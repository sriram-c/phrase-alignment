----
00429	Increasing Model Sizes Another key reason that neural networks are wildly successful today after enjoying comparatively little success since the 1980s is that we have the computational resources to run much larger models today.	 baDZawI moYdala AkAra eka Ora pramuKa kAraNa hE ki waMwrikA netavarka Aja apekRAkqwa kama saPalawA kA AnaMxa lene ke bAxa bewahASA saPala rahe hEM 1980 ke xaSaka se hE ki hama Aja bahuwa badZe moYdala calAne ke lie kampyUteSanala saMsAXanoM hE .		
428	428
S1 Increasing Model Sizes Another key reason that neural networks are wildly successful today after enjoying comparatively little success since the 1980s is that we have the computational resources to run much larger models today .	baDZawA huA moYdala AkAra eka Ora pramuKa vajaha hE ki waMwrikA netavarka 1980 ke xaSaka se wulanAwmaka rUpa se kama saPalawA kA AnaMxa lene ke bAxa Aja kAPI saPala
NP2 Increasing Model	baDZawA huA moYdala
VP5_LWG Sizes	AkAra
NP7 Another key reason that neural networks are wildly successful	eka Ora pramuKa vajaha hE ki waMwrikA netavarka behaxa saPala hEM
NP8 Another key reason	eka Ora pramuKa kAraNa
SBAR12 that neural networks are wildly successful	vaha waMwrikA netavarka behaxa saPala hEM
S14 neural networks are wildly successful	waMwrikA netavarka behaxa saPala hEM
NP15 neural networks	waMwrikA netavarka
NNS17 networks	netavarka
VP18_LWG are	hEM
ADJP20 wildly successful	behaxa saPala
NP-TMP23 today	Aja
PP25 after enjoying comparatively little success since the 1980s is that we have the computational resources to run much larger models today	1980 ke xaSaka se wulanAwmaka rUpa se kama saPalawA kA AnaMxa lene ke bAxa yaha hE ki Aja hamAre pAsa bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana
S27 enjoying comparatively little success since the 1980s is that we have the computational resources to run much larger models today	1980 ke xaSaka se wulanAwmaka rUpa se kama saPalawA kA AnaMxa lenA yaha hE ki Aja hamAre pAsa bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana hEM
VP28_LWG enjoying	AnaMxa le rahA hE
NP30 comparatively little success	wulanAwmaka rUpa se kama saPalawA
ADJP31 comparatively little	wulanAwmaka rUpa se kama
SBAR35 since the 1980s is that we have the computational resources to run much larger models today	1980 ke xaSaka se yaha hE ki Aja hamAre pAsa bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana hEM
S37 the 1980s is that we have the computational resources to run much larger models today	1980 kA xOra yaha hE ki Aja hamAre pAsa bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana hEM
NP38 the 1980s	1980 ke xaSaka
NNS40 1980s	1980 ke xaSaka meM
VP41_LWG is	hE
SBAR43 that we have the computational resources to run much larger models today	hamAre pAsa Aja bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana hEM
S45 we have the computational resources to run much larger models today	hamAre pAsa Aja bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana hEM
NP46 we	hama
VP48_LWG have	pAsa hE
NP50 the computational resources to run much larger models today	bahuwa badZe moYdala calAne ke lie kampyUteSanala saMsAXana Aja
NNS53 resources	saMsAXana
S54 to run much larger models today	bahuwa badZe moYdala calAne ke lie Aja
VP55_LWG to run	xOdZane ke lie
NP59 much larger models	bahuwa badZe moYdala
ADJP60 much larger	bahuwa badZA
NNS63 models	moYdala
NP-TMP64 today	Aja

