----	----
0001	Chapter 1 Introduction Inventors have long dreamed of creating machines that think.	 aXyAya 1 paricaya niveSakoM ne laMbe samaya se EsI maSIneM banAne kA sapanA xeKA hE jo socawI hEM 		0	0
0
S1 Chapter 1 Introduction Inventors have long dreamed of creating machines that think .	aXyAya 1 paricaya upayogakarwAoM ne laMbe samaya waka maSInoM kA nirmANa karane kA sapanA xeKA hE jo socawe hEM
NP2 Chapter 1 Introduction Inventors	aXyAya 1 paricaya AviRkAraka
NML3 Chapter 1	aXyAya 1
VP8_LWG have long dreamed	laMbe samaya se sapane xeKa cuke hEM
ADVP10 long	laMbe samaya waka
PP14 of creating machines that think	maSInoM kA nirmANa karanA jo socawe hEM
S16 creating machines that think	maSIneM banAnA jo socawe hEM
VP17_LWG creating	banA rahA hE racanA
NP19 machines that think	maSIneM jo socawI hEM
NP20 machines	maSIneM
NNS21 machines	maSIneM
SBAR22 that think	EsA lagawA hE ki
WHNP23 that	vaha
S25 think	socie
VP26_LWG think	socie

----
0002	This desire dates back to at least the time of ancient Greece.	 yaha icCA kama se kama prAcIna grIsa ke samaya ke lie vApasa wiWiyAz .		
1	1
S1 This desire dates back to at least the time of ancient Greece .	yaha icCA prAcIna grIsa ke kama se kama samaya waka vApasa AwI hE
NP2 This desire	yaha icCA
VP5_LWG dates back	wArIKeM vApasa
PRT7 back	vApasa
PP9 to at least the time of ancient Greece	kama se kama prAcIna grIsa ke samaya ke lie
NP11 at least the time of ancient Greece	kama se kama prAcIna grIsa kA samaya
NP12 at least the time	kama se kama samaya
QP13 at least	kama se kama
ADVP14 at least	kama se kama
PP19 of ancient Greece	prAcIna grIsa kA
NP21 ancient Greece	prAcIna grIsa

----
0003	The mythical gures Pygmalion, Daedalus, and Hephaestus may all be interpreted as legendary inventors, and Galatea, Talos, and Pandora may all be regarded as articial life (Ovid and Martin, 2004; Sparkes, 1996; Tandy, 1997).	 miWakIya AMkadZe pigmaliyana , xAxalasa , Ora hePestasa saBI ko praKyAwa AviRkAraka Ora gEletA , tElosa , Ora pAMdorA saBI ko kqwrima jIvana mAnA jA sakawA hE ( ovida Ora mArtina , 1997 , kAuntI , 1996 )		
2	2
S1 The mythical gures Pygmalion , Daedalus , and Hephaestus may all be interpreted as legendary inventors , and Galatea , Talos , and Pandora may all be regarded as articial life ( Ovid and Martin , 2004 ; Sparkes , 1996 ; Tandy , 1997 ) .	pOrANika AMkadZe pAyagamAiliyana, dedalasa, Ora hePestasa saBI ko mahAna AviRkAraka ke rUpa meM vyAKyA kiyA jA sakawA hE, Ora gEletA, tElo,
S2 The mythical gures Pygmalion , Daedalus , and Hephaestus may all be interpreted as legendary inventors	pOrANika AMkadZe pAyagamAiliyana, xAxAlasa, Ora hePestasa saBI ko mahAna AviRkAraka ke rUpa meM mAnA jA sakawA hE
NP3 The mythical gures Pygmalion , Daedalus , and Hephaestus	pOrANika AMkadZe pAyagamalIyana, xAxAlasa Ora hePEstasa
NP4 The mythical gures	pOrANika AMkadZe
NNS7 gures	AMkadZe
NP8 Pygmalion , Daedalus , and Hephaestus	pAyagaminIyana, xAxAlasa, Ora hePestasa
,10 ,	,
,12 ,	,
CC13 and	Ora
NNP14 Hephaestus	hePestasa
VP15_LWG may all be interpreted	saBI kI vyAKyA ho sakawI hE
PP22 as legendary inventors	mahAna AviRkAraka ke rUpa meM
NP24 legendary inventors	mahAna AviRkAraka
NNS26 inventors	AviRkAraka
,27 ,	,
CC28 and	Ora
S29 Galatea , Talos , and Pandora may all be regarded as articial life ( Ovid and Martin , 2004 ; Sparkes , 1996 ; Tandy , 1997 )	gEletA, wAlosa, Ora pEMxrA saBI ko kqwrima jIvana (ovida Ora mArtina, 2004) ke rUpa meM mAnA jA sakawA hE; spArksa, 1996 ; tAMdI, 1997
NP30 Galatea , Talos , and Pandora	gEletA, wAlosa, Ora pEMxrA
,32 ,	,
,34 ,	,
CC35 and	Ora
NNP36 Pandora	pEMxrA
VP37_LWG may all be regarded	ho sakawA hE saBI kA saMbaMXa
PP44 as articial life ( Ovid and Martin , 2004 ; Sparkes , 1996 ; Tandy , 1997 )	kqwrima jIvana (ovida Ora mArtina, 2004 ; spArksa, 1996 ; tAMdI, 1997 )
NP46 articial life ( Ovid and Martin , 2004 ; Sparkes , 1996 ; Tandy , 1997 )	kqwrima jIvana (ovida Ora mArtina, 2004 ; spArksa, 1996 ; tAMdI, 1997 )
PRN49 ( Ovid and Martin , 2004 ; Sparkes , 1996 ; Tandy , 1997 )	(ovida Ora mArtina, 2004 ; spArksa, 1996 ; tAMdI, 1997 )
NP51 Ovid and Martin	ovida Ora mArtina
CC53 and	Ora
NNP52 Ovid	ovida
NNP54 Martin	mArtina
,55 ,	,
NP56 2004	2004
NP59 Sparkes	spArka
,61 ,	,
NP62 1996	1996
NP65 Tandy	taMdI
,67 ,	,
NP68 1997	1997

----
0004	When programmable computers were rst conceived, people wondered whether such machines might become intelligent, over a hundred years before one was built (Lovelace, 1842).	 jaba pahalI bAra progrAma yogya kaMpyUtaroM kI kalpanA kI gaI wo logoM ne socA ki kyA EsI maSIneM buxXimAna ho sakawI hEM , eka ke nirmANa se sO sAla pahale ( Lovlace , 1842 )		
3	3
S1 When programmable computers were rst conceived , people wondered whether such machines might become intelligent , over a hundred years before one was built ( Lovelace , 1842 ) .	jaba progrAmabala kaMpyUtaroM kI pahalI kalpanA kI gaI WI, wo logoM ne socA ki kyA EsI maSIneM buxXimAna ho sakawI hEM, jo eka sO sAla pahale bana sakawI hEM
SBAR2 When programmable computers were rst conceived	jaba progrAmabala kaMpyUtaroM kI pahalI kalpanA kI gaI WI
WHADVP3 When	kaba
S5 programmable computers were rst conceived	progrAmabala kaMpyUtaroM kI pahalI kalpanA kI gaI WI
NP6 programmable computers	progrAmabala kaMpyUtara
NNS8 computers	kaMpyUtara
VP9_LWG were	We
ADJP11 rst conceived	pahalI kalpanA
,14 ,	,
NP15 people	logoM
NNS16 people	logoM
VP17_LWG wondered	socA
SBAR19 whether such machines might become intelligent , over a hundred years before one was built ( Lovelace , 1842 )	cAhe EsI maSIneM buxXimAna ho sakawI hEM, cAhe eka sO sAla pahale (lavalesa, 1843)
S21 such machines might become intelligent , over a hundred years before one was built ( Lovelace , 1842 )	EsI maSIneM buxXimAna ho sakawI hEM, eka sO sAla pahale hI (lavalesa, 1843)
NP22 such machines	EsI maSIneM
NNS24 machines	maSIneM
VP25_LWG might become	bana sakawe hEM
ADJP29 intelligent	buxXimAna
,31 ,	,
SBAR32 over a hundred years before one was built	banane se sO sAla pahale eka sO sAla se jyAxA
NP33 over a hundred years	sO sAla se jyAxA
QP34 over a hundred	sO se jyAxA
NNS38 years	sAla-ba-ba-ba-ba-ba-ba-ba.
S40 one was built	eka banAyA gayA WA
NP41 one	eka
VP43_LWG was built	banAyA gayA WA
PRN47 ( Lovelace , 1842 )	(lavalIsa, 1843)
NP49 Lovelace	lavalesa
,51 ,	,
NP52 1842	1843

----
0005	Today, articial intelligence (AI) is a thriving eld with many practical applications and active research topics.	 Aja , kqwrima buxXi ( eeAI ) eka PalawA - PUlawA kRewra hE jisameM kaI vyAvahArika anuprayoga Ora sakriya anusaMXAna viRaya hEM 		
4	4
S1 Today , articial intelligence ( AI ) is a thriving eld with many practical applications and active research topics .	Aja kqwrima buxXimawwA (eAI) kaI vyAvahArika Avexana Ora sakriya anusaMXAna viRayoM ke sAWa eka saMpanna kRewra hE
NP-TMP2 Today	Aja
,4 ,	,
NP5 articial intelligence ( AI )	kqwrima buxXimawwA (eAI)
NP6 articial intelligence	kqwrima buxXimawwA
NP10 AI	E.AI.
VP13_LWG is	hE
NP15 a thriving eld with many practical applications and active research topics	kaI vyAvahArika Avexana Ora sakriya anusaMXAna viRayoM ke sAWa saMpanna kRewra
NP16 a thriving eld	eka saMpanna kRewra
PP20 with many practical applications and active research topics	kaI vyAvahArika Avexana Ora sakriya anusaMXAna viRayoM ke sAWa
NP22 many practical applications and active research topics	kaI vyAvahArika Avexana Ora sakriya anusaMXAna viRaya
NML24 practical applications and active research	vyAvahArika Avexana Ora sakriya SoXa
NML25 practical applications	vyAvahArika Avexana
NNS27 applications	Avexana
CC28 and	Ora
NML29 active research	sakriya SoXa
NNS32 topics	viRaya

----
0006	We look to intelligent software to automate routine labor, understand speech or images, make diagnoses in medicine and support basic scientic research.	 hama sAmAnya kArya ko svacAliwa karane ke lie buxXimawwApUrNa soYPZtaveyara kI ora xeKawe hEM , BARaNa yA CaviyoM ko samaJawe hEM , cikiwsA meM nixAna karawe hEM Ora mUla vEjFAnika anusaMXAna ko samarWana xewe hEM 		
5	5
S1 We look to intelligent software to automate routine labor , understand speech or images , make diagnoses in medicine and support basic scientic research .	hama niyamiwa Srama ko svacAliwa karane ke lie buxXimAna soYPtaveyara ko xeKawe hEM, BARaNa yA CaviyAM samaJawe hEM, cikiwsA meM nixAna karawe hEM Ora buniyAxI vE
NP2 We	hama
VP4_LWG look	xeKo
PP6 to intelligent software	buxXimAna soYPtaveyara ko
NP8 intelligent software	buxXimAna soYPtaveyara
S11 to automate routine labor , understand speech or images , make diagnoses in medicine and support basic scientic research	niyamiwa rUpa se Srama ko svacAliwa karane ke lie, BARaNa yA CaviyAM samaJeM, cikiwsA meM nixAna kareM Ora buniyAxI vEjFAnika anusaMXAna kA samarWana kareM
VP12_LWG to automate understand make support	svacAliwa samaJane ke lie samarWana banAne ke lie
NP17 routine labor	niyamiwa Srama
,20 ,	,
NP23 speech or images	BARaNa yA CaviyAM
CC25 or	yA
NN24 speech	BARaNa
NNS26 images	CaviyAM
,27 ,	,
NP30 diagnoses	nixAna
NNS31 diagnoses	nixAna
PP32 in medicine	xavA meM
NP34 medicine	xavA
CC36 and	Ora
VP28 make diagnoses in medicine	xavA meM kareM nixAna
VP37 support basic scientic research	buniyAxI vEjFAnika anusaMXAna kA samarWana
NP39 basic scientic research	buniyAxI vEjFAnika anusaMXAna

----
0007	In the early days of articial intelligence, the eld rapidly tackled and solved problems that are intellectually dicult for human beings but relatively straight- forward for computers	 kqwrima buxXi ke prAraMBika xinoM meM , kRewra wejI se hala kiyA Ora samasyAoM hE ki bOxXika rUpa se mAnava ke lie muSkila hE , lekina kaMpyUtara ke lie apekRAkqwa sIXe Age -		
6	6
S1 In the early days of articial intelligence , the eld rapidly tackled and solved problems that are intellectually dicult for human beings but relatively straight - forward for computers 	kqwrima buxXimawwA ke prAraMBika xinoM meM, kRewra ne wejI se nipatakara samasyAoM kA samAXAna kiyA jo manuRyoM ke lie bOxXika rUpa se muSkila hEM lekina apekRAkqwa
PP2 In the early days of articial intelligence	kqwrima buxXimawwA ke SuruAwI xinoM meM
NP4 the early days of articial intelligence	kqwrima buxXimawwA ke SuruAwI xina
NP5 the early days	SuruAwI xina
NNS8 days	xina-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba.
PP9 of articial intelligence	kqwrima buxXimawwA kA
NP11 articial intelligence	kqwrima buxXimawwA
,14 ,	,
NP15 the eld	Kewa
ADVP18 rapidly	wejI se
VP20_LWG tackled solved	nipatA huA hala
CC22 and	Ora
VBD21 tackled	nipatA
VBD23 solved	hala huA
NP24 problems that are intellectually dicult for human beings but relatively straight - forward for computers	EsI samasyAeM jo iMsAnoM ke lie bOxXika rUpa se muSkila hEM lekina apekRAkqwa sIXe- kaMpyUtaroM ke lie Age
NP25 problems	samasyAeM
NNS26 problems	samasyAeM
SBAR27 that are intellectually dicult for human beings but relatively straight - forward for computers	yaha iMsAnoM ke lie bOxXika rUpa se muSkila hEM lekina apekRAkqwa sIXe- kaMpyUtaroM ke lie Age
WHNP28 that	vaha
S30 are intellectually dicult for human beings but relatively straight - forward for computers	iMsAnoM ke lie bOxXika rUpa se muSkila hEM lekina apekRAkqwa sIXA- kaMpyUtaroM ke lie Age
VP31_LWG are	hEM
ADJP33 intellectually dicult for human beings but relatively straight - forward	iMsAnoM ke lie bOxXika rUpa se muSkila lekina apekRAkqwa sIXA-Age
ADJP34 intellectually dicult for human beings	iMsAnoM ke lie bOxXika rUpa se muSkila
PP37 for human beings	iMsAnoM ke lie
NP39 human beings	manuRya
NNS41 beings	prANI
CC42 but	lekina
RB43 relatively	apekRAkqwa
ADJP44 straight - forward	sIXA- Age
PP48 for computers	kaMpyUtaroM ke lie
NP50 computers	kaMpyUtara
NNS51 computers	kaMpyUtara
,52 	

----
0008	problems that can be described by a list of formal, math- ematical rules.	 samasyAoM hE ki OpacArika , gaNiwa - viRayaka niyamoM kI eka sUcI xvArA varNiwa kiyA jA sakawA hE .		
7	7
NP1 problems that can be described by a list of formal , math - ematical rules .	jina samasyAoM kA varNana OpacArika, gaNiwa- AxarSa niyamoM kI sUcI se kiyA jA sakawA hE
NP2 problems	samasyAeM
NNS3 problems	samasyAeM
SBAR4 that can be described by a list of formal , math - ematical rules	jise OpacArika, gaNiwa kI sUcI se varNiwa kiyA jA sakawA hE - AxarSa niyama
WHNP5 that	vaha
S7 can be described by a list of formal , math - ematical rules	OpacArika, gaNiwa- niyamoM kI sUcI se varNiwa kiyA jA sakawA hE
VP8_LWG can be described	bawAyA jA sakawA hE varNana
PP14 by a list of formal , math - ematical rules	OpacArika, gaNiwa- niyamoM kI sUcI se
NP16 a list of formal , math - ematical rules	OpacArika, gaNiwa- niyamoM kI sUcI
NP17 a list	eka sUcI
PP20 of formal , math - ematical rules	OpacArika, gaNiwa- AxarSa niyamoM kA
NP22 formal , math - ematical rules	OpacArika, gaNiwa- AxarSa niyama
,24 ,	,
ADJP25 math - ematical	gaNiwa- gaNiwIya
NP26 math	gaNiwa
NNS30 rules	niyama

----
0009	The true challenge to articial intelligence proved to be solving the tasks that are easy for people to perform but hard for people to describe formallyproblems that we solve intuitively, that feel automatic, like recognizing spoken words or faces in images.	 kqwrima buxXi ke lie saccI cunOwI una kAryoM ko hala karane kI sAbiwa huI jo logoM ke lie AsAna hEM , lekina OpacArika rUpa se varNana karane ke lie logoM ke lie kaTina hEM - una prawIkoM ko , jo hama anwarjFAnAwmaka rUpa se hala karawe hEM , jo bolacAla ke SabxoM yA wasvIroM meM cehare ko pahacAnane kI waraha svawaH mahasUsa karawe hEM 		
8	8
S1 The true challenge to articial intelligence proved to be solving the tasks that are easy for people to perform but hard for people to describe formally  problems that we solve intuitively , that feel automatic , like recognizing spoken words or faces in images .	kqwrima buxXimawwA ke lie saccI cunOwI una kAryoM ko hala karanA sAbiwa huI jo logoM ke lie AsAna hEM lekina logoM ke lie OpacArika rUpa se varNana kara
NP2 The true challenge to articial intelligence	kqwrima buxXimawwA ko saccI cunOwI
NP3 The true challenge	saccI cunOwI
PP7 to articial intelligence	kqwrima buxXimawwA ko
NP9 articial intelligence	kqwrima buxXimawwA
VP12_LWG proved	sAbiwa huA
S14 to be solving the tasks that are easy for people to perform but hard for people to describe formally  problems that we solve intuitively , that feel automatic , like recognizing spoken words or faces in images	una kAryoM ko hala karanA jo logoM ke lie AsAna hEM lekina logoM ke lie OpacArika rUpa se varNana karanA muSkila hE lekina samasyAoM kA samAXAna karanA hama sahaja
VP15_LWG to be solving	hala honA hE
NP21 the tasks that are easy for people to perform but hard for people to describe formally  problems that we solve intuitively , that feel automatic , like recognizing spoken words or faces in images	logoM ke lie jo kArya karanA AsAna hE, lekina logoM ke lie OpacArika rUpa se varNana karanA kaTina hE samasyAeM jo hama sahaja DaMga se hala karawe hEM,
NP22 the tasks	kAryoM
NNS24 tasks	kAryoM
SBAR25 that are easy for people to perform but hard for people to describe formally  problems that we solve intuitively , that feel automatic , like recognizing spoken words or faces in images	logoM ke lie yaha praxarSana karanA AsAna hE lekina logoM ke lie OpacArika rUpa se varNana karanA kaTina hE samasyAeM jo hama sahaja rUpa se hala karawe hEM,
WHNP26 that	vaha
S28 are easy for people to perform but hard for people to describe formally  problems that we solve intuitively , that feel automatic , like recognizing spoken words or faces in images	logoM ke lie praxarSana karanA AsAna hE lekina logoM ke lie OpacArika rUpa se varNana karanA kaTina hE samasyAeM jo hama sahaja DaMga se hala karawe hEM, jE
VP29_LWG are	hEM
ADJP31 easy for people to perform but hard for people to describe formally  problems that we solve intuitively , that feel automatic , like recognizing spoken words or faces in images	logoM ke lie praxarSana karanA AsAna lekina logoM ke lie OpacArika rUpa se varNana karanA kaTina hE samasyAeM jo hama sahaja rUpa se hala karawe hEM, jEse
PP33 for people to perform but hard for people to describe formally  problems that we solve intuitively , that feel automatic , like recognizing spoken words or faces in images	logoM ke lie OpacArika rUpa se logoM ke lie praxarSana karanA lekina kaTina praxarSana karanA hogA samasyAeM jo hama sahaja rUpa se hala karawe hEM, jEse ciwroM
NP35 people to perform but hard for people to describe formally  problems that we solve intuitively , that feel automatic , like recognizing spoken words or faces in images	loga OpacArika rUpa se logoM ke lie praxarSana karanA lekina kaTina praxarSana karanA cAhawe hEM samasyAeM jo hama sahaja rUpa se hala karawe hEM, jo svacAliwa mahasUsa kara
NNS36 people	logoM
S37 to perform but hard for people to describe formally  problems that we solve intuitively , that feel automatic , like recognizing spoken words or faces in images	logoM ke lie OpacArika rUpa se varNana karane ke lie kaTina praxarSana karanA lekina kaTina praxarSana karanA samasyAeM jo hama sahaja rUpa se hala karawe hEM, jE
VP38_LWG to perform	praxarSana karane ke lie
PP42 but hard for people to describe formally  problems that we solve intuitively , that feel automatic , like recognizing spoken words or faces in images	lekina logoM ke lie OpacArika rUpa se varNana karanA kaTina hE samasyAeM jo hama sahaja rUpa se hala karawe hEM, jEse ki CaviyoM meM bolI huI SabxoM yA

----
0010	This book is about a solution to these more intuitive problems.	 yaha puswaka ina aXika sahaja samasyAoM ke eka samAXAna ke bAre meM hE .		
9	9
S1 This book is about a solution to these more intuitive problems .	yaha puswaka ina aXika sahaja samasyAoM ke samAXAna ke bAre meM hE
NP2 This book	yaha puswaka
VP5_LWG is	hE
PP7 about a solution to these more intuitive problems	ina aXika sahaja samasyAoM ke samAXAna ke bAre meM
NP9 a solution to these more intuitive problems	ina aXika sahaja samasyAoM kA samAXAna
NP10 a solution	eka samAXAna
PP13 to these more intuitive problems	ina Ora aXika sahaja samasyAoM ke lie
NP15 these more intuitive problems	ye aXika sahaja samasyAeM
ADJP17 more intuitive	aXika sahaja
NNS20 problems	samasyAeM

----
00101	Many of these factors of variation, such as a speakers accent, can be identied only using sophisticated, nearly human-level understanding of the data.	 inameM se bahuwa se kAraka , jEse ki vakwA ke lahaje , kI pahacAna kevala pariRkqwa , lagaBaga mAnava - swara kI detA kI samaJa kA upayoga karake kI jA sakawI hE .		
100	100
S1 Many of these factors of variation , such as a speaker s accent , can be identied only using sophisticated , nearly human - level understanding of the data .	viviXawA ke inameM se kaI kAraka, jEse ki eka spIkara kA uccAraNa, kevala pariRkqwa rUpa se upayoga karake pahacAna kI jA sakawI hE, lagaBaga mAnava - detA
NP2 Many of these factors of variation , such as a speaker s accent ,	viviXawA ke inameM se kaI kAraka, jEse ki eka spIkara kA uccAraNa,
NP3 Many	kaI
PP5 of these factors of variation	viviXawA ke ina kAraka
NP7 these factors of variation	viviXawA ke ye kAraka
NP8 these factors	ye kAraka
NNS10 factors	kAraKAne
PP11 of variation	viviXawA kA
NP13 variation	viviXawA
,15 ,	,
PP16 such as a speaker s accent	jEse ki eka spIkara kA lahajA
NP19 a speaker s accent	eka spIkara kA lahajA
NML21 speaker s	spIkara kI
,25 ,	,
VP26_LWG can be identied	pahacAna kI jA sakawI hE
S32 only using sophisticated , nearly human - level understanding of the data	kevala pariRkqwa, lagaBaga mAnava - detA kI swarIya samaJa
ADVP33 only	kevala
VP35_LWG using	iswemAla karanA
UCP37 sophisticated , nearly human - level understanding	pariRkqwa, lagaBaga mAnava-swarIya samaJa
ADJP38 sophisticated	pariRkqwa
,40 ,	,
NP41 nearly human - level understanding	lagaBaga mAnava-swarIya samaJa
NML43 human - level	mAnava-swara
PP48 of the data	detA kA
NP50 the data	detA
NNS52 data	detA

----
00102	When it is nearly as dicult to obtain a representation as to solve the original problem, representation learning does not, at rst glance, seem to help us.	 jaba mUla samasyA ko hala karane ke lie prawiniXiwva prApwa karanA lagaBaga uwanA hI kaTina howA hE , waba aByAvexana SikRaNa pahalI najara meM hamArI sahAyawA nahIM karawA 		
101	101
S1 When it is nearly as dicult to obtain a representation as to solve the original problem , representation learning does not , at rst glance , seem to help us .	jaba mUla samasyA ke samAXAna ke lie prawiniXiwva prApwa karanA lagaBaga muSkila howA hE, wo pahalI najara meM prawiniXiwva sIKane se hamArI maxaxa milawI hE
SBAR2 When it is nearly as dicult to obtain a representation as to solve the original problem	jaba mUla samasyA ke samAXAna ke lie prawiniXiwva prApwa karanA lagaBaga uwanA hI muSkila howA hE
WHADVP3 When	kaba
S5 it is nearly as dicult to obtain a representation as to solve the original problem	mUla samasyA ke samAXAna ke lie prawiniXiwva prApwa karanA lagaBaga uwanA hI muSkila hE
NP6 it	yaha
VP8_LWG is	hE
ADJP10 nearly as dicult to obtain a representation as to solve the original problem	mUla samasyA ke samAXAna ke lie prawiniXiwva prApwa karanA lagaBaga uwanA hI muSkila
S14 to obtain a representation as to solve the original problem	mUla samasyA ke samAXAna ke lie prawiniXiwva prApwa karane ke lie
VP15_LWG to obtain	prApwa karane ke lie
NP19 a representation	eka prawiniXiwva
SBAR22 as to solve the original problem	mUla samasyA ke samAXAna ke lie
S24 to solve the original problem	mUla samasyA kA samAXAna karane ke lie
VP25_LWG to solve	hala karane ke lie
NP29 the original problem	mUla samasyA
,33 ,	,
NP34 representation learning	prawiniXiwva sIKanA
VP37_LWG does not seem	EsA nahIM lagawA
,41 ,	,
PP42 at rst glance	pahalI najara meM
NP44 rst glance	pahalI najara
,47 ,	,
S49 to help us	hamArI maxaxa karane ke lie
VP50_LWG to help	maxaxa ke lie
NP54 us	hameM

----
00103	Deep learning solves this central problem in representation learning by intro- ducing representations that are expressed in terms of other, simpler representations.	 gahana aXyayana isa keMxrIya samasyA kA samAXAna karawA hE prawiniXiwva sIKane meM AwmanirIkRaNa xvArA , jo anya , sarala nirUpaNa ke saMxarBa meM vyakwa kie jAwe hEM .		
102	102
S1 Deep learning solves this central problem in representation learning by intro - ducing representations that are expressed in terms of other , simpler representations .	gaharI sIKa aMwarxqRti se sIKane vAle prawiniXiwva meM isa keMxrIya samasyA kA samAXAna karawA hE  kamajora prawiniXiwva jo anya, sarala prawiniXiwva ke mAmale meM vyakwa kie jAwe
NP2 Deep learning	gaharI sIKa
VP5_LWG solves	hala karawA hE
NP7 this central problem in representation learning	prawiniXiwva sIKane meM yaha keMxrIya samasyA
NP8 this central problem	yaha keMxrIya samasyA
PP12 in representation learning	prawiniXiwva meM sIKa
NP14 representation learning	prawiniXiwva sIKanA
PP17 by intro - ducing representations that are expressed in terms of other , simpler representations	xvArA - xUsare, sAXAraNa prawiniXiwva ke mAmale meM vyakwa kie jAwe hEM ucca prawiniXiwva
NP19 intro - ducing representations that are expressed in terms of other , simpler representations	iMtro - xUsare, sarala prawiniXiwva ke mAmale meM vyakwa kiye jAwe hEM vo kamajora prawiniXiwva
NP20 intro - ducing representations	iMtro - kamajora prawiniXiwva
NML21 intro - ducing	iMtaravyU- liMgana
NNS25 representations	prawiniXiwva
SBAR26 that are expressed in terms of other , simpler representations	jinheM xUsare, sAXAraNa prawiniXiwva ke mAmale meM vyakwa kiyA jAwA hE
WHNP27 that	vaha
S29 are expressed in terms of other , simpler representations	xUsare, sAXAraNa prawiniXiwva ke mAmale meM vyakwa kiye jAwe hEM
VP30_LWG are expressed	vyakwa kiye jAwe hEM
PP34 in terms of other , simpler representations	xUsare ke mAmale meM, sAXAraNa prawiniXiwva
NP36 terms of other , simpler representations	anya, sarala prawiniXiwva kI SarweM
NP37 terms	SarwoM
NNS38 terms	SarwoM
PP39 of other , simpler representations	xUsare kI, sAXAraNa prawiniXiwva
NP41 other , simpler representations	anya, sarala prawiniXiwva
,43 ,	,
ADJP44 simpler	sarala
NNS46 representations	prawiniXiwva

----
00104	Deep learning enables the computer to build complex concepts out of simpler con- cepts.	 gahana aXyayana kaMpyUtara ko sarala saMkalpanAoM se jatila avaXAraNAoM kA nirmANa karane meM sakRama banAwA hE 		
103	103
S1 Deep learning enables the computer to build complex concepts out of simpler con - cepts .	gaharI sIKa kaMpyUtara ko sarala koYna-septa se bAhara jatila koYnsepta banAne meM sakRama banAwA hE
NP2 Deep learning	gaharI sIKa
VP5_LWG enables	sakRama
NP7 the computer	kaMpyUtara
S10 to build complex concepts out of simpler con - cepts	sarala koYna se jatila koYnsepta banAne ke lie  pracalana
VP11_LWG to build out	banAne ke lie
NP15 complex concepts	jatila parikalpanAeM
NNS17 concepts	avaXAraNAez
PRT18 out	bAhara
PP20 of simpler con - cepts	sAXAraNa cora- petIema kI
NP22 simpler con - cepts	sarala cona - peteMta
ADJP23 simpler	sarala
NNS27 cepts	septsa

----
00105	Figure 1.2 shows how a deep learning system can represent the concept of an image of a person by combining simpler concepts, such as corners and contours, which are in turn dened in terms of edges.	 ciwra 1 . 2 se pawA calawA hE ki kEse eka gaharI sIKane kI praNAlI sarala avaXAraNAoM , jEse konoM Ora contours , jo bArI meM kinAroM ke saMxarBa meM pariBARiwa kara rahe hEM ke saMyojana ke xvArA eka vyakwi kI Cavi kI avaXAraNA kA prawiniXiwva kara sakawe hEM .		
104	104
S1 Figure 1.2 shows how a deep learning system can represent the concept of an image of a person by combining simpler concepts , such as corners and contours , which are in turn dened in terms of edges .	Pigara 1.2 se pawA calawA hE ki kEse eka gaharI sIKa praNAlI sarala avaXArAoM, jEse kone Ora kroYuMdara ko eka sAWa jodZakara vyakwi kI Cavi kI avaXAraNA kA
NP2 Figure 1.2	AMkadZA 1.2
VP5_LWG shows	xiKAwA hE So
SBAR7 how a deep learning system can represent the concept of an image of a person by combining simpler concepts , such as corners and contours , which are in turn dened in terms of edges	kEse eka gaharI sIKa praNAlI eka vyakwi kI Cavi kI avaXAraNA kA prawiniXiwva kara sakawI hE, jEse ki kone Ora kroYuMdara, jo ki kinAroM ke mAmale meM pariBARiwa ho
WHADVP8 how	kEse
S10 a deep learning system can represent the concept of an image of a person by combining simpler concepts , such as corners and contours , which are in turn dened in terms of edges	eka gaharI sIKa praNAlI eka vyakwi kI eka Cavi kI avaXAraNA kA prawiniXiwva kara sakawI hE, jEse ki kone Ora kroYuMdara, jo ki kinAroM ke mAmale meM pariBARiwa hEM,
NP11 a deep learning system	eka gaharI sIKa praNAlI
NML13 deep learning	gaharI sIKa
VP17_LWG can represent	prawiniXiwva kara sakawe hEM
NP21 the concept of an image of a person	eka vyakwi kI Cavi kI avaXAraNA
NP22 the concept	avaXAraNA
PP25 of an image of a person	eka vyakwi kI Cavi kA
NP27 an image of a person	eka vyakwi kI Cavi
NP28 an image	eka Cavi
PP31 of a person	eka vyakwi kA
NP33 a person	eka vyakwi
PP36 by combining simpler concepts , such as corners and contours , which are in turn dened in terms of edges	sarala avaXArAoM ko milAkara, jEse kone Ora kroYuMdarsa, jo ki kinAroM ke mAmale meM pariBARiwa hEM modZa meM
S38 combining simpler concepts , such as corners and contours , which are in turn dened in terms of edges	sarala avaXArAoM ko jodZanA, jEse konoM Ora kaMXoM, jo ki kinAroM ke mAmale meM pariBARiwa hEM modZa meM
VP39_LWG combining	saMyukwa
NP41 simpler concepts , such as corners and contours , which are in turn dened in terms of edges	sarala avaXAraNA, jEse kone Ora kaMXoM, jo ki kinAroM ke mAmale meM pariBARiwa hEM modZa meM
NP42 simpler concepts	sarala avaXAraNAez
NNS44 concepts	avaXAraNAez
,45 ,	,
PP46 such as corners and contours , which are in turn dened in terms of edges	jEse kone Ora kaMXoM, jo ki kinAroM ke mAmale meM pariBARiwa hEM modZa meM
NP49 corners and contours , which are in turn dened in terms of edges	kinAroM Ora kinAroM ke mAmale meM pariBARiwa hone vAle konoM Ora kAMursa, jo ki kinAroM ke mAmale meM pariBARiwa hEM
NP50 corners and contours	kone Ora kaMXoM
NNS51 corners	kone-kone
CC52 and	Ora
NNS53 contours	wahaKAne
,54 ,	,
SBAR55 which are in turn dened in terms of edges	jo ki kinAroM ke mAmale meM pariBARiwa hEM modZa meM
WHNP56 which	jo ki
S58 are in turn dened in terms of edges	kinAroM ke mAmale meM pariBARiwa hEM modZa meM
VP59_LWG are	hEM
PP61 in turn dened in terms of edges	kinAroM ke mAmale meM pariBARiwa bArI meM
NP63 turn dened in terms of edges	kinAroM ke mAmale meM pariBARiwa kareM modZa
NP64 turn	bArI
VP66_LWG dened	pariBARiwa
PP68 in terms of edges	kinAroM ke mAmale meM
NP70 terms of edges	kinAroM kI SarweM
NP71 terms	SarwoM
NNS72 terms	SarwoM
PP73 of edges	kinAroM kI
NP75 edges	kinAroM
NNS76 edges	kinAroM

----
00106	The quintessential example of a deep learning model is the feedforward deep network, or multilayer perceptron (MLP).	 eka gahana aXigama moYdala kA sAraBUwa uxAharaNa hE PIdalara gaharA netavarka yA maltIleyara parseptoYna ( emaelapI ) 		
105	105
S1 The quintessential example of a deep learning model is the feedforward deep network , or multilayer perceptron ( MLP ) .	eka gahare sIKane ke moYdala kA kviMtala PIdaPoYravarda gahare netavarka, yA maltIletara seMsara (emaelapI) hE
NP2 The quintessential example of a deep learning model	gahare sIKane ke moYdala kA kviMteriyala uxAharaNa
NP3 The quintessential example	kviMtalajarUrI uxAharaNa
PP7 of a deep learning model	eka gaharI sIKa moYdala kI
NP9 a deep learning model	eka gaharI sIKa moYdala
NML11 deep learning	gaharI sIKa
VP15_LWG is	hE
NP17 the feedforward deep network , or multilayer perceptron	PIdaraPoYravarda gaharA netavarka, yA maltIletara avacewana
NP18 the feedforward deep network	PIdaraPoYravarda gaharA netavarka
,23 ,	,
CC24 or	yA
NP25 multilayer perceptron	bahuAyAmI avaXAraNA
PRN28 ( MLP )	(emaelapI)
NP30 MLP	emaelapI

----
00107	A multilayer perceptron is just a mathematical function mapping some set of input values to output values.	 eka maltIleyara parseptoYna sirPa eka gaNiwIya Palana mAnaciwraNa hE jo Autaputa mUlyoM ke lie inaputa mUlyoM ke kuCa seta ko mEpa karawA hE .		
106	106
S1 A multilayer perceptron is just a mathematical function mapping some set of input values to output values .	eka bahuswarIya avaXAraNA sirPa eka gaNiwIya kArya hE jo mUlyoM ko bAhara karane ke lie inaputa mUlyoM ke kuCa seta kA mEpiMga karawA hE
NP2 A multilayer perceptron	eka bahuAyAmI avaXAraNA
VP6_LWG is just	basa hE
ADVP8 just	basa
NP10 a mathematical function mapping some set of input values to output values	eka gaNiwIya kArya mUlyoM ko Autaputa karane ke lie inaputa mUlyoM ke kuCa seta kA mEpiMga karawA hE
NP11 a mathematical function	eka gaNiwIya samAroha
VP15_LWG mapping	mEpiMga
NP17 some set of input values	inaputa mUlyoM kA kuCa seta
NP18 some set	kuCa seta
PP21 of input values	inaputa mUlyoM ke
NP23 input values	inaputa mUlyoM
NNS25 values	mUlyoM
PP26 to output values	mUlyoM kA uwpAxana karane ke lie
NP28 output values	Autaputa mUlyoM
NNS30 values	mUlyoM

----
00108	The function is formed by composing many simpler functions.	 samAroha kaI sarala kAryoM kI racanA ke xvArA nirmiwa hE .		
107	107
S1 The function is formed by composing many simpler functions .	isa samAroha kA nirmANa kaI sAXAraNa kAryoM kI racanA karake kiyA jAwA hE
NP2 The function	samAroha
VP5_LWG is formed	banAyA gayA hE
PP9 by composing many simpler functions	kaI sAXAraNa kAryoM kI racanA kara
S11 composing many simpler functions	kaI sAXAraNa kAryoM kI racanA
VP12_LWG composing	racanA
NP14 many simpler functions	kaI sAXAraNa kArya
NNS17 functions	samAroha

----
00109	We can think of each application of a dierent mathematical function as providing a new representation of the input.	 hama inaputa kA eka nayA prawiniXiwva praxAna karane ke rUpa meM eka alaga gaNiwIya samAroha ke prawyeka Avexana ke bAre meM soca sakawe hEM .		
108	108
S1 We can think of each application of a dierent mathematical function as providing a new representation of the input .	hama inaputa kA nayA prawiniXiwva upalabXa karAne ke rUpa meM eka alaga gaNiwIya kArya ke prawyeka Avexana ke bAre meM soca sakawe hEM
NP2 We	hama
VP4_LWG can think	soca sakawe hEM
PP8 of each application of a dierent mathematical function	eka alaga gaNiwIya kArya ke prawyeka Avexana kA
NP10 each application of a dierent mathematical function	eka alaga gaNiwIya kArya kA prawyeka Avexana
NP11 each application	prawyeka Avexana
PP14 of a dierent mathematical function	eka alaga gaNiwIya samAroha kA
NP16 a dierent mathematical function	eka alaga gaNiwIya kArya
PP21 as providing a new representation of the input	inaputa kA nayA prawiniXiwva upalabXa karAne ke rUpa meM
S23 providing a new representation of the input	inaputa kA nayA prawiniXiwva upalabXa karAne
VP24_LWG providing	praxAna karanA
NP26 a new representation of the input	inaputa kA nayA prawiniXiwva
NP27 a new representation	eka nayA prawiniXiwva
PP31 of the input	inaputa kA
NP33 the input	inaputa

----
00110	The idea of learning the right representation for the data provides one per- spective on deep learning.	 detA ke lie sahI prawiniXiwva sIKane kA vicAra gahana aXyayana para eka prawi - parixqSya praxAna karawA hE .		
109	109
S1 The idea of learning the right representation for the data provides one per - spective on deep learning .	detA ke lie sahI prawiniXiwva sIKane kA vicAra eka prawi - gaharI sIKa para BaviRyavANI praxAna karawA hE
NP2 The idea of learning the right representation for the data	detA ke lie sahI prawiniXiwva sIKane kA vicAra
NP3 The idea	vicAra
PP6 of learning the right representation for the data	detA ke lie sahI prawiniXiwva sIKanA
S8 learning the right representation for the data	detA ke lie sahI prawiniXiwva sIKanA
VP9_LWG learning	sIKanA
NP11 the right representation	sahI prawiniXiwva
PP15 for the data	detA ke lie
NP17 the data	detA
NNS19 data	detA
VP20_LWG provides	praxAna karawA hE
NP22 one per - spective	eka prawi - saMBAviwa
NP23 one	eka
PP25 per - spective	prawi- saMBAvanAwmaka
NP28 spective	BaviRyavANI
PP30 on deep learning	gaharI sIKa para
NP32 deep learning	gaharI sIKa

----
0011	This solution is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept dened through its relation to simpler concepts.	 yaha samAXAna kaMpyUtara ko anuBava se sIKane Ora avaXAraNAoM ke eka paxAnukrama ke saMxarBa meM xuniyA ko samaJane kI anumawi xene ke lie hE , prawyeka avaXAraNA ke sAWa sarala avaXAraNAoM ke apane saMbaMXa ke mAXyama se pariBARiwa .		
10	10
S1 This solution is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts , with each concept dened through its relation to simpler concepts .	yaha samAXAna kaMpyUtaroM ko avaXAraNAoM kI eka pravqwwi ke mAmale meM anuBava se sIKane Ora xuniyA ko samaJane kI anumawi xenA hE, prawyeka avaXAraNA ke sAWa usake
NP2 This solution	yaha samAXAna
VP5_LWG is	hE
S7 to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts , with each concept dened through its relation to simpler concepts	kaMpyUtaroM ko anuBava se sIKane Ora avaXArAoM kI eka pravqwwi ke mAmale meM xuniyA ko samaJane kI anumawi xene ke lie, prawyeka avaXAraNA ke sAWa sarala avaXArAoM
VP8_LWG to allow	anumawi xene ke lie
NP12 computers	kaMpyUtara
NNS13 computers	kaMpyUtara
S14 to learn from experience and understand the world in terms of a hierarchy of concepts	avaXArAoM kI eka reKA ke mAmale meM anuBava se sIKeM Ora xuniyA ko samaJeM
VP15_LWG to learn understand	samaJane ke lie
PP20 from experience	anuBava se
NP22 experience	anuBava
CC24 and	Ora
VP18 learn from experience	anuBava se sIKeM
VP25 understand the world in terms of a hierarchy of concepts	avaXArAoM kI eka paxAnukrama ke mAmale meM xuniyA ko samaJeM
NP27 the world	xuniyA
PP30 in terms of a hierarchy of concepts	avaXArAoM kI eka paxAnukrama ke mAmale meM
NP32 terms of a hierarchy of concepts	avaXArAoM kI eka paxAnukrama kI SarweM
NP33 terms	SarwoM
NNS34 terms	SarwoM
PP35 of a hierarchy of concepts	avaXArAoM kI eka pravqwwi kA
NP37 a hierarchy of concepts	avaXArAoM kI eka pravqwwi
NP38 a hierarchy	eka paxAnukrama
PP41 of concepts	avaXArAoM kI
NP43 concepts	avaXAraNAez
NNS44 concepts	avaXAraNAez
,45 ,	,
PP46 with each concept dened through its relation to simpler concepts	prawyeka avaXAraNA ke sAWa sarala avaXArAoM ke saMbaMXa ke mAXyama se pariBARiwa
NP48 each concept dened through its relation to simpler concepts	prawyeka avaXAraNA ko sarala avaXArAoM ke saMbaMXa meM pariBARiwa
NP49 each concept	prawyeka avaXAraNA
VP52_LWG dened	pariBARiwa
PP54 through its relation	usake riSwe ke mAXyama se
NP56 its relation	isakA saMbaMXa
PP59 to simpler concepts	sarala avaXArAoM ko
NP61 simpler concepts	sarala avaXAraNAez
NNS63 concepts	avaXAraNAez

----
00111	Another perspective on deep learning is that depth enables the computer to learn a multistep computer program.	 gahare sIKane para eka Ora pariprekRya hE ki gaharAI kaMpyUtara ko eka maltIstepa kaMpyUtara progrAma sIKane meM sakRama banAwA hE .		
110	110
S1 Another perspective on deep learning is that depth enables the computer to learn a multistep computer program .	gaharI sIKa para eka Ora pariprekRya yaha hE ki gaharAI kaMpyUtara ko bahuAyAmI kaMpyUtara progrAma sIKane meM sakRama banAwI hE
NP2 Another perspective on deep learning	gaharI sIKa para eka Ora najariyA
NP3 Another perspective	eka Ora pariprekRya
PP6 on deep learning	gaharI sIKa para
NP8 deep learning	gaharI sIKa
VP11_LWG is	hE
SBAR13 that depth enables the computer to learn a multistep computer program	yaha gaharAI kaMpyUtara ko bahuAyAmI kaMpyUtara progrAma sIKane meM sakRama banAwI hE
S15 depth enables the computer to learn a multistep computer program	kaMpyUtara ko bahuAyAmI kaMpyUtara progrAma sIKane meM sakRama banAwA hE gaharAI
NP16 depth	gaharAI
VP18_LWG enables	sakRama
NP20 the computer	kaMpyUtara
S23 to learn a multistep computer program	maltIstrepa kaMpyUtara progrAma sIKane ke lie
VP24_LWG to learn	sIKane ke lie
NP28 a multistep computer program	eka bahuAyAmI kaMpyUtara progrAma

----
00112	Each layer of the representation can be thought of as the state of the computers memory after 5 CHAPTER 1.	 prawiniXiwva kI prawyeka parawa ko 5 CHAPTER 1 ke bAxa kaMpyUtara kI smqwi kI sWiwi ke rUpa meM socA jA sakawA hE .		
111	111
S1 Each layer of the representation can be thought of as the state of the computer s memory after 5 CHAPTER 1 .	prawiniXiwva kI prawyeka parawa ko 5 cEptara 1 ke bAxa kaMpyUtara kI smqwi kI sWiwi ke rUpa meM socA jA sakawA hE
NP2 Each layer of the representation	prawiniXiwva kI prawyeka parawa
NP3 Each layer	prawyeka parawa
PP6 of the representation	prawiniXiwva kA
NP8 the representation	prawiniXiwva
VP11_LWG can be thought	socA jA sakawA hE
PP17 of as the state of the computer s memory	kaMpyUtara kI yAxaxASwa kI sWiwi ke rUpa meM
PP19 as the state of the computer s memory	jEsA ki kaMpyUtara kI yAxaxASwa kI sWiwi
NP21 the state of the computer s memory	kaMpyUtara kI yAxaxASwa kI sWiwi
NP22 the state	praxeSa
PP25 of the computer s memory	kaMpyUtara kI memorI
NP27 the computer s memory	kaMpyUtara kI memorI
NML29 computer s	kaMpyUtara kI
PP33 after 5 CHAPTER 1	5 cEptara 1 ke bAxa
NP35 5 CHAPTER 1	5 cEptara 1

----
00113	INTRODUCTION Visible layer (input pixels)	 xqSya parawa ( inaputa pikselsa )		
112	112
FRAG1 INTRODUCTION Visible layer ( input pixels )	paricaya spaRta leyara (inaputa piksala)
NP2 INTRODUCTION Visible layer	paricaya spaRta parawa
NP7 input pixels	inaputa piksela
NNS9 pixels	piksela

----
00114	1st hidden layer (edges)	 praWama CipA huA		
113	113
FRAG1 1st hidden layer ( edges )	pahalI CipI huI parawa (kinAroM)
NP2 1st hidden layer	pahalI CipI huI parawa
NP6 ( edges )	(kinAre)

----
00115	2nd hidden layer (corners and contours) 3rd hidden layer (object parts)	 xviwIya gupwa parawa ( saMxUka Ora saMxUka )		
114	114
FRAG1 2nd hidden layer ( corners and contours ) 3rd hidden layer ( object parts )	xUsarI CipI huI parawa (kone Ora koYrnara) 3 vIM CipI huI parawa (vaswu BAga)
NP2 2nd hidden layer ( corners and contours ) 3rd	xUsarI CipI huI parawa (kone Ora kaMXoM) 3 vIM
NP3 2nd hidden layer ( corners and contours )	xUsarI CipI huI parawa (kone Ora koYrnara)
NP4 2nd hidden layer	xUsarI CipI huI parawa
NP9 corners and contours	kone Ora kaMXoM
NNS10 corners	kone-kone
CC11 and	Ora
NNS12 contours	wahaKAne
NP14 3rd	3 vIM
NP16 hidden layer ( object parts )	CipI huI parawa (vaswu BAga)
NP17 hidden layer	CipI huI parawa
NP21 object parts	vaswuoM ke hisse
NNS23 parts	BAgoM

----
00116	CAR PERSON ANIMAL Output (object identity)	 CAR PERSON ANIMAL Autaputa ( viRayagawa pahacAna )		
115	115
NP1 CAR PERSON ANIMAL Output ( object identity )	kAra vyakwi AjIvika Autaputa (vaswu pahacAna)
NP2 CAR PERSON ANIMAL	kAra vyakwi AxivAsI
NP6 Output ( object identity )	Autaputa (vaswu pahacAna)
PRN8 ( object identity )	(vaswu pahacAna)
NP10 object identity	vaswu pahacAna

----
00117	Figure 1.2: Illustration of a deep learning model.	 ciwra 1 . 2 : gahare sIKane ke moYdala kA praxarSana		
116	116
FRAG1 Figure 1.2 : Illustration of a deep learning model .	Pigara 1.2 H gahare sIKane ke moYdala kI ciwrakalA
NP2 Figure 1.2	AMkadZA 1.2
NP6 Illustration of a deep learning model	eka gaharI sIKa moYdala kI CaviyAM
NP7 Illustration	ciwrakalA
PP9 of a deep learning model	eka gaharI sIKa moYdala kI
NP11 a deep learning model	eka gaharI sIKa moYdala

----
00118	It is dicult for a computer to understand the meaning of raw sensory input data, such as this image represented as a collection of pixel values.	 eka kaMpyUtara ke lie kacce saMvexI inaputa detA ke arWa ko samaJanA kaTina howA hE , jEse ki yaha Cavi piksela mUlyoM ke saMgraha ke rUpa meM prawiniXiwva karawI hE .		
117	117
S1 It is dicult for a computer to understand the meaning of raw sensory input data , such as this image represented as a collection of pixel values .	kaMpyUtara ke lie kacce seMsara inaputa dAtA ke arWa ko samaJanA muSkila hE, jEse ki yaha Cavi piksala mUlyoM ke saMgraha ke rUpa meM xarSAyI gaI
NP2 It	yaha
VP4_LWG is	hE
ADJP6 dicult for a computer	kaMpyUtara ke lie muSkila
PP8 for a computer	eka kaMpyUtara ke lie
NP10 a computer	eka kaMpyUtara
S13 to understand the meaning of raw sensory input data	kacce seMsara inaputa dAtA ke arWa ko samaJane ke lie
VP14_LWG to understand	samaJane ke lie
NP18 the meaning of raw sensory input data	kacce seMsara inaputa dAtA kA arWa
NP19 the meaning	mawalaba
PP22 of raw sensory input data	kacce seMsara inaputa dAtA kA
NP24 raw sensory input data	kaccA seMsara inaputa dAtA
NML26 sensory input	seMsararI inaputa
NNS29 data	detA
,30 ,	,
SBAR31 such as this image represented as a collection of pixel values	jEse ki yaha Cavi piksala mUlyoM ke saMgraha ke rUpa meM xarSAyI gaI
S34 this image represented as a collection of pixel values	yaha Cavi piksala mUlyoM ke saMgraha ke rUpa meM xarSAyI gaI
NP35 this image	yaha Cavi
VP38_LWG represented	prawiniXiwva
PP40 as a collection of pixel values	piksala mUlyoM ke saMgraha ke rUpa meM
NP42 a collection of pixel values	piksala mUlyoM kA saMgraha
NP43 a collection	eka saMgraha
PP46 of pixel values	piksala mUlyoM kI
NP48 pixel values	piksala ke mUlya
NNS50 values	mUlyoM

----
00119	The function mapping from a set of pixels to an object identity is very complicated.	 kisI vaswu kI pahacAna ke lie piksala ke samuccaya se Palana prawiciwraNa bahuwa jatila howA hE 		
118	118
S1 The function mapping from a set of pixels to an object identity is very complicated .	piksela ke seta se lekara eka vaswu pahacAna waka PaMkSana mEpiMga behaxa jatila hE
NP2 The function mapping from a set of pixels to an object identity	piksela ke eka seta se lekara eka vaswu pahacAna waka kA samAroha mEpiMga
NP3 The function mapping	PaMkSana mEpiMga
PP7 from a set of pixels to an object identity	piksela ke seta se lekara eka vaswu pahacAna waka
NP9 a set of pixels to an object identity	eka vaswu pahacAna ke lie piksela kA seta
NP10 a set	eka seta
PP13 of pixels to an object identity	eka vaswu pahacAna ke lie piksela kA
NP15 pixels to an object identity	eka vaswu pahacAna ke lie piksela
NP16 pixels	piksela
NNS17 pixels	piksela
PP18 to an object identity	eka vaswu pahacAna ke lie
NP20 an object identity	eka vaswu pahacAna
VP24_LWG is	hE
ADJP26 very complicated	bahuwa jatila

----
00120	Learning or evaluating this mapping seems insurmountable if tackled directly.	 isa mAnaciwraNa ko sIKanA yA mUlyAMkana karanA yaxi sIXe hala kara liyA jAe wo yaha asaMBava prawIwa howA hE 		
119	119
S1 Learning or evaluating this mapping seems insurmountable if tackled directly .	sIXe nipatane para isa mEpiMga ko sIKanA yA mUlyAMkana karanA aprawyASiwa lagawA hE
S2 Learning or evaluating this mapping	isa mEpiMga kA sIKanA yA mUlyAMkana
VP3_LWG Learning evaluating	mUlyAMkana sIKanA
CC5 or	yA
VBG4 Learning	sIKanA
VBG6 evaluating	mUlyAMkana
NP7 this mapping	yaha mEpiMga
VP10_LWG seems	lagawA hE lagawA hE
NP12 insurmountable	nirBara karane yogya
SBAR14 if tackled directly	agara sIXe nipatAyA wo
S16 tackled directly	sIXA nipatAyA
VP17_LWG tackled directly	sIXA nipatAyA
ADVP19 directly	sIXe wOra para

----
0012	By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all the knowledge that the computer needs.	 anuBava se jFAna ikatTA karake , yaha xqRtikoNa mAnava oYparetaroM ke lie una saBI jFAna ko OpacArika rUpa se nirxiRta karane kI AvaSyakawA se bacawA hE jinakI kaMpyUtara ko AvaSyakawA howI hE .		
11	11
S1 By gathering knowledge from experience , this approach avoids the need for human operators to formally specify all the knowledge that the computer needs .	anuBava se jFAna jutAkara, yaha xqRtikoNa mAnava saMcAlakoM kI AvaSyakawA se bacawA hE ki vaha saBI jFAna ko OpacArika rUpa se viSiRta kare jo kaMpyUtara kI jarUrawa hE
PP2 By gathering knowledge from experience	anuBava se jFAna jutAkara
S4 gathering knowledge from experience	anuBava se jFAna jutAnA
VP5_LWG gathering	ikatTA honA
NP7 knowledge	jFAna
PP9 from experience	anuBava se
NP11 experience	anuBava
,13 ,	,
NP14 this approach	yaha xqRtikoNa
VP17_LWG avoids	isase bacawe hEM
NP19 the need for human operators	mAnava saMcAlakoM kI jarUrawa
NP20 the need	jarUrawa
PP23 for human operators	mAnava saMcAlakoM ke lie
NP25 human operators	mAnava saMcAlaka
NNS27 operators	saMcAlaka
S28 to formally specify all the knowledge that the computer needs	OpacArika rUpa se una saBI jFAnoM ko viSiRta karane ke lie jinheM kaMpyUtara ko cAhie
VP29_LWG to formally specify	OpacArika rUpa se viSiRtawA ke lie
ADVP31 formally	OpacArika wOra para
NP35 all the knowledge	sArA jFAna
SBAR39 that the computer needs	ki kaMpyUtara kI jarUrawa
S41 the computer needs	kaMpyUtara kI jarUrawa
NP42 the computer	kaMpyUtara
VP45_LWG needs	jarUraweM

----
00121	Deep learning resolves this diculty by breaking the desired complicated mapping into a series of nested simple mappings, each described by a dierent layer of the model.	 gahana aXigama isa kaTinAI ko nIdZiwa sarala mAnaciwraNoM kI SrqMKalA meM vAMCiwa jatila mAnaciwraNa ko wodZakara hala karawA hE , prawyeka moYdala kI eka alaga parawa xvArA varNiwa howA hE 		
120	120
S1 Deep learning resolves this diculty by breaking the desired complicated mapping into a series of nested simple mappings , each described by a dierent layer of the model .	gaharI sIKa isa kaTinAI ko nAyAba sAXAraNa mEpyuPikeSana kI eka SrqMKalA meM tUtakara, prawyeka ko moYdala kI eka alaga-alaga parawa xvArA bawAyA gayA hE
NP2 Deep learning	gaharI sIKa
VP5_LWG resolves	hala karawA hE
NP8 this diculty	yaha kaTinAI
PP11 by breaking the desired complicated mapping into a series of nested simple mappings	vAMCiwa sAXAraNa mEpyuleksa kI eka SrqMKalA meM vAMCiwa jatila mEpiMga ko wodZakara
S13 breaking the desired complicated mapping into a series of nested simple mappings	vAMCiwa sAXAraNa mEpyuleksa kI eka SrqMKalA meM vAMCiwa jatila mEpiMga ko wodZanA
VP14_LWG breaking	wodZanA
NP16 the desired complicated mapping	vAMCiwa jatila mEpiMga
PP21 into a series of nested simple mappings	nihiwa sAXAraNa mEpyuleksa kI eka SrqMKalA meM
NP23 a series of nested simple mappings	nihiwa sAXAraNa mEpyuleksa kI eka SrqMKalA
NP24 a series	eka SrqMKalA
PP27 of nested simple mappings	nihiwa sAXAraNa mEpyuPikeSana kA
NP29 nested simple mappings	sarala mEpyuPikeSana kA SikAra huA
NNS32 mappings	mEpyuleksa
,33 ,	,
NP34 each described by a dierent layer of the model	moYdala kI eka alaga parawa se varNiwa prawyeka
NP35 each	prawyeka
VP37_LWG described	varNiwa
PP39 by a dierent layer of the model	moYdala kI eka alaga parawa se
NP41 a dierent layer of the model	moYdala kI eka alaga parawa
NP42 a dierent layer	eka alaga parawa
PP46 of the model	moYdala kI
NP48 the model	moYdala

----
00122	The input is presented at the visible layer , so named because it contains the variables that we are able to observe.	 inaputa ko xqSya swara para praswuwa kiyA jAwA hE , isalie nAma xiyA jAwA hE kyoMki isameM ve cara howe hEM jo hama nirIkRaNa karane meM sakRama howe hEM .		
121	121
S1 The input is presented at the visible layer , so named because it contains the variables that we are able to observe .	inaputa xqSya parawa para peSa kiyA jAwA hE, isalie nAma xiyA jAwA hE kyoMki isameM veriebala mOjUxa howe hEM jinheM hama xeKane meM sakRama howe hEM
NP2 The input	inaputa
VP5_LWG is presented so named	iwanA nAma peSa kiyA jAwA hE
PP10 at the visible layer	xiKane vAlI parawa para
NP12 the visible layer	xiKane vAlI parawa
,16 ,	,
ADVP17 so	Ese meM
SBAR21 because it contains the variables that we are able to observe	kyoMki isameM veriebala mOjUxa howe hEM jinheM hama xeKa pAwe hEM
S23 it contains the variables that we are able to observe	isameM ve vEriebala mOjUxa howe hEM jinheM hama xeKa pAwe hEM
NP24 it	yaha
VP26_LWG contains	isameM SAmila
NP28 the variables	veriebala
NNS30 variables	vEriebala
SBAR31 that we are able to observe	ki hama nirIkRaNa kara pA rahe hEM
S33 we are able to observe	hama nirIkRaNa kara pA rahe hEM
NP34 we	hama
VP36_LWG are	hEM
ADJP38 able to observe	nirIkRaNa karane meM sakRama
S40 to observe	nirIkRaNa ke lie
VP41_LWG to observe	nirIkRaNa ke lie

----
00123	Then a series of hidden layers extracts increasingly abstract features from the image.	 Pira CupI parawoM kI eka SrqMKalA Cavi se baDZawI huI amUrwa viSeRawAoM ko nikAlawI hE .		
122	122
S1 Then a series of hidden layers extracts increasingly abstract features from the image .	Pira CipI huI parawoM kI eka SrqMKalA Cavi se badZe pEmAne para asaMgawa viSeRawAoM ko nikAlawI hE
ADVP2 Then	Pira
NP4 a series of hidden layers extracts increasingly abstract	CipI huI parawoM kI eka SrqMKalA wejI se amUrwa nikAlawI hE
NP5 a series	eka SrqMKalA
PP8 of hidden layers extracts increasingly abstract	CipI huI parawoM kI paraweM wejI se amUrwa nikAlawI hEM
NP10 hidden layers extracts increasingly abstract	CipI huI paraweM wejI se amUrwa nikAlawI hEM
NP11 hidden layers extracts	CipI huI paraweM nikAlawI hEM
NML12 hidden layers	CipI huI paraweM
NNS14 layers	paraweM
NNS15 extracts	nikAlawA hE
ADJP16 increasingly abstract	wejI se amUrwa
VP19_LWG features	PIcarsa
PP21 from the image	Cavi se
NP23 the image	Cavi

----
00124	These layers are called hidden because their values are not given in the data; instead the model must determine which concepts are useful for explaining the relationships in the observed data.	 ina parawoM ko hidden kahA jAwA hE kyoMki unake mUlya detA meM nahIM xie gae hEM , kyoMki moYdala ko una avaXAraNAoM kA nirXAraNa karanA cAhie jo prekRiwa AMkadZoM meM saMbaMXoM kI vyAKyA karane ke lie upayogI hEM .		
123	123
S1 These layers are called  hidden  because their values are not given in the data ; instead the model must determine which concepts are useful for explaining the relationships in the observed data .	ina parawoM ko  CipA huA kahA jAwA hE kyoMki unake mUlyoM ko AMkadZoM meM nahIM xiyA jAwA hE; isake bajAya moYdala nirXAriwa karanA cAhie ki nihiwa AMkadZe meM
S2 These layers are called  hidden  because their values are not given in the data	ina parawoM ko  CipA huA kahA jAwA hE kyoMki unake mUlyoM ko AMkadZoM meM nahIM xiyA jAwA
NP3 These layers	ye paraweM
NNS5 layers	paraweM
VP6_LWG are called	kahawe hEM
S10  hidden 	 CipA huA
ADJP11  hidden 	 CipA huA
SBAR15 because their values are not given in the data	kyoMki unake mUlyoM ko AMkadZoM meM nahIM xiyA jAwA
S17 their values are not given in the data	unake mUlyoM ko AMkadZoM meM nahIM xiyA jAwA
NP18 their values	unake mUlya
NNS20 values	mUlyoM
VP21_LWG are not given	nahIM xiyA jAwA
PP26 in the data	AMkadZoM meM
NP28 the data	detA
NNS30 data	detA
S32 instead the model must determine which concepts are useful for explaining the relationships in the observed data	isake bajAya moYdala nirXAriwa karanA cAhie ki avalokiwa AMkadZoM meM riSwoM ko samaJAne ke lie kOna sI avaXArAez upayogI hEM
ADVP33 instead	isake bajAya
NP35 the model	moYdala
VP38_LWG must determine	nirXAriwa karanA cAhie
SBAR42 which concepts are useful for explaining the relationships in the observed data	nirIkRiwa AMkadZoM meM riSwoM ko samaJAne ke lie upayogI kOna sI avaXArAez upayogI
WHNP43 which	jo ki
S45 concepts are useful for explaining the relationships in the observed data	avaXAriwa AMkadZoM meM riSwoM ko samaJAne ke lie upayogI hEM avaXArAez
NP46 concepts	avaXAraNAez
NNS47 concepts	avaXAraNAez
VP48_LWG are	hEM
ADJP50 useful for explaining the relationships in the observed data	nirIkRiwa AMkadZoM meM riSwoM ko samaJAne ke lie upayogI
PP52 for explaining the relationships in the observed data	avalokiwa AMkadZoM meM riSwoM ko samaJAne ke lie
S54 explaining the relationships in the observed data	avalokiwa AMkadZoM meM saMbaMXoM kI vyAKyA
VP55_LWG explaining	samaJAwe hue
NP57 the relationships in the observed data	nirIkRiwa AMkadZoM meM riSwe
NP58 the relationships	riSwoM
NNS60 relationships	riSwoM
PP61 in the observed data	avalokiwa AMkadZoM meM
NP63 the observed data	avalokiwa AMkadZe
NNS66 data	detA

----
00125	The images here are visualizations of the kind of feature represented by each hidden unit.	 yahAz CaviyoM prawyeka CipA ikAI xvArA prawiniXiwva viSeRawA ke prakAra ke xqSya hEM .		
124	124
S1 The images here are visualizations of the kind of feature represented by each hidden unit .	yahAM kI CaviyAM prawyeka CipI huI ikAI xvArA xarSAe gae viSeRawA ke vyAKyAna hEM
NP2 The images	CaviyAM
NNS4 images	CaviyAM
ADVP5 here	yahAM
VP7_LWG are	hEM
NP9 visualizations of the kind of feature represented by each hidden unit	prawyeka CipI huI yUnita xvArA xarSAe gae viSeRawA ke vyAKyAna
NP10 visualizations	xqSyIkaraNa
NNS11 visualizations	xqSyIkaraNa
PP12 of the kind of feature represented by each hidden unit	prawyeka CipI huI yUnita xvArA xarSAyA gayA viSeRawA kA
NP14 the kind of feature represented by each hidden unit	prawyeka CipI huI yUnita xvArA xarSAyA gayA viSeRawA
NP15 the kind	xayAlu
PP18 of feature represented by each hidden unit	prawyeka CipI huI yUnita xvArA xarSAyA gayA PIcara
NP20 feature represented by each hidden unit	prawyeka CipI huI yUnita xvArA xarSAyA gayA PIcara
NP21 feature	PIcara
VP23_LWG represented	prawiniXiwva
PP25 by each hidden unit	prawyeka CipI huI yUnita xvArA
NP27 each hidden unit	prawyeka CipI huI yUnita

----
00126	Given the pixels, the rst layer can easily identify edges, by comparing the brightness of neighboring pixels.	 piksela ko xeKawe hue , pahalI parawa AsAnI se kinAroM kI pahacAna kara sakawe hEM , padZosI piksela kI camaka kI wulanA karake .		
125	125
S1 Given the pixels , the rst layer can easily identify edges , by comparing the brightness of neighboring pixels .	piksela ko xeKawe hue pahalI parawa AsAnI se padZosI piksela kI camaka kI wulanA karake kinAroM kI pahacAna kara sakawI hE
PP2 Given the pixels	piksela ko xeKawe hue
NP4 the pixels	piksela
NNS6 pixels	piksela
,7 ,	,
NP8 the rst layer	pahalI parawa
VP12_LWG can easily identify	AsAnI se pahacAna sakawe hEM
ADVP14 easily	AsAnI se
NP18 edges	kinAroM
NNS19 edges	kinAroM
,20 ,	,
PP21 by comparing the brightness of neighboring pixels	padZosI piksela kI camaka kI wulanA kara
S23 comparing the brightness of neighboring pixels	padZosI piksela kI camaka kI wulanA
VP24_LWG comparing	wulanA karanA
NP26 the brightness of neighboring pixels	padZosI piksela kI ujjvalawA
NP27 the brightness	ujjvalawA
PP30 of neighboring pixels	padZosI piksela kI
NP32 neighboring pixels	padZosI piksela
NNS34 pixels	piksela

----
00127	Given the rst hidden layers description of the edges, the second hidden layer can easily search for corners and extended contours, which are recognizable as collections of edges.	 kinAroM ke pahale CipA parawa ke varNana ko xeKawe hue , xUsarI CipA parawa AsAnI se konoM Ora viswAriwa contours ke lie Koja kara sakawe hEM , jo kinAroM ke saMgraha ke rUpa meM pahacAnane yogya hEM .		
126	126
S1 Given the rst hidden layer s description of the edges , the second hidden layer can easily search for corners and extended contours , which are recognizable as collections of edges .	pahalI CipI huI parawa ke kinAroM kA vivaraNa xeKawe hue xUsarI CipI huI parawa AsAnI se konoM Ora viswAriwa kroYuMda kI walASa kara sakawI hE, jo kinAroM
PP2 Given the rst hidden layer s description of the edges	pahalI CipI huI parawa ke kinAroM kA vivaraNa xeKawe hue
NP4 the rst hidden layer s description of the edges	pahalI CipI huI parawa kA kinAroM kA vivaraNa
NP5 the rst hidden layer s description	pahalI CipI huI parawa kA vivaraNa
NML8 hidden layer	CipI huI parawa
PP13 of the edges	kinAroM kI
NP15 the edges	kinAroM
NNS17 edges	kinAroM
,18 ,	,
NP19 the second hidden layer	xUsarI CipI huI parawa
VP24_LWG can easily search	AsAnI se Koja sakawe hEM
ADVP26 easily	AsAnI se
PP30 for corners and extended contours , which are recognizable as collections of edges	konoM ke lie Ora viswAriwa parisaroM ke lie, jo ki kinAroM ke saMgraha ke rUpa meM mAnyawA prApwa hEM
NP32 corners and extended contours , which are recognizable as collections of edges	kone Ora viswAriwa parisara, jo ki kinAroM ke saMgraha ke rUpa meM mAnyawA prApwa hEM
NP33 corners and extended contours	konoM Ora viswAriwa kroYuMda
NP34 corners	kone-kone
NNS35 corners	kone-kone
CC36 and	Ora
NP37 extended contours	viswAriwa parisara
NNS39 contours	wahaKAne
,40 ,	,
SBAR41 which are recognizable as collections of edges	jo ki kinAroM ke saMgraha ke rUpa meM mAnyawA prApwa hEM
WHNP42 which	jo ki
S44 are recognizable as collections of edges	kinAroM ke saMgraha ke rUpa meM mAnyawA prApwa hEM
VP45_LWG are	hEM
ADJP47 recognizable as collections of edges	kinAroM ke saMgraha ke rUpa meM mAnyawA prApwa
PP49 as collections of edges	kinAroM ke saMgraha ke rUpa meM
NP51 collections of edges	kinAroM ke saMgraha
NP52 collections	saMgraha
NNS53 collections	saMgraha
PP54 of edges	kinAroM kI
NP56 edges	kinAroM
NNS57 edges	kinAroM

----
00128	Given the second hidden layers description of the image in terms of corners and contours, the third hidden layer can detect entire parts of specic objects, by nding specic collections of contours and corners.	 konoM Ora saMcaroM ke saMxarBa meM Cavi ke xUsare Cipe hue parawa ke varNana ko xeKawe hue , wIsarI CupI parawa viSiRta vaswuoM ke saMpUrNa BAgoM kA pawA lagA sakawI hE , jisameM saMcaroM Ora konoM kA viSiRta saMgraha pAyA jA sakawA hE .		
127	127
S1 Given the second hidden layer s description of the image in terms of corners and contours , the third hidden layer can detect entire parts of specic objects , by nding specic collections of contours and corners .	konoM Ora kaMXoM ke mAmale meM Cavi ke xUsarI CipI huI parawa ko xeKawe hue wIsarI CipI huI parawa viSiRta vaswuoM ke saMpUrNa hissoM kA pawA lagA sakawI hE
PP2 Given the second hidden layer s description of the image in terms of corners and contours	konoM Ora kaMXoM ke mAmale meM xUsarI CipI huI parawa ke vivaraNa ko xeKawe hue
NP4 the second hidden layer s description of the image in terms of corners and contours	xUsarI CipI huI parawa kA konoM Ora kaMXoM ke mAmale meM Cavi kA vivaraNa
NP5 the second hidden layer s description	xUsarI CipI huI parawa kA vivaraNa
NML8 hidden layer	CipI huI parawa
PP13 of the image in terms of corners and contours	konoM Ora kaMXoM ke mAmale meM Cavi kI
NP15 the image in terms of corners and contours	konoM Ora kaMXoM ke mAmale meM Cavi
NP16 the image	Cavi
PP19 in terms of corners and contours	konoM Ora kaMXoM ke mAmale meM
NP21 terms of corners and contours	konoM Ora kaMXoM kI SarweM
NP22 terms	SarwoM
NNS23 terms	SarwoM
PP24 of corners and contours	konoM Ora kaMXoM kI
NP26 corners and contours	kone Ora kaMXoM
NNS27 corners	kone-kone
CC28 and	Ora
NNS29 contours	wahaKAne
,30 ,	,
NP31 the third hidden layer	wIsarI CipI huI parawa
VP36_LWG can detect	pawA lagA sakawe hEM
NP40 entire parts of specic objects	viSiRta vaswuoM ke pUre hisse
NP41 entire parts	pUre hisse
NNS43 parts	BAgoM
PP44 of specic objects	viSiRta vaswuoM kA
NP46 specic objects	viSiRta vaswueM
NNS48 objects	vaswueM
,49 ,	,
PP50 by nding specic collections of contours and corners	kaMXoM Ora konoM ke viSiRta saMgraha Koja kara
S52 nding specic collections of contours and corners	kaMXoM Ora konoM ke viSiRta saMgraha Kojane
VP53_LWG nding	mila rahA hE
NP55 specic collections of contours and corners	koYrnara Ora konoM ke viSiRta saMgraha
NP56 specic collections	viSiRta saMgraha
NNS58 collections	saMgraha
PP59 of contours and corners	kaMXoM Ora konoM kI
NP61 contours and corners	wahaKAne Ora kone
NNS62 contours	wahaKAne
CC63 and	Ora
NNS64 corners	kone-kone

----
00129	Finally, this description of the image in terms of the object parts it contains can be used to recognize the objects present in the image.	 aMwa meM , Cavi ke isa varNana ko vaswu BAgoM ke saMxarBa meM , jisameM yaha samAhiwa hE , Cavi meM mOjUxa vaswuoM ko pahacAnane ke lie prayoga kiyA jA sakawA hE .		
128	128
S1 Finally , this description of the image in terms of the object parts it contains can be used to recognize the objects present in the image .	aMwa meM isameM mOjUxa vaswuoM ke hisAba se Cavi kA yaha vivaraNa Cavi meM upasWiwa vaswuoM ko pahacAnane ke lie iswemAla kiyA jA sakawA hE
ADVP2 Finally	aMwa meM
,4 ,	,
NP5 this description of the image in terms of the object parts it contains	vaswuoM ke hissoM ke mAmale meM Cavi kA yaha vivaraNa hE isameM
NP6 this description	yaha vivaraNa
PP9 of the image in terms of the object parts it contains	vaswuoM ke hissoM ke mAmale meM Cavi kI
NP11 the image in terms of the object parts it contains	vaswuoM ke hissoM ke mAmale meM Cavi isameM SAmila
NP12 the image	Cavi
PP15 in terms of the object parts it contains	vaswuoM ke hissoM ke mAmale meM isameM SAmila
NP17 terms of the object parts it contains	vaswuoM ke hissoM kI SarwoM meM isameM SAmila
NP18 terms	SarwoM
NNS19 terms	SarwoM
PP20 of the object parts it contains	vaswuoM ke hissoM meM isameM SAmila
NP22 the object parts it contains	isake vaswu BAgoM meM hE
NP23 the object parts	vaswuoM ke hissoM
NNS26 parts	BAgoM
SBAR27 it contains	isameM SAmila
S28 it contains	isameM SAmila
NP29 it	yaha
VP31_LWG contains	isameM SAmila
VP33_LWG can be used	iswemAla ho sakawA hE
S39 to recognize the objects present in the image	Cavi meM mOjUxa vaswuoM ko pahacAnane ke lie
VP40_LWG to recognize	pahacAnane ke lie
S44 the objects present in the image	Cavi meM mOjUxa vaswueM
NP45 the objects	vaswueM
NNS47 objects	vaswueM
ADJP48 present in the image	Cavi meM mOjUxa
PP50 in the image	Cavi meM
NP52 the image	Cavi

----
00130	Images reproduced with permission from Zeiler and Fergus (2014).	 jZIlara Ora Pargasa (2014 ) kI anumawi se ciwra praswuwa kiye gaye 		
129	129
S1 Images reproduced with permission from Zeiler and Fergus ( 2014 ) .	jZIlara Ora Pargasa ( 2014) se anumawi ke sAWa prajanana kie gae ciwra
NP2 Images	CaviyAM
NNS3 Images	CaviyAM
VP4_LWG reproduced	prajanana
PP6 with permission	anumawi se
NP8 permission	anumawi
PP10 from Zeiler and Fergus	jZIlara Ora Pargasa se
NP12 Zeiler and Fergus	jZelara Ora Pargasa
CC14 and	Ora
NNP13 Zeiler	jZIlara
NNP15 Fergus	Pargasa
PRN16 ( 2014 )	( 2014)
NP18 2014	2014

----
0013	The hierarchy of concepts enables the computer to learn complicated concepts by building them out of simpler ones.	 avaXAraNAoM kA paxAnukrama kaMpyUtara ko jatila avaXAraNAoM ko sarala avaXAraNAoM se nirmiwa karake sIKane meM sakRama banAwA hE .		
12	12
S1 The hierarchy of concepts enables the computer to learn complicated concepts by building them out of simpler ones .	avaXArAoM kI pravqwwi kaMpyUtara ko sarala logoM se bAhara banAkara jatila avaXArAoM ko sIKane meM sakRama banAwI hE
NP2 The hierarchy of concepts	avaXArAoM kI reKA
NP3 The hierarchy	paxAnukrama
PP6 of concepts	avaXArAoM kI
NP8 concepts	avaXAraNAez
NNS9 concepts	avaXAraNAez
VP10_LWG enables	sakRama
NP12 the computer	kaMpyUtara
S15 to learn complicated concepts by building them out of simpler ones	inheM sAXAraNa logoM se banAkara sIKane ke lie jatila avaXArAeM
VP16_LWG to learn	sIKane ke lie
NP20 complicated concepts	jatila parikalpanAeM
NNS22 concepts	avaXAraNAez
PP23 by building them out of simpler ones	unheM sarala logoM se banAkara
S25 building them out of simpler ones	unheM sarala logoM se banAnA
VP26_LWG building out	bildiMga Auta
NP28 them	unheM
PRT30 out	bAhara
PP32 of simpler ones	sarala vAloM kI
NP34 simpler ones	sarala vAle
NNS36 ones	vAle

----
00131	6 CHAPTER 1.	 6 CHAPTER 1 .		
130	130
FRAG1 6 CHAPTER 1 .	6 cEptara 1.
NP2 6	6
NP4 CHAPTER 1	cEptara 1

----
00132	INTRODUCTION executing another set of instructions in parallel.	 nirxeSa ke eka anya seta ko samAnAMwara rUpa se niRpAxiwa karane vAlA INTRODUCING 		
131	131
FRAG1 INTRODUCTION executing another set of instructions in parallel .	parisImana meM nirxeSoM ke eka Ora seta ko niRpAxiwa karawe hue paricaya
NP2 INTRODUCTION	paricaya
VP4_LWG executing	niRpAxiwa
NP6 another set of instructions in parallel	samAnAMwara meM nirxeSoM kA eka Ora seta
NP7 another set	eka Ora seta
PP10 of instructions in parallel	samAnAMwara meM nirxeSa
NP12 instructions in parallel	samAnAMwara meM nirxeSa
NP13 instructions	nirxeSa
NNS14 instructions	nirxeSa
PP15 in parallel	samAnAMwara meM
NP17 parallel	samAnAMwara

----
00133	Networks with greater depth can execute more instructions in sequence.	 aXika gaharAI vAle netavarka aXika anuxeSoM ko anukrama meM niRpAxiwa kara sakawe hEM 		
132	132
S1 Networks with greater depth can execute more instructions in sequence .	aXika gaharAI vAle netavarka krama meM aXika nirxeSoM ko aMjAma xe sakawe hEM
NP2 Networks with greater depth	aXika gaharAI vAle netavarka
NP3 Networks	netavarka
PP5 with greater depth	jyAxA gaharAI ke sAWa
NP7 greater depth	jyAxA gaharAI
VP10_LWG can execute	niRpAxiwa kara sakawe hEM
NP14 more instructions	aXika nirxeSa
NNS16 instructions	nirxeSa
PP17 in sequence	krama meM
NP19 sequence	kramAMka

----
00134	Sequential instructions oer great power because later instructions can refer back to the results of earlier instructions.	 anukramika anuxeSa mahAna Sakwi praxAna karawe hEM kyoMki bAxa ke anuxeSa pUrva nirxeSoM ke pariNAmoM ko vApasa saMxarBiwa kara sakawe hEM .		
133	133
S1 Sequential instructions oer great power because later instructions can refer back to the results of earlier instructions .	silasilevAra nirxeSa mahAna Sakwi praxAna karawe hEM kyoMki bAxa meM nirxeSa pahale ke nirxeSoM ke pariNAma para vApasa Beja sakawe hEM
NP2 Sequential instructions	silasilevAra nirxeSa
NNS4 instructions	nirxeSa
VP5_LWG oer	oYPara
NP7 great power	mahAna Sakwi
SBAR10 because later instructions can refer back to the results of earlier instructions	kyoMki bAxa meM nirxeSa pahale ke nirxeSoM ke pariNAma para vApasa Beja sakawe hEM
S12 later instructions can refer back to the results of earlier instructions	bAxa meM nirxeSa pahale ke nirxeSoM ke pariNAma para vApasa Beja sakawe hEM
NP13 later instructions	bAxa meM nirxeSa
NNS15 instructions	nirxeSa
VP16_LWG can refer back	vApasa kara sakawe hEM saMxarBa
PRT20 back	vApasa
PP22 to the results of earlier instructions	pahale ke nirxeSoM ke pariNAmoM ke lie
NP24 the results of earlier instructions	pahale ke nirxeSoM ke pariNAma
NP25 the results	pariNAma
NNS27 results	pariNAma
PP28 of earlier instructions	pahale ke nirxeSoM kA
NP30 earlier instructions	pahale ke nirxeSa
NNS32 instructions	nirxeSa

----
00135	Ac-	 Eka -		
134	134
NP1 Ac -	Eka-

----
00136	cording to this view of deep learning, not all the information in a layers activations necessarily encodes factors of variation that explain the input.	 gahana aXigama ke isa xqRtikoNa ke anurUpa , eka parawa ke sakriyaNa meM saBI jAnakArI AvaSyaka rUpa se inaputa kI vyAKyA karane vAle viBinnawA ke kArakoM ko kUtabaxXa nahIM karawI hE .		
135	135
S1 cording to this view of deep learning , not all the information in a layer s activations necessarily encodes factors of variation that explain the input .	gaharI sIKa ke isa xqSya para GamaMda, eka parawa kI saBI jAnakAriyoM meM AvaSyaka rUpa se viviXawA ke kArakoM ko saMSoXana xewA hE jo inaputa ko samaJAwA hE
S2 cording to this view of deep learning	gaharI sIKa ke isa xqSya para GamaMda
VP3_LWG cording	GerAbaMxI
PP5 to this view of deep learning	gaharI sIKa ke isa najZarie para
NP7 this view of deep learning	gaharI sIKa kA yaha najArA
NP8 this view	yaha najArA
PP11 of deep learning	gaharI sIKa kA
NP13 deep learning	gaharI sIKa
,16 ,	,
NP17 not all the information in a layer s activations	eka parawa kI sakriyawA meM saBI jAnakArI nahIM
NP18 not all the information	sArI jAnakArI nahIM
PP23 in a layer s activations	eka parawa kI sakriyawA meM
NP25 a layer s activations	eka parawa kI sakriyawA
NML27 layer s	parawa kI
NNS30 activations	sakriyawA
ADVP31 necessarily	jarUrI
VP33_LWG encodes	enkoda
NP35 factors of variation that explain the input	parivarwana ke kAraka jo inaputa ko samaJAwe hEM
NP36 factors	kAraKAne
NNS37 factors	kAraKAne
PP38 of variation that explain the input	vinimaya jo inaputa ko samaJAwA hE
NP40 variation that explain the input	vinimaya jo inaputa ko samaJAwA hE
NP41 variation	viviXawA
SBAR43 that explain the input	yaha inaputa kI vyAKyA
WHNP44 that	vaha
S46 explain the input	inaputa kI vyAKyA kareM
VP47_LWG explain	samaJAwe hEM
NP49 the input	inaputa

----
00137	The representation also stores state information that helps to execute a program that can make sense of the input.	 aByAvexana rAjya sUcanA ko BI saMgrahiwa karawA hE jo eka Ese progrAma ko niRpAxiwa karane meM maxaxa karawA hE jo inaputa kA boXa karA sakawA hE .		
136	136
S1 The representation also stores state information that helps to execute a program that can make sense of the input .	prawiniXiwva rAjya kI jAnakArI ko BI BaMdAra karawA hE jo eka kAryakrama ko niRpAxiwa karane meM maxaxa karawA hE jo inaputa kA arWa banA sakawA hE
NP2 The representation	prawiniXiwva
ADVP5 also	sAWa hI
VP7_LWG stores	stora
NP9 state information that helps to execute a program that can make sense of the input	rAjya jAnakArI jo eka kAryakrama ko niRpAxiwa karane meM maxaxa karawA hE jo inaputa kA arWa banA sakawA hE
NP10 state information	rAjya sUcanAeM
SBAR13 that helps to execute a program that can make sense of the input	yaha eka kAryakrama ko niRpAxiwa karane meM maxaxa karawA hE jo inaputa kI BAvanA banA sakawA hE
WHNP14 that	vaha
S16 helps to execute a program that can make sense of the input	eka kAryakrama ko niRpAxiwa karane meM maxaxa karawA hE jo inaputa kI BAvanA banA sakawA hE
VP17_LWG helps	maxaxa karawA hE
S19 to execute a program that can make sense of the input	eka kAryakrama ko niRpAxiwa karane ke lie jo inaputa kA arWa banA sakawA hE
VP20_LWG to execute	niRpAxiwa karane ke lie
NP24 a program that can make sense of the input	eka kAryakrama jo inaputa kI BAvanA banA sakawA hE
NP25 a program	eka kAryakrama
SBAR28 that can make sense of the input	jo inaputa kI BAvanA banA sakawA hE
WHNP29 that	vaha
S31 can make sense of the input	inaputa kI BAvanA banA sakawe hEM
VP32_LWG can make	banA sakawe hEM
NP36 sense of the input	inaputa kI BAvanA
NP37 sense	samaJaxArI
PP39 of the input	inaputa kA
NP41 the input	inaputa

----
00138	This state information could be analogous to a counter or pointer in a traditional computer program.	 yaha rAjya jAnakArI eka pAraMparika kaMpyUtara progrAma meM eka kAuMtara yA sUcaka ke anurUpa ho sakawI hE .		
137	137
S1 This state information could be analogous to a counter or pointer in a traditional computer program .	yaha rAjya jAnakArI eka pAraMparika kaMpyUtara kAryakrama meM eka kAuMtara yA poYiMtara ke anurUpa ho sakawI hE
NP2 This state information	yaha rAjya sUcanA
VP6_LWG could be	ho sakawA hE
ADJP10 analogous to a counter or pointer in a traditional computer program	eka pAraMparika kaMpyUtara progrAma meM kAuMtara yA poYiMtara ke samAna
PP12 to a counter or pointer in a traditional computer program	eka pAraMparika kaMpyUtara kAryakrama meM kAuMtara yA citTI karane ke lie
NP14 a counter or pointer in a traditional computer program	eka pAraMparika kaMpyUtara progrAma meM kAuMtara yA citTI
NP15 a counter or pointer	eka kAuMtara yA saMkewa
CC18 or	yA
NN17 counter	kAuMtara
NN19 pointer	XyAna xene vAlA
PP20 in a traditional computer program	eka pAraMparika kaMpyUtara progrAma meM
NP22 a traditional computer program	eka pAraMparika kaMpyUtara progrAma

----
00139	It has nothing to do with the content of the input specically, but it helps the model to organize its processing.	 isakA inaputa kI sAmagrI ke sAWa viSeRa rUpa se koI saMbaMXa nahIM hE , lekina yaha moYdala ko apane prasaMskaraNa ko vyavasWiwa karane meM maxaxa karawA hE .		
138	138
S1 It has nothing to do with the content of the input specically , but it helps the model to organize its processing .	isakA viSeRa rUpa se inaputa kI sAmagrI se koI lenA-xenA nahIM hE, lekina yaha moYdala ko isake prosesiMga kA Ayojana karane meM maxaxa karawA hE
S2 It has nothing to do with the content of the input specically	isakA inaputa kI sAmagrI se koI lenA-xenA nahIM hE KAsakara
NP3 It	yaha
VP5_LWG has	usake pAsa hE
NP7 nothing to do with the content of the input specically	inaputa kI sAmagrI se koI lenA-xenA nahIM
S9 to do with the content of the input specically	inaputa kI sAmagrI se karanA viSeRa rUpa se
VP10_LWG to do	karane ke lie
PP14 with the content of the input specically	inaputa kI sAmagrI ke sAWa viSeRa rUpa se
NP16 the content of the input specically	inaputa kI sAmagrI KAsa
NP17 the content	sAmagrI
PP20 of the input specically	inaputa kA viSeRa rUpa se
NP22 the input specically	viSeRa rUpa se inaputa
,26 ,	,
CC27 but	lekina
S28 it helps the model to organize its processing	yaha moYdala ko isake prasaMskaraNa kA Ayojana karane meM maxaxa karawA hE
NP29 it	yaha
VP31_LWG helps	maxaxa karawA hE
NP33 the model	moYdala
S36 to organize its processing	isake prasaMskaraNa kA Ayojana karane ke lie
VP37_LWG to organize	Ayojana ke lie
NP41 its processing	isakA prasaMskaraNa

----
00140	There are two main ways of measuring the depth of a model.	 eka moYdala kI gaharAI mApane ke xo muKya warIke hEM .		
139	139
S1 There are two main ways of measuring the depth of a model .	eka moYdala kI gaharAI ko mApane ke xo muKya warIke hEM
NP2 There	vahAM
VP4_LWG are	hEM
NP6 two main ways of measuring the depth of a model	eka moYdala kI gaharAI ko mApane ke xo muKya warIke
NP7 two main ways	xo muKya warIke
NNS10 ways	warIke
PP11 of measuring the depth of a model	eka moYdala kI gaharAI ko mApane kI
S13 measuring the depth of a model	eka moYdala kI gaharAI ko mApawe hue
VP14_LWG measuring	mApa
NP16 the depth of a model	eka moYdala kI gaharAI
NP17 the depth	gaharAI
PP20 of a model	eka moYdala kI
NP22 a model	eka moYdala

----
0014	If we draw a graph showing how these concepts 1 CHAPTER 1.	 yaxi hama eka grAPa banAwe hEM jisase pawA calawA hE ki kEse ina avaXAraNAoM 1 CHAPTER 1 .		
13	13
S1 If we draw a graph showing how these concepts 1 CHAPTER 1 .	yaxi hama eka grAPa KIMcawe hEM jisameM xiKAyA gayA hE ki ye avaXArAez kEse 1 aXyAya 1
SBAR2 If we draw a graph showing how these concepts 1	agara hama eka grAPa KIMcawe hEM jisameM xiKAyA gayA hE ki ye avaXArAez kEsI hEM
S4 we draw a graph showing how these concepts 1	hama eka grAPa KIMcawe hEM jisameM xiKAyA gayA hE ki ye avaXArAez kEse 1
NP5 we	hama
VP7_LWG draw	AkarRiwa
NP9 a graph	eka grAPa
S12 showing how these concepts 1	yaha xiKA rahA hE ki ye avaXArAez kEse 1
VP13_LWG showing	xiKA rahA hE
NP15 how	kEse
NP17 these concepts	ye avaXArAeM
NNS19 concepts	avaXAraNAez
NP20 1	1
VP22_LWG CHAPTER	cEptara
NP24 1	1

----
00141	The rst view is based on the number of sequential instructions that must be executed to evaluate the architecture.	 pahalA xqSya anukramika anuxeSoM kI saMKyA para AXAriwa hE jise vAswukalA kA mUlyAMkana karane ke lie niRpAxiwa kiyA jAnA cAhie .		
140	140
S1 The rst view is based on the number of sequential instructions that must be executed to evaluate the architecture .	pahalA xqSya kramika nirxeSoM kI saMKyA para AXAriwa hE jise vAswukalA kA mUlyAMkana karane ke lie niRpAxiwa kiyA jAnA cAhie
NP2 The rst view	pahalA najArA
VP6_LWG is based	AXAriwa
PP10 on the number of sequential instructions that must be executed to evaluate the architecture	vAswukalA ke mUlyAMkana ke lie jo wawkAla nirxeSoM kI saMKyA ko aMjAma xiyA jAnA cAhie, usa para
NP12 the number of sequential instructions that must be executed to evaluate the architecture	vAswukalA kA mUlyAMkana karane ke lie jina wawkAla nirxeSoM kI saMKyA ko aMjAma xiyA jAnA cAhie
NP13 the number	naMbara
PP16 of sequential instructions that must be executed to evaluate the architecture	wawkAla nirxeSoM kA jise vAswukalA kA mUlyAMkana karane ke lie niRpAxiwa kiyA jAnA cAhie
NP18 sequential instructions that must be executed to evaluate the architecture	BaviRyavANI kA mUlyAMkana karane ke lie wawkAla nirxeSoM ko niRpAxiwa kiyA jAnA cAhie
NP19 sequential instructions	wawkAla nirxeSa
NNS21 instructions	nirxeSa
SBAR22 that must be executed to evaluate the architecture	vAswukalA kA mUlyAMkana karane ke lie niRpAxiwa kiyA jAnA cAhie
WHNP23 that	vaha
S25 must be executed to evaluate the architecture	vAswukalA kA mUlyAMkana karane ke lie niRpAxiwa kiyA jAnA cAhie
VP26_LWG must be executed	niRpAxiwa honA cAhie
S32 to evaluate the architecture	vAswukalA kA mUlyAMkana karane ke lie
VP33_LWG to evaluate	mUlyAMkana ke lie
NP37 the architecture	vAswukalA

----
00142	We can think of this as the length of the longest path through a ow chart that describes how to compute each of the models outputs given its inputs.	 hama isake bAre meM eka pravAha cArta ke mAXyama se sabase laMbe paWa kI laMbAI ke rUpa meM soca sakawe hEM jo yaha varNana karawA hE ki kEse prawyeka moYdala ke Autaputa apane inaputa xiyA kI gaNanA karane ke lie .		
141	141
S1 We can think of this as the length of the longest path through a ow chart that describes how to compute each of the model s outputs given its inputs .	hama isake bAre meM pravAha cArta ke mAXyama se sabase laMbe mArga kI laMbAI ke rUpa meM soca sakawe hEM jisameM varNana kiyA gayA hE ki isake inaputa ke
NP2 We	hama
VP4_LWG can think	soca sakawe hEM
PP8 of this as the length of the longest path	isakI laMbAI sabase laMbI rAha ke rUpa meM
NP10 this as the length of the longest path	yaha sabase laMbe mArga kI laMbAI ke rUpa meM
NP11 this	yaha
PP13 as the length of the longest path	sabase laMbe mArga kI laMbAI ke rUpa meM
NP15 the length of the longest path	sabase laMbe mArga kI laMbAI
NP16 the length	laMbAI
PP19 of the longest path	sabase laMbe rAswe kA
NP21 the longest path	sabase laMbA rAswA
PP25 through a ow chart that describes how to compute each of the model s outputs given its inputs	eka pravAha cArta ke mAXyama se yaha varNana karawA hE ki isake inaputa ke xie gae moYdala ke prawyeka Autaputa ko kEse kampyUta karanA hE
NP27 a ow chart that describes how to compute each of the model s outputs given its inputs	eka pravAha cArta jisameM varNana kiyA gayA hE ki isake inaputa ke xie gae moYdala ke prawyeka Autaputa ko kEse kampyUta kiyA jAe
NP28 a ow chart	eka pravAha cArta
SBAR32 that describes how to compute each of the model s outputs given its inputs	yaha bawAwA hE ki isake inaputa ke xie gae moYdala ke prawyeka Autaputa ko kEse kampyUta karanA hE
WHNP33 that	vaha
S35 describes how to compute each of the model s outputs given its inputs	varNana karawA hE ki isake inaputa xie gae moYdala ke prawyeka Autaputa kA kEse kareM gaNanA
VP36_LWG describes	varNana
SBAR38 how to compute each of the model s outputs given its inputs	isake inaputa xie gae moYdala ke prawyeka Autaputa kA gaNanA kEse kareM
WHADVP39 how	kEse
S41 to compute each of the model s outputs given its inputs	apane inaputa xie gae moYdala ke prawyeka Autaputa kA kaMpyUtara karane ke lie
VP42_LWG to compute	gaNanA karane ke lie
NP46 each of the model s outputs given its inputs	moYdala ke prawyeka Autaputa ne apane inaputa xie
NP47 each	prawyeka
PP49 of the model s outputs given its inputs	moYdala ke Autaputa ne apane inaputa xie
NP51 the model s outputs given its inputs	moYdala ke Autaputa ne apane inaputa xie
NP52 the model s outputs	moYdala kA Autaputa
NNS56 outputs	Autaputa
VP57_LWG given	xiyA gayA
NP59 its inputs	isake inaputa
NNS61 inputs	inaputa

----
00143	Just as two equivalent computer programs will have dierent lengths depending on which language the program is written in, the same function may be drawn as a owchart with dierent depths depending on which functions we allow to be used as individual steps in the owchart.	 jisa prakAra xo samawulya kaMpyUtara progrAmoM kI laMbAI Binna howI hE , yaha nirBara karawA hE ki progrAma kisa BARA meM liKA gayA hE , usI prakAra eka hI Palana ko viBinna gaharAiyoM ke sAWa Plo cArta ke rUpa meM KIMcA jA sakawA hE , jisa para nirBara karawA hE ki hama pravAha saMciwra meM alaga - alaga caraNoM ke rUpa meM prayoga kI anumawi xewe hEM 		
142	142
S1 Just as two equivalent computer programs will have dierent lengths depending on which language the program is written in , the same function may be drawn as a owchart with dierent depths depending on which functions we allow to be used as individual steps in the owchart .	jisa waraha xo samakakRa kaMpyUtara kAryakramoM meM alaga-alaga laMbAI hogI, isake AXAra para ki kAryakrama kisa BARA meM liKA gayA hE, usI samAroha ko alaga-alaga ga
SBAR2 Just as two equivalent computer programs will have dierent lengths depending on which language the program is written in	jisa waraha xo samakakRa kaMpyUtara kAryakramoM meM alaga-alaga laMbAI hogI, jisase kAryakrama meM liKA gayA hE,
ADVP3 Just	basa
S6 two equivalent computer programs will have dierent lengths depending on which language the program is written in	xo samakakRa kaMpyUtara kAryakramoM meM alaga-alaga laMbAI hogI, jisa para kAryakrama meM liKA gayA hE, isa para nirBara hogA
NP7 two equivalent computer programs	xo samakakRa kaMpyUtara kAryakrama
NNS11 programs	kAryakrama
VP12_LWG will have	hogA
NP16 dierent lengths	alaga-alaga laMbAI
NNS18 lengths	laMbAI
PP19 depending on which language the program is written in	kisa BARA para liKA hE kAryakrama
PP21 on which language the program is written in	kisa BARA para kAryakrama liKA hE
SBAR23 which language the program is written in	kAryakrama meM kOna sI BARA liKI jAwI hE
WHNP24 which language	kOna sI BARA
S27 the program is written in	kAryakrama meM liKA hE
NP28 the program	kAryakrama
VP31_LWG is written	liKA hE
PP35 in	meM
,37 ,	,
NP38 the same function	vahI samAroha
VP42_LWG may be drawn	KIMce jA sakawe hEM
PP48 as a owchart with dierent depths	alaga-alaga gaharAI ke sAWa eka PlocArta ke rUpa meM
NP50 a owchart with dierent depths	alaga-alaga gaharAI ke sAWa eka PlocArta
NP51 a owchart	eka PlocArta
PP54 with dierent depths	alaga-alaga gaharAI ke sAWa
NP56 dierent depths	alaga-alaga gaharAI
NNS58 depths	gaharAI
PP59 depending on which functions we allow to be used as individual steps in the owchart	PlocArta meM hama kina kAryoM kA iswemAla vyakwigawa kaxamoM ke rUpa meM karane kI anumawi xewe hEM
PP61 on which functions we allow to be used as individual steps in the owchart	jisa para hama PlocArta meM vyakwigawa kaxamoM ke rUpa meM iswemAla karane kI anumawi xewe hEM
SBAR63 which functions we allow to be used as individual steps in the owchart	jina kAryoM kA upayoga hama pravAhaciwra meM vyakwigawa kaxamoM ke rUpa meM kiyA jAnA anumawi xewe hEM
WHNP64 which functions	kOna sA kArya karawA hE
NNS66 functions	samAroha
S67 we allow to be used as individual steps in the owchart	hama PlocArta meM vyakwigawa kaxamoM ke rUpa meM iswemAla karane kI anumawi xewe hEM
NP68 we	hama
VP70_LWG allow	anumawi
S72 to be used as individual steps in the owchart	PlocArta meM vyakwigawa kaxamoM ke rUpa meM iswemAla kiyA jAnA
VP73_LWG to be used	iswemAla honA hE
PP79 as individual steps in the owchart	PlocArta meM vyakwigawa kaxama ke rUpa meM
NP81 individual steps in the owchart	PlocArta meM vyakwigawa kaxama
NP82 individual steps	vyakwigawa kaxama
NNS84 steps	kaxama
PP85 in the owchart	PlocArta meM
NP87 the owchart	PlocArta

----
00144	Figure 1.3 illustrates how this choice of language can give two dierent measurements for the same architecture.	 ciwra 1 . 3 spaRta karawA hE ki kEse BARA kA yaha cayana eka hI vAswukalA ke lie xo alaga alaga mApa xe sakawA hE 		
143	143
S1 Figure 1.3 illustrates how this choice of language can give two dierent measurements for the same architecture .	AMkadZA 1.3 xarSAwA hE ki BARA kA yaha vikalpa eka hI vAswukalA ke lie xo alaga-alaga mApa kEse xe sakawA hE
NP2 Figure 1.3	AMkadZA 1.3
VP5_LWG illustrates	xarSAwA hE
SBAR7 how this choice of language can give two dierent measurements for the same architecture	BARA kA yaha vikalpa eka hI vAswukalA ke lie xo alaga-alaga mApa kEse xe sakawA hE
WHADVP8 how	kEse
S10 this choice of language can give two dierent measurements for the same architecture	BARA kA yaha vikalpa eka hI vAswukalA ke lie xo alaga-alaga mApa xe sakawA hE
NP11 this choice of language	BARA kA yaha vikalpa
NP12 this choice	yaha vikalpa
PP15 of language	BARA kA
NP17 language	BARA
VP19_LWG can give	xe sakawe hEM
NP23 two dierent measurements	xo alaga-alaga mApa
NNS26 measurements	mApa
PP27 for the same architecture	usI vAswukalA ke lie
NP29 the same architecture	vahI vAswukalA

----
00145	x 1 x 1  w 1 w 1	 x 1 x 1  w 1 w 1 w		
144	144
FRAG1 x 1 x 1  w 1 w 1	eksa 1 eksa 1 dablyU 1 dablyU 1 dablyU 1
SYM2 x	eksa
NP3 1 x 1	1 eksa 1
SYM5 x	eksa
NP7  w	esa dablyU
NP10 1 w 1	1 dablyU 1
NP11 1	1
PP13 w 1	dablyU 1
NP15 1	1

----
00146		 		
145	145
FRAG1 	
NP2 	
SYM3 	

----
00147	x 2	 x 2		
146	146
NP1 x 2	eksa 2

----
00148	x 2	 x 2		
147	147
NP1 x 2	eksa 2

----
00149	w	 wyOrI		
148	148
NP1 w	dablyU

----
00150	2	 2		
149	149
X1 2	2

----
0015	INTRODUCTION are built on top of each other, the graph is deep, with many layers.	 INTRODUCTION eka xUsare ke Upara nirmiwa hEM , grAPa gaharA hE , kaI parawoM ke sAWa 		
14	14
S1 INTRODUCTION are built on top of each other , the graph is deep , with many layers .	eka xUsare ke Upara se paricaya banAyA jAwA hE, grAPa gaharA hE, kaI parawoM ke sAWa
S2 INTRODUCTION are built on top of each other	eka xUsare ke SIrRa para banAyA gayA hE paricaya
NP3 INTRODUCTION	paricaya
VP5_LWG are built	banAe gae hEM
PP9 on top of each other	eka xUsare ke Upara
NP11 top of each other	eka xUsare ke SIrRa
NP12 top	SIrRa
PP14 of each other	eka xUsare kI
NP16 each other	eka xUsare ko
,19 ,	,
NP20 the graph	grAPa
VP23_LWG is	hE
ADJP25 deep	gaharA
,27 ,	,
PP28 with many layers	kaI parawoM ke sAWa
NP30 many layers	kaI paraweM
NNS32 layers	paraweM

----
00151	w 2  +	 w 2  +		
150	150
NP1 w 2  +	dablyU 2 +
NP2 w	dablyU
NP4 2  +	2 +
QP5 2 	2
SYM7 	
SYM8 +	+

----
00152	Element Set + 	 wawva seta + 		
151	151
FRAG1 Element Set + 	wawva seta +
NP2 Element Set	wawva seta
NP5 +	+
SYM6 +	+
NP7 	
SYM8 	

----
00153	 xx ww	  xx  www		
152	152
NP1  xx ww	esa 0 0 5
NML2  xx	esa 0

----
00154	Element Set Logistic Regression Logistic Regression Figure 1.3:	 wawva samuccaya loYjistika prawigamana ciwra 1 . 3 :		
153	153
FRAG1 Element Set Logistic Regression Logistic Regression Figure 1.3 :	wawva seta loYjistika rispeMSana loYjikala rispeMSana AMkadZA 1.3
NP2 Element Set Logistic	wawva seta loYjistika
NP6 Regression Logistic Regression	prawiPala loYjistika prawiPala
NP10 Figure 1.3	AMkadZA 1.3

----
00155	Illustration of computational graphs mapping an input to an output where each node performs an operation.	 kampyUteSanala reKAMkana kA praxarSana eka Autaputa ke lie inaputa mEpiMga karawA hE jahAM prawyeka noda eka saMkriyA niRpAxiwa karawA hE 		
154	154
NP1 Illustration of computational graphs mapping an input to an output where each node performs an operation .	kampyUteSanala grAPa kI CavaraNa eka Autaputa ke lie eka inaputa ko mEpa karawI hE jahAM prawyeka neda eka oYpareSana karawA hE
NP2 Illustration of computational graphs	kampyUteSanala grAPa kI CaviyAM
NP3 Illustration	ciwrakalA
PP5 of computational graphs	kampyUteSanala grAPa kI
NP7 computational graphs	kampyUteSanala grAPa
NNS9 graphs	grAPa
VP10_LWG mapping	mEpiMga
NP12 an input	eka inaputa
PP15 to an output where each node performs an operation	eka Autaputa ke lie jahAM prawyeka noda oYpareSana karawA hE
NP17 an output where each node performs an operation	eka Autaputa jahAM prawyeka noda oYpareSana karawA hE
NP18 an output	eka Autaputa
SBAR21 where each node performs an operation	jahAM prawyeka noda oYpareSana karawA hE
WHADVP22 where	kahAM
S24 each node performs an operation	prawyeka noda oYpareSana karawA hE
NP25 each node	prawyeka noda
VP28_LWG performs	praxarSana karawA hE
NP30 an operation	eka oYpareSana

----
00156	Depth is the length of the longest path from input to output but depends on the denition of what constitutes a possible computational step.	 gaharAI inaputa se Autaputa waka sabase laMbe paWa kI laMbAI hE , lekina kyA eka saMBAviwa kampyUteSanala kaxama kA gaTana karawA hE kI pariBARA para nirBara karawA hE .		
155	155
S1 Depth is the length of the longest path from input to output but depends on the denition of what constitutes a possible computational step .	depoWa inaputa se lekara Autaputa waka sabase laMbe rAswe kI laMbAI hE lekina isakI pariBARA para nirBara karawA hE ki eka saMBAviwa kaMpyUtieSanala kaxama kyA
NP2 Depth	xIpwi
VP4_LWG is depends	nirBara karawA hE
NP7 the length of the longest path from input to output	inaputa se lekara Autaputa waka sabase laMbe mArga kI laMbAI
NP8 the length	laMbAI
PP11 of the longest path from input to output	inaputa se lekara Autaputa waka kA sabase laMbA rAswA
NP13 the longest path from input to output	inaputa se lekara Autaputa waka sabase laMbA rAswA
NP14 the longest path	sabase laMbA rAswA
PP18 from input to output	inaputa se lekara Autaputa waka
NP20 input to output	Autaputa ke lie inaputa
NP21 input	inaputa
PP23 to output	Autaputa karane ke lie
NP25 output	Autaputa
CC27 but	lekina
VP5 is the length of the longest path from input to output	inaputa se lekara Autaputa waka sabase laMbe mArga kI laMbAI hE
VP28 depends on the denition of what constitutes a possible computational step	saMBAviwa gaNanASIla kaxama kyA banAwA hE isakI pariBARA para nirBara karawA hE
PP30 on the denition of what constitutes a possible computational step	saMBAviwa kaMpyUtiSanala kaxama kyA banAwA hE isakI pariBARA para
NP32 the denition of what constitutes a possible computational step	kyA saMBAviwa kaMpyUtiSanala kaxama banAwA hE usakI pariBARA
NP33 the denition	pariBARA
PP36 of what constitutes a possible computational step	kyA saMBAviwa kaMpyUtiSanala kaxama kA gaTana karawA hE
SBAR38 what constitutes a possible computational step	kyA saMBAviwa kaMpyUtiSanala kaxama banAwA hE
WHNP39 what	kyA
S41 constitutes a possible computational step	saMBAviwa kaMpyUtiSanala kaxama kA gaTana
VP42_LWG constitutes	gaTiwa
NP44 a possible computational step	eka saMBAviwa gaNanASIla kaxama

----
00157	The computation depicted in these graphs is the output of a logistic regression model,  ( w T x ), where  is the logistic sigmoid function.	 ina reKAMkana meM xarSAyA gayA aBikalana eka loYjistika prawigamana moYdala kA Autaputa hE ,  ( w T x ) , jahAM  loYjistika avagrahI Palana hE .		
156	156
S1 The computation depicted in these graphs is the output of a logistic regression model ,  ( w T x ) , where  is the logistic sigmoid function .	ina grAPa meM xarSAyI gaI prawiRTA eka loYjista riPyUjana moYdala, esa (dablyU tI eksa) kA Autaputa hE, jahAM esa loYjistika sigoYida PaMkSana hE
NP2 The computation depicted in these graphs	ina grAPoM meM xiKAI gaI prawiRTA
NP3 The computation	prawiRTA
VP6_LWG depicted	xarSAyA gayA
PP8 in these graphs	ina grAPoM meM
NP10 these graphs	ye grAPa
NNS12 graphs	grAPa
VP13_LWG is	hE
NP15 the output of a logistic regression model ,  ( w T x ) , where  is the logistic sigmoid function	eka loYjista riPyUjana moYdala kA Autaputa, esa (dablyU tI eksa), jahAM loYjistika sigamAuda PaMkSana hE
NP16 the output	Autaputa
PP19 of a logistic regression model ,  ( w T x ) , where  is the logistic sigmoid function	eka loYjista riPyUjana moYdala kI, esa (dablyU tI eksa), jahAM para hE loYjistika sigamAuda PaMkSana
NP21 a logistic regression model ,  ( w T x ) , where  is the logistic sigmoid function	eka loYjista riPyUjana moYdala, esa (dablyU tI eksa), jahAM loYjistika sigamAuda PaMkSana hE
NP22 a logistic regression model	eka loYjista riPyUjana moYdala
,27 ,	,
NP28  ( w T x ) , where  is the logistic sigmoid function	esa (dablyU tI eksa), jahAM loYjistika sigamAuda PaMkSana hE
NP29  ( w T x )	esa (dablyU tI eksa)
PRN31 ( w T x )	(dablyU tI eksa)
NP33 w T x	dablyU tI eksa
SYM36 x	eksa
,38 ,	,
SBAR39 where  is the logistic sigmoid function	jahAM esa hE loYjistika sigamAuda PaMkSana
WHADVP40 where	kahAM
S42  is the logistic sigmoid function	esa loYjistika sigamAuda PaMkSana
NP43 	esa
VP45_LWG is	hE
NP47 the logistic sigmoid function	loYjistika sigamAuda PaMkSana

----
00158	If we use addition, multiplication and logistic sigmoids as the elements of our computer language, then this model has depth three.	 yaxi hama jodZa , guNana Ora rasaxa sigmida kA upayoga hamAre kaMpyUtara BARA ke wawvoM ke rUpa meM karawe hEM , wo isa moYdala gaharAI wIna		
157	157
S1 If we use addition , multiplication and logistic sigmoids as the elements of our computer language , then this model has depth three .	yaxi hama apanI kaMpyUtara BARA ke wawvoM ke rUpa meM awirikwa, viBAjana Ora loYjistika sigoYida kA upayoga karawe hEM, wo isa moYdala meM gaharAI
SBAR2 If we use addition , multiplication and logistic sigmoids as the elements of our computer language	yaxi hama apanI kaMpyUtara BARA ke wawvoM ke rUpa meM awirikwa, viBAjana Ora loYjistika sigoYida kA upayoga karawe hEM
S4 we use addition , multiplication and logistic sigmoids as the elements of our computer language	hama apanI kaMpyUtara BARA ke wawvoM ke rUpa meM awirikwa, viBAjana Ora loYjistika sigoYida kA upayoga karawe hEM
NP5 we	hama
VP7_LWG use	upayoga
NP9 addition , multiplication and logistic sigmoids	isake alAvA, viBAjana Ora loYjistika sigamAoida
NP10 addition	isake alAvA
,12 ,	,
NP13 multiplication	viBAjana
CC15 and	Ora
NP16 logistic sigmoids	loYjistika sigamAyoida
NNS18 sigmoids	sigamAoidsa
PP19 as the elements of our computer language	hamAre kaMpyUtara BARA ke wawvoM ke rUpa meM
NP21 the elements of our computer language	hamArI kaMpyUtara BARA ke wawva
NP22 the elements	wawva
NNS24 elements	wawva
PP25 of our computer language	hamArI kaMpyUtara BARA kA
NP27 our computer language	hamArI kaMpyUtara BARA
,31 ,	,
ADVP32 then	Pira
NP34 this model	ye moYdala
VP37_LWG has	usake pAsa hE
NP39 depth three	gaharAI wIna

----
00159	If we view logistic regression as an element itself, then this model has depth one.	 yaxi hama rasaxa prawigamana ko svayaM eka wawva ke rUpa meM xeKawe hEM , wo isa moYdala kI gaharAI eka hE .		
158	158
S1 If we view logistic regression as an element itself , then this model has depth one .	yaxi hama eka wawva ke rUpa meM loYjistika prawiSoXa ko xeKawe hEM, wo isa moYdala meM gaharAI howI hE
SBAR2 If we view logistic regression as an element itself	yaxi hama eka wawva ke rUpa meM loYjistika prawiPala ko xeKawe hEM
S4 we view logistic regression as an element itself	hama eka wawva ke rUpa meM loYjistika prawiPala ko xeKawe hEM
NP5 we	hama
VP7_LWG view	xqSya
S9 logistic regression as an element itself	eka wawva ke rUpa meM loYjistika prawiPala svayaM
NP10 logistic regression as an element	eka wawva ke rUpa meM loYjistika prawiPala
NP11 logistic regression	loYjistika prawiPala
PP14 as an element	eka wawva ke rUpa meM
NP16 an element	eka wawva
NP19 itself	hI Kuxa
,21 ,	,
ADVP22 then	Pira
NP24 this model	ye moYdala
VP27_LWG has	usake pAsa hE
NP29 depth one	gaharAI eka

----
00160	7 CHAPTER 1.	 7 CHAPTER 1 .		
159	159
FRAG1 7 CHAPTER 1 .	7 cEptara 1.
NP2 7	7
NP4 CHAPTER 1	cEptara 1

----
0016	For this reason, we call this approach to AI deep learning.	 isa kAraNa se , hama isa xqRtikoNa eAI gaharI sIKane ke lie kahawe hEM .		
15	15
S1 For this reason , we call this approach to AI deep learning .	isI vajaha se hama isa xqRtikoNa ko eAI gaharI sIKa ko bulAwe hEM
PP2 For this reason	isI vajaha se
NP4 this reason	isI vajaha se
,7 ,	,
NP8 we	hama
VP10_LWG call	koYla
NP12 this approach	yaha xqRtikoNa
PP15 to AI deep learning	eAI ko gaharI sIKa
NP17 AI deep learning	eAI gaharI sIKa
NML19 deep learning	gaharI sIKa

----
00161	INTRODUCTION Another approach, used by deep probabilistic models, regards the depth of a model as being not the depth of the computational graph but the depth of the graph describing how concepts are related to each other.	 INTRODUCTER eka anya xqRtikoNa , jisakA prayoga gahare vyAvahArika moYdaloM xvArA kiyA jAwA hE , moYdala kI gaharAI ko kampyUteSanala grAPa kI gaharAI nahIM , balki grAPa kI gaharAI ko yaha varNiwa karawA hE ki avaXAraNAeM eka xUsare se kEse saMbaMXiwa hEM .		
160	160
S1 INTRODUCTION Another approach , used by deep probabilistic models , regards the depth of a model as being not the depth of the computational graph but the depth of the graph describing how concepts are related to each other .	uxGAtana eka Ora xqRtikoNa, jisakA upayoga gahare saMBAvanAvAxI moYdaloM xvArA kiyA gayA hE, eka moYdala kI gaharAI ko saMboXiwa karawA hE kyoMki kampa
S2 INTRODUCTION Another approach , used by deep probabilistic models , regards the depth of a model as being not the depth of the computational graph	paricaya eka Ora xqRtikoNa, jisakA upayoga gahare saMBAvanAvAxI moYdaloM xvArA kiyA jAwA hE, eka moYdala kI gaharAI ko mAnA jAwA hE kyoMki gaNiwIya
NP3 INTRODUCTION Another approach , used by deep probabilistic models ,	eka Ora xqRtikoNa, jisakA upayoga gahare saMBAvanAvAxI moYdaloM xvArA kiyA jAwA hE,
NP4 INTRODUCTION Another approach	paricaya eka Ora xqRtikoNa
NP5 INTRODUCTION	paricaya
NP7 Another approach	eka Ora xqRtikoNa
,10 ,	,
VP11_LWG used	iswemAla
PP13 by deep probabilistic models	gahare saMBAvanAwmaka moYdaloM xvArA
NP15 deep probabilistic models	gaharA saMBAvanAvAxI moYdala
ADJP16 deep probabilistic	gaharA saMBAvanAvAxI
NNS19 models	moYdala
,20 ,	,
VP21_LWG regards	saMbaMXiwa
NP23 the depth of a model	eka moYdala kI gaharAI
NP24 the depth	gaharAI
PP27 of a model	eka moYdala kI
NP29 a model	eka moYdala
PP32 as being not the depth of the computational graph	jEsA ki kaMpyUteSanala grAPa kI gaharAI nahIM hE
S34 being not the depth of the computational graph	kampyUteSanala grAPa kI gaharAI nahIM honA
VP35_LWG being	hone ke nAwe
NP37 not the depth of the computational graph	kampyUteSanala grAPa kI gaharAI nahIM
NP38 not the depth	gaharAI nahIM
PP42 of the computational graph	kampyUteSanala grAPa kA
NP44 the computational graph	kampyUteSanala grAPa
CC48 but	lekina
FRAG49 the depth of the graph describing how concepts are related to each other	grAPZa kI gaharAI yaha bawAwe hue ki kEse eka xUsare se judZI avaXArAeM
NP50 the depth of the graph	grAPa kI gaharAI
NP51 the depth	gaharAI
PP54 of the graph	grAPa kA
NP56 the graph	grAPa
VP59_LWG describing	bawAwe hue
SBAR61 how concepts are related to each other	kEse eka xUsare se judZI avaXArAeM
WHADVP62 how	kEse
S64 concepts are related to each other	avaXArAeM eka xUsare se saMbaMXiwa
NP65 concepts	avaXAraNAez
NNS66 concepts	avaXAraNAez
VP67_LWG are related	saMbaMXiwa hEM
PP71 to each other	eka xUsare ko
NP73 each other	eka xUsare ko

----
00162	In this case, the depth of the owchart of the computations needed to compute the representation of each concept may be much deeper than the graph of the concepts themselves.	 isa mAmale meM , prawyeka avaXAraNA ke prawiniXiwva kI gaNanA karane ke lie AvaSyaka saMgaNanA ke pravAha saMciwra kI gaharAI svayaM avaXAraNAoM ke grAPa se kahIM aXika gaharI ho sakawI hE .		
161	161
S1 In this case , the depth of the owchart of the computations needed to compute the representation of each concept may be much deeper than the graph of the concepts themselves .	isa mAmale meM prawyeka avaXAraNA ke prawiniXiwva ko gaNanA karane ke lie AvaSyaka kampyUteSana kI PlocArta kI gaharAI svayaM avaXAriwAoM ke grAPa se bahuwa gaharI ho saka
PP2 In this case	isa mAmale meM
NP4 this case	yaha mAmalA
,7 ,	,
NP8 the depth of the owchart of the computations needed to compute the representation of each concept	prawyeka avaXAraNA ke prawiniXiwva ko gaNanA karane ke lie kampyUteSana ke pravAhapawra kI gaharAI jarUrawa
NP9 the depth	gaharAI
PP12 of the owchart of the computations needed to compute the representation of each concept	prawyeka avaXAraNA ke prawiniXiwva ko gaNanA karane ke lie AvaSyaka kampyUteSana kI pravAha citTI kI AvaSyakawA
NP14 the owchart of the computations needed to compute the representation of each concept	prawyeka avaXAraNA ke prawiniXiwva kA gaNanA karane ke lie kampyUteSana kI pravAha citTI kI AvaSyakawA
NP15 the owchart	PlocArta
PP18 of the computations needed to compute the representation of each concept	prawyeka avaXAraNA ke prawiniXiwva kA gaNanA karane ke lie kampyUteSana kI AvaSyakawA
NP20 the computations needed to compute the representation of each concept	prawyeka avaXAraNA ke prawiniXiwva kA gaNanA karane ke lie kampyUteSana kI AvaSyakawA
NP21 the computations	kampyUteSana
NNS23 computations	kampyUteSana
VP24_LWG needed	jarUrawa
S26 to compute the representation of each concept	prawyeka avaXAraNA ke prawiniXiwva kA gaNanA karane ke lie
VP27_LWG to compute	gaNanA karane ke lie
NP31 the representation of each concept	prawyeka avaXAraNA kA prawiniXiwva
NP32 the representation	prawiniXiwva
PP35 of each concept	prawyeka avaXAraNA kI
NP37 each concept	prawyeka avaXAraNA
VP40_LWG may be	ho sakawA hE
ADJP44 much deeper than the graph of the concepts	avaXArAoM ke grAPa se bahuwa gaharA
ADJP45 much deeper	bahuwa gaharA
PP48 than the graph of the concepts	avaXArAoM ke grAPa se jyAxA
NP50 the graph of the concepts	avaXArAoM kA grAPa
NP51 the graph	grAPa
PP54 of the concepts	avaXArAoM kI
NP56 the concepts	avaXAraNAez
NNS58 concepts	avaXAraNAez
NP59 themselves	Kuxa

----
00163	This is because the systems understanding of the simpler concepts can be rened given information about the more complex concepts.	 isakA kAraNa yaha hE ki praNAlI kI sarala avaXAraNAoM kI samaJa ko aXika jatila avaXAraNAoM ke bAre meM jAnakArI xekara pariRkqwa kiyA jA sakawA hE 		
162	162
S1 This is because the system s understanding of the simpler concepts can be rened given information about the more complex concepts .	EsA isalie hE kyoMki sistama kI sAXAraNa avaXArAoM kI samaJa ko aXika jatila avaXAriwAoM ke bAre meM jAnakArI xI jA sakawI hE
NP2 This	yaha
VP4_LWG is	hE
SBAR6 because the system s understanding of the simpler concepts can be rened given information about the more complex concepts	kyoMki sarala avaXArAoM kI vyavasWA kI samaJa ko aXika jatila avaXArAoM ke bAre meM xie jA sakawe hEM pariRkqwa kiyA jA sakawA hE
S8 the system s understanding of the simpler concepts can be rened given information about the more complex concepts	sarala avaXArAoM kI praNAlI kI samaJa ko aXika jatila avaXArAoM ke bAre meM xie jA sakawe hEM pariRkqwa kiyA jA sakawA hE
NP9 the system s understanding of the simpler concepts	sistama kI sAXAraNa avaXArAoM kI samaJa
NP10 the system s understanding	sistama kI samaJa
PP15 of the simpler concepts	sarala avaXArAoM kI
NP17 the simpler concepts	sarala avaXAraNAez
NNS20 concepts	avaXAraNAez
VP21_LWG can be rened	pariRkqwa kiyA jA sakawA hE
PP27 given information about the more complex concepts	aXika jatila avaXArAoM ke bAre meM jAnakArI xI
NP29 information about the more complex concepts	aXika jatila avaXArAoM ke bAre meM jAnakArI
NP30 information	jAnakArI
PP32 about the more complex concepts	aXika jatila avaXArAoM ke bAre meM
NP34 the more complex concepts	aXika jatila avaXAraNA
ADJP36 more complex	aXika jatila
NNS39 concepts	avaXAraNAez

----
00164	For example, an AI system observing an image of a face with one eye in shadow may initially see only one eye.	 uxAharaNa ke lie , eka eAI sistama jo CAyA meM eka AzKa vAle cehare kI Cavi ko xeKawA hE , prAraMBa meM kevala eka AzKa hI xeKa sakawA hE .		
163	163
S1 For example , an AI system observing an image of a face with one eye in shadow may initially see only one eye .	uxAharaNa ke lie, CAyA meM eka AMKa ke sAWa cehare kI Cavi kA avalokana karane vAlI eAI praNAlI SurU meM kevala eka AMKa xeKa sakawI hE
PP2 For example	uxAharaNa ke lie
NP4 example	uxAharaNa
,6 ,	,
NP7 an AI system observing an image of a face with one eye in shadow	CAyA meM eka AMKa ke sAWa cehare kI Cavi kA avalokana kara rahI eAI praNAlI
NP8 an AI system	eka eAI praNAlI
VP12_LWG observing	xeKa rahA hUz
NP14 an image of a face with one eye in shadow	CAyA meM eka AMKa ke sAWa cehare kI Cavi
NP15 an image	eka Cavi
PP18 of a face with one eye in shadow	CAyA meM eka AMKa ke sAWa cehare kA
NP20 a face with one eye in shadow	CAyA meM eka AMKa vAlA ceharA
NP21 a face	eka ceharA
PP24 with one eye in shadow	CAyA meM eka AMKa ke sAWa
NP26 one eye in shadow	CAyA meM eka AMKa
NP27 one eye	eka AMKa
PP30 in shadow	CAyA meM
NP32 shadow	CAyA
VP34_LWG may initially see	prAraMBika rUpa se xeKa sakawe hEM
ADVP36 initially	SuruAwa meM
NP40 only one eye	kevala eka AMKa

----
00165	After detecting that a face is present, the system can then infer that a second eye is probably present as well.	 yaha pawA lagAne ke bAxa ki eka ceharA mOjUxa hE , sistama waba anumAna lagA sakawA hE ki eka xUsarI AMKa SAyaxa BI mOjUxa hE .		
164	164
S1 After detecting that a face is present , the system can then infer that a second eye is probably present as well .	eka ceharA mOjUxa hE, isakA pawA lagAne ke bAxa sistama waba anumAna lagA sakawA hE ki xUsarI AMKa SAyaxa mOjUxa hE
PP2 After detecting that a face is present	yaha pawA lagAne ke bAxa ki ceharA mOjUxa hE
S4 detecting that a face is present	yaha pawA lagAwe hue ki ceharA mOjUxa hE
VP5_LWG detecting	kA pawA lagAnA
SBAR7 that a face is present	ki eka ceharA mOjUxa
S9 a face is present	eka ceharA mOjUxa
NP10 a face	eka ceharA
VP13_LWG is	hE
ADJP15 present	mOjUxa
,17 ,	,
NP18 the system	sistama
VP21_LWG can then infer	Pira anumAna lagA sakawe hEM
ADVP23 then	Pira
SBAR27 that a second eye is probably present as well	ki xUsarI AMKa SAyaxa mOjUxa hE
S29 a second eye is probably present as well	eka xUsarI AMKa SAyaxa mOjUxa hE
NP30 a second eye	eka xUsarI AMKa
VP34_LWG is probably	SAyaxa hE
ADVP36 probably	SAyaxa
ADJP38 present as well	sAWa hI mOjUxa
PP40 as well	sAWa hI
ADVP42 well	acCI waraha se

----
00166	In this case, the graph of concepts includes only two layersa layer for eyes and a layer for facesbut the graph of computations includes 2 n layers if we rene our estimate of each concept given the other n times.	 isa mAmale meM , avaXAraNAoM ke grAPa meM kevala xo paraweM SAmila hEM - AMKoM ke lie eka parawa Ora ceharoM ke lie eka parawa - lekina aBikalana ke grAPa meM 2 n paraweM SAmila hEM yaxi hama anya n samaya xie gae prawyeka avaXAraNA ke apane anumAna ko pariRkqwa karawe hEM .		
165	165
S1 In this case , the graph of concepts includes only two layers  a layer for eyes and a layer for faces  but the graph of computations includes 2 n layers if we rene our estimate of each concept given the other n times .	isa mAmale meM avaXArAoM ke grAPa meM kevala xo parawoM kI parawiyAM SAmila hEMAMKoM ke lie eka parawa Ora cehare ke lie eka parawa howI hE lekina kampyUteSana
S2 In this case , the graph of concepts includes only two layers  a layer for eyes and a layer for faces	isa mAmale meM avaXAraNAoM ke grAPa meM kevala xo paraweM SAmila hEMAMKoM ke lie eka parawa Ora cehare ke lie eka parawa
PP3 In this case	isa mAmale meM
NP5 this case	yaha mAmalA
,8 ,	,
NP9 the graph of concepts	avaXAraNA kA grAPa
NP10 the graph	grAPa
PP13 of concepts	avaXArAoM kI
NP15 concepts	avaXAraNAez
NNS16 concepts	avaXAraNAez
VP17_LWG includes	isameM SAmila
NP19 only two layers  a layer for eyes and a layer for faces	kevala xo parawoM- AMKoM ke lie eka parawa Ora cehare ke lie eka parawa
NP20 only two layers  a layer for eyes	kevala xo parawoM- AMKoM ke lie eka parawa
NP21 only two layers	kevala xo paraweM
QP22 only two	kevala xo hI
NNS25 layers	paraweM
NP27 a layer for eyes	AMKoM kI parawa
NP28 a layer	eka parawa
PP31 for eyes	AMKoM ke lie
NP33 eyes	AMKeM
NNS34 eyes	AMKeM
CC35 and	Ora
NP36 a layer for faces	cehare ke lie eka parawa
NP37 a layer	eka parawa
PP40 for faces	cehare ke lie
NP42 faces	cehare
NNS43 faces	cehare
,44 	
CC45 but	lekina
S46 the graph of computations includes 2 n layers if we rene our estimate of each concept given the other n times	kampyUteSana ke grAPa meM 2 ena paraweM SAmila hEM yaxi hama eka-eka avaXAraNA ke apane anumAna ko xUsare ena samaya meM svIkqwa karawe hEM
NP47 the graph of computations	kampyUteSana kA grAPa
NP48 the graph	grAPa
PP51 of computations	kampyUteSana kA
NP53 computations	kampyUteSana
NNS54 computations	kampyUteSana
VP55_LWG includes	isameM SAmila
NP57 2 n layers	2 n paraweM
NNS60 layers	paraweM
SBAR61 if we rene our estimate of each concept given the other n times	agara hama xUsare ena bAra xie gae prawyeka avaXAraNA ke apane anumAna ko pariRkqwa karawe hEM
S63 we rene our estimate of each concept given the other n times	hama xUsare ena bAra xie gae prawyeka avaXAraNA ke apane anumAna ko pariRkqwa karawe hEM
NP64 we	hama
VP66_LWG rene	riPAina
NP68 our estimate of each concept given the other n times	xUsare ena bAra xie gae prawyeka avaXAraNA kA hamArA anumAna
NP69 our estimate	hamArA anumAna
PP72 of each concept given the other n times	xUsare ena samaya meM xI gaI prawyeka avaXAraNA kI
NP74 each concept given the other n times	xUsare ena samaya meM xI gaI prawyeka avaXAraNA
NP75 each concept	prawyeka avaXAraNA
VP78_LWG given	xiyA gayA
NP-TMP80 the other n times	xUsare ena bAra
NNS84 times	samaya

----
00167	Because it is not always clear which of these two viewsthe depth of the computational graph, or the depth of the probabilistic modeling graphis most relevant, and because dierent people choose dierent sets of smallest elements from which to construct their graphs, there is no single correct value for the depth of an architecture, just as there is no single correct value for the length of a computer program.	 kyoMki yaha hameSA spaRta nahIM hE ki ina xo xqSyoM meM se kOna sA hE - kampyUteSanala grAPa kI gaharAI yA probAbilistika moYdaliMga grAPa kI gaharAI - sabase aXika prAsaMgika hE , Ora kyoMki viBinna loga apane grAPoM kA nirmANa karane ke lie sabase Cote wawvoM ke viBinna seta cunawe hEM , vahAz eka vAswukalA kI gaharAI ke lie koI ekala sahI mUlya nahIM hE		
166	166
S1 Because it is not always clear which of these two views  the depth of the computational graph , or the depth of the probabilistic modeling graph  is most relevant , and because dierent people choose dierent sets of smallest elements from which to construct their graphs , there is no single correct value for the depth of an architecture , just as there is no single correct value for the length of a computer program .	kyoMki yaha hameSA spaRta nahIM howA hE ki ina xonoM vicAroM meM se kisameM se kaMpyUtaranala grAPa kI gaharAI, yA saMBAviwa moYdaliMga grAPa kI gaharAI sabase
SBAR2 Because it is not always clear which of these two views  the depth of the computational graph , or the depth of the probabilistic modeling graph  is most relevant , and because dierent people choose dierent sets of smallest elements from which to construct their graphs	kyoMki yaha hameSA spaRta nahIM howA hE ki ina xonoM vicAroM meM se kOna sA hE kaMpyUtarikala grAPa kI gaharAI, yA saMBAvanAvAxI moYdaliMga grAPa kI gaharAI
S4 it is not always clear which of these two views  the depth of the computational graph , or the depth of the probabilistic modeling graph  is most relevant , and because dierent people choose dierent sets of smallest elements from which to construct their graphs	yaha hameSA spaRta nahIM howA hE ki ina xonoM vicAroM meM se kOna sA hE kaMpyUtarikala grAPa kI gaharAI, yA saMBAvanAvAxI moYdaliMga grAPa kI gaharAI sabase
NP5 it	yaha
VP7_LWG is not always	hameSA nahIM howA
ADVP10 always	hameSA
ADJP12 clear	spaRta
SBAR14 which of these two views  the depth of the computational graph , or the depth of the probabilistic modeling graph  is most relevant , and because dierent people choose dierent sets of smallest elements from which to construct their graphs	ina xonoM vicAroM meM se kOna-kaMpyUSanala grAPa kI gaharAI, yA saMBAvanAvAxI moYdaliMga grAPa kI gaharAIsabase prAsaMgika hE, Ora kyoMki viBinna lo
SBAR15 which of these two views  the depth of the computational graph , or the depth of the probabilistic modeling graph  is most relevant	ina xonoM vicAroM meM se kOna-kaMpyUSanala grAPa kI gaharAI, yA saMBAvanAvAxI moYdaliMga grAPa kI gaharAI sabase aXika prAsaMgika hE
WHNP16 which	jo ki
S18 of these two views  the depth of the computational graph , or the depth of the probabilistic modeling graph  is most relevant	ina xonoM vicAroM kI gaharAI kaMpyUteSanala grAPa kI gaharAI, yA saMBAvanAvAxI moYdaliMga grAPa kI gaharAIsabase aXika prAsaMgika hE
NP19 of these two views  the depth of the computational graph , or the depth of the probabilistic modeling graph 	ina xonoM vicAroM kI gaharAI kaMpyUteSanala grAPa kI gaharAI, yA saMBAviwa moYdaliMga grAPa kI gaharAI
NP20 of these two views  the depth of the computational graph	ina xonoM vicAroM kI kaMpyUtiSanala grAPa kI gaharAI
PP21 of these two views	ina xonoM vicAroM kA
NP23 these two views	ina xonoM vicAroM
NNS26 views	vicAra
,27 	
NP28 the depth	gaharAI
PP31 of the computational graph	kampyUteSanala grAPa kA
NP33 the computational graph	kampyUteSanala grAPa
,37 ,	,
CC38 or	yA
NP39 the depth of the probabilistic modeling graph	saMBAviwa moYdaliMga grAPa kI gaharAI
NP40 the depth	gaharAI
PP43 of the probabilistic modeling graph	saMBAviwa moYdaliMga grAPa kA
NP45 the probabilistic modeling graph	saMBAviwa moYdaliMga grAPa
ADJP47 probabilistic modeling	saMBAvanAvAxI moYdaliMga
VP52_LWG is	hE
ADJP54 most relevant	sabase prAsaMgika
,57 ,	,
CC58 and	Ora
SBAR59 because dierent people choose dierent sets of smallest elements from which to construct their graphs	kyoMki viBinna loga sabase Cote wawvoM ke alaga-alaga seta cunawe hEM jinase apane grAPa banAne ke lie
S61 dierent people choose dierent sets of smallest elements from which to construct their graphs	viBinna loga sabase Cote wawvoM ke alaga-alaga seta cunawe hEM jinase apane grAPa banAne ke lie
NP62 dierent people	alaga-alaga loga
NNS64 people	logoM
VP65_LWG choose	cuneM
NP67 dierent sets of smallest elements	sabase Cote wawvoM ke alaga seta
NP68 dierent sets	alaga-alaga seta
NNS70 sets	seta
PP71 of smallest elements	sabase Cote wawvoM kA
NP73 smallest elements	sabase Cote wawva
NNS75 elements	wawva
PP76 from which to construct their graphs	jisase unake grAPa banAne ke lie
SBAR78 which to construct their graphs	jo apane grAPa kA nirmANa kareM
WHNP79 which	jo ki
S81 to construct their graphs	unake grAPa kA nirmANa karane ke lie
VP82_LWG to construct	banAne ke lie
NP86 their graphs	unake grAPa
NNS88 graphs	grAPa
,89 ,	,
NP90 there	vahAM
VP92_LWG is	hE
NP94 no single correct value for the depth of an architecture	eka vAswukalA kI gaharAI ke lie koI BI sahI mUlya nahIM
NP95 no single correct value	koI BI sahI mUlya nahIM
PP100 for the depth of an architecture	eka vAswukalA kI gaharAI ke lie
NP102 the depth of an architecture	eka vAswukalA kI gaharAI
NP103 the depth	gaharAI
PP106 of an architecture	eka vAswukalA kI
NP108 an architecture	eka vAswukalA
,111 ,	,
SBAR112 just as there is no single correct value for the length of a computer program	jEse kisI kaMpyUtara progrAma kI laMbAI ke lie eka BI sahI mUlya nahIM hE
S115 there is no single correct value for the length of a computer program	kampyUtara progrAma kI laMbAI ke lie eka BI sahI mUlya nahIM hE
NP116 there	vahAM
VP118_LWG is	hE
NP120 no single correct value for the length of a computer program	kaMpyUtara progrAma kI laMbAI ke lie eka BI sahI mUlya nahIM
NP121 no single correct value	koI BI sahI mUlya nahIM
PP126 for the length of a computer program	kampyUtara progrAma kI laMbAI ke lie
NP128 the length of a computer program	kampyUtara progrAma kI laMbAI
NP129 the length	laMbAI
PP132 of a computer program	eka kaMpyUtara progrAma kA
NP134 a computer program	eka kaMpyUtara progrAma

----
00168	Nor is there a consensus about how much depth a model requires to qualify as deep.	 na hI isa bAwa para Ama sahamawi hE ki kisI moYdala ko xIpAvalI ke rUpa meM kiwanI gaharAI se yogya banAne kI AvaSyakawA hE 		
167	167
SINV1 Nor is there a consensus about how much depth a model requires to qualify as  deep .	na hI isa bAre meM eka sahamawi hE ki gaharI ke rUpa meM yogya hone ke lie eka moYdala kI kiwanI gaharAI kI AvaSyakawA howI hE

----
00169	However, deep learning can be safely regarded as the study of models that involve a greater amount of composition of either learned functions or learned concepts than traditional machine learning does.	 hAlAMki , gaharI sIKane surakRiwa rUpa se moYdala hE ki yA wo sIKA kAryoM yA pAraMparika maSIna sIKane kI wulanA meM sIKA avaXAraNAoM ke saMyojana kI eka aXika rASi SAmila ke aXyayana ke rUpa meM mAnA jA sakawA hE .		
168	168
S1 However , deep learning can be safely regarded as the study of models that involve a greater amount of composition of either learned functions or learned concepts than traditional machine learning does .	hAlAMki gaharI sIKa ko surakRiwa rUpa se moYdaloM ke aXyayana ke rUpa meM mAnA jA sakawA hE jisameM yA wo sIKA huA kAryoM kI racanA ho yA Pira pAraMparika maSIna
ADVP2 However	hAlAMki
,4 ,	,
NP5 deep learning	gaharI sIKa
VP8_LWG can be safely regarded	surakRiwa rUpa se ho sakawA hE saMbaMXa
ADVP12 safely	surakRiwa
PP16 as the study of models that involve a greater amount of composition of either learned functions or learned concepts than traditional machine learning does	jEsA ki moYdaloM kA aXyayana hE jisameM yA wo sIKA huA kAryoM kI racanA kI aXika mAwrA SAmila howI hE yA paraMparAgawa maSIna sIKane kI wulanA meM sIKI ava
NP18 the study of models that involve a greater amount of composition of either learned functions or learned concepts than traditional machine learning does	moYdaloM kA aXyayana jisameM yA wo sIKA huA kAryoM kI racanA ho yA Pira pAraMparika maSIna sIKane kI wulanA meM sIKI avaXArAoM kI aXika mAwrA howI hE
NP19 the study	aXyayana
PP22 of models that involve a greater amount of composition of either learned functions or learned concepts than traditional machine learning does	moYdaloM kI jinameM yA wo sIKA huA kAryoM kI racanA ho yA paraMparAgawa maSIna sIKane kI wulanA meM sIKI avaXArAoM kI aXika mAwrA SAmila howI hE
NP24 models that involve a greater amount of composition of either learned functions or learned concepts than traditional machine learning does	moYdala jinameM yA wo sIKA huA kAryoM kI racanA ho yA paraMparAgawa maSIna sIKane kI wulanA meM sIKI avaXArAoM kI aXika mAwrA howI hE
NP25 models	moYdala
NNS26 models	moYdala
SBAR27 that involve a greater amount of composition of either learned functions or learned concepts than traditional machine learning does	jisameM pAraMparika maSIna sIKane kI wulanA meM yA wo sIKA huA kAryoM kI racanA aXika mAwrA meM SAmila hE
WHNP28 that	vaha
S30 involve a greater amount of composition of either learned functions or learned concepts than traditional machine learning does	pAraMparika maSIna sIKane kI wulanA meM yA wo sIKe gae kAryoM kI racanA aXika mAwrA meM SAmila
VP31_LWG involve	SAmila
SBAR33 a greater amount of composition of either learned functions or learned concepts than traditional machine learning does	pAraMparika maSIna sIKane kI wulanA meM yA wo sIKe gae kAryoM yA sIKe hue avaXArAoM kI racanA kI aXika mAwrA
S34 a greater amount of composition of either learned functions or learned concepts than traditional machine learning does	pAraMparika maSIna sIKane kI wulanA meM yA wo sIKe gae kAryoM yA sIKe hue avaXArAoM kI racanA kI aXika mAwrA
NP35 a greater amount of composition of either learned functions or learned concepts than traditional machine learning	pAraMparika maSIna sIKane kI wulanA meM yA wo sIKe gae kAryoM kI racanA kI aXika mAwrA
NP36 a greater amount	aXika rASi
PP40 of composition of either learned functions or learned concepts than traditional machine learning	pAraMparika maSIna sIKane kI wulanA meM yA wo sIKA huA kAryoM yA sIKA avaXAraNA kI racanA
NP42 composition of either learned functions or learned concepts than traditional machine learning	paraMparAgawa maSIna sIKane kI wulanA meM yA wo sIKe gae kAryoM kI racanA yA Pira sIKI avaXArAoM kI racanA
NP43 composition	racanA
PP45 of either learned functions or learned concepts than traditional machine learning	paraMparAgawa maSIna sIKane kI wulanA meM yA wo sIKe gae kArya yA avaXAriwa avaXArAoM kI
NP47 either learned functions or learned concepts than traditional machine learning	yA wo sIKe hue kAma yA paraMparAgawa maSIna sIKane kI wulanA meM sIKI avaXArAeM
NML48 either learned functions or learned concepts than traditional machine	yA wo sIKe hue kAma yA paraMparAgawa maSIna kI wulanA meM sIKI avaXArAeM

----
00170	To summarize, deep learning, the subject of this book, is an approach to AI.	 saMkRepa karane ke lie , gaharI sIKane , isa puswaka kA viRaya hE , eAI ke lie eka xqRtikoNa hE .		
169	169
S1 To summarize , deep learning , the subject of this book , is an approach to AI .	varNana karane ke lie, gaharI sIKa, isa puswaka kA viRaya, eAI kA xqRtikoNa hE
S2 To summarize	varNana karane ke lie
VP3_LWG To summarize	varNana karane ke lie
,7 ,	,
NP8 deep learning , the subject of this book ,	gaharI sIKa, isa puswaka kA viRaya,
NP9 deep learning	gaharI sIKa
,12 ,	,
NP13 the subject of this book	isa puswaka kA viRaya
NP14 the subject	viRaya
PP17 of this book	isa puswaka kI
NP19 this book	yaha puswaka
,22 ,	,
VP23_LWG is	hE
NP25 an approach to AI	eAI kA xqRtikoNa
NP26 an approach	eka xqRtikoNa
PP29 to AI	eAI ko
NP31 AI	E.AI.

----
0017	Many of the early successes of AI took place in relatively sterile and formal environments and did not require computers to have much knowledge about the world.	 eAI kI kaI prAraMBika saPalawAeM apekRAkqwa baMXya Ora OpacArika vAwAvaraNa meM huI Ora isake lie kaMpyUtara ko xuniyA ke bAre meM aXika jAnakArI raKane kI AvaSyakawA nahIM WI .		
16	16
S1 Many of the early successes of AI took place in relatively sterile and formal environments and did not require computers to have much knowledge about the world .	eAI kI kaI prAraMBika saPalawAeM apekRAkqwa svaswa Ora OpacArika vAwAvaraNa meM huIM Ora xuniyA ke bAre meM jyAxA jAnakArI raKane ke lie kaMpyUtaroM kI
NP2 Many of the early successes of AI	eAI kI kaI SuruAwI saPalawAeM
NP3 Many	kaI
PP5 of the early successes of AI	eAI kI SuruAwI saPalawAoM kI
NP7 the early successes of AI	eAI kI SuruAwI saPalawAeM
NP8 the early successes	SuruAwI saPalawAeM
NNS11 successes	saPalawAeM
PP12 of AI	eAI kA
NP14 AI	E.AI.
VP16_LWG took did not require	liyA jarUrawa nahIM WI
NP19 place	jagaha
PP21 in relatively sterile and formal environments	apekRAkqwa asWira Ora OpacArika vAwAvaraNa meM
NP23 relatively sterile and formal environments	apekRAkqwa asWira Ora OpacArika vAwAvaraNa
ADJP24 relatively sterile and formal	apekRAkqwa asWira Ora OpacArika
CC27 and	Ora
JJ26 sterile	niRpraBAvI
JJ28 formal	OpacArika
NNS29 environments	paryAvaraNa
CC30 and	Ora
VP17 took place in relatively sterile and formal environments	apekRAkqwa asWira Ora OpacArika vAwAvaraNa meM huA WA
VP31 did not require computers to have much knowledge about the world	xuniyA ke bAre meM jyAxA jAnakArI raKane ke lie kaMpyUtaroM kI jarUrawa nahIM WI
NP36 computers	kaMpyUtara
NNS37 computers	kaMpyUtara
S38 to have much knowledge about the world	xuniyA ke bAre meM jyAxA jAnakArI raKane ke lie
VP39_LWG to have	karane ke lie
NP43 much knowledge about the world	xuniyA ke bAre meM bahuwa kuCa jFAna
NP44 much knowledge	bahuwa jFAna
PP47 about the world	xuniyA ke bAre meM
NP49 the world	xuniyA

----
00171	Specically, it is a type of machine learning, a technique that enables computer systems to improve with experience and data.	 viSeRa rUpa se , yaha maSIna aXigama kA eka prakAra hE , eka wakanIka hE jo kaMpyUtara praNAliyoM ko anuBava Ora detA ke sAWa suXAra karane meM sakRama banAwI hE .		
170	170
S1 Specically , it is a type of machine learning , a technique that enables computer systems to improve with experience and data .	viSeRa rUpa se, yaha eka prakAra kA maSIna sIKane, eka wakanIka hE jo kaMpyUtara sistama ko anuBava Ora detA ke sAWa behawara karane meM sakRama banAwA hE
ADVP2 Specically	viSeRa rUpa se
,4 ,	,
NP5 it	yaha
VP7_LWG is	hE
NP9 a type of machine learning , a technique that enables computer systems to improve with experience and data	eka prakAra kI maSIna sIKane, eka wakanIka jo anuBava Ora detA ke sAWa kaMpyUtara sistama ko behawara banAne meM sakRama banAwI hE
NP10 a type	eka prakAra kA
PP13 of machine learning , a technique that enables computer systems to improve with experience and data	maSIna sIKane kI, eka wakanIka jo anuBava Ora detA ke sAWa kampyUtara sistama ko behawara banAne meM sakRama banAwI hE
NP15 machine learning , a technique that enables computer systems to improve with experience and data	maSIna sIKane, eka EsI wakanIka jo anuBava Ora detA ke sAWa kaMpyUtara sistama ko behawara banAne meM sakRama banAwI hE
NP16 machine learning	maSIna sIKanA
,19 ,	,
NP20 a technique that enables computer systems to improve with experience and data	eka wakanIka jo anuBava Ora detA ke sAWa kampyUtara sistama ko behawara karane meM sakRama banAwA hE
NP21 a technique	eka wakanIka
SBAR24 that enables computer systems to improve with experience and data	yaha anuBava Ora detA ke sAWa kampyUtara sistama ko behawara banAne meM sakRama
WHNP25 that	vaha
S27 enables computer systems to improve with experience and data	anuBava Ora detA ke sAWa suXAra karane meM sakRama kaMpyUtara sistama
VP28_LWG enables	sakRama
NP30 computer systems	kampyUtara sistama
NNS32 systems	sistama
S33 to improve with experience and data	anuBava Ora AMkadZoM ke sAWa suXAra karane ke lie
VP34_LWG to improve	suXAra karane ke lie
PP38 with experience and data	anuBava Ora dAtA ke sAWa
NP40 experience and data	anuBava Ora detA
CC42 and	Ora
NN41 experience	anuBava
NNS43 data	detA

----
00172	We contend that machine learning is the only viable approach to building AI systems that can operate in complicated real-world environments.	 hama warka xewe hEM ki maSInI SikRA hI eAI praNAliyoM ke nirmANa kA ekamAwra vyavahArya warIkA hE jo jatila vAswavika xuniyA ke vAwAvaraNa meM kArya kara sakawA hE 		
171	171
S1 We contend that machine learning is the only viable approach to building AI systems that can operate in complicated real - world environments .	hama xAvA karawe hEM ki maSIna sIKanA eAI praNAliyoM ke nirmANa ke lie ekamAwra vExyakIya xqRtikoNa hE jo jatila vAswavika - viSva vAwAvaraNa meM kAma kara sakawA hE
NP2 We	hama
VP4_LWG contend	xAvexAra
SBAR6 that machine learning is the only viable approach to building AI systems that can operate in complicated real - world environments	vaha maSIna sIKanA eAI praNAliyoM ke nirmANa ke lie ekamAwra vExyakIya xqRtikoNa hE jo jatila vAswavika - viSva paryAvaraNa meM saMcAliwa kara sakawA hE
S8 machine learning is the only viable approach to building AI systems that can operate in complicated real - world environments	maSIna sIKanA eAI praNAliyoM ke nirmANa ke lie ekamAwra vExyakIya xqRtikoNa hE jo jatila vAswavika rUpa meM saMcAliwa kara sakawA hE - viSva paryAvaraNa
NP9 machine learning	maSIna sIKanA
VP12_LWG is	hE
NP14 the only viable approach to building AI systems that can operate in complicated real - world environments	eAI sistama banAne ke lie ekamAwra viSvasanIya xqRtikoNa jo jatila vAswavika - viSva paryAvaraNa meM kAma kara sakawA hE
NP15 the only viable approach to building AI systems	eAI sistama banAne ke lie ekamAwra vExyakIya xqRtikoNa
NP16 the only viable approach	ekamAwra vExyakIya xqRtikoNa
ADJP18 only viable	kevala vyavahArya
NP23 building AI systems	eAI sistama kA nirmANa
NML24 building AI	nirmANa eAI
NNS27 systems	sistama
SBAR28 that can operate in complicated real - world environments	jo jatila vAswavika - viSva vAwAvaraNa meM saMcAliwa kara sakawA hE
WHNP29 that	vaha
S31 can operate in complicated real - world environments	jatila vAswavika meM saMcAlana kara sakawe hEM - viSva vAwAvaraNa
VP32_LWG can operate	saMcAlana kara sakawe hEM
PP36 in complicated real - world environments	jatila vAswavika - viSva vAwAvaraNa
NP38 complicated real - world environments	jatila vAswavika - viSva vAwAvaraNa
NML40 real - world	asalI-xuniyA
NNS44 environments	paryAvaraNa

----
00173	Deep learning is a particular kind of machine learning that achieves great power and exibility by representing the world as a nested hierarchy of concepts, with each concept dened in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.	 gahana aXigama , eka viSeRa prakAra kA maSIna aXigama hE , jo avaXAraNAoM ke nIdZiwa paxAnukrama ke rUpa meM viSva kA prawiniXiwva karake mahAna Sakwi Ora lacIlApana prApwa karawA hE , jisameM prawyeka avaXAraNA sarala avaXAraNAoM ke saMbaMXa meM pariBARiwa howI hE , Ora kama amUrwa nirUpaNa ke saMxarBa meM parikaliwa aXika amUrwa nirUpaNa howA hE 		
172	172
S1 Deep learning is a particular kind of machine learning that achieves great power and exibility by representing the world as a nested hierarchy of concepts , with each concept dened in relation to simpler concepts , and more abstract representations computed in terms of less abstract ones .	gaharI sIKa eka viSeRa prakAra kI maSIna sIKane vAlI hE jo saMBAvanAoM kA prawiniXiwva karake mahAna Sakwi Ora lacIlApana hAsila karawI hE, prawyeka avaXAraNA ko sarala
S2 Deep learning is a particular kind of machine learning that achieves great power and exibility by representing the world as a nested hierarchy of concepts , with each concept dened in relation to simpler concepts	gaharI sIKa eka viSeRa prakAra kI maSIna sIKane vAlI hE jo saMBAvanAoM ke eka prANI ke rUpa meM viSva kA prawiniXiwva karake mahAna Sakwi Ora lacIlApana hAsila karawI hE
NP3 Deep learning	gaharI sIKa
VP6_LWG is	hE
NP8 a particular kind of machine learning that achieves great power and exibility by representing the world as a nested hierarchy of concepts , with each concept dened in relation to simpler concepts	eka viSeRa prakAra kI maSIna sIKane vAlI Sakwi Ora lacIlApana ko avaXAriwAoM kI eka nasIhawa ke rUpa meM viSva kA prawiniXiwva karake mahAna Sakwi Ora lacIlA
NP9 a particular kind	eka viSeRa prakAra kA
PP13 of machine learning that achieves great power and exibility by representing the world as a nested hierarchy of concepts , with each concept dened in relation to simpler concepts	maSIna sIKane vAlI jo saMBAvanAoM ke eka sUkRma graMWa ke rUpa meM viSva kA prawiniXiwva karake mahAna Sakwi Ora lacIlApana hAsila karawI hE, prawyeka avaXAraNA ke
NP15 machine learning that achieves great power and exibility by representing the world as a nested hierarchy of concepts , with each concept dened in relation to simpler concepts	maSIna sIKane vAlI jo saMBAvanAoM kA prawiniXiwva karake mahAna Sakwi Ora lacIlApana hAsila karawI hE, prawyeka avaXAraNA ke sAWa sarala avaXArAoM ke saMbaMXa meM pariBARa
NP16 machine learning	maSIna sIKanA
SBAR19 that achieves great power and exibility by representing the world as a nested hierarchy of concepts , with each concept dened in relation to simpler concepts	yaha sAMsArika avaXArAoM ke saMbaMXa meM prawyeka avaXAraNA ke rUpa meM viSva kA prawiniXiwva karake mahAna Sakwi Ora lacIlApana hAsila karawA hE, prawyeka avaXAraNA ke sAWa sarala
WHNP20 that	vaha
S22 achieves great power and exibility by representing the world as a nested hierarchy of concepts , with each concept dened in relation to simpler concepts	saMBAvanAoM kI eka pramuKa avaXAraNA ke rUpa meM viSva kA prawiniXiwva karake mahAna Sakwi Ora lacIlApana hAsila karawA hE, prawyeka avaXAraNA ke sAWa sarala avaXArAoM ke saMbaMXa
VP23_LWG achieves	hAsila karawA hE
NP25 great power and exibility	mahAna Sakwi Ora lacIlApana
CC28 and	Ora
NN27 power	bijalI
NN29 exibility	lacIlApana
PP30 by representing the world as a nested hierarchy of concepts , with each concept dened in relation to simpler concepts	saMBAvanAoM kI eka prANI ke rUpa meM xuniyA kA prawiniXiwva karake, prawyeka avaXAraNA ke sAWa sarala avaXArAoM ke saMbaMXa meM pariBARiwa
S32 representing the world as a nested hierarchy of concepts , with each concept dened in relation to simpler concepts	saMBAvanAoM kI eka prANI ke rUpa meM xuniyA kA prawiniXiwva karanA, prawyeka avaXAraNA ke sAWa sarala avaXArAoM ke saMbaMXa meM pariBARiwa
VP33_LWG representing	prawiniXiwva karawe hue
NP35 the world	xuniyA
PP38 as a nested hierarchy of concepts	avaXArAoM kI eka nqSaMkuwA ke rUpa meM
NP40 a nested hierarchy of concepts	avaXArAoM kI eka nqSaMkuwA
NP41 a nested hierarchy	eka newqwva vAle paxAnukrama
PP45 of concepts	avaXArAoM kI
NP47 concepts	avaXAraNAez
NNS48 concepts	avaXAraNAez
,49 ,	,
PP50 with each concept dened in relation to simpler concepts	prawyeka avaXAraNA ke sAWa sarala avaXArAoM ke saMbaMXa meM pariBARiwa
NP52 each concept dened in relation to simpler concepts	sarala avaXArAoM ke saMbaMXa meM pariBARiwa prawyeka avaXAraNA
NP53 each concept	prawyeka avaXAraNA
VP56_LWG dened	pariBARiwa
PP58 in relation	saMbaMXa meM
NP60 relation	saMbaMXa
PP62 to simpler concepts	sarala avaXArAoM ko
NP64 simpler concepts	sarala avaXAraNAez
NNS66 concepts	avaXAraNAez
,67 ,	,
CC68 and	Ora
S69 more abstract representations computed in terms of less abstract ones	kama avawAra vAle SabxoM ke mAmale meM aXika amUrwa prawiniXiwva kA gaNanA
NP70 more abstract representations	aXika amUrwa prawiniXiwva
NNS73 representations	prawiniXiwva
VP74_LWG computed	kampyUta kiyA gayA
PP76 in terms of less abstract ones	kama avawAra vAloM ke mAmale meM
NP78 terms of less abstract ones	kama avawAra vAloM kI SarweM
NP79 terms	SarwoM
NNS80 terms	SarwoM
PP81 of less abstract ones	kama avawAra vAle logoM kI
NP83 less abstract ones	kama amUrwa vAle
ADJP84 less abstract	kama sAraxarSI
NNS87 ones	vAle

----
00174	Figure 1.4 illustrates the relationship between these dierent AI disciplines.	 ciwra 1 . 4 ina viBinna eAI viXAoM ke bIca saMbaMXoM ko spaRta karawA hE 		
173	173
S1 Figure 1.4 illustrates the relationship between these dierent AI disciplines .	AMkadZA 1.4 ina alaga-alaga eAI anuSAsanoM ke bIca saMbaMXoM ko praxarSiwa karawA hE
NP2 Figure 1.4	AMkadZA 1.4
VP5_LWG illustrates	xarSAwA hE
NP7 the relationship between these dierent AI disciplines	ina alaga-alaga eAI anuSAsanoM ke bIca riSwe
NP8 the relationship	riSwe
PP11 between these dierent AI disciplines	ina alaga-alaga eAI anuSAsanoM ke bIca
NP13 these dierent AI disciplines	ye alaga-alaga eAI anuSAsana
NNS17 disciplines	anuSAsana

----
00175	Figure 1.5 gives a high-level schematic of how each works.	 ciwra 1 . 5 prawyeka kqwi ke bAre meM ucca swara kI yojanA banAwA hE 		
174	174
S1 Figure 1.5 gives a high - level schematic of how each works .	Pigara 1.5 meM eka ucca-swarIya lEpatoYpika xiyA gayA hE ki prawyeka kEse kAma karawA hE
NP2 Figure 1.5	AMkadZA 1.5
VP5_LWG gives	xewA hE
NP7 a high - level schematic of how each works	eka ucca - swarIya gaNiwa kEse prawyeka kArya karawA hE
NP8 a high - level schematic	eka ucca-swarIya gaNiwAwmaka
NML10 high - level	ucca - swara
PP15 of how each works	kEse kAma karawA hE prawyeka
SBAR17 how each works	kEse kAma karawA hE prawyeka kArya
WHADVP18 how	kEse
S20 each works	prawyeka kAma karawA hE
NP21 each	prawyeka
VP23_LWG works	kAma karawA hE

----
00176	1.1	 1 . 1		
175	175
FRAG1 1.1	1.1
NP2 1.1	1.1

----
00177	Who Should Read This Book?	 kOna isa kiwAba ko paDZanA cAhie ?		
176	176
SBARQ1 Who Should Read This Book ?	isa puswaka ko kOna paDZanA cAhie?
WHNP2 Who	kOna
SQ4 Should Read This Book	isa puswaka ko paDZanA cAhie
VP6_LWG Read	paDZeM
NP8 This Book	yaha puswaka

----
00178	This book can be useful for a variety of readers, but we wrote it with two target audiences in mind.	 yaha puswaka pATakoM kI eka kisma ke lie upayogI ho sakawA hE , lekina hama mana meM xo lakRiwa xarSakoM ke sAWa liKA WA .		
177	177
S1 This book can be useful for a variety of readers , but we wrote it with two target audiences in mind .	yaha kiwAba kaI waraha ke pATakoM ke lie upayogI ho sakawI hE, lekina hamane ise xo tArageta xarSakoM ke mana meM liKA hE
S2 This book can be useful for a variety of readers	yaha kiwAba kaI waraha ke pATakoM ke lie upayogI ho sakawI hE
NP3 This book	yaha puswaka
VP6_LWG can be	ho sakawA hE
ADJP10 useful for a variety of readers	kaI waraha ke pATakoM ke lie upayogI
PP12 for a variety of readers	kaI waraha ke pATakoM ke lie
NP14 a variety of readers	kaI waraha ke pATaka
NP15 a variety	eka kisma kI
PP18 of readers	pATakoM kI
NP20 readers	pATaka
NNS21 readers	pATaka
,22 ,	,
CC23 but	lekina
S24 we wrote it with two target audiences in mind	hamane ise xo tArageta xarSakoM ke mana meM liKA
NP25 we	hama
VP27_LWG wrote	liKA WA
NP29 it	yaha
PP31 with two target audiences in mind	xo tArageta xarSakoM ke mana meM
NP33 two target audiences in mind	xo niSAne para xarSakoM kA mana
NP34 two target audiences	xo tArageta xarSakoM
NNS37 audiences	xarSaka
PP38 in mind	XyAna meM raKeM
NP40 mind	mana

----
00179	One of these target audiences is university students (under- graduate or graduate) learning about machine learning, including those who are beginning a career in deep learning and articial intelligence research.	 ina lakRiwa xarSakoM meM se eka hE viSvavixyAlaya ke CAwra ( snAwaka yA snAwaka kI paDZAI maSIna sIKane ke bAre meM , una logoM sahiwa jo gaharI sIKane Ora kqwrima buxXi anusaMXAna meM eka kEriyara kI SuruAwa kara rahe hEM )		
178	178
S1 One of these target audiences is university students ( under - graduate or graduate ) learning about machine learning , including those who are beginning a career in deep learning and articial intelligence research .	inameM se eka lakRya xarSaka viSvavixyAlaya ke CAwra (- snAwaka yA snAwaka) maSIna sIKane ke bAre meM sIKa rahe hEM, jinameM una logoM ko gaharI sIKa Ora kqwri
NP2 One of these target audiences	inameM se eka lakRya xarSaka
NP3 One	eka
PP5 of these target audiences	ina lakRya xarSakoM kI
NP7 these target audiences	ye lakRya xarSakoM
NNS10 audiences	xarSaka
VP11_LWG is	hE
NP13 university students ( under - graduate or graduate ) learning about machine learning , including those who are beginning a career in deep learning and articial intelligence research	yUnivarsitI ke CAwra (- snAwaka yA snAwaka) maSIna sIKane ke bAre meM sIKa rahe hEM, jinameM gaharI sIKa Ora kqwrima KuPiyA anusaMXAna meM kariyara SurU
NP14 university students ( under - graduate or graduate ) learning	viSvavixyAlaya ke CAwra (aXIkRaka-grejueta yA snAwaka) kI sIKa
NP15 university students	yUnivarsitI ke CAwra
NNS17 students	CAwra-CAwrAeM
NML19 under - graduate or graduate	aMdara - snAwaka yA snAwaka
NML20 under - graduate	aMdara - snAwaka
CC24 or	yA
NML25 graduate	snAwaka
PP29 about machine learning	maSIna sIKane ke bAre meM
NP31 machine learning	maSIna sIKanA
,34 ,	,
PP35 including those who are beginning a career in deep learning and articial intelligence research	jina logoM ko gaharI sIKa Ora kqwrima KuPiyA anusaMXAna meM kariara kI SuruAwa ho rahI hE unameM SAmila
NP37 those who are beginning a career in deep learning and articial intelligence research	jinheM gaharI sIKa Ora kqwrima KuPiyA anusaMXAna meM kariyara kI SuruAwa kara rahe hEM
NP38 those	una logoM
SBAR40 who are beginning a career in deep learning and articial intelligence research	jo gaharI sIKa Ora kqwrima KuPiyA anusaMXAna meM kariyara kI SuruAwa kara rahe hEM
WHNP41 who	kOna
S43 are beginning a career in deep learning and articial intelligence research	gaharI sIKa Ora kqwrima KuPiyA anusaMXAna meM kariyara kI SuruAwa kara rahe hEM
VP44_LWG are beginning	SurU ho rahe hEM
NP48 a career in deep learning and articial intelligence research	gaharI sIKa Ora kqwrima KuPiyA anusaMXAna meM kariara
NP49 a career	eka kariyara
PP52 in deep learning and articial intelligence research	gaharI sIKa Ora kqwrima KuPiyA anusaMXAna meM
NP54 deep learning and articial intelligence research	gaharI sIKa Ora kqwrima buxXimawwA anusaMXAna
NML55 deep learning and articial intelligence	gaharI sIKa Ora kqwrima buxXimawwA
NML56 deep learning	gaharI sIKa
CC59 and	Ora
NML60 articial intelligence	kqwrima buxXimawwA

----
00180	The other 8 CHAPTER 1.	 anya 8 CHAPTER 1 .		
179	179
FRAG1 The other 8 CHAPTER 1 .	anya 8 cEptara 1
NP2 The other 8	xUsare 8
NP6 CHAPTER 1	cEptara 1

----
0018	For example, IBMs Deep Blue chess-playing system defeated world champion Garry Kasparov in 1997 (Hsu, 2002).	 uxAharaNa ke lie , AIbIema ke dIpa blU SawaraMja Kela praNAlI ne 1997 meM viSva cEMpiyana gErI kAspArova ko harAyA ( hAsu , 2002 )		
17	17
S1 For example , IBM s Deep Blue chess - playing system defeated world champion Garry Kasparov in 1997 ( Hsu , 2002 ) .	uxAharaNa ke lie, AIbIema kI xIpa blU SawaraMja- Kela praNAlI ne viSva cEMpiyana gErI kesarova ko 1997 meM (hasu, 2002) se parAjiwa kiyA
PP2 For example	uxAharaNa ke lie
NP4 example	uxAharaNa
,6 ,	,
NP7 IBM s Deep Blue chess - playing system	AIbIema kI gaharI nIlI SawaraMja- Kela praNAlI
NP8 IBM s Deep Blue	AIbIema kI dIpa blU
NP13 chess - playing system	SawaraMja- Kela praNAlI
VP18_LWG defeated	parAjiwa
NP20 world champion Garry Kasparov in 1997	viSva cEMpiyana gErI kApasarAva ne 1997 meM
NP21 world champion Garry Kasparov	viSva cEMpiyana gErI kApasarAva
NML22 world champion	viSva cEMpiyana
PP27 in 1997	1997 meM
NP29 1997	1997
PRN31 ( Hsu , 2002 )	(hasu, 2002)
NP33 Hsu	hU
,35 ,	,
NP36 2002	2002

----
00181	INTRODUCTION AI Machine learning Representation learning Deep learning Example: Knowledge bases Example:	 INTRODUCTION AI maSIna larniMga prawiniXiwva aXigama gahana aXigama uxAharaNaH jFAna AXAra uxAharaNaH		
180	180
FRAG1 INTRODUCTION AI Machine learning Representation learning Deep learning Example : Knowledge bases Example :	paricaya eAI maSIna sIKanA riprajeMteSana xIpa sIKanA uxAharaNa sIKanA H jFAna AXAriwa uxAharaNa -
NP2 INTRODUCTION AI Machine learning Representation learning Deep learning Example	paricaya eAI maSIna sIKa riprajeMteSana sIKanA gaharI sIKa kA uxAharaNa
NP13 Knowledge bases Example	jFAna AXAriwa karawA hE uxAharaNa
NP14 Knowledge bases	jFAna AXAra
NNS16 bases	TikAne
NP17 Example	uxAharaNa

----
00182	Logistic regression	 wArkika prawigamana		
181	181
NP1 Logistic regression	loYjistika prawiPala

----
00183	Example:	 uxAharaNaH		
182	182
NP1 Example :	uxAharaNaH

----
00184	Shallow autoencoders	 SElo oYtonakodarsa		
183	183
NP1 Shallow autoencoders	XImI oYtoenakodara
NNS3 autoencoders	oYtonakodara

----
00185	Example:	 uxAharaNaH		
184	184
NP1 Example :	uxAharaNaH

----
00186	MLPs Figure 1.4:	 MLPs ciwra 1 . 4 :		
185	185
FRAG1 MLPs Figure 1.4 :	emaelapIesa kA AMkadZA 1.4H
NP2 MLPs	emaelapI
NNS3 MLPs	emaelapI
NP4 Figure 1.4	AMkadZA 1.4

----
00187	A Venn diagram showing how deep learning is a kind of representation learning, which is in turn a kind of machine learning, which is used for many but not all approaches to AI.	 eka vena AreKa yaha xarSAwA hE ki kiwanA gaharA SikRaNa eka prakAra kA prawiniXiwva aXigama hE , jo baxale meM eka prakAra kA maSIna aXigama hE , jo kaI ke lie prayoga kiyA jAwA hE lekina saBI xqRtikoNa eAI ke lie nahIM .		
186	186
S1 A Venn diagram showing how deep learning is a kind of representation learning , which is in turn a kind of machine learning , which is used for many but not all approaches to AI .	eka vena ciwra jisameM xiKAyA gayA hE ki kiwanI gaharI sIKa eka waraha kI prawiniXiwva sIKane kI waraha hE, jo eka waraha kI maSIna sIKane kI waraha hE, jisakA isa
NP2 A Venn diagram showing how deep learning is a kind of representation learning , which is in turn a kind of machine learning , which is used for many but not all	eka vena nirxeSana jisameM xiKAyA gayA hE ki kiwanI gaharI sIKa eka waraha kI prawiniXiwva sIKane vAlI sIKa hE, jo eka waraha kI maSIna sIKane kI waraha hE,
NP3 A Venn diagram	eka vena ciwra
VP7_LWG showing	xiKA rahA hE
SBAR9 how deep learning is a kind of representation learning , which is in turn a kind of machine learning , which is used for many but not all	kiwanI gaharI sIKa eka waraha kI prawiniXiwva sIKa hE, jo ki eka waraha kI maSIna sIKane kI waraha hE, jisakA iswemAla kaI logoM ke lie kiyA jAwA hE lekina
WHADJP10 how deep	kiwanA gaharA
S13 learning is a kind of representation learning , which is in turn a kind of machine learning , which is used for many but not all	sIKanA eka waraha kA prawiniXiwva sIKanA hE, jo eka waraha kI maSIna sIKane kI waraha hE, jisakA iswemAla kaI logoM ke lie kiyA jAwA hE lekina saBI nahIM
NP14 learning	sIKanA
VP16_LWG is	hE
NP18 a kind of representation learning , which is in turn a kind of machine learning , which is used for many but not all	eka waraha kI prawiniXiwva sIKa, jo eka waraha kI maSIna sIKane kI waraha hE, jisakA iswemAla kaI logoM ke lie kiyA jAwA hE lekina yaha saba nahIM
NP19 a kind of representation learning	eka waraha kA prawiniXiwva sIKanA
NP20 a kind	eka waraha kA
PP23 of representation learning	prawiniXiwva kI sIKa
NP25 representation learning	prawiniXiwva sIKanA
,28 ,	,
SBAR29 which is in turn a kind of machine learning , which is used for many but not all	jo ki bArI meM hE eka waraha kI maSIna sIKane kI, jisakA iswemAla kaI logoM ke lie kiyA jAwA hE lekina saBI nahIM
WHNP30 which	jo ki
S32 is in turn a kind of machine learning , which is used for many but not all	eka waraha kI maSIna sIKane kI bArI hE, jisakA iswemAla kaI logoM ke lie kiyA jAwA hE lekina saBI nahIM
VP33_LWG is	hE
PP35 in turn	bArI-bArI meM
NP37 turn	bArI
NP39 a kind of machine learning , which is used for many but not all	eka waraha kI maSIna sIKa rahI hE, jisakA iswemAla kaI logoM ke lie kiyA jAwA hE lekina saBI nahIM
NP40 a kind of machine learning	eka waraha kI maSIna sIKa rahI hE
NP41 a kind	eka waraha kA
PP44 of machine learning	maSIna sIKane kI
NP46 machine learning	maSIna sIKanA
,49 ,	,
SBAR50 which is used for many but not all	jisakA iswemAla kaI logoM ke lie kiyA jAwA hE lekina saBI nahIM
WHNP51 which	jo ki
S53 is used for many but not all	kaI logoM ke lie iswemAla kiyA jAwA hE lekina saBI nahIM
VP54_LWG is used	iswemAla howA hE
PP58 for many but not all	kaI logoM ke lie lekina saBI nahIM
NP60 many but not all	kaI lekina saBI nahIM
NP61 many	kaI
CONJP63 but not	lekina nahIM

----
00188	Each section of the Venn diagram includes an example of an AI technology.	 vena AreKa ke prawyeka anuBAga meM eka eAI prOxyogikI kA eka uxAharaNa SAmila hE .		
187	187
S1 Each section of the Venn diagram includes an example of an AI technology .	vena dijAina ke prawyeka varga meM eka eAI wakanIka kA uxAharaNa SAmila hE
NP2 Each section of the Venn diagram	vena ciwra ke prawyeka varga
NP3 Each section	prawyeka varga
PP6 of the Venn diagram	vena nirxeSa kA
NP8 the Venn diagram	vena kA ciwra
VP12_LWG includes	isameM SAmila
NP14 an example of an AI technology	eka eAI wakanIka kA uxAharaNa
NP15 an example	eka uxAharaNa
PP18 of an AI technology	eka eAI wakanIka kI
NP20 an AI technology	eka eAI wakanIka

----
00189	target audience is software engineers who do not have a machine learning or statis- tics background but want to rapidly acquire one and begin using deep learning in their product or platform.	 lakRya xarSaka soYPtaveyara iMjIniyaroM hE jo eka maSIna sIKane yA sWEwika tika pqRTaBUmi nahIM hE , lekina wejI se eka prApwa karanA cAhawe hEM Ora apane uwpAxa yA maMca meM gaharI sIKane kA upayoga SurU .		
188	188
S1 target audience is software engineers who do not have a machine learning or statis - tics background but want to rapidly acquire one and begin using deep learning in their product or platform .	lakRya xarSaka soYPtaveyara iMjIniyarsa hEM, jinakA maSIna sIKane yA stetasa-tika pqRTaBUmi nahIM hE lekina wejI se eka aXigrahaNa karanA cAhawe hEM Ora apane uwpAxa
NP2 target audience	xarSakoM ko niSAnA
VP5_LWG is	hE
NP7 software engineers who do not have a machine learning or statis - tics background but want to rapidly acquire one and begin using deep learning in their product or platform	soYPtaveyara iMjIniyarsa jinake pAsa maSIna kI sIKa yA stetasa - tika pqRTaBUmi nahIM hE lekina wejI se eka aXigrahaNa karanA cAhawe hEM Ora apane uwpAxa yA pla
NP8 software engineers	soYPtaveyara iMjIniyarsa
NNS10 engineers	iMjIniyarsa
SBAR11 who do not have a machine learning or statis - tics background but want to rapidly acquire one and begin using deep learning in their product or platform	jinake pAsa maSIna sIKane yA stetasa - tika pqRTaBUmi nahIM hE lekina wejI se eka aXigrahaNa karanA cAhawe hEM Ora apane uwpAxa yA pletaPoYrma meM gaharI sIKa
WHNP12 who	kOna
S14 do not have a machine learning or statis - tics background but want to rapidly acquire one and begin using deep learning in their product or platform	maSIna sIKane yA stetasa na hoM- tika pqRTaBUmi lekina wejI se eka aXigrahaNa karanA cAhawe hEM Ora apane uwpAxa yA pletaPoYrma meM gaharI sIKa kA upayoga kara
VP15_LWG do not have want	nahIM cAhawe We
NP21 a machine learning or statis - tics background	eka maSIna sIKane yA stetasa- tika pqRTaBUmi
NML23 machine learning or statis - tics	maSIna sIKanA yA stetasa - tika
NML24 machine learning	maSIna sIKanA
CC27 or	yA
NML28 statis - tics	stetasa- tika
CC33 but	lekina
VP16 do not have a machine learning or statis - tics background	maSIna sIKane yA stetasa na hoM - tika pqRTaBUmi
VP34 want to rapidly acquire one and begin using deep learning in their product or platform	wejI se eka aXigrahaNa karanA cAhawe hEM Ora apane uwpAxa yA pletaPoYrma meM gaharI sIKa kA iswemAla karanA SurU karawe hEM
S36 to rapidly acquire one and begin using deep learning in their product or platform	wejI se eka aXigrahaNa karanA Ora apane uwpAxa yA pletaPoYrma meM gaharI sIKa kA upayoga karanA SurU karanA
VP37_LWG to rapidly acquire begin	wejI se aXigrahaNa karanA SurU
ADVP39 rapidly	wejI se
NP44 one	eka
CC46 and	Ora
VP42 acquire one	eka kA aXigrahaNa
VP47 begin using deep learning in their product or platform	apane uwpAxa yA pletaPoYrma meM gaharI sIKa kA iswemAla karanA SurU kareM
S49 using deep learning in their product or platform	apane uwpAxa yA pletaPoYrma meM gaharI sIKa kA iswemAla
VP50_LWG using	iswemAla karanA
NP52 deep learning	gaharI sIKa
PP55 in their product or platform	unake uwpAxa yA pletaPoYrma meM
NP57 their product or platform	unakA uwpAxa yA pletaPoYrma
CC60 or	yA
NN59 product	uwpAxa
NN61 platform	pletaPoYrma

----
00190	Deep learning has already proved useful in many soft- ware disciplines, including computer vision, speech and audio processing, natural language processing, robotics, bioinformatics and chemistry, video games, search engines, online advertising and nance.	 dIpa larniMga pahale se hI kaI soYPta veyara viXAoM meM upayogI sAbiwa huI hE , jisameM kaMpyUtara vijana , BARaNa Ora oYdiyo prosesiMga , prAkqwika BARA saMsAXana , robotiksa , jEva sUcanA vijFAna Ora rasAyana vijFAna , vIdiyo gema , Koja iMjana , oYnalAina vijFApana Ora viwwa SAmila hEM .		
189	189
S1 Deep learning has already proved useful in many soft - ware disciplines , including computer vision , speech and audio processing , natural language processing , robotics , bioinformatics and chemistry , video games , search engines , online advertising and nance .	gaharI sIKa pahale se hI kaI narama - veyara anuSAsanoM meM upayogI sAbiwa huI hE, jisameM kaMpyUtara vijana, BARaNa Ora oYdiyo prosesiMga, prAkqwika
NP2 Deep learning	gaharI sIKa
VP5_LWG has already proved	pahale hI sAbiwa ho cukA hE
ADVP7 already	pahale se hI
S11 useful	upayogI
ADJP12 useful	upayogI
PP14 in many soft - ware disciplines	kaI narama - jAgarUka anuSAsanoM meM
NP16 many soft - ware disciplines	kaI narama - jAgarUka anuSAsana
NML18 soft - ware	narama - veyara
NNS22 disciplines	anuSAsana
,23 ,	,
PP24 including computer vision , speech and audio processing , natural language processing , robotics , bioinformatics and chemistry , video games , search engines , online advertising and nance	jisameM kaMpyUtara vijana, BARaNa Ora oYdiyo prosesiMga, prAkqwika BARA prasaMskaraNa, robotiksa, bAyoteknomika Ora kemistrI, vIdiyo gema, sarca iM
NP26 computer vision , speech and audio processing , natural language processing , robotics , bioinformatics and chemistry , video games , search engines , online advertising and nance	kaMpyUtara xqRti, BARaNa Ora oYdiyo prosesiMga, prAkqwika BARA prasaMskaraNa, robotiksa, bAyo iMtaranetiksa Ora kemistrI, vIdiyo gema, sarca iMjana,
NP27 computer vision	kaMpyUtara kI xqRti
,30 ,	,
NP31 speech and audio processing	BARaNa Ora oYdiyo prasaMskaraNa
NML32 speech and audio	BARaNa Ora oYdiyo
CC34 and	Ora
NN33 speech	BARaNa
NN35 audio	oYdiyo
,37 ,	,
NP38 natural language processing	prAkqwika BARA prasaMskaraNa
NML39 natural language	prAkqwika BARA
,43 ,	,
NP44 robotics	robotiksa
NNS45 robotics	robotiksa
,46 ,	,
NP47 bioinformatics and chemistry	bAyo iMtaranetiksa Ora kemistrI
CC49 and	Ora
NN48 bioinformatics	bAyo iMtaraPaMkSanika
NN50 chemistry	rasAyana vijFAna
,51 ,	,
NP52 video games	vIdiyo gema
NNS54 games	Kela
,55 ,	,
NP56 search engines	Koja iMjana
NNS58 engines	iMjana
,59 ,	,
NP60 online advertising	oYnalAina vijFApana
CC63 and	Ora
NP64 nance	viwwa

----
0019	Chess is of course a very simple world, containing only sixty-four locations and thirty-two pieces that can move in only rigidly circumscribed ways.	 SawaraMja beSaka eka bahuwa hI sarala xuniyA hE , jisameM kevala sATa sWAna Ora wIsa - bawwIsa tukadZe hEM , jo kevala kaTora DaMga se cala sakawe hEM 		
18	18
S1 Chess is of course a very simple world , containing only sixty - four locations and thirty - two pieces that can move in only rigidly circumscribed ways .	SawaraMja beSaka eka bahuwa hI sAXAraNa xuniyA hE, jisameM kevala sATa-cAra sWAna Ora wIsa-wIna tukadZe SAmila hEM jo kevala DaMga se pariBARiwa warIkoM se Age baDZa
NP2 Chess	SawaraMja
VP4_LWG is of	kA hE
ADVP6 of course	beSaka
NP9 a very simple world , containing only sixty - four locations and thirty - two pieces that can move in only rigidly circumscribed ways	eka bahuwa hI sAXAraNa xuniyA, jisameM kevala sATa-cAra sWAna Ora wIsa-wIna-xo tukadZe SAmila hEM jo kevala DaMga se KadZe warIkoM se Age baDZa sakawe hEM
NP10 a very simple world	eka bahuwa hI sAXAraNa xuniyA
ADJP12 very simple	bahuwa hI sarala
,16 ,	,
VP17_LWG containing	jisameM
NP19 only sixty - four locations and thirty - two pieces that can move in only rigidly circumscribed ways	kevala sATa-cAra sWAna Ora wIsa-wIna-xo tukadZe jo sirPa DaMga se KadZe warIkoM se Age baDZa sakawe hEM
NP20 only sixty - four locations and thirty - two pieces	kevala sATa-cAra sWAna Ora wIsa-xo tukadZe
NP21 only sixty - four locations	kevala sATa- cAra sWAna
QP22 only sixty - four	kevala sATa-cAra
NNS27 locations	sWAna
CC28 and	Ora
NP29 thirty - two pieces	wIsa-xo tukadZe
QP30 thirty - two	wIsa-xo
NNS34 pieces	tukadZe
SBAR35 that can move in only rigidly circumscribed ways	vaha kevala gaMBIrawA se pariBARiwa warIkoM se Age baDZa sakawA hE
WHNP36 that	vaha
S38 can move in only rigidly circumscribed ways	kevala DaMga se KadZe warIkoM se Age baDZa sakawe hEM
VP39_LWG can move	Age baDZa sakawe hEM
PP43 in only rigidly circumscribed ways	kevala DaMga se pariBARiwa warIkoM meM
NP45 only rigidly circumscribed ways	kevala DaMga se pariBARiwa warIke
QP46 only rigidly	sirPa majabUwI se
NNS50 ways	warIke

----
00191	This book has been organized into three parts to best accommodate a variety of readers.	 isa puswaka ko wIna BAgoM meM saMgaTiwa kiyA gayA hE wAki viBinna prakAra ke pATakoM ko sarvowwama rUpa se samAyojiwa kiyA jA sake 		
190	190
S1 This book has been organized into three parts to best accommodate a variety of readers .	isa puswaka kA Ayojana wIna BAgoM meM kiyA gayA hE wAki pATakoM kI eka prakAra kA Ayojana kiyA jA sake
NP2 This book	yaha puswaka
VP5_LWG has been organized	Ayojana kiyA gayA hE
PP11 into three parts	wIna BAgoM meM
NP13 three parts	wIna hisse
NNS15 parts	BAgoM
S16 to best accommodate a variety of readers	pATakoM kI eka kisma ko sabase behawara DaMga se samAyojiwa karane ke lie
VP17_LWG to best accommodate	sabase acCA samAyojana karane ke lie
ADVP19 best	sarvaSreRTa
NP23 a variety of readers	kaI waraha ke pATaka
NP24 a variety	eka kisma kI
PP27 of readers	pATakoM kI
NP29 readers	pATaka
NNS30 readers	pATaka

----
00192	Part I introduces basic mathematical tools and machine learning concepts.	 BAga mEM buniyAxI gaNiwIya upakaraNa Ora maSIna sIKane avaXAraNAoM kA paricaya .		
191	191
S1 Part I introduces basic mathematical tools and machine learning concepts .	BAga mEM mUla gaNiwIya upakaraNoM Ora maSIna sIKa avaXAraNAoM ko praswuwa karawA hUM
NP2 Part	BAga
NP4 I	mEMne
VP6_LWG introduces	peSa karawA hE paricaya
NP8 basic mathematical tools and machine learning concepts	buniyAxI gaNiwIya upakaraNa Ora maSIna sIKane kI avaXArAeM
NML10 mathematical tools and machine learning	gaNiwIya upakaraNa Ora maSIna sIKanA
NML11 mathematical tools	gaNiwIya upakaraNa
NNS13 tools	upakaraNa
CC14 and	Ora
NML15 machine learning	maSIna sIKanA
NNS18 concepts	avaXAraNAez

----
00193	Part II describes the most established deep learning algorithms, which are essentially solved technologies.	 BAga 2 sabase sWApiwa gaharI sIKane elgorixama , jo anivArya rUpa se prOxyogikiyoM hala kara rahe hEM kA varNana karawA hE .		
192	192
S1 Part II describes the most established deep learning algorithms , which are essentially solved technologies .	BAga xviwIya meM sabase sWApiwa gaharI sIKa elgorixama kA varNana kiyA gayA hE, jo anivArya rUpa se hala kI gaI wakanIkoM kA varNana karawe hEM
NP2 Part II	BAga xviwIya
VP5_LWG describes	varNana
NP7 the most established deep learning algorithms , which are essentially solved technologies	sabase sWApiwa gaharI sIKa elgorixama, jo anivArya rUpa se hala kI gaI wakanIkoM
NP8 the most established deep learning algorithms	sabase sWApiwa gaharI sIKa elgorixama
ADJP10 most established	sabase sWApiwa
NML13 deep learning	gaharI sIKa
NNS16 algorithms	elgorixama
,17 ,	,
SBAR18 which are essentially solved technologies	jo anivArya rUpa se hala kI gaI wakanIkoM
WHNP19 which	jo ki
S21 are essentially solved technologies	anivArya rUpa se hala kI gaI wakanIkoM
VP22_LWG are essentially solved	anivArya rUpa se hala howe hEM
ADVP24 essentially	anivArya rUpa se
NP28 technologies	wakanIkoM
NNS29 technologies	wakanIkoM

----
00194	Part III describes more speculative ideas that are widely believed to be important for future research in deep learning.	 BAga III aXika sattA vicAroM kA varNana karawA hE jo vyApaka rUpa se gahare sIKane meM BaviRya ke anusaMXAna ke lie mahawvapUrNa mAnA jAwA hE .		
193	193
S1 Part III describes more speculative ideas that are widely believed to be important for future research in deep learning .	BAga III meM aXika anumAna lagAne vAle vicAroM kA varNana kiyA gayA hE jo gaharI sIKa meM BaviRya ke anusaMXAna ke lie vyApaka rUpa se mahawvapUrNa mAnA jAwA hE
NP2 Part III	BAga III
VP5_LWG describes	varNana
NP7 more speculative ideas that are widely believed to be important for future research in deep learning	aXika anumAna lagAne vAle vicAra jinheM gaharI sIKa meM BaviRya ke anusaMXAna ke lie bahuwa mahawvapUrNa mAnA jAwA hE
NP8 more speculative ideas	aXika atakaleM lagAne vAle vicAra
ADJP9 more speculative	aXika atakaleM
NNS12 ideas	vicAra
SBAR13 that are widely believed to be important for future research in deep learning	jise gaharI sIKa meM BaviRya ke anusaMXAna ke lie vyApaka rUpa se mahawvapUrNa mAnA jA rahA hE
WHNP14 that	vaha
S16 are widely believed to be important for future research in deep learning	gaharI sIKa meM BaviRya ke anusaMXAna ke lie vyApaka rUpa se mahawvapUrNa mAnA jA rahA hE
VP17_LWG are widely believed	badZe pEmAne para hEM mAnA
ADVP19 widely	vyApaka rUpa se
S23 to be important for future research in deep learning	gaharI sIKa meM BaviRya ke anusaMXAna ke lie mahawvapUrNa honA
VP24_LWG to be	honA cAhie
ADJP28 important for future research in deep learning	gaharI sIKa meM BaviRya ke anusaMXAna ke lie mahawvapUrNa
PP30 for future research in deep learning	gaharI sIKa meM BaviRya ke SoXa ke lie
NP32 future research in deep learning	gaharI sIKa meM BaviRya kA anusaMXAna
NP33 future research	BaviRya kA anusaMXAna
PP36 in deep learning	gaharI sIKa meM
NP38 deep learning	gaharI sIKa

----
00195	9 CHAPTER 1.	 9 CHAPTER 1 .		
194	194
FRAG1 9 CHAPTER 1 .	9 cEptara 1.
NP2 9	9
NP4 CHAPTER 1	cEptara 1

----
00196	INTRODUCTION Input	 InTRODUC inaputa		
195	195
NP1 INTRODUCTION Input	paricaya inaputa

----
00197	Hand- designed program	 haswanirmiwa progrAma		
196	196
NP1 Hand - designed program	hAWa- dijAina kiyA gayA kAryakrama
ADJP2 Hand - designed	hAWa- dijAina

----
00198	Output Input	 Autaputa inaputa		
197	197
NP1 Output Input	Autaputa inaputa

----
00199	Hand- designed features Mapping from features Output Input Features Mapping from features Output Input	 PIcara Autaputa inaputa se Autaputa inaputa PIcara mEpiMga se hAWa dijAina viSeRawAoM mEpiMga		
198	198
FRAG1 Hand - designed features Mapping from features Output Input Features Mapping from features Output Input	hAWa - dijAina kie gae PIcarsa mEpiMga se PIcarsa Autaputa inaputa PIcarsa mEpiMga se PIcarsa Autaputa inaputa
NP2 Hand	hAWa
VP5_LWG designed	dijAina
NP7 features Mapping from features Output Input Features Mapping from features Output Input	PIcarsa se mEpiMga PIcarsa Autaputa inaputa PIcarsa mEpiMga se PIcarsa Autaputa inaputa
NML8 features Mapping from features	PIcarsa se PIcarsa mEpiMga
NML9 features Mapping	PIcarsa mEpiMga
NNS10 features	PIcarsa
PP12 from features	PIcarsa se
NP14 features	PIcarsa
NNS15 features	PIcarsa
NML17 Input Features Mapping from features	inaputa ke PIcarsa PIcarsa se mEpiMga
NML18 Input Features Mapping	inaputa ke PIcarsa mEpiMga
NML19 Input Features	inaputa ke PIcarsa
PP23 from features	PIcarsa se
NP25 features	PIcarsa
NNS26 features	PIcarsa

----
00200	Simple features Mapping from features Output Additional layers of more abstract features Rule-based systems Classic machine learning Representation learning Deep learning Figure 1.5: Flowcharts showing how the dierent parts of an AI system relate to each other within dierent AI disciplines.	 viSeRawAoM Autaputa awirikwa parawoM se mEpiMga Ora aXika amUrwa suviXAoM niyama - paswa sistama klAsika maSIna sIKane prawiniXiwva ciwra 1 . 5 . Plocara yaha xarSAwA hE ki kEse eka eAI praNAlI ke viBinna BAgoM viBinna eAI viXAoM ke BIwara eka xUsare se saMbaMXiwa hE .		
199	199
FRAG1 Simple features Mapping from features Output Additional layers of more abstract features Rule - based systems Classic machine learning Representation learning Deep learning Figure 1.5 : Flowcharts showing how the dierent parts of an AI system relate to each other within dierent AI disciplines .	sarala suviXAoM se mEpaliMga niyama-AXAriwa praNAliyoM kI awirikwa paraweM niyama-AXAriwa praNAliyoM klAsika maSIna sIKanA riprajeMteSana dIpa larniMga A
NP2 Simple features Mapping from features Output	PIcarsa Autaputa se siMpala PIcarsa mEpiMga
NP3 Simple features	sAXAraNa suviXAeM
NNS5 features	PIcarsa
NP6 Mapping from features Output	PIcarsa Autaputa se mEpiMga
NML7 Mapping from features	PIcarsa se mEpiMga
NML8 Mapping	mEpiMga
PP10 from features	PIcarsa se
NP12 features	PIcarsa
NNS13 features	PIcarsa
NP15 Additional layers of more abstract features	aXika amUrwa viSeRawAoM kI awirikwa paraweM
NP16 Additional layers	awirikwa paraweM
NNS18 layers	paraweM
PP19 of more abstract features	aXika amUrwa viSeRawAoM kI
NP21 more abstract features	aXika amUrwa viSeRawAeM
NNS24 features	PIcarsa
NP25 Rule - based systems	niyama- AXAriwa praNAliyoM
NML26 Rule - based	niyama- AXAriwa
NNS30 systems	sistama
NP31 Classic machine learning Representation learning Deep learning Figure 1.5 : Flowcharts showing how the dierent parts of an AI system relate to each other within dierent AI disciplines	klAsika maSIna sIKa riprajeMteSana xIpa sIKane kA AMkadZA 1.5 sIKa rahA hE H PlocArta yaha xiKA rahA hE ki eAI praNAlI ke alaga-alaga hissa
NP32 Classic machine learning Representation learning Deep learning Figure 1.5	klAsika maSIna sIKa riprajeMteSana sIKa rahA hE dIpa larniMga Pigara 1.5
S43 Flowcharts showing how the dierent parts of an AI system relate to each other within dierent AI disciplines	PlocArta xiKAwe hEM ki viBinna eAI anuSAsanoM ke BIwara eka-xUsare se kEse saMbaMXiwa howA hE eAI praNAlI ke alaga-alaga hisse
NP44 Flowcharts	PlocArta
VP46_LWG showing	xiKA rahA hE
SBAR48 how the dierent parts of an AI system relate to each other within dierent AI disciplines	kEse alaga-alaga eAI anuSAsanoM ke BIwara eka-xUsare se saMbaMXiwa howA hE eAI praNAlI ke alaga-alaga hisse
WHADVP49 how	kEse
S51 the dierent parts of an AI system relate to each other within dierent AI disciplines	eka eAI praNAlI ke alaga-alaga hisse alaga-alaga eAI anuSAsanoM ke BIwara eka xUsare se saMbaMXiwa
NP52 the dierent parts of an AI system	eAI praNAlI ke alaga-alaga hissoM
NP53 the dierent parts	alaga-alaga hissoM
NNS56 parts	BAgoM
PP57 of an AI system	eka eAI praNAlI kI
NP59 an AI system	eka eAI praNAlI
VP63_LWG relate	saMbaMXiwa
PP65 to each other within dierent AI disciplines	alaga-alaga eAI anuSAsanoM ke BIwara eka xUsare ko
NP67 each other within dierent AI disciplines	eka xUsare ko alaga-alaga eAI anuSAsanoM ke BIwara
NP68 each other	eka xUsare ko
PP71 within dierent AI disciplines	alaga-alaga eAI anuSAsanoM ke BIwara
NP73 dierent AI disciplines	alaga-alaga eAI anuSAsana
NNS76 disciplines	anuSAsana

----
0020	Devising a successful chess strategy is a tremendous accomplishment, but the challenge is not due to the diculty of describing the set of chess pieces and allowable moves to the computer.	 eka saPala SawaraMja raNanIwi ko CodZa xenA eka jabaraxaswa upalabXi hE , lekina cunOwI SawaraMja tukadZe ke seta kA varNana karane Ora kaMpyUtara ke lie anumawi yogya cAla kI kaTinAI ke kAraNa nahIM hE .		
19	19
S1 Devising a successful chess strategy is a tremendous accomplishment , but the challenge is not due to the diculty of describing the set of chess pieces and allowable moves to the computer .	saPala SawaraMja kI raNanIwi ko xevI-xevawA karanA eka jabaraxaswa upalabXi hE, lekina cunOwI SawaraMja tukadZoM ke seta Ora kaMpyUtara ke anumawi xene kI ka
S2 Devising a successful chess strategy is a tremendous accomplishment	saPala SawaraMja kI raNanIwi ko xevI-xevawA karanA eka jabaraxaswa upalabXi hE
S3 Devising a successful chess strategy	eka saPala SawaraMja kI raNanIwi ko xevI-xevawA
VP4_LWG Devising	xevI-xevawA
NP6 a successful chess strategy	eka saPala SawaraMja kI raNanIwi
VP11_LWG is	hE
NP13 a tremendous accomplishment	eka jabaraxaswa upalabXi
,17 ,	,
CC18 but	lekina
S19 the challenge is not due to the diculty of describing the set of chess pieces and allowable moves to the computer	cunOwI SawaraMja tukadZoM ke seta Ora kaMpyUtara ko anumawi xene vAle kaxamoM ke varNana kI kaTinAI ke kAraNa nahIM hE
NP20 the challenge	cunOwI
VP23_LWG is not	nahIM hE
ADJP26 due to the diculty of describing the set of chess pieces and allowable moves to the computer	SawaraMja tukadZoM ke seta kA varNana karane kI kaTinAI Ora kaMpyUtara meM anumawi xene yogya cAla
PP28 to the diculty of describing the set of chess pieces and allowable moves to the computer	SawaraMja tukadZoM ke seta kA varNana karane kI kaTinAI Ora kaMpyUtara ke lie anumawi xene yogya cAla
NP30 the diculty of describing the set of chess pieces and allowable moves to the computer	SawaraMja tukadZoM ke seta kA varNana karane kI kaTinAI Ora kaMpyUtara ko anumawi xene yogya cAla
NP31 the diculty	muSkila
PP34 of describing the set of chess pieces and allowable moves to the computer	SawaraMja tukadZoM ke seta kA varNana karane Ora kaMpyUtara meM anumawi xene yogya cAla
S36 describing the set of chess pieces and allowable moves to the computer	SawaraMja tukadZoM ke seta kA varNana karawe hue Ora kaMpyUtara ko anumawi xene yogya kaxama
VP37_LWG describing	bawAwe hue
NP39 the set of chess pieces and allowable moves	SawaraMja tukadZoM kA seta Ora anumawipUrNa cAloM kA
NP40 the set	seta
PP43 of chess pieces and allowable moves	SawaraMja ke tukadZe Ora anumawipUrNa cAleM
NP45 chess pieces and allowable moves	SawaraMja ke tukadZe Ora anumawipUrNa cAleM
NP46 chess pieces	SawaraMja ke tukadZe
NNS48 pieces	tukadZe
CC49 and	Ora
NP50 allowable moves	anumawi xene yogya cAla
NNS52 moves	calawA hE
PP53 to the computer	kaMpyUtara ko
NP55 the computer	kaMpyUtara

----
00201	Shaded boxes indicate components that are able to learn from data.	 CAyAMkiwa bakse una GatakoM ko sUciwa karawe hEM jo detA se sIKane meM sakRama howe hEM 		
200	200
S1 Shaded boxes indicate components that are able to learn from data .	CAyA huA bakse una GatakoM ko iMgiwa karawA hE jo detA se sIKa sakawe hEM
NP2 Shaded boxes	CAyA huA bakse
NNS4 boxes	bakse
VP5_LWG indicate	saMkewa
NP7 components that are able to learn from data	Gataka jo detA se sIKa pA rahe hEM
NP8 components	Gataka
NNS9 components	Gataka
SBAR10 that are able to learn from data	jo detA se sIKa pA rahe hEM
WHNP11 that	vaha
S13 are able to learn from data	detA se sIKa pA rahe hEM
VP14_LWG are	hEM
ADJP16 able to learn from data	detA se sIKa pA rahe hEM
S18 to learn from data	detA se sIKane ke lie
VP19_LWG to learn	sIKane ke lie
PP23 from data	AMkadZoM se
NP25 data	detA
NNS26 data	detA

----
00202	Readers should feel free to skip parts that are not relevant given their interests or background.	 pATakoM ko una hissoM ko CodZane ke lie svawaMwra mahasUsa karanA cAhie jo unake hiwoM yA pqRTaBUmi ko XyAna meM raKawe hue prAsaMgika nahIM hEM .		
201	201
S1 Readers should feel free to skip parts that are not relevant given their interests or background .	pATakoM ko una hissoM ko CodZane ke lie svawaMwra mahasUsa karanA cAhie jo apane hiwa yA pqRTaBUmi ko xie gae nahIM hEM
NP2 Readers	pATaka
NNS3 Readers	pATaka
VP4_LWG should feel	mahasUsa karanA cAhie
ADJP8 free to skip parts that are not relevant	BAgoM ko CodZane ke lie svawaMwra jo prAsaMgika nahIM hEM
S10 to skip parts that are not relevant	Ese hissoM ko CodZane ke lie jo prAsaMgika nahIM hEM
VP11_LWG to skip	CodZane ke lie
NP15 parts that are not relevant	Ese hisse jo prAsaMgika nahIM hEM
NP16 parts	BAgoM
NNS17 parts	BAgoM
SBAR18 that are not relevant	vo prAsaMgika nahIM
WHNP19 that	vaha
S21 are not relevant	prAsaMgika nahIM hEM
VP22_LWG are not	nahIM hEM
ADJP25 relevant	prAsaMgika
PP27 given their interests or background	apane hiwoM yA pqRTaBUmi ko xeKawe hue
NP29 their interests or background	unake hiwa yA pqRTaBUmi
NP30 their interests	unake hiwa
NNS32 interests	hiwoM
CC33 or	yA
NP34 background	pqRTaBUmi

----
00203	Readers familiar with linear algebra, probability, and fundamental machine learning concepts can skip part I, for example, while those who just want to implement a working system need not read beyond part II.	 rEKika bIjagaNiwa , prAyikawA , Ora mUlaBUwa maSIna SikRaNa avaXAraNAoM se pariciwa pATaka BAga I ko CodZa sakawe hEM , uxAharaNa ke lie , jabaki jo loga sirPa eka kArya praNAlI ko kAryAnviwa karanA cAhawe hEM unheM BAga II se Age paDZane kI AvaSyakawA nahIM hE 		
202	202
S1 Readers familiar with linear algebra , probability , and fundamental machine learning concepts can skip part I , for example , while those who just want to implement a working system need not read beyond part II .	laGu aMkagaNiwa, saMBAvanA Ora mOlika maSIna sIKane vAle pATaka uxAharaNa ke lie BAga I ko CodZa sakawe hEM, uxAharaNa ke lie, jabaki jo sirPa eka
NP2 Readers familiar with linear algebra , probability , and fundamental machine learning concepts	laGu aMkagaNiwa, saMBAvanA Ora mOlika maSIna sIKane vAle avaXArAoM se pariciwa pATaka
NP3 Readers	pATaka
NNS4 Readers	pATaka
ADJP5 familiar with linear algebra , probability , and fundamental machine learning concepts	laGu aMkagaNiwa, saMBAvanA Ora mOlika maSIna sIKane vAlI avaXArAoM se pariciwa
PP7 with linear algebra , probability , and fundamental machine learning concepts	lAinara bIjagaNiwa, saMBAvanA Ora mOlika maSIna sIKane vAlI avaXArAoM ke sAWa
NP9 linear algebra , probability , and fundamental machine learning concepts	laGu aMkagaNiwa, saMBAvanA Ora mOlika maSIna sIKane vAlI avaXArAez
NP10 linear algebra	laGugaNiwa buxXijIvI
,13 ,	,
NP14 probability	saMBAvanA
,16 ,	,
CC17 and	Ora
NP18 fundamental machine learning concepts	mOlika maSIna sIKane kI avaXArAeM
NML20 machine learning	maSIna sIKanA
NNS23 concepts	avaXAraNAez
VP24_LWG can skip	CodZa sakawe hEM
NP28 part	hissA
S30 I	mEMne
NP31 I	mEMne
,33 ,	,
PP34 for example	uxAharaNa ke lie
NP36 example	uxAharaNa
,38 ,	,
SBAR39 while those who just want to implement a working system need not read beyond part II	jabaki jo loga sirPa eka kArya praNAlI ko lAgU karanA cAhawe hEM, unheM BAga xviwIya se pare nahIM paDZanA cAhie
S41 those who just want to implement a working system need not read beyond part II	jo loga sirPa eka kArya praNAlI ko lAgU karanA cAhawe hEM, unheM BAga xviwIya se pare nahIM paDZanA cAhie
NP42 those who just want to implement a working system	jo sirPa eka kArya praNAlI lAgU karanA cAhawe hEM
NP43 those	una logoM
SBAR45 who just want to implement a working system	jo sirPa eka kArya praNAlI lAgU karanA cAhawe hEM
WHNP46 who	kOna
S48 just want to implement a working system	sirPa eka kArya praNAlI lAgU karanA cAhawe hEM
ADVP49 just	basa
VP51_LWG want	cAhawe hEM
S53 to implement a working system	kArya praNAlI lAgU karane ke lie
VP54_LWG to implement	lAgU karane ke lie
NP58 a working system	eka kArya praNAlI
VP62_LWG need not read	paDZane kI jarUrawa nahIM
PP67 beyond part II	BAga xviwIya se pare
NP69 part II	BAga xviwIya

----
00204	To help choose which 10 CHAPTER 1.	 cunane meM maxaxa karane ke lie jo 10 CHAPTER 1 .		
203	203
S1 To help choose which 10 CHAPTER 1 .	cunane meM maxaxa karane ke lie jo 10 ceptara 1 hE
S2 To help	maxaxa ke lie
VP3_LWG To help	maxaxa ke lie
VP7_LWG choose	cuneM
SBAR9 which 10 CHAPTER 1	jo ki 10 cEptara 1
WHNP10 which	jo ki
S12 10 CHAPTER 1	10 cEptara 1
NP13 10 CHAPTER 1	10 cEptara 1

----
00205	INTRODUCTION 1.	 INTRODUCTION 1 .		
204	204
NP1 INTRODUCTION 1 .	paricaya 1.

----
00206	Introduction Part I: Applied Math and Machine Learning Basics 2.	 paricaya BAga I : anuprayukwa maTa Ora maSIna larniMga besika 2 .		
205	205
FRAG1 Introduction Part I : Applied Math and Machine Learning Basics 2 .	paricaya BAga I H eplAida mEWa eMda maSIna larniMga besika 2.
NP2 Introduction	paricaya
NP4 Part	BAga
FRAG6 I : Applied Math	mEM H eplAida mEWa
NP7 I	mEMne
NP10 Applied Math	aplAida mEWa
CC13 and	Ora
FRAG14 Machine Learning Basics 2	maSIna sIKanA besika 2
NP15 Machine Learning Basics	maSIna sIKanA besika
NP19 2	2

----
00207	Linear Algebra 3.	 rEKika bIjagaNiwa 3 .		
206	206
FRAG1 Linear Algebra 3 .	lAinara eljAbe_raja 3
NP2 Linear Algebra	lAinara egroPija
NP5 3	3

----
00208	Probability and Information Theory 4.	 saMBAvanA Ora sUcanA sixXAMwa 4 .		
207	207
FRAG1 Probability and Information Theory 4 .	saMBAvanA Ora sUcanA WyorI 4
NP2 Probability and Information Theory	saMBAvanA Ora sUcanA WyorI
CC4 and	Ora
NNP3 Probability	saMBAvanA
NNP5 Information	jAnakArI
NP7 4	4

----
00209	Numerical Computation 5.	 saMKyAwmaka kaMpyUtiMga 5 .		
208	208
FRAG1 Numerical Computation 5 .	saMKyAwmaka prawiRTA 5.
NP2 Numerical Computation	saMKyAwmaka prawiRTA
NP5 5	5

----
00210	Machine Learning Basics Part II:	 maSIna larniMga besika pArta II :		
209	209
FRAG1 Machine Learning Basics Part II :	maSIna larniMga besika BAga xviwIyaH
NP2 Machine Learning	maSIna sIKanA
NP5 Basics Part II	besika BAga xviwIya

----
0021	Chess can be completely described by a very brief list of completely formal rules, easily provided ahead of time by the programmer.	 SawaraMja pUrI waraha se pUrI waraha se OpacArika niyamoM kI eka bahuwa saMkRipwa sUcI xvArA varNiwa kiyA jA sakawA hE , AsAnI se progrAmara xvArA samaya se Age praxAna kI .		
20	20
S1 Chess can be completely described by a very brief list of completely formal rules , easily provided ahead of time by the programmer .	pUrI waraha se OpacArika niyamoM kI eka bahuwa saMkRipwa sUcI se SawaraMja ko pUrI waraha se varNiwa kiyA jA sakawA hE, jise progrAmara xvArA AsAnI se samaya se Age
NP2 Chess	SawaraMja
VP4_LWG can be completely described easily provided ahead	Age AsAnI se praxAna kiyA jA sakawA hE pUrI waraha se varNiwa
ADVP9 completely	pUrI waraha se
PP13 by a very brief list of completely formal rules	pUrI waraha se OpacArika niyamoM kI eka bahuwa saMkRipwa sUcI se
NP15 a very brief list of completely formal rules	pUrI waraha se OpacArika niyamoM kI eka bahuwa saMkRipwa sUcI
NP16 a very brief list	eka bahuwa hI saMkRipwa sUcI
ADJP18 very brief	bahuwa saMkRipwa
PP22 of completely formal rules	pUrI waraha se OpacArika niyamoM kA
NP24 completely formal rules	pUrI waraha se OpacArika niyama
ADJP25 completely formal	pUrI waraha se OpacArika
NNS28 rules	niyama
,29 ,	,
ADVP30 easily	AsAnI se
ADVP34 ahead of time	samaya se Age
PP36 of time	samaya kI
NP38 time	samaya
PP40 by the programmer	progrAmara xvArA
NP42 the programmer	progrAmara

----
00211	Deep Networks: Modern Practices 6.	 dIpa netavarksaH AXunika paxXawiyAz 6 .		
210	210
FRAG1 Deep Networks : Modern Practices 6 .	xIpa netavarka H AXunika aByAsa 6.
NP2 Deep	gaharA
NP4 Networks	netavarka
NP7 Modern Practices 6	AXunika aByAsa 6
NML8 Modern Practices	AXunika aByAsa

----
00212	Deep Feedforward Networks 7.	 dIpa PIda sarala netavarka 7 .		
211	211
FRAG1 Deep Feedforward Networks 7 .	dIpa PedaraPoYravarda netavarka 7
NP2 Deep Feedforward Networks	gaharI PIdaraPoYravarda netavarka
NP6 7	7

----
00213	Regularization 8.	 niyamiwIkaraNa 8 .		
212	212
NP1 Regularization 8 .	niyamiwIkaraNa 8

----
00214	Optimization 9.	 oYptImAijeSana 9 .		
213	213
NP1 Optimization 9 .	sWirIkaraNa 9.

----
00215	CNNs 10.	 sIenaena 10 .		
214	214
NP1 CNNs 10 .	sIenaena 10.
NNS2 CNNs	sIenaenaesa

----
00216	RNNs 11.	 RNNs 11 .		
215	215
NP1 RNNs 11 .	RBY 11.
NNS2 RNNs	Araenaenaesa

----
00217	Practical Methodology 12.	 vyAvahArika paxXawi 12 .		
216	216
FRAG1 Practical Methodology 12 .	vyAvahArika viXiSAswra 12
NP2 Practical Methodology	vyAvahArika paxXawiSAswra
NP5 12	12

----
00218	Applications Part III:	 anuprayoga BAga III :		
217	217
NP1 Applications Part III :	Avexana BAga IIIH

----
00219	Deep Learning Research 13.	 gahana aXyayana anusaMXAna 13 .		
218	218
FRAG1 Deep Learning Research 13 .	dIpa larniMga risarca 13.
NP2 Deep Learning Research	dIpa larniMga risarca
NP6 13	13

----
00220	Linear Factor Models 14.	 reKIya PEktara moYdala 14 .		
219	219
FRAG1 Linear Factor Models 14 .	lAinara PEktara moYdala 14
NP2 Linear Factor Models	lAinara PEktara moYdala
NP6 14	14

----
0022	Ironically, abstract and formal tasks that are among the most dicult mental undertakings for a human being are among the easiest for a computer.	 vidaMbanA yaha hE ki , amUrwa Ora OpacArika kArya jo eka mAnava ke lie sabase kaTina mAnasika upakramoM meM se hEM , eka kaMpyUtara ke lie sabase AsAna meM se hEM .		
21	21
S1 Ironically , abstract and formal tasks that are among the most dicult mental undertakings for a human being are among the easiest for a computer .	vidaMbanA Ora OpacArika kArya jo manuRya ke lie sabase muSkila mAnasika upakramoM meM SAmila hEM, vaha kaMpyUtara ke lie sabase AsAna meM se hEM
ADVP2 Ironically	vidaMbanA se
,4 ,	,
NP5 abstract and formal tasks that are among the most dicult mental undertakings for a human being	amUrwa Ora OpacArika kArya jo manuRya ke lie sabase muSkila mAnasika upakramoM meM SAmila hEM
NP6 abstract and formal tasks	amUrwa Ora OpacArika kArya
ADJP7 abstract and formal	amUrwa Ora OpacArika
CC9 and	Ora
JJ8 abstract	sAraxarSI
JJ10 formal	OpacArika
NNS11 tasks	kAryoM
SBAR12 that are among the most dicult mental undertakings for a human being	jo iMsAna ke lie sabase muSkila mAnasika upakramoM meM SAmila
WHNP13 that	vaha
S15 are among the most dicult mental undertakings for a human being	iMsAna ke lie sabase muSkila mAnasika upakramoM meM SAmila hEM
VP16_LWG are	hEM
PP18 among the most dicult mental undertakings for a human being	iMsAna ke lie sabase muSkila mAnasika upakramoM meM SAmila
NP20 the most dicult mental undertakings for a human being	iMsAna ke lie sabase muSkila mAnasika upakrama
NP21 the most dicult mental undertakings	sabase muSkila mAnasika upakrama
ADJP23 most dicult	sabase muSkila
NNS27 undertakings	upakrama
PP28 for a human being	iMsAna ke lie
NP30 a human being	eka iMsAna
VP34_LWG are	hEM
PP36 among the easiest for a computer	kaMpyUtara ke lie sabase AsAna meM se
NP38 the easiest for a computer	kaMpyUtara ke lie sabase AsAna
NP39 the easiest	sabase AsAna
PP42 for a computer	eka kaMpyUtara ke lie
NP44 a computer	eka kaMpyUtara

----
00221	Autoencoders 15.	 oYtonakodarsa 15 .		
220	220
NP1 Autoencoders 15 .	oYtonakodara 15.
NNS2 Autoencoders	oYtonakodara

----
00222	Representation Learning 16.	 prawiniXiwva sIKanA 16 .		
221	221
FRAG1 Representation Learning 16 .	riprajeMteSana sIKanA 16
NP2 Representation Learning	riprajeMteSana sIKanA
NP5 16	16

----
00223	Structured Probabilistic Models 17.	 saMraciwa saMBAvI moYdala 17 .		
222	222
FRAG1 Structured Probabilistic Models 17 .	saMraciwa prAsaMgika moYdala 17
NP2 Structured Probabilistic Models	saMraciwa prAsaMgika moYdala
NP6 17	17

----
00224	Monte Carlo Methods 18.	 moMte kArlo meWadsa 18		
223	223
FRAG1 Monte Carlo Methods 18 .	moMte kArlo warIke 18
NP2 Monte Carlo Methods	moMte kArlo warIke
NP6 18	18

----
00225	Partition Function 19.	 viBAjana kArya 19 .		
224	224
FRAG1 Partition Function 19 .	viBAjana kArya 19
NP2 Partition	viBAjana
NP4 Function 19	PaMkSana 19

----
00226	Inference 20.	 anumAna 20 .		
225	225
S1 Inference 20 .	anupAwa 20.
VP2_LWG Inference	inxriyawA
NP4 20	20

----
00227	Deep Generative Models Figure 1.6: The high-level organization of the book.	 dIpa jenareSana moYdala ciwra 1 . 6 : puswaka kA ucca swarIya saMgaTana 		
226	226
NP1 Deep Generative Models Figure 1.6 : The high - level organization of the book .	xIpa uxAra AxarSoM kA AMkadZA 1.6H puswaka kA ucca-swarIya saMgaTana
NP2 Deep Generative Models Figure 1.6	xIpa uxAra AxarSoM kA AMkadZA 1.6
NP3 Deep Generative Models	xIpa uxAra moYdala
NP7 Figure 1.6	AMkadZA 1.6
NP11 The high - level organization of the book	puswaka kA ucca swarIya saMgaTana
NP12 The high - level organization	ucca swarIya saMgaTana
NML14 high - level	ucca - swara
PP19 of the book	kiwAba kI
NP21 the book	kiwAba

----
00228	An arrow from one chapter to another indicates that the former chapter is prerequisite material for understanding the latter.	 eka aXyAya se xUsare aXyAya waka kA wIra isa bAwa kA saMkewa xewA hE ki pUrva aXyAya uwwarArxXa ko samaJane ke lie AvaSyaka sAmagrI hE 		
227	227
S1 An arrow from one chapter to another indicates that the former chapter is prerequisite material for understanding the latter .	eka aXyAya se xUsare aXyAya waka eka wIra saMkewa xewA hE ki pUrva aXyAya paScAwa ko samaJane ke lie AvaSyaka sAmagrI hE
NP2 An arrow from one chapter to another	eka aXyAya se xUsare aXyAya waka eka wIra
NP3 An arrow	eka wIra
PP6 from one chapter to another	eka aXyAya se xUsare aXyAya
NP8 one chapter to another	eka aXyAya xUsare ko
NP9 one chapter	eka aXyAya
PP12 to another	xUsare ko
NP14 another	eka Ora
VP16_LWG indicates	iMgiwa karawA hE
SBAR18 that the former chapter is prerequisite material for understanding the latter	pUrva aXyAya bAxa meM samaJane ke lie anivArya sAmagrI hE
S20 the former chapter is prerequisite material for understanding the latter	pUrva aXyAya bAxa meM samaJane ke lie anivArya sAmagrI hE
NP21 the former chapter	pUrva aXyAya
VP25_LWG is	hE
NP27 prerequisite material for understanding the latter	bAxa meM samaJane ke lie AvaSyaka sAmagrI
NP28 prerequisite material	AvaSyaka sAmagrI
PP31 for understanding the latter	bAxa meM samaJane ke lie
S33 understanding the latter	bAxa meM samaJanA
VP34_LWG understanding	samaJa
NP36 the latter	bAxa meM

----
00229	11 CHAPTER 1.	 11 CHAPTER 1 .		
228	228
FRAG1 11 CHAPTER 1 .	11 cEptara 1.
NP2 11	11
NP4 CHAPTER 1	cEptara 1

----
00230	INTRODUCTION chapters to read, gure 1.6 provides a owchart showing the high-level organization of the book.	 aXyAyoM ko paDZane ke lie , AMkadZA 1 . 6 eka pravAha saMciwra praxAna karawA hE jo puswaka ke ucca swara ke saMgaTana ko xarSAwA hE 		
229	229
S1 INTRODUCTION chapters to read , gure 1.6 provides a owchart showing the high - level organization of the book .	paDZane ke lie paricaya, AMkadZA 1.6 puswaka ke ucca-swarIya saMgaTana ko xiKAne vAlA pravAhapawra praxAna karawA hE
NP2 INTRODUCTION chapters to read , gure 1.6	paDZane ke lie aXyAya, AMkadZA 1.6
NNS4 chapters	aXyAya
S5 to read , gure 1.6	paDZane ke lie, AMkadZA 1.6
VP6_LWG to read gure	AMkadZA paDZane ke lie
,10 ,	,
NP12 1.6	1.6
VP14_LWG provides	praxAna karawA hE
NP16 a owchart showing the high - level organization of the book	puswaka ke ucca swarIya saMgaTana ko xiKA rahA eka PlocArta
NP17 a owchart	eka PlocArta
VP20_LWG showing	xiKA rahA hE
NP22 the high - level organization of the book	puswaka kA ucca swarIya saMgaTana
NP23 the high - level organization	ucca swarIya saMgaTana
NML25 high - level	ucca - swara
PP30 of the book	kiwAba kI
NP32 the book	kiwAba

----
0023	Computers have long been able to defeat even the best human chess player but only recently have begun matching some of the abilities of average human beings to recognize objects or speech.	 kaMpyUtara laMbe samaya se sarvaSreRTa mAnava SawaraMja KilAdZI ko BI harAne meM saPala rahe hEM , lekina kevala hAla hI meM Osawa mAnava kI kuCa kRamawAoM kA milAna vaswuoM yA BARaNa ko pahacAnane ke lie SurU kara xiyA hE .		
22	22
S1 Computers have long been able to defeat even the best human chess player but only recently have begun matching some of the abilities of average human beings to recognize objects or speech .	kaMpyUtara laMbe samaya se mAnava SawaraMja ke sarvaSreRTa KilAdZI ko BI harAne meM sakRama rahe hEM lekina hAla hI meM Osawa iMsAna kI kuCa kRamawAoM kA milAna kara
NP2 Computers	kaMpyUtara
VP4_LWG have long been only have begun	laMbe samaya se hI SurU hue hEM
ADVP7 long	laMbe samaya waka
ADJP11 able to defeat even the best human chess player	yahAM waka ki sarvaSreRTa mAnava SawaraMja KilAdZI ko BI harAne meM sakRama
S13 to defeat even the best human chess player	yahAM waka ki sarvaSreRTa mAnava SawaraMja KilAdZI ko harAne ke lie
VP14_LWG to defeat even	parAjaya ke lie BI
ADVP18 even	yahAM waka ki
NP20 the best human chess player	sarvaSreRTa mAnava SawaraMja KilAdZI
CC26 but	lekina
VP5 have long been able to defeat even the best human chess player	laMbe samaya se mAnava SawaraMja ke sarvaSreRTa KilAdZI ko BI harAne meM sakRama rahe hEM
ADVP27 only recently	kevala hAla hI meM
S34 matching some of the abilities of average human beings to recognize objects or speech	vaswuoM yA BARaNa ko pahacAnane ke lie Osawa iMsAna kI kuCa kRamawAoM kA milAna
VP35_LWG matching	milAna
NP37 some of the abilities of average human beings	Osawa iMsAna kI kuCa kRamawAeM
NP38 some	kuCa
PP40 of the abilities of average human beings	Osawa iMsAna kI kRamawAoM kA
NP42 the abilities of average human beings	Osawa iMsAna kI kRamawAeM
NP43 the abilities	kRamawAoM
NNS45 abilities	kRamawAoM
PP46 of average human beings	Osawa iMsAna kA
NP48 average human beings	Osawa iMsAna
NNS51 beings	prANI
S52 to recognize objects or speech	vaswuoM yA BARaNa ko pahacAnane ke lie
VP53_LWG to recognize	pahacAnane ke lie
NP57 objects or speech	vaswueM yA BARaNa
NNS58 objects	vaswueM
CC59 or	yA
NN60 speech	BARaNa

----
00231	We do assume that all readers come from a computer science background.	 hama yaha jarUra mAnawe hEM ki saBI pATaka kaMpyUtara vijFAna kI pqRTaBUmi se Awe hEM 		
230	230
S1 We do assume that all readers come from a computer science background .	hama mAnawe hEM ki saBI pATaka kaMpyUtara sAiMsa bEkagrAuMda se Awe hEM
NP2 We	hama
VP4_LWG do assume	mAna lenA
SBAR8 that all readers come from a computer science background	ki kampyUtara sAiMsa bEkagrAuMda se Awe hEM saBI pATaka
S10 all readers come from a computer science background	saBI pATaka kampyUtara sAiMsa bEkagrAuMda se Awe hEM
NP11 all readers	saBI pATaka
NNS13 readers	pATaka
VP14_LWG come	Ao
PP16 from a computer science background	eka kaMpyUtara sAiMsa bEkagrAuMda se
NP18 a computer science background	eka kaMpyUtara sAiMsa bEkagrAuMda
NML20 computer science	kampyUtara sAiMsa

----
00232	We assume familiarity with programming, a basic understanding of computational performance issues, complexity theory, introductory level calculus and some of the terminology of graph theory.	 hama progrAmiMga se pariciwa hEM , kampyUteSanala niRpAxana muxxoM kI eka buniyAxI samaJa , jatilawA sixXAMwa , paricayAwmaka swara kI paWarI Ora grAPa sixXAMwa kI kuCa SabxAvalI .		
231	231
S1 We assume familiarity with programming , a basic understanding of computational performance issues , complexity theory , introductory level calculus and some of the terminology of graph theory .	hama progrAmiMga ke sAWa pariciwawA, kaMpyUtarIya praxarSana ke muxxoM kI eka mUla samaJa, jatilawA sixXAMwa, paricaya swarIya gaNanA Ora grAPa sixXAMwa kI kuCa aMwar
NP2 We	hama
VP4_LWG assume	mAna lIjie
NP6 familiarity with programming , a basic understanding of computational performance issues , complexity theory , introductory level calculus and some of the terminology of graph theory	progrAmiMga se pariciwawA, kampyUteSanala praxarSana ke muxxoM kI eka mUla samaJa, jatilawA sixXAMwa, paricaya swarIya gaNanA Ora grAPa sixXAMwa kI kuCa avaXAraNA
NP7 familiarity with programming	progrAmiMga se pariciwawA
NP8 familiarity	pariciwawA
PP10 with programming	progrAmiMga ke sAWa
NP12 programming	progrAmiMga
,14 ,	,
NP15 a basic understanding of computational performance issues	kampyUteSanala praxarSana ke muxxoM kI buniyAxI samaJa
NP16 a basic understanding	eka buniyAxI samaJa
PP20 of computational performance issues	kampyUteSanala praxarSana ke muxxe
NP22 computational performance issues	kampyUteSanala praxarSana ke muxxe
NNS25 issues	muxxe
,26 ,	,
NP27 complexity theory	jatilawA sixXAMwa
,30 ,	,
NP31 introductory level calculus	paricaya swarIya gaNanA
CC35 and	Ora
NP36 some of the terminology of graph theory	grAPa sixXAMwa kI kuCa samApwi
NP37 some	kuCa
PP39 of the terminology of graph theory	grAPa sixXAMwa kI samApwi kA
NP41 the terminology of graph theory	grAPa sixXAMwa kA aMwarjFAna
NP42 the terminology	aMwarjFAna
PP45 of graph theory	grAPa sixXAMwa kA
NP47 graph theory	grAPa sixXAMwa

----
00233	1.2 Historical Trends in Deep Learning	 dIpa larniMga meM 1 . 2 EwihAsika wanAva		
232	232
NP1 1.2 Historical Trends in Deep Learning	1.2 gIpa larniMga meM EwihAsika treMda
NP2 1.2 Historical Trends	1.2 EwihAsika ruJAna
PP6 in Deep Learning	dIpa larniMga meM
NP8 Deep Learning	gaharI sIKa

----
00234	It is easiest to understand deep learning with some historical context.	 gahare aXyayana ko kisI EwihAsika saMxarBa ke sAWa samaJanA sabase AsAna hE 		
233	233
S1 It is easiest to understand deep learning with some historical context .	kuCa EwihAsika saMxarBa ke sAWa gaharI sIKa ko samaJanA sabase AsAna hE
NP2 It	yaha
VP4_LWG is	hE
ADJP6 easiest	sabase AsAna
S8 to understand deep learning with some historical context	kuCa EwihAsika saMxarBa se gaharI sIKa samaJane ke lie
VP9_LWG to understand	samaJane ke lie
NP13 deep learning	gaharI sIKa
PP16 with some historical context	kuCa EwihAsika saMxarBa ke sAWa
NP18 some historical context	kuCa EwihAsika saMxarBa

----
00235	Rather than providing a detailed history of deep learning, we identify a few key trends:	 gaharI sIKane kA eka viswqwa iwihAsa praxAna karane ke bajAya , hama kuCa pramuKa pravqwwiyoM kI pahacAnaH		
234	234
S1 Rather than providing a detailed history of deep learning , we identify a few key trends :	gaharI sIKa kA viswqwa iwihAsa upalabXa karAne ke bajAya, hama kuCa pramuKa ruJAnoM kI pahacAna karawe hEM?
PP2 Rather than providing a detailed history of deep learning	gaharI sIKa kA viswqwa iwihAsa upalabXa karAne ke bajAya balki
S5 providing a detailed history of deep learning	gaharI sIKa kA viswqwa iwihAsa praxAna karawe hue
VP6_LWG providing	praxAna karanA
NP8 a detailed history of deep learning	gaharI sIKa kA viswqwa iwihAsa
NP9 a detailed history	eka viswqwa iwihAsa
PP13 of deep learning	gaharI sIKa kA
NP15 deep learning	gaharI sIKa
,18 ,	,
NP19 we	hama
VP21_LWG identify	pahacAna
NP23 a few key trends	kuCa pramuKa ruJAna
QP24 a few	kuCa
NNS28 trends	ruJAna

----
00236	 Deep learning has had a long and rich history, but has gone by many names, reecting dierent philosophical viewpoints, and has waxed and waned in popularity.	  gahana aXyayana kA eka laMbA Ora samqxXa iwihAsa rahA hE , lekina kaI nAmoM se calA gayA hE , viBinna xArSanika xqRtikoNoM ko prawibiMbiwa , Ora lokapriyawA meM vqxXi huI hE Ora icCuka hE .		
235	235
S1  Deep learning has had a long and rich history , but has gone by many names , reecting dierent philosophical viewpoints , and has waxed and waned in popularity .	 gaharI sIKa kA laMbA Ora samqxXa iwihAsa rahA hE, lekina kaI nAmoM se calA gayA hE, alaga-alaga xArSanika xqRtikoNoM ko xarSAwA hE, Ora
NP2  Deep learning	 gaharI sIKa
NML3  Deep	 gaharA
VP7_LWG has had has gone has waxed waned	calA gayA hE calA gayA hE moma laharAyA
NP12 a long and rich history	eka laMbe Ora samqxXa iwihAsa
ADJP14 long and rich	laMbe Ora amIra
CC16 and	Ora
JJ15 long	laMbe samaya waka
JJ17 rich	amIra
,19 ,	,
CC20 but	lekina
VP21 has gone by many names , reecting dierent philosophical viewpoints	kaI nAmoM se calA gayA hE, alaga-alaga xArSanika xqRtikoNoM ko xarSAwe hue
PP25 by many names	kaI nAmoM se
NP27 many names	kaI nAma
NNS29 names	nAma
,30 ,	,
S31 reecting dierent philosophical viewpoints	alaga-alaga xArSanika xqRtikoNoM ko prawibiMbiwa karanA
VP32_LWG reecting	prawibiMbiwa
NP34 dierent philosophical viewpoints	alaga-alaga xArSanika xqRtikoNa
NNS37 viewpoints	xqRtikoNa
,38 ,	,
CC39 and	Ora
VP40 has waxed and waned in popularity	lokapriyawA meM moma Ora laharAyA hE
CC44 and	Ora
VBN43 waxed	momabawwI
VBN45 waned	jAma kara xiyA gayA
PP46 in popularity	lokapriyawA meM
NP48 popularity	lokapriyawA

----
00237	 Deep learning has become more useful as the amount of available training data has increased.	  gahana aXyayana aXika upayogI ho gayA hE kyoMki upalabXa praSikRaNa detA kI mAwrA meM vqxXi huI hE .		
236	236
S1  Deep learning has become more useful as the amount of available training data has increased .	 upalabXa praSikRaNa AMkadZe kI mAwrA baDanYe se gaharI sIKa aXika upayogI ho gaI hE
NP2  Deep learning	 gaharI sIKa
NML3  Deep	 gaharA
VP7_LWG has become	bana gayA hE
S11 more useful	aXika upayogI
ADJP12 more useful	aXika upayogI
SBAR15 as the amount of available training data has increased	jEse-jEse upalabXa praSikRaNa ke AMkadZoM kI mAwrA baDZI hE
S17 the amount of available training data has increased	upalabXa praSikRaNa AMkadZe kI rASi baDZI
NP18 the amount of available training data	upalabXa praSikRaNa detA kI rASi
NP19 the amount	rASi
PP22 of available training data	upalabXa praSikRaNa ke AMkadZe
NP24 available training data	upalabXa praSikRaNa kA AMkadZA
NNS27 data	detA
VP28_LWG has increased	baDZa gaI hE

----
00238	 Deep learning models have grown in size over time as computer infrastructure (both hardware and software) for deep learning has improved.	  dIpa larniMga moYdala samaya ke sAWa AkAra meM vqxXi huI hE ke rUpa meM kaMpyUtara buniyAxI DAMce ( gaharI sIKane ke lie hArdaveyara Ora soYPtaveyara ) meM suXAra huA hE .		
237	237
S1  Deep learning models have grown in size over time as computer infrastructure ( both hardware and software ) for deep learning has improved .	 gaharI sIKa ke lie kaMpyUtara iMPrAstrakcara (hArdaveyara Ora soYPtaveyara xonoM) jEse samaya ke AkAra meM gaharI sIKa baDZI hE
NP2  Deep learning models	 gaharI sIKa ke moYdala
NML3  Deep learning	 gaharI sIKa
NML4  Deep	 gaharA
NNS8 models	moYdala
VP9_LWG have grown	badZe ho gae hEM
PP13 in size over time	samaya se aXika AkAra meM
NP15 size over time	samaya se AkAra
NP16 size	AkAra
PP18 over time	samaya ke sAWa
NP20 time	samaya
SBAR22 as computer infrastructure ( both hardware and software ) for deep learning has improved	kyoMki gaharI sIKa ke lie kaMpyUtara iMPrAstrakcara (hArdaveyara Ora soYPtaveyara xonoM) meM suXAra huA hE
S24 computer infrastructure ( both hardware and software ) for deep learning has improved	gaharI sIKa ke lie kaMpyUtara iMPrAstrakcara (hArdaveyara Ora soYPtaveyara xonoM) meM suXAra huA hE
NP25 computer infrastructure ( both hardware and software ) for deep learning	gaharI sIKa ke lie kaMpyUtara iMPrAstrakcara (hArdaveyara Ora soYPtaveyara xonoM)
NP26 computer infrastructure ( both hardware and software )	kaMpyUtara iMPrAstrakcara (hArdaveyara Ora soYPtaveyara xonoM)
NML27 computer infrastructure	kaMpyUtara iMPrAstrakcara
NML31 both hardware and software	hArdaveyara Ora soYPtaveyara xonoM

----
00239	 Deep learning has solved increasingly complicated applications with increasing accuracy over time.	  gahana SikRaNa samaya ke sAWa baDZawI satIkawA ke sAWa jatila anuprayogoM ko hala kiyA hE .		
238	238
S1  Deep learning has solved increasingly complicated applications with increasing accuracy over time .	 gaharI sIKa ne samaya ke sAWa baDZawI satIkawA ke sAWa wejI se jatila AvexanoM ko hala kara xiyA hE
NP2  Deep learning	 gaharI sIKa
NML3  Deep	 gaharA
VP7_LWG has solved	sulaJa cukA hE
NP11 increasingly complicated applications	wejI se jatila Avexana
ADJP12 increasingly complicated	wejI se jatila
NNS15 applications	Avexana
PP16 with increasing accuracy over time	samaya ke sAWa baDZawI satIkawA ke sAWa
NP18 increasing accuracy over time	samaya ke sAWa baDZa rahI satIkawA
NP19 increasing accuracy	baDZawI satIkawA
PP22 over time	samaya ke sAWa
NP24 time	samaya

----
00240	1.2.1	 1 . 2		
239	239
FRAG1 1.2.1	1. 1.
NP2 1.2.1	1. 1.

----
0024	A persons everyday life requires an immense amount of knowledge about the world.	 vyakwi ke xEnika jIvana ke lie viSva ke bAre meM awyaXika jFAna kI AvaSyakawA howI hE 		
23	23
S1 A person s everyday life requires an immense amount of knowledge about the world .	eka vyakwi kI rojamarrA kI jiMxagI xuniyA ke bAre meM asIma mAwrA meM jFAna kI AvaSyakawA howI hE
NP2 A person s everyday life	eka vyakwi kI rojZamarrA kI jiMxagI
NML4 person s	vyakwi kA
VP9_LWG requires	jarUrawa hE
NP11 an immense amount of knowledge	jFAna kI awyaXika mAwrA
NP12 an immense amount	eka apAra rASi
PP16 of knowledge	jFAna kA
NP18 knowledge	jFAna
PP20 about the world	xuniyA ke bAre meM
NP22 the world	xuniyA

----
00241	The Many Names and Changing Fortunes of Neural Net- works	 nyurala neta ke aneka nAma Ora parivarwanakArI kArya		
240	240
UCP1 The Many Names and Changing Fortunes of Neural Net - works	waMwrikA neta ke kaI nAma Ora baxalawe PoYrcyUna- kAma
NP2 The Many Names	kaI nAma
NNS5 Names	nAma
CC6 and	Ora
S7 Changing Fortunes of Neural Net - works	waMwrikA neta ke PoYrcyUjZa baxalanA- kAma karawA hE
VP8_LWG Changing	baxala rahA hE
NP10 Fortunes of Neural Net - works	waMwrikA jAla ke PoYrcyU- kAma karawA hE
NP11 Fortunes	PoYrcyUjZa
PP13 of Neural Net - works	waMwrikA jAla kA- kAma
NP15 Neural Net - works	waMwrikA jAla- kAma karawA hE
NNS19 works	kAma karawA hE

----
00242	We expect that many readers of this book have heard of deep learning as an exciting new technology, and are surprised to see a mention of history in a book about an emerging eld.	 hama ummIxa karawe hEM ki isa puswaka ke kaI pATakoM ne eka romAMcaka naI prOxyogikI ke rUpa meM gahana SikRaNa ke bAre meM sunA hE , Ora eka uBarawe kRewra ke bAre meM eka puswaka meM gEwihAsika kA ulleKa xeKakara AScaryacakiwa hEM .		
241	241
S1 We expect that many readers of this book have heard of deep learning as an exciting new technology , and are surprised to see a mention of  history  in a book about an emerging eld .	hameM ummIxa hE ki isa puswaka ke kaI pATakoM ne eka romAMcaka naI wakanIka ke rUpa meM gaharI sIKa ke bAre meM sunA hE, Ora eka uBarawe kRewra ke bAre meM eka
NP2 We	hama
VP4_LWG expect	ummIxa
SBAR6 that many readers of this book have heard of deep learning as an exciting new technology , and are surprised to see a mention of  history  in a book about an emerging eld	isa kiwAba ke kaI pATakoM ne eka romAMcaka naI wakanIka ke rUpa meM gaharI sIKa ke bAre meM sunA hE, Ora uBarawe kRewra ke bAre meM eka kiwAba meM iwihAsa kA
S8 many readers of this book have heard of deep learning as an exciting new technology , and are surprised to see a mention of  history  in a book about an emerging eld	isa puswaka ke kaI pATakoM ne eka romAMcaka naI wakanIka ke rUpa meM gaharI sIKa ke bAre meM sunA hE, Ora uBarawe kRewra ke bAre meM eka kiwAba meM iwihAsa kA
NP9 many readers of this book	isa kiwAba ke kaI pATaka
NP10 many readers	kaI pATaka
NNS12 readers	pATaka
PP13 of this book	isa puswaka kI
NP15 this book	yaha puswaka
VP18_LWG have heard are	sunA hE nA.
PP23 of deep learning	gaharI sIKa kA
NP25 deep learning	gaharI sIKa
PP28 as an exciting new technology	eka romAMcaka naI wakanIka ke rUpa meM
NP30 an exciting new technology	eka romAMcaka naI wakanIka
,35 ,	,
CC36 and	Ora
VP37 are surprised to see a mention of  history  in a book about an emerging eld	uBarawe kRewra ke bAre meM eka kiwAba meM iwihAsa kA jikra xeKakara hErAna
ADJP39 surprised to see a mention of  history  in a book about an emerging eld	eka uBarawe kRewra ke bAre meM eka kiwAba meM iwihAsa kA jikra xeKakara hErAna
S41 to see a mention of  history  in a book about an emerging eld	eka uBarawe kRewra ke bAre meM eka kiwAba meM iwihAsa kA ulleKa xeKane ke lie
VP42_LWG to see	xeKane ke lie
NP46 a mention of  history 	iwihAsa kA ulleKa
NP47 a mention	eka ulleKa
PP50 of  history 	iwihAsa kA
NP52  history 	iwihAsa
PP56 in a book about an emerging eld	eka uBarawe kRewra ke bAre meM eka kiwAba meM
NP58 a book about an emerging eld	eka uBarawe kRewra ke bAre meM eka kiwAba
NP59 a book	eka kiwAba
PP62 about an emerging eld	eka uBarawe kRewra ke bAre meM
NP64 an emerging eld	eka uBarawA huA kRewra

----
00243	In fact, deep learning dates back to the 1940s.	 vAswava meM , gaharI sIKane kI wArIKeM 1940 ke xaSaka meM vApasa .		
242	242
S1 In fact , deep learning dates back to the 1940s .	vAswava meM, gaharI sIKa vApasa 1940 ke xaSaka meM AwI hE
PP2 In fact	vAswava meM
NP4 fact	waWya
,6 ,	,
NP7 deep learning	gaharI sIKa
VP10_LWG dates back	wArIKeM vApasa
PRT12 back	vApasa
PP14 to the 1940s	1940 ke xaSaka waka
NP16 the 1940s	1940 ke xaSaka meM
NNS18 1940s	1940 ke xaSaka meM

----
00244	Deep learning only appears to be new, because it was relatively unpopular for several years preceding its current popularity, and because it has gone through many dierent names, only recently being called deep learning.	 gahana aXyayana kevala nayA prawIwa howA hE , kyoMki yaha apanI varwamAna lokapriyawA se pahale kaI varRoM ke lie apekRAkqwa alokapriya WA , Ora kyoMki yaha kaI alaga alaga nAmoM ke mAXyama se calA gayA hE , hAla hI meM xepa aXigama kahA jA rahA hE .		
243	243
S1 Deep learning only appears to be new , because it was relatively unpopular for several years preceding its current popularity , and because it has gone through many dierent names , only recently being called  deep learning .	gaharI sIKa kevala naI prawIwa howI hE, kyoMki yaha apanI mOjUxA lokapriyawA se pahale kaI varRoM ke lie apekRAkqwa apracaliwa WA, Ora kyoMki yaha
NP2 Deep learning	gaharI sIKa
ADVP5 only	kevala
VP7_LWG appears	xiKAI xewA hE
S9 to be new , because it was relatively unpopular for several years preceding its current popularity , and because it has gone through many dierent names , only recently being called  deep learning	nayA honA, kyoMki yaha apanI mOjUxA lokapriyawA se pahale kaI varRoM ke lie apekRAkqwa apracaliwa WA, Ora kyoMki yaha kaI alaga-alaga nAmoM se
VP10_LWG to be	honA cAhie
ADJP14 new	nayA
,16 ,	,
SBAR17 because it was relatively unpopular for several years preceding its current popularity , and because it has gone through many dierent names , only recently being called  deep learning	kyoMki yaha apanI mOjUxA lokapriyawA se pahale kaI varRoM ke lie apekRAkqwa apracaliwa WA, Ora kyoMki yaha kaI alaga-alaga nAmoM se gujara cukA hE,
SBAR18 because it was relatively unpopular for several years preceding its current popularity	kyoMki yaha apanI mOjUxA lokapriyawA se pahale kaI varRoM ke lie apekRAkqwa aprawyASiwa WA
S20 it was relatively unpopular for several years preceding its current popularity	yaha apanI mOjUxA lokapriyawA se pahale kaI varRoM se aprawyASiwa WA
NP21 it	yaha
VP23_LWG was	WA
ADJP25 relatively unpopular for several years preceding its current popularity	apanI mOjUxA lokapriyawA se pahale kaI varRoM ke lie apekRAkqwa aprawyASiwa
PP28 for several years preceding its current popularity	apanI mOjUxA lokapriyawA se pahale kaI varRoM se
NP30 several years preceding its current popularity	apanI mOjUxA lokapriyawA se pahale kaI sAla
NP31 several years	kaI sAla
NNS33 years	sAla-ba-ba-ba-ba-ba-ba-ba.
VP34_LWG preceding	Age kI ora
NP36 its current popularity	isakI mOjUxA lokapriyawA
,40 ,	,
CC41 and	Ora
SBAR42 because it has gone through many dierent names , only recently being called  deep learning	kyoMki yaha kaI alaga-alaga nAmoM se gujarA hE, kevala hAla hI meM kahA jA rahA hE gaharI sIKa
S44 it has gone through many dierent names , only recently being called  deep learning	yaha kaI alaga-alaga nAmoM se gujarA hE, kevala hAla hI meM kahA jA rahA hE gaharI sIKa
NP45 it	yaha
VP47_LWG has gone	calA gayA hE
PP51 through many dierent names	kaI alaga-alaga nAmoM ke mAXyama se
NP53 many dierent names	kaI alaga-alaga nAma
NNS56 names	nAma
,57 ,	,
S58 only recently being called  deep learning	kevala hAla hI meM kahA jA rahA hE gaharI sIKa
ADVP59 only	kevala
ADVP61 recently	hAla hI meM
VP63_LWG being called	bulAyA jA rahA hE
NP67  deep learning	gaharI sIKa

----
00245	The eld has been rebranded many times, reecting the inuence of dierent researchers and dierent perspectives.	 kRewra kaI bAra rebranded kiyA gayA hE , viBinna SoXakarwAoM Ora viBinna pariprekRyoM ke praBAva ko prawibiMbiwa .		
244	244
S1 The eld has been rebranded many times , reecting the inuence of dierent researchers and dierent perspectives .	viBinna SoXakarwAoM Ora alaga-alaga pariprekRyoM ke praBAva ko xarSAwe hue kRewra ko kaI bAra Pira se baxalA gayA hE
NP2 The eld	Kewa
VP5_LWG has been rebranded	Pira se baxalA gayA hE
NP-TMP11 many times	kaI bAra
NNS13 times	samaya
,14 ,	,
S15 reecting the inuence of dierent researchers and dierent perspectives	alaga-alaga SoXakarwAoM Ora alaga-alaga pariprekRyoM ke praBAva ko prawibiMbiwa karawe hue
VP16_LWG reecting	prawibiMbiwa
NP18 the inuence of dierent researchers and dierent perspectives	alaga-alaga SoXakarwAoM kA praBAva Ora alaga-alaga pariprekRya
NP19 the inuence	praBAva
PP22 of dierent researchers and dierent perspectives	alaga-alaga SoXakarwAoM Ora alaga-alaga pariprekRyoM kI
NP24 dierent researchers and dierent perspectives	alaga-alaga SoXakarwAoM Ora alaga-alaga pariprekRya
NP25 dierent researchers	alaga-alaga SoXakarwA
NNS27 researchers	SoXakarwAoM
CC28 and	Ora
NP29 dierent perspectives	alaga-alaga pariprekRya
NNS31 perspectives	pariprekRya

----
00246	A comprehensive history of deep learning is beyond the scope of this textbook.	 gahana SikRaNa kA eka vyApaka iwihAsa isa pATyapuswaka ke xAyare se bAhara hE 		
245	245
S1 A comprehensive history of deep learning is beyond the scope of this textbook .	gaharI sIKa kA vyApaka iwihAsa isa pATyapuswaka ke xAyare se pare hE
NP2 A comprehensive history of deep learning	gaharI sIKa kA vyApaka iwihAsa
NP3 A comprehensive history	eka vyApaka iwihAsa
PP7 of deep learning	gaharI sIKa kA
NP9 deep learning	gaharI sIKa
VP12_LWG is	hE
PP14 beyond the scope of this textbook	isa pATyapuswaka ke xAyare se pare
NP16 the scope of this textbook	isa pATyapuswaka kA xAyarA
NP17 the scope	xAyarA
PP20 of this textbook	isa pATyapuswaka kI
NP22 this textbook	yaha pATyapuswaka

----
00247	Some basic context, however, is useful for understanding deep learning.	 kuCa mUla saMxarBa , waWApi , gaharI sIKane ke lie upayogI hE .		
246	246
S1 Some basic context , however , is useful for understanding deep learning .	hAlAMki kuCa mUla saMxarBa, gaharI sIKa ko samaJane ke lie upayogI hE
NP2 Some basic context	kuCa mUla saMxarBa
,6 ,	,
ADVP7 however	hAlAMki
,9 ,	,
VP10_LWG is	hE
ADJP12 useful for understanding deep learning	gaharI sIKa samaJane ke lie upayogI
PP14 for understanding deep learning	gaharI sIKa samaJane ke lie
S16 understanding deep learning	gaharI sIKa samaJanA
VP17_LWG understanding	samaJa
NP19 deep learning	gaharI sIKa

----
00248	Broadly speaking, there have been three waves of development:	 mote wOra para kaheM wo vikAsa kI wIna lahareM rahI hEM :		
247	247
S1 Broadly speaking , there have been three waves of development :	badZe pEmAne para bolawe hue vikAsa kI wIna lahareM huI hEMH
S2 Broadly speaking	badZe pEmAne para bolawe hEM
ADVP3 Broadly	badZe pEmAne para
VP5_LWG speaking	bola rahA hUz
,7 ,	,
NP8 there	vahAM
VP10_LWG have been	kara cuke hEM
NP14 three waves of development	vikAsa kI wIna lahareM
NP15 three waves	wIna lahareM
NNS17 waves	lahareM
PP18 of development	vikAsa kI
NP20 development	vikAsa

----
00249	deep learning known as cybernetics in the 1940s1960s, deep learning known as connectionism in the 12 CHAPTER 1.	 1940 -1960 ke xaSaka meM sAibaranetiksa ke nAma se jAnI jAne vAlI gaharI SikRA , jise 12 CHAPTER 1 meM kanekSana ke rUpa meM jAnA jAwA hE .		
248	248
FRAG1 deep learning known as cybernetics in the 1940s  1960s , deep learning known as connectionism in the 12 CHAPTER 1 .	1940 ke xaSaka- 1960 ke xaSaka meM sAibaranetika ke rUpa meM jAnI jAwI gaharI sIKa, 12 cEptara 1 meM saMbaMXavAxa ke rUpa meM jAnA jAwA hE
FRAG2 deep learning known as cybernetics in the 1940s  1960s	1940 ke xaSaka meM sAibaranetika ke rUpa meM jAnA jAwA gaharA jFAna
NP3 deep learning	gaharI sIKa
VP6_LWG known	jAnA jAwA
PP8 as cybernetics in the 1940s  1960s	1940 ke xaSaka meM sAibaranetika ke rUpa meM
NP10 cybernetics in the 1940s  1960s	1940 ke xaSaka meM sAibaranetika
NP11 cybernetics	sAibaranetika
NNS12 cybernetics	sAibaranetika
PP13 in the 1940s  1960s	1940 ke xaSaka meM- 1960 ke xaSaka meM
NP15 the 1940s  1960s	1940 ke xaSaka- 1960 ke xaSaka
NP16 the 1940s	1940 ke xaSaka meM
NNS18 1940s	1940 ke xaSaka meM
PP19  1960s	 1960 ke xaSaka meM
SYM20 	
NP21 1960s	1960 ke xaSaka meM
NNS22 1960s	1960 ke xaSaka meM
,23 ,	,
FRAG24 deep learning known as connectionism in the 12 CHAPTER 1	12 cEptara 1 meM kanekSanavAxa ke rUpa meM jAnA jAwA hE gaharI sIKa
NP25 deep learning	gaharI sIKa
VP28_LWG known	jAnA jAwA
PP30 as connectionism in the 12 CHAPTER 1	12 cEptara 1 meM saMbaMXavAxa ke rUpa meM
NP32 connectionism in the 12 CHAPTER 1	12 cEptara 1 meM kanekSana
NP33 connectionism	kanekSanavAxa
PP35 in the 12 CHAPTER	12 cEptara meM
NP37 the 12 CHAPTER	12 cEptara
NP-TMP41 1	1

----
00250	INTRODUCTION 1940 1950 1960 1970 1980 1990 2000 Year	 paricaya 1940 1950 1960 1980 1990 2000 varRa		
249	249
NP1 INTRODUCTION 1940195019601970 198019902000 Year	paricaya 1940 1950 1960 1970 1980 1990 2000 varRa
NP2 INTRODUCTION 1940195019601970	paricaya 1940 1950 1960 1970
NP-TMP5 198019902000 Year	1980 1990 2000 varRa

----
0025	Much of this knowledge is subjective and intuitive, and therefore dicult to articulate in a formal way.	 isa jFAna kA aXikAMSa hissA vyakwiparaka Ora anwarjFAnAwmaka hE , Ora isalie OpacArika rUpa se vyakwa karanA kaTina hE 		
24	24
S1 Much of this knowledge is subjective and intuitive , and therefore dicult to articulate in a formal way .	isa jFAna kA bahuwa kuCa viRayAwmaka Ora sahaja hE, Ora isalie OpacArika warIke se wAlamela banAnA muSkila hE
NP2 Much of this knowledge	isa jFAna kA bahuwa kuCa
NP3 Much	bahuwa kuCa
PP5 of this knowledge	isa jFAna kA
NP7 this knowledge	yaha jFAna
VP10_LWG is therefore dicult	isalie muSkila hE
ADJP13 subjective and intuitive	viRayAwmaka Ora sahaja
CC15 and	Ora
JJ14 subjective	viRayAwmaka
JJ16 intuitive	sahaja
,17 ,	,
CC18 and	Ora
ADVP19 therefore	isalie
S23 to articulate in a formal way	OpacArika warIke se wAlamela banAne ke lie
VP24_LWG to articulate	wAlamela banAne ke lie
PP28 in a formal way	OpacArika warIke se
NP30 a formal way	OpacArika warIke se

----
00251	0.000000 0.000050 0.000100 0.000150	 0 . 000 0 . 00050 0 . 000100 0 . 000150		
250	250
FRAG1 0.000000 0.000050 0.000100 0.000150	gaNanA xEtyika gaNanA kqwrima 0.000 0.000
NP2 0.000000 0.000050 0.000100 0.000150	gaNanA xEtyika gaNanA kqwrima 0.000 0.000

----
00252	0.000200 0.000250 Frequency of Word or Phrase cybernetics (connectionism + neural networks)	 0 . 000200 0 . 000250 Avqwwi Sabxa yA vAkyAMSa sAibaranetiksa ( saMbaMXavAxa + waMwrikA netavarka )		
251	251
NP1 0.000200 0.000250 Frequency of Word or Phrase cybernetics ( connectionism + neural networks )	April200 0.0250 Sabxa yA Pira Presa sAibaranetika (kanekSanavAxa + waMwrikA netavarka) kI wIvrawA
NP2 0.000200 0.000250 Frequency	kqpANa 0.000 wIvrawA
QP3 0.000200 0.000250	kqpayA 0.000
PP7 of Word or Phrase cybernetics ( connectionism + neural networks )	Sabxa yA Brasa sAibaranetika (kanekSanavAxa + waMwrikA netavarka)
NP9 Word or Phrase cybernetics ( connectionism + neural networks )	Sabxa yA Brasa sAibaranetika (kanekSanavAxa + waMwrikA netavarka)
NML10 Word or Phrase	Sabxa yA carasa
CC12 or	yA
NNP11 Word	Sabxa
NNP13 Phrase	Xruva
NNS14 cybernetics	sAibaranetika
PRN15 ( connectionism + neural networks )	(kanekSanavAxa + waMwrikA netavarka)
NP17 connectionism + neural networks	kanekSana + waMwrikA netavarka
NP18 connectionism	kanekSanavAxa
SYM20 +	+
NP21 neural networks	waMwrikA netavarka
NNS23 networks	netavarka

----
00253	Figure 1.7:	 ciwra 1 . 7 :		
252	252
NP1 Figure 1.7 :	AMkadZA 1.7H

----
00254	Two of the three historical waves of articial neural nets research, as measured by the frequency of the phrases cybernetics and connectionism or neural networks, according to Google Books (the third wave is too recent to appear).	 kqwrima waMwrikA jAla anusaMXAna kI wIna EwihAsika waraMgoM meM se xo , jEsA ki vAkyAMSoM kI Avqwwi se mApA jAwA hE sAibaranetiksa Ora saMyojana yA nerala netavarka , gUgala buksa ke anusAra ( wIsarI lahara aBI hAla hI meM prakata huI hE )		
253	253
S1 Two of the three historical waves of articial neural nets research , as measured by the frequency of the phrases  cybernetics  and  connectionism  or  neural networks ,  according to Google Books ( the third wave is too recent to appear ) .	kqwrima waMwrikA netarsa anusaMXAna kI wIna EwihAsika laharoM meM se xo, jEsA ki SabxoM kI Avqwwi sAibaranetiksa Ora kanekSanavAxa yA waMwrikA ne
NP2 Two of the three historical waves of articial neural nets	kqwrima waMwrikA jAla kI wIna EwihAsika laharoM meM se xo
NP3 Two	xo
PP5 of the three historical waves of articial neural nets	kqwrima waMwrikA jAla kI wIna EwihAsika laharoM kI
NP7 the three historical waves of articial neural nets	kqwrima waMwrikA jAla kI wIna EwihAsika lahareM
NP8 the three historical waves	wIna EwihAsika lahareM
NNS12 waves	lahareM
PP13 of articial neural nets	kqwrima waMwrikA jAla kA
NP15 articial neural nets	kqwrima waMwrikA jAla
NNS18 nets	jAla
VP19_LWG research	SoXa
,21 ,	,
SBAR22 as measured by the frequency of the phrases  cybernetics  and  connectionism  or  neural networks ,  according to Google Books	jEsA ki gUgala puswakoM ke anusAra sAibaranetiksa Ora kanekSanavAxa yA waMwrikA netavarka, SabxoM kI Avqwwi se mApA gayA
S24 measured by the frequency of the phrases  cybernetics  and  connectionism  or  neural networks ,  according to Google Books	gUgala puswakoM ke anusAra sAibaranetiksa Ora kanekSanavAxa yA waMwrikA netavarka, SabxoM kI Avqwwi se mApA gayA
VP25_LWG measured	mApa liyA gayA
PP27 by the frequency of the phrases  cybernetics  and  connectionism  or  neural networks	vAkyAMSoM kI Avqwwi se sAibaranetiksa Ora kanekSanavAxa yA waMwrikA netavarka
NP29 the frequency of the phrases  cybernetics  and  connectionism  or  neural networks	vAkyAMSoM kI Avqwwi sAibaranetiksa Ora kanekSanavAxa yA waMwrikA netavarka
NP30 the frequency	Avqwwi
PP33 of the phrases	vAkyAMSoM kA
NP35 the phrases	vAkyAMSa
NNS37 phrases	vAkyAMSa
NP39 cybernetics	sAibaranetika
NNS40 cybernetics	sAibaranetika
CC42 and	Ora
''41 	
``43 	
NP44 connectionism	kanekSanavAxa
CC47 or	yA
''46 	
``48 	
NP49 neural networks	waMwrikA netavarka
NNS51 networks	netavarka
,52 ,	,
PP54 according to Google Books	gUgala buksa ke anusAra
PP56 to Google Books	gUgala buksa ko
NP58 Google Books	gUgala buksa
PRN61 ( the third wave is too recent to appear )	(xarSane ke lie wIsarI lahara bahuwa hAla hI meM hE)
S63 the third wave is too recent to appear	xiKane ke lie wIsarI lahara bahuwa hAla hI meM hE
NP64 the third wave	wIsarI lahara
VP68_LWG is	hE
ADJP70 too recent	bahuwa hAla hI meM
S73 to appear	xiKane ke lie
VP74_LWG to appear	xiKane ke lie

----
00255	The rst wave started with cybernetics in the 1940s1960s, with the development of theories of biological learning (McCulloch and Pitts, 1943; Hebb, 1949) and implementations of the rst models, such as the perceptron (Rosenblatt, 1958), enabling the training of a single neuron.	 praWama waraMga kI SuruAwa 1940 -1960 ke xaSaka meM jEva vijFAna ke sixXAMwoM ke vikAsa ke sAWa huI ( waMwrikA vijFAna Ora pitsa , 1943 , 1949 , hebbana moYdaloM ke praWama anukUlIkaraNa Ora kAryAnvayana jEse ekala sakRamIkaraNa praSikRaNa )		
254	254
S1 The rst wave started with cybernetics in the 1940s  1960s , with the development of theories of biological learning ( McCulloch and Pitts , 1943 ; Hebb , 1949 ) and implementations of the rst models , such as the perceptron ( Rosenblatt , 1958 ) , enabling the training of a single neuron .	pahalI lahara 1940 ke xaSaka meM sAibaranetika ke sAWa SurU huI, jEse ki jEvika sIKa ke sixXAMwa (mEkkulloca Ora pitsa, 1943 ; hebba, 1949) Ora
NP2 The rst wave	pahalI lahara
VP6_LWG started	SurU
PP8 with cybernetics in the 1940s  1960s	1940 ke xaSaka meM sAibaranetika ke sAWa
NP10 cybernetics in the 1940s  1960s	1940 ke xaSaka meM sAibaranetika
NP11 cybernetics	sAibaranetika
NNS12 cybernetics	sAibaranetika
PP13 in the 1940s  1960s	1940 ke xaSaka meM- 1960 ke xaSaka meM
NP15 the 1940s  1960s	1940 ke xaSaka- 1960 ke xaSaka
NP16 the 1940s	1940 ke xaSaka meM
NNS18 1940s	1940 ke xaSaka meM
PP19  1960s	 1960 ke xaSaka meM
SYM20 	
NP21 1960s	1960 ke xaSaka meM
NNS22 1960s	1960 ke xaSaka meM
,23 ,	,
PP24 with the development of theories of biological learning ( McCulloch and Pitts , 1943 ; Hebb , 1949 ) and implementations of the rst models , such as the perceptron ( Rosenblatt , 1958 )	jEvika SikRaNa ke sixXAMwoM ke vikAsa ke sAWa (mEkkulloca Ora pitsa, 1943 ; hebba, 1949) Ora pahale moYdaloM ke kAryAnvayana, jEse ki eksaperi
NP26 the development of theories of biological learning ( McCulloch and Pitts , 1943 ; Hebb , 1949 ) and implementations of the rst models , such as the perceptron ( Rosenblatt , 1958 )	jEvika SikRaNa ke sixXAMwoM kA vikAsa (mEkkulloca Ora pitsa, 1943 ; hebba, 1949) Ora pahale moYdaloM ke kriyAnvayana, jEse ki eksapartrona
NP27 the development	vikAsa
PP30 of theories of biological learning ( McCulloch and Pitts , 1943 ; Hebb , 1949 ) and implementations of the rst models , such as the perceptron ( Rosenblatt , 1958 )	jEvika jFAna ke sixXAMwoM (mEkkulloca Ora pitsa, 1943 ; hebba, 1949 ) Ora pahale moYdaloM ke kriyAnvayana, jEse ki eksapartrona (rose
NP32 theories of biological learning ( McCulloch and Pitts , 1943 ; Hebb , 1949 ) and implementations of the rst models , such as the perceptron ( Rosenblatt , 1958 )	jEvika jFAna ke sixXAMwa (mEkkulloca Ora pitsa, 1943 ; hebba, 1949 ) Ora pahale moYdaloM ke kriyAnvayana, jEse ki eksapartrona (rosebla
NP33 theories	sixXAMwa
NNS34 theories	sixXAMwa
PP35 of biological learning ( McCulloch and Pitts , 1943 ; Hebb , 1949 ) and implementations of the rst models , such as the perceptron ( Rosenblatt , 1958 )	jEvika sIKa (mEkkulloca Ora pitsa, 1943 ; hebba, 1949 ) Ora pahale moYdaloM ke kriyAnvayana, jEse ki eksaperina (rosebleta, 1958
NP37 biological learning ( McCulloch and Pitts , 1943 ; Hebb , 1949 ) and implementations of the rst models , such as the perceptron ( Rosenblatt , 1958 )	jEvika sIKa (mEkkulloca Ora pitsa, 1943 ; hebba, 1949 ) Ora pahale moYdaloM ke kriyAnvayana, jEse ki eksaperina (rosebleta, 1958
NP38 biological learning ( McCulloch and Pitts , 1943 ; Hebb , 1949 )	jEvika sIKa (mEkakulloca Ora pitsa, 1943 ; hebba, 1949 )
PRN41 ( McCulloch and Pitts , 1943 ; Hebb , 1949 )	(mEkakulloca Ora pitsa, 1943 ; hebba, 1949 )
NP43 McCulloch and Pitts	mEkkulloca Ora pitsa
CC45 and	Ora
NNP44 McCulloch	mEkakulaloca
NNP46 Pitts	pitsa
,47 ,	,
NP48 1943	1943
NP51 Hebb	hebba
,53 ,	,
NP54 1949	1949
CC57 and	Ora
NP58 implementations of the rst models , such as the perceptron ( Rosenblatt , 1958 )	pahale moYdaloM ke kAryAnvayana, jEse ki eksapartrona (rosebleta, 1958)
NP59 implementations	kriyAnvayana
NNS60 implementations	kriyAnvayana
PP61 of the rst models	pahale moYdaloM kI
NP63 the rst models	pahale moYdala
NNS66 models	moYdala
,67 ,	,
PP68 such as the perceptron	jEse ki avaXAraNA
NP71 the perceptron	avaXAraNA
PRN74 ( Rosenblatt , 1958 )	(rosenableta, 1958)
NP76 Rosenblatt	rosenableta
,78 ,	,
NP79 1958	1958
,82 ,	,
S83 enabling the training of a single neuron	ekala nyUrona ke praSikRaNa ko baDZAvA
VP84_LWG enabling	Anana-PAnana
NP86 the training of a single neuron	ekala nyUrona kA praSikRaNa
NP87 the training	praSikRaNa
PP90 of a single neuron	ekala nyUrona kI
NP92 a single neuron	eka hI nyUrona

----
00256	The second wave started with the connectionist approach of the 19801995 period, with back- propagation (Rumelhart et al., 1986a) to train a neural network with one or two hidden layers.	 xUsarI lahara kI SuruAwa 1980 -1995 ke saMparkavAxI xqRtikoNa se huI , jisameM paSca - pracAra ( Rumelhart eta ala , 1986 ) eka yA xo Cipe parawoM vAle waMwrikA netavarka ko praSikRiwa karane ke lie huA 		
255	255
S1 The second wave started with the connectionist approach of the 1980  1995 period , with back - propagation ( Rumelhart et al. , 1986a ) to train a neural network with one or two hidden layers .	yaha xUsarI lahara 1980- 1995 ke xOra ke kanekSanavAxI xqRtikoNa se SurU huI WI, isake sAWa pICe-pracAra (rumAlahArta eta ala., 1986a) ke sAWa
NP2 The second wave	xUsarI lahara
VP6_LWG started	SurU
PP8 with the connectionist approach of the 1980  1995 period , with back - propagation ( Rumelhart et al. , 1986a )	1980- 1995 kI avaXi ke kanekSanavAxI xqRtikoNa ke sAWa, pICe se-pracAra (rUmahArta eta ala., 1986a)
NP10 the connectionist approach of the 1980  1995 period , with back - propagation ( Rumelhart et al. , 1986a )	1980- 1995 kI avaXi kA kanekSanavAxI xqRtikoNa, pICe se-pracAra ( Rumelhartet ala., 1986a)
NP11 the connectionist approach	kanekSanavAxI xqRtikoNa
PP15 of the 1980  1995 period , with back - propagation ( Rumelhart et al. , 1986a )	1980- 1995 ke xOra kI, pICe se-pracAra (rumAlahArta eta ala., 1986a)
NP17 the 1980  1995 period , with back - propagation ( Rumelhart et al. , 1986a )	1980- 1995 kI avaXi, pICe se-pracAra (rumAlahArta eta ala., 1986a)
NP18 the 1980	1980 kA xOra
PP21  1995 period	 1995 kI avaXi
SYM22 	
NP23 1995 period	1995 kI avaXi
,26 ,	,
PP27 with back - propagation ( Rumelhart et al. , 1986a )	vApasa- pracAra-prasAra ( Rumelhartita eta ala., 1986a)
NP29 back - propagation ( Rumelhart et al. , 1986a )	pICe - pracAra (rumAlahArta eta ala., 1986a)
PRN33 ( Rumelhart et al. , 1986a )	(rumAlahArta eta ala., 1986a)
NP35 Rumelhart et al. , 1986a	rUmAlahArta eta ala., 1986a
NP36 Rumelhart et al.	Rumelhart eta ala
NP37 Rumelhart	rUmAlahArta
ADVP39 et al.	eta ala
,42 ,	,
NP43 1986a	1986a
NNS44 1986a	1986a
S46 to train a neural network with one or two hidden layers	eka yA xo CipI parawoM vAle waMwrikA netavarka ko praSikRiwa karane ke lie
VP47_LWG to train	treniMga ke lie
NP51 a neural network	eka waMwrikA netavarka
PP55 with one or two hidden layers	eka-xo CipI huI parawoM ke sAWa
NP57 one or two hidden layers	eka-xo CipI huI paraweM
QP58 one or two	eka-xo
CC60 or	yA
CD59 one	eka
CD61 two	xo
NNS63 layers	paraweM

----
00257	The current and third wave, deep learning, started around 2006 (Hinton et al., 2006; Bengio et al.	 varwamAna wIsarI lahara , gahana aXyayana , 2006 ke AsapAsa SurU huA ( ecina eta ala , 2006 beMjiyo eta		
256	256
S1 The current and third wave , deep learning , started around 2006 ( Hinton et al. , 2006 ; Bengio et al. .	mOjUxA Ora wIsarI lahara, gaharI sIKa, 2006 ke AsapAsa SurU huI (hiMtana eta ala, 2006 ; beMgiyo eta ala.
NP2 The current and third wave , deep learning ,	mOjUxA Ora wIsarI lahara, gaharI sIKa,
NP3 The current and third wave	mOjUxA Ora wIsarI lahara
NML5 current and third	varwamAna Ora wIsarA
CC7 and	Ora
JJ6 current	varwamAna
JJ8 third	wIsarA
,10 ,	,
NP11 deep learning	gaharI sIKa
,14 ,	,
VP15_LWG started	SurU
PP17 around 2006	2006 ke AsapAsa
NP19 2006	2006
PRN21 ( Hinton et al. , 2006 ; Bengio et al.	(hiMtana eta ala, 2006 ; beMgiyo eta ala
FRAG23 Hinton et al. , 2006	hiMtana eta ala., 2006
NP24 Hinton et al.	hiMtana eta ala
ADVP26 et al.	eta ala
,29 ,	,
NP30 2006	2006
FRAG33 Bengio et al.	beMjiyo eta ala
NP34 Bengio et	beMjiyo eta
ADVP36 et	ewa.
ADVP38 al.	ala

----
00258	, 2007; Ranzato et al., 2007a)	 eta ala Ranzato & # 44 ; 2007 & # 44 ;		
257	257
FRAG1 , 2007 ; Ranzato et al. , 2007a )	, 2007 ; raNajAwo eta ala., 2007a )
NP2 , 2007	, 2007
,3 ,	,
FRAG6 Ranzato et al. , 2007a	raNajAwo eta ala., 2007 e
NP7 Ranzato et al.	raNajAwo eta ala
ADVP9 et al.	eta ala
,12 ,	,
NP13 2007a	2007 e.

----
00259	and is just now appearing in book form as of 2016.	 Ora aBI 2016 ke rUpa meM puswaka ke rUpa meM prakata ho rahA hE .		
258	258
S1 and is just now appearing in book form as of 2016 .	Ora aBI 2016 kI waraha puswaka rUpa meM xiKAI xe rahA hE

----
00260	The other two waves similarly appeared in book form much later than the corresponding scientic activity occurred.	 anya xo waraMgeM BI isI prakAra puswakAkAra rUpa meM prakASiwa huI , jo waxanurUpa vEjFAnika gawiviXi se bahuwa bAxa meM prApwa huI 		
259	259
S1 The other two waves similarly appeared in book form much later than the corresponding scientic activity occurred .	saMvAxiwa vEjFAnika gawiviXi kI wulanA meM anya xo waraMge puswaka rUpa meM bahuwa bAxa xiKAI xie
NP2 The other two waves	xUsarI xo lahareM
NNS6 waves	lahareM
ADVP7 similarly	isI waraha
VP9_LWG appeared	xiKAI xiyA
PP11 in book form	kiwAba ke PoYrma meM
NP13 book form	puswaka PArma
SBAR16 much later than the corresponding scientic activity occurred	bahuwa bAxa meM huI saMbaMXiwa vEjFAnika gawiviXi
ADVP17 much later	bahuwa bAxa
S21 the corresponding scientic activity occurred	saMbaMXiwa vEjFAnika gawiviXi huI
NP22 the corresponding scientic activity	saMbaMXiwa vEjFAnika gawiviXi
VP27_LWG occurred	huA WA

----
0026	Computers need to capture this same knowledge in order to behave in an intelligent way.	 kaMpyUtaroM ko eka buxXimAna warIke se vyavahAra karane ke lie isa eka hI jFAna para kabjA karane kI jarUrawa hE .		
25	25
S1 Computers need to capture this same knowledge in order to behave in an intelligent way .	kaMpyUtaroM ko eka buxXimAna warIke se vyavahAra karane ke lie isI jFAna ko pakadZane kI AvaSyakawA howI hE
NP2 Computers	kaMpyUtara
VP4_LWG need	jarUrawa
S6 to capture this same knowledge in order to behave in an intelligent way	buxXimAna warIke se vyavahAra karane ke lie isI jFAna ko pakadZane ke lie
VP7_LWG to capture	kabjA karane ke lie
NP11 this same knowledge	yahI jFAna
PP15 in order to behave in an intelligent way	eka buxXimAna warIke se vyavahAra karane ke lie
NP17 order to behave in an intelligent way	eka buxXimAna warIke se vyavahAra karane kA AxeSa
S19 to behave in an intelligent way	eka buxXimAna warIke se vyavahAra karanA
VP20_LWG to behave	vyavahAra karane ke lie
PP24 in an intelligent way	buxXimAna warIke se
NP26 an intelligent way	eka buxXimAna warIkA

----
00261	1980s1990s, and the current resurgence under the name deep learning beginning in 2006.	 1980 -1990 ke xaSaka meM , Ora varwamAna punaruwWAna 2006 meM gahare sIKane ke nAma se SurU huA .		
260	260
FRAG1 1980s  1990s , and the current resurgence under the name deep learning beginning in 2006 .	1980 ke xaSaka- 1990 ke xaSaka meM, Ora 2006 meM gaharI sIKa ke nAma ke wahawa varwamAna punarmilana
NP2 1980s	1980 ke xaSaka meM
NP5 1990s , and the current resurgence under the name deep learning beginning in 2006	1990 ke xaSaka meM, Ora 2006 meM gaharI sIKa ke nAma ke wahawa mOjUxA punarmilana
NP6 1990s , and the current resurgence under the name deep learning	1990 ke xaSaka meM, Ora gaharI sIKa ke wahawa varwamAna punarmilana
NP7 1990s	1990 ke xaSaka
NNS8 1990s	1990 ke xaSaka
,9 ,	,
CC10 and	Ora
NP11 the current resurgence under the name	nAma ke wahawa mOjUxA punarmilana
NP12 the current resurgence	mOjUxA punarmilana
PP16 under the name	nAma ke wahawa
NP18 the name	nAma
NML21 deep learning	gaharI sIKa
VP24_LWG beginning	SuruAwa
PP26 in 2006	2006 meM
NP28 2006	2006

----
00262	This is quantitatively illustrated in gure 1.7.	 yaha mAwrAwmaka rUpa se aMka 1 . 7 meM xarSAyA gayA hE 		
261	261
S1 This is quantitatively illustrated in gure 1.7 .	yaha mAwrAwmaka rUpa se AMkadZe 1.7 meM pariBARiwa hE
NP2 This	yaha
VP4_LWG is quantitatively illustrated	mAwrAwmaka rUpa se xiKAyA gayA hE
ADVP6 quantitatively	mAwrAwmaka rUpa se
PP10 in gure 1.7	AMkadZe 1.7 meM
NP12 gure 1.7	AMkadZA 1.7

----
00263	Some of the earliest learning algorithms we recognize today were intended to be computational models of biological learning, that is, models of how learning happens or could happen in the brain.	 kuCa prAraMBika SikRaNa elgorixama , jinheM hama Aja pahacAnawe hEM , kA uxxeSya jEvika SikRaNa ke kampyUteSanala moYdala honA WA , arWAwa sIKane ke warIke maswiRka meM kEse howe hEM yA ho sakawe hEM .		
262	262
S1 Some of the earliest learning algorithms we recognize today were intended to be computational models of biological learning , that is , models of how learning happens or could happen in the brain .	Aja hama jina kuCa pahale se mAnyawA prApwa karawe hEM, unameM se kuCa jEvika sIKa ke kampyUteSanala moYdala hone kA irAxA WA, yAnI ki kEse sIKane
NP2 Some of the earliest learning algorithms we recognize today	kuCa sabase pahale sIKane vAle elgorixama jinheM Aja hama pahacAnawe hEM
NP3 Some	kuCa
PP5 of the earliest learning algorithms we recognize today	sabase pahale sIKane vAle elgorixama ke hama Aja pahacAnawe hEM
NP7 the earliest learning algorithms we recognize today	sabase pahale sIKane vAle elgorixama hama Aja pahacAnawe hEM
NP8 the earliest learning algorithms	sabase pahale sIKane vAle elgorixama
NNS12 algorithms	elgorixama
SBAR13 we recognize today	hama Aja pahacAnawe hEM
S14 we recognize today	hama Aja pahacAnawe hEM
NP15 we	hama
VP17_LWG recognize	pahacAna
NP-TMP19 today	Aja
VP21_LWG were intended	We irAxA
S25 to be computational models of biological learning , that is , models of how learning happens or could happen in the brain	jEvika SikRaNa ke kampyUteSanala moYdala hone ke lie, yAnI maswiRka meM kEse howA hE sIKane ke moYdala ho sakawe hEM yA ho sakawe hEM
VP26_LWG to be	honA cAhie
NP30 computational models of biological learning , that is , models of how learning happens or could happen in the brain	jEvika SikRaNa ke kampyUteSanala moYdala, yAnI ki kEse howA hE sIKane ke moYdala yA maswiRka meM ho sakawe hEM
NP31 computational models	kampyUteSanala moYdala
NNS33 models	moYdala
PP34 of biological learning , that is , models of how learning happens or could happen in the brain	jEvika sIKane ke, yAnI maswiRka meM kEse howA hE sIKane ke moYdala yA ho sakawe hEM
NP36 biological learning , that is , models of how learning happens or could happen in the brain	jEvika sIKa, yAnI maswiRka meM kEse howA hE sIKane ke moYdala yA ho sakawe hEM
NP37 biological learning	jEvika sIKa
,40 ,	,
SBAR41 that is , models of how learning happens or could happen in the brain	yAnI maswiRka meM kEse howA hE sIKane ke moYdala yA ho sakawe hEM
WHNP42 that	vaha
S44 is , models of how learning happens or could happen in the brain	hE, maswiRka meM kEse howA hE sIKane ke moYdala yA ho sakawe hEM
VP45_LWG is	hE
,47 ,	,
FRAG48 models of how learning happens or could happen in the brain	maswiRka meM kEse howA hE sIKane ke moYdala yA ho sakawe hEM
NP49 models of how learning happens or could happen in the brain	maswiRka meM kEse howA hE sIKane ke moYdala yA ho sakawe hEM
NP50 models	moYdala
NNS51 models	moYdala
PP52 of how learning happens or could happen in the brain	kEse howA hE sIKanA yA ho sakawA hE maswiRka meM
SBAR54 how learning happens or could happen in the brain	kEse howA hE sIKanA yA ho sakawA hE maswiRka meM
WHADVP55 how	kEse
S57 learning happens or could happen in the brain	sIKawA howA hE yA maswiRka meM ho sakawA hE
NP58 learning	sIKanA
VP60_LWG happens could happen	EsA ho sakawA hE howA hE
CC63 or	yA
VP61 happens	EsA howA hE
VP64 could happen in the brain	maswiRka meM ho sakawA hE ho sakawA hE
PP68 in the brain	maswiRka meM
NP70 the brain	maswiRka

----
00264	As a result, one of the names that deep learning has gone by is articial neural networks (ANNs).	 eka pariNAma ke rUpa meM , eka nAma hE ki gaharI sIKane ke xvArA calA gayA hE kqwrima waMwrikA netavarka ( eenaenaesa )		
263	263
S1 As a result , one of the names that deep learning has gone by is articial neural networks ( ANNs ) .	pariNAmasvarUpa jina nAmoM se gaharI sIKa gaI hE unameM se eka kqwrima waMwrikA netavarka (eenaena) hE
PP2 As a result	PalasvarUpa
NP4 a result	eka pariNAma
,7 ,	,
NP8 one of the names that deep learning has gone by	jina nAmoM se gaharI sIKa calI gaI hE unameM se eka
NP9 one	eka
PP11 of the names that deep learning has gone by	jina nAmoM kI gaharI sIKa calI gaI hE unakA
NP13 the names that deep learning has gone by	jina nAmoM se gaharI sIKa calI gaI hE vo
NP14 the names	nAma
NNS16 names	nAma
SBAR17 that deep learning has gone by	ki gaharI sIKa calI gaI hE
S19 deep learning has gone by	gaharI sIKa calI gaI hE
NP20 deep learning	gaharI sIKa
VP23_LWG has gone	calA gayA hE
PP27 by	xvArA
VP29_LWG is	hE
NP31 articial neural networks ( ANNs )	kqwrima waMwrikA netavarka (eenaena)
NP32 articial neural networks	kqwrima waMwrikA netavarka
NNS35 networks	netavarka
NP37 ANNs	eenaena

----
00265	The corresponding perspective on deep learning models is that they are engineered systems inspired by the biological brain (whether the human brain or the brain of another animal).	 gahare sIKane ke moYdaloM para saMgawa pariprekRya yaha hE ki ve jEvika maswiRka ( cAhe mAnava maswiRka ho yA kisI anya paSu ke maswiRka ) se preriwa sistama iMjIniyara hEM .		
264	264
S1 The corresponding perspective on deep learning models is that they are engineered systems inspired by the biological brain ( whether the human brain or the brain of another animal ) .	gaharI sIKa ke moYdaloM para saMxarBiwa pariprekRya yaha hE ki ve jEvika maswiRka se preriwa iMjIniyariMga sistama hEM ( cAhe mAnava maswiRka ho yA xUsare jAnavara
NP2 The corresponding perspective on deep learning models	gaharI sIKa ke moYdaloM para saMbaMXiwa pariprekRya
NP3 The corresponding perspective	saMbaMXiwa pariprekRya
PP7 on deep learning models	gahare larniMga moYdala para
NP9 deep learning models	gaharI sIKa ke moYdala
NML10 deep learning	gaharI sIKa
NNS13 models	moYdala
VP14_LWG is	hE
SBAR16 that they are engineered systems inspired by the biological brain ( whether the human brain or the brain of another animal )	ki ve jEvika maswiRka se preriwa iMjIniyariMga sistama hEM ( cAhe mAnava maswiRka ho yA kisI anya jAnavara kA maswiRka)
S18 they are engineered systems inspired by the biological brain ( whether the human brain or the brain of another animal )	ve jEvika maswiRka se preriwa iMjIniyariMga sistama hEM ( cAhe mAnava maswiRka ho yA xUsare jAnavara kA maswiRka)
NP19 they	ve
VP21_LWG are	hEM
NP23 engineered systems inspired by the biological brain ( whether the human brain or the brain of another animal )	jEvika maswiRka se preriwa iMjIniyariMga sistama ( cAhe mAnava maswiRka ho yA xUsare jAnavara kA maswiRka)
NP24 engineered systems	iMjIniyariMga sistama
NNS26 systems	sistama
VP27_LWG inspired	preriwa
PP29 by the biological brain	jEvika maswiRka se
NP31 the biological brain	jEvika maswiRka
SBAR35 ( whether the human brain or the brain of another animal )	( cAhe mAnava maswiRka ho yA xUsare jAnavara kA maswiRka)
FRAG38 the human brain or the brain of another animal	mAnava maswiRka yA xUsare jAnavara kA maswiRka
NP39 the human brain or the brain of another animal	mAnava maswiRka yA xUsare jAnavara kA maswiRka
NP40 the human brain	mAnava maswiRka
CC44 or	yA
NP45 the brain of another animal	xUsare jAnavara kA maswiRka
NP46 the brain	maswiRka
PP49 of another animal	xUsare jAnavara kA
NP51 another animal	eka Ora jAnavara

----
00266	While the kinds of neural networks used for machine learning have sometimes been used to understand brain function (Hinton and Shallice, 1991), they are generally not designed to be realistic models of biological function.	 jabaki maSInI SikRaNa ke lie prayukwa waMwrikA netavarkoM ke prakAroM kA upayoga kaBI - kaBI maswiRka ke kArya ko samaJane ke lie kiyA gayA hE ( hEnatana Ora SElisa , 1991 ) , ve Ama wOra para jEvika kArya ke vAswavika moYdala nahIM banane ke lie dijZAina kie gae hEM .		
265	265
S1 While the kinds of neural networks used for machine learning have sometimes been used to understand brain function ( Hinton and Shallice , 1991 ) , they are generally not designed to be realistic models of biological function .	jabaki maSIna sIKane ke lie iswemAla hone vAle waMwrikA netavarka kaBI-kaBI brena PaMkSana (hiMtana Ora SAlsI, 1991) ko samaJane ke lie
SBAR2 While the kinds of neural networks used for machine learning have sometimes been used to understand brain function ( Hinton and Shallice , 1991 )	jabaki maSIna sIKane ke lie iswemAla hone vAle waMwrikA netavarka kaBI-kaBI brena PaMkSana (hiMtana Ora SAlsI, 1991) ko samaJane ke lie
S4 the kinds of neural networks used for machine learning have sometimes been used to understand brain function ( Hinton and Shallice , 1991 )	maSIna sIKane ke lie iswemAla hone vAle waMwrikA netavarka kaBI-kaBI brena PaMkSana (hiMtana Ora SAlsI, 1991) ko samaJane ke lie iswemAla
NP5 the kinds of neural networks used for machine learning	maSIna sIKane ke lie iswemAla kie gae waMwrikA netavarka
NP6 the kinds	waraha-waraha kI waraha
NNS8 kinds	waraha-waraha kI waraha
PP9 of neural networks used for machine learning	maSIna sIKane ke lie iswemAla kie gae waMwrikA netavarka ke
NP11 neural networks used for machine learning	maSIna sIKane ke lie iswemAla huA waMwrikA netavarka
NP12 neural networks	waMwrikA netavarka
NNS14 networks	netavarka
VP15_LWG used	iswemAla
PP17 for machine learning	maSIna sIKane ke lie
NP19 machine learning	maSIna sIKanA
VP22_LWG have sometimes been used	kaBI kaBI iswemAla howA rahA hE
ADVP24 sometimes	kaBI-kaBI
S30 to understand brain function	brena PaMkSana ko samaJane ke lie
VP31_LWG to understand	samaJane ke lie
NP35 brain function	brena PaMkSana
PRN38 ( Hinton and Shallice , 1991 )	(hiMtana Ora SAlsI, 1991)
NP40 Hinton and Shallice	hiMtana Ora SAlsI
CC42 and	Ora
NNP41 Hinton	hiMtana
NNP43 Shallice	SElIce
,44 ,	,
NP45 1991	1991
,48 ,	,
NP49 they	ve
VP51_LWG are generally not designed	Ama wOra para dijAina nahIM howe
ADVP53 generally	Ama wOra para
S58 to be realistic models of biological function	jEvika samAroha ke yaWArWavAxI moYdala banane ke lie
VP59_LWG to be	honA cAhie
NP63 realistic models of biological function	jEvika samAroha ke yaWArWavAxI moYdala
NP64 realistic models	yaWArWavAxI moYdala
NNS66 models	moYdala
PP67 of biological function	jEvika samAroha kA
NP69 biological function	jEvika samAroha

----
00267	The neural perspective on deep learning is motivated by two main ideas.	 gahana aXyayana para waMwrikA pariprekRya xo muKya vicAroM se preriwa hE 		
266	266
S1 The neural perspective on deep learning is motivated by two main ideas .	gaharI sIKa para waMwrikA xqRtikoNa xo muKya vicAroM se preriwa hE
NP2 The neural perspective on deep learning	gaharI sIKa para waMwrikA pariprekRya
NP3 The neural perspective	waMwrikA pariprekRya
PP7 on deep learning	gaharI sIKa para
NP9 deep learning	gaharI sIKa
VP12_LWG is motivated	preriwa hE
PP16 by two main ideas	xo muKya vicAroM se
NP18 two main ideas	xo muKya vicAra
NNS21 ideas	vicAra

----
00268	One idea is that the brain provides a proof by example that intelligent behavior is possible, and a conceptually straightforward path to building intelligence is to reverse engineer the computational principles behind the brain and duplicate its functionality.	 eka vicAra yaha hE ki maswiRka uxAharaNa ke xvArA eka sabUwa praxAna karawA hE ki buxXimAna vyavahAra saMBava hE , Ora buxXi ke nirmANa ke lie eka vEcArika rUpa se sIXA rAswA hE maswiRka ke pICe kampyUteSanala sixXAMwoM ko rivarsa iMjIniyara Ora usakI kAryakRamawA dupliketa .		
267	267
S1 One idea is that the brain provides a proof by example that intelligent behavior is possible , and a conceptually straightforward path to building intelligence is to reverse engineer the computational principles behind the brain and duplicate its functionality .	eka vicAra yaha hE ki maswiRka eka pramANa praxAna karawA hE ki buxXijIvI vyavahAra saMBava hE, Ora buxXi ke nirmANa ke lie eka vyAvahArika rUpa se sIXA rAswA iMjIniyara ko ma
S2 One idea is that the brain provides a proof by example that intelligent behavior is possible	eka vicAra yaha hE ki maswiRka uxAharaNa se pramANa praxAna karawA hE ki buxXimAna vyavahAra saMBava hE
NP3 One idea	eka vicAra
VP6_LWG is	hE
SBAR8 that the brain provides a proof by example that intelligent behavior is possible	yaha maswiRka uxAharaNa se pramANa praxAna karawA hE ki buxXimAna vyavahAra saMBava hE
S10 the brain provides a proof by example that intelligent behavior is possible	maswiRka uxAharaNa se pramANa praxAna karawA hE ki buxXimAna vyavahAra saMBava hE
NP11 the brain	maswiRka
VP14_LWG provides	praxAna karawA hE
NP16 a proof	eka pramANa
PP19 by example	uxAharaNa se
NP21 example	uxAharaNa
SBAR23 that intelligent behavior is possible	vaha buxXimAna vyavahAra saMBava hE
S25 intelligent behavior is possible	buxXimAna vyavahAra saMBava
NP26 intelligent behavior	buxXimAna vyavahAra
VP29_LWG is	hE
ADJP31 possible	saMBava
,33 ,	,
CC34 and	Ora
S35 a conceptually straightforward path to building intelligence is to reverse engineer the computational principles behind the brain and duplicate its functionality	buxXi ke nirmANa ke lie eka vyAvahArika rUpa se sIXA rAswA iMjIniyara ko maswiRka ke pICe ke kaMpyUtara sixXAMwoM ko ultA karanA Ora isakI suviXA ko xoharAnA
NP36 a conceptually straightforward path to building intelligence	buxXi ke nirmANa ke lie eka vyAvahArika rUpa se sIXA rAswA
NP37 a conceptually straightforward path	eka vyAvahArika rUpa se sIXA rAswA
ADJP39 conceptually straightforward	vyAvahArika rUpa se sIXA
PP43 to building intelligence	buxXi banAne ke lie
NP45 building intelligence	buxXi kA nirmANa
VP48_LWG is	hE
S50 to reverse engineer the computational principles behind the brain and duplicate its functionality	iMjIniyara ko maswiRka ke pICe ke kaMpyUtara sixXAMwoM ko ultA karane Ora isakI suviXA ko xoharAne ke lie
VP51_LWG to reverse engineer duplicate	rivarsa iMjIniyara dupliketa karane ke lie
ADVP53 reverse	ultA
NP58 the computational principles behind the brain	maswiRka ke pICe gaNiwIya sixXAMwa
NP59 the computational principles	kampyUteSanala sixXAMwa
NNS62 principles	sixXAMwa
PP63 behind the brain	maswiRka ke pICe
NP65 the brain	maswiRka
CC68 and	Ora
VP56 engineer the computational principles behind the brain	iMjIniyara maswiRka ke pICe kampyUteSanala sixGAMwoM kA iMjIniyara
VP69 duplicate its functionality	isakI suviXA ko duplIketa kareM
NP71 its functionality	isakI suviXA

----
00269	Another 13 CHAPTER 1.	 eka Ora 13 CHAPTER 1 .		
268	268
FRAG1 Another 13 CHAPTER 1 .	eka Ora 13 ceptara 1
NP2 Another 13	eka Ora 13
NP5 CHAPTER 1	cEptara 1

----
00270	INTRODUCTION perspective is that it would be deeply interesting to understand the brain and the principles that underlie human intelligence, so machine learning models that shed light on these basic scientic questions are useful apart from their ability to solve engineering applications.	 aMwaHviRaya pariprekRya yaha hE ki maswiRka Ora una sixXAMwoM ko samaJanA bahuwa xilacaspa hogA jo mAnava buxXi ko kamajora karawe hEM , isalie maSInI SikRaNa moYdala jo ina buniyAxI vEjFAnika praSnoM para prakASa dAlawe hEM , iMjIniyariMga anuprayogoM ko hala karane kI unakI kRamawA ke alAvA upayogI hEM .		
269	269
S1 INTRODUCTION perspective is that it would be deeply interesting to understand the brain and the principles that underlie human intelligence , so machine learning models that shed light on these basic scientic questions are useful apart from their ability to solve engineering applications .	paricaya pariprekRya yaha hE ki maswiRka Ora mAnava buxXi ko kamajora karane vAle sixXAMwoM ko samaJanA gaharA xilacaspa hogA, isalie ina buniyAxI vEjFAnika praSnoM para
S2 INTRODUCTION perspective is that it would be deeply interesting to understand the brain and the principles that underlie human intelligence	paricaya pariprekRya yaha hE ki maswiRka Ora mAnava buxXi ko kamajora karane vAle sixXAMwoM ko samaJanA gaharA xilacaspa hogA
NP3 INTRODUCTION perspective	paricaya pariprekRya
VP6_LWG is	hE
SBAR8 that it would be deeply interesting to understand the brain and the principles that underlie human intelligence	yaha maswiRka Ora mAnava buxXi ko kamajora karane vAle sixXAMwoM ko samaJanA bahuwa xilacaspa hogA
S10 it would be deeply interesting to understand the brain and the principles that underlie human intelligence	maswiRka Ora mAnava buxXi ko kamajora karane vAle sixXAMwoM ko samaJanA bahuwa xilacaspa hogA
NP11 it	yaha
VP13_LWG would be	howA.
ADJP17 deeply interesting to understand the brain and the principles that underlie human intelligence	maswiRka Ora mAnava buxXi ko kamajora karane vAle sixXAMwoM ko samaJanA gaharA xilacaspa
S20 to understand the brain and the principles that underlie human intelligence	maswiRka Ora mAnava buxXi ko kamajora karane vAle sixXAMwoM ko samaJane ke lie
VP21_LWG to understand	samaJane ke lie
NP25 the brain and the principles that underlie human intelligence	maswiRka Ora mAnava buxXi ko kamajora karane vAle sixXAMwa
NP26 the brain and the principles	maswiRka Ora sixXAMwa
NP27 the brain	maswiRka
CC30 and	Ora
NP31 the principles	sixXAMwa
NNS33 principles	sixXAMwa
SBAR34 that underlie human intelligence	vaha mAnava buxXi kA aMdaralI
WHNP35 that	vaha
S37 underlie human intelligence	mAnava buxXimawwA ko aMdara lIjie
VP38_LWG underlie	aMdaralI
NP40 human intelligence	mAnava buxXi
,43 ,	,
CC44 so	Ese meM
S45 machine learning models that shed light on these basic scientic questions are useful apart from their ability to solve engineering applications	ina buniyAxI vEjFAnika praSnoM para prakASa dAlane vAle maSIna sIKane vAle moYdala iMjIniyariMga AvexanoM ke samAXAna kI unakI kRamawA ke alAvA upayogI hEM
NP46 machine learning models that shed light on these basic scientic questions	ina buniyAxI vEjFAnika savAloM para prakASa dAlane vAle maSIna sIKane vAle moYdala
NP47 machine learning models	maSIna sIKane ke moYdala
NML48 machine learning	maSIna sIKanA
NNS51 models	moYdala
SBAR52 that shed light on these basic scientic questions	ina buniyAxI vEjFAnika praSnoM para prakASa dAlA
WHNP53 that	vaha
S55 shed light on these basic scientic questions	ina buniyAxI vEjFAnika savAloM para prakASa dAlA
VP56_LWG shed	Seda
NP58 light	prakASa
PP60 on these basic scientic questions	ina buniyAxI vEjFAnika praSnoM para
NP62 these basic scientic questions	ye mUla vEjFAnika praSna
NNS66 questions	savAla
VP67_LWG are	hEM
ADJP69 useful apart from their ability to solve engineering applications	iMjIniyariMga AvexanoM ke samAXAna kI unakI kRamawA ke alAvA upayogI
PP71 apart from their ability to solve engineering applications	iMjIniyariMga AvexanoM ke samAXAna kI unakI kRamawA ke alAvA
ADVP72 apart	alaga
NP75 their ability to solve engineering applications	iMjIniyariMga AvexanoM ke samAXAna kI unakI kRamawA
S78 to solve engineering applications	iMjIniyariMga AvexanoM ke samAXAna ke lie
VP79_LWG to solve	hala karane ke lie
NP83 engineering applications	iMjIniyariMga ke Avexana
NNS85 applications	Avexana

----
0027	One of the key challenges in articial intelligence is how to get this informal knowledge into a computer.	 kqwrima buxXi meM eka pramuKa cunOwiyoM meM se eka hE ki kEse isa anOpacArika jFAna ko eka kaMpyUtara meM prApwa karane ke lie .		
26	26
S1 One of the key challenges in articial intelligence is how to get this informal knowledge into a computer .	kqwrima buxXimawwA meM pramuKa cunOwiyoM meM se eka yaha hE ki isa anOpacArika jFAna ko kaMpyUtara meM kEse prApwa kareM
NP2 One of the key challenges in articial intelligence	kqwrima buxXimawwA meM pramuKa cunOwiyoM meM se eka
NP3 One	eka
PP5 of the key challenges in articial intelligence	kqwrima buxXimawwA meM pramuKa cunOwiyoM kI
NP7 the key challenges in articial intelligence	kqwrima buxXimawwA meM pramuKa cunOwiyAM
NP8 the key challenges	pramuKa cunOwiyAM
NNS11 challenges	cunOwiyAM
PP12 in articial intelligence	kqwrima buxXimawwA meM
NP14 articial intelligence	kqwrima buxXimawwA
VP17_LWG is	hE
SBAR19 how to get this informal knowledge into a computer	isa anOpacArika jFAna ko kaMpyUtara meM kEse prApwa kareM
WHADVP20 how	kEse
S22 to get this informal knowledge into a computer	isa anOpacArika jFAna ko kaMpyUtara meM lAne ke lie
VP23_LWG to get	prApwa karane ke lie
NP27 this informal knowledge	yaha anOpacArika jFAna
PP31 into a computer	eka kaMpyUtara meM
NP33 a computer	eka kaMpyUtara

----
00271	The modern term deep learning goes beyond the neuroscientic perspective on the current breed of machine learning models.	 AXunika Sabxa xIpwa SikRaNa maSIna SikRaNa moYdala kI varwamAna nasla para waMwrikA vijFAnI pariprekRya se pare calA jAwA hE .		
270	270
S1 The modern term  deep learning  goes beyond the neuroscientic perspective on the current breed of machine learning models .	AXunika Sabxa gaharI sIKa maSIna sIKane ke moYdaloM kI mOjUxA nasla para nyUrosAijFAnika xqRtikoNa se pare jAwA hE
NP2 The modern term  deep learning 	AXunika Sabxa gaharI sIKa
NML5 term  deep learning 	Sabxa gaharI sIKa
VP11_LWG goes	jAwA hE
PP13 beyond the neuroscientic perspective on the current breed of machine learning models	maSIna sIKane ke moYdaloM kI mOjUxA nasla para nyUrosAijFAnika najarie se pare
NP15 the neuroscientic perspective on the current breed of machine learning models	maSIna sIKane vAle moYdaloM kI mOjUxA nasla para nyUrosAijFAnika pariprekRya
NP16 the neuroscientic perspective	nyUroSEjFAnika pariprekRya
PP20 on the current breed of machine learning models	maSIna larniMga moYdala kI mOjUxA nasla para
NP22 the current breed of machine learning models	maSIna larniMga moYdala kI mOjUxA nasla
NP23 the current breed	varwamAna nasla
PP27 of machine learning models	maSIna sIKane ke moYdala
NP29 machine learning models	maSIna sIKane ke moYdala
NML30 machine learning	maSIna sIKanA
NNS33 models	moYdala

----
00272	It appeals to a more general principle of learning multiple levels of composition, which can be applied in machine learning frameworks that are not necessarily neurally inspired.	 yaha saMyojana ke kaI swaroM ko sIKane ke eka aXika sAmAnya sixXAMwa ke lie apIla karawA hE , jo maSIna sIKane ke DAMce meM lAgU kiyA jA sakawA hE jo AvaSyaka rUpa se waMwrikA preriwa nahIM hEM .		
271	271
S1 It appeals to a more general principle of learning multiple levels of composition , which can be applied in machine learning frameworks that are not necessarily neurally inspired .	yaha racanA ke kaI swaroM ko sIKane ke eka aXika sAmAnya sixXAMwa kI apIla karawA hE, jise maSIna sIKane ke DAMce meM lAgU kiyA jA sakawA hE jo jarUrI nahIM hE
NP2 It	yaha
VP4_LWG appeals	apIla
PP6 to a more general principle of learning multiple levels of composition , which can be applied in machine learning frameworks that are not necessarily neurally inspired	racanA ke kaI swaroM ko sIKane ke aXika sAmAnya sixXAMwa ke lie, jise maSIna sIKane ke DAMce meM lAgU kiyA jA sakawA hE jo jarUrI rUpa se preriwa nahIM howe
NP8 a more general principle of learning multiple levels of composition , which can be applied in machine learning frameworks that are not necessarily neurally inspired	racanA ke kaI swara sIKane kA eka Ora sAmAnya sixXAMwa, jise maSIna sIKane ke DAMce meM lAgU kiyA jA sakawA hE jo jarUrI rUpa se preriwa nahIM howe
NP9 a more general principle	eka Ora sAmAnya sixXAMwa
ADJP11 more general	aXika sAmAnya
PP15 of learning multiple levels of composition , which can be applied in machine learning frameworks that are not necessarily neurally inspired	racanA ke kaI swara sIKane ke, jise maSIna sIKane ke DAMce meM lagAyA jA sakawA hE jinheM jarUrI rUpa se preriwa nahIM kiyA jAwA
S17 learning multiple levels of composition , which can be applied in machine learning frameworks that are not necessarily neurally inspired	racanA ke kaI swara sIKanA, jise maSIna sIKane ke DAMce meM lagAyA jA sakawA hE jo jarUrI nahIM honA jarUrI hE
VP18_LWG learning	sIKanA
NP20 multiple levels of composition , which can be applied in machine learning frameworks that are not necessarily neurally inspired	racanA ke kaI swara, jise maSIna sIKane ke DAMce meM lAgU kiyA jA sakawA hE jo jarUrI nahIM hE nyUrolI preriwa
NP21 multiple levels of composition	racanA ke kaI swara
NP22 multiple levels	kaI swara
NNS24 levels	swara
PP25 of composition	racanA kA
NP27 composition	racanA
,29 ,	,
SBAR30 which can be applied in machine learning frameworks that are not necessarily neurally inspired	jise maSIna sIKane ke DAMce meM lagAyA jA sakawA hE jinheM jarUrI nahIM honA jarUrI hE
WHNP31 which	jo ki
S33 can be applied in machine learning frameworks that are not necessarily neurally inspired	maSIna sIKane ke DAMce meM lagAyA jA sakawA hE jo jarUrI nahIM hE nyUrolI preriwa
VP34_LWG can be applied	lagAyA jA sakawA hE Avexana
PP40 in machine learning frameworks that are not necessarily neurally inspired	maSIna sIKane ke DAMce meM jo jarUrI nahIM howe anivArya rUpa se preriwa
NP42 machine learning frameworks that are not necessarily neurally inspired	maSIna sIKane ke DAMce jo jarUrI nahIM howe anivArya rUpa se preriwa
NP43 machine learning frameworks	maSIna sIKane ke DAMce
NML44 machine learning	maSIna sIKanA
NNS47 frameworks	DAMce
SBAR48 that are not necessarily neurally inspired	yaha jarUrI nahIM ki anivArya rUpa se preriwa hoM
WHNP49 that	vaha
S51 are not necessarily neurally inspired	jarUrI nahIM hE anivArya rUpa se preriwa
VP52_LWG are not necessarily	jarUrI nahIM
ADVP55 necessarily	jarUrI
ADJP57 neurally inspired	anAvaSyaka rUpa se preriwa

----
00273	The earliest predecessors of modern deep learning were simple linear models motivated from a neuroscientic perspective.	 AXunika gaharI sIKane ke sabase pahale pUrvavarwiyoM sarala rEKika moYdala We jo eka waMwrikA vijFAnI pariprekRya se preriwa We .		
272	272
S1 The earliest predecessors of modern deep learning were simple linear models motivated from a neuroscientic perspective .	AXunika gaharI sIKa ke sabase pahale pUrvavarwI sarala lAinara moYdala nyUrosAijFAnika xqRtikoNa se preriwa We
NP2 The earliest predecessors of modern deep learning	AXunika gaharI sIKa ke sabase pahale pUrvavarwI
NP3 The earliest predecessors	sabase pahale pUrvavarwI
NNS6 predecessors	pUrvavarwI
PP7 of modern deep learning	AXunika gaharI sIKa kA
NP9 modern deep learning	AXunika gaharI sIKa
VP13_LWG were	We
NP15 simple linear models motivated from a neuroscientic perspective	eka nyUrosoYlajI xqRtikoNa se preriwa sarala lAinara moYdala
NP16 simple linear models	sAXAraNa lAinara moYdala
NNS19 models	moYdala
VP20_LWG motivated	preriwa
PP22 from a neuroscientic perspective	nyUrosAijFAnika xqRtikoNa se
NP24 a neuroscientic perspective	eka nyUroSEjFAnika xqRtikoNa

----
00274	These models were designed to take a set of n input values x 1 , . .	 ina moYdaloM ko n inaputa mUlyoM kA eka seta lene ke lie dijZAina kiyA gayA WA x 1 , .		
273	273
S1 These models were designed to take a set of n input values x 1 , . .	ina moYdaloM ko ena inaputa mUlyoM kA seta lene ke lie dijAina kiyA gayA WA..
S2 These models were designed to take a set of n input values x 1	ina moYdaloM ko ena inaputa mUlyoM kA seta lene ke lie dijAina kiyA gayA WA
NP3 These models	ye moYdala
NNS5 models	moYdala
VP6_LWG were designed	We dijAina
S10 to take a set of n input values x 1	ena inaputa mUlyoM kA seta lene ke lie x 1
VP11_LWG to take	lene ke lie
NP15 a set of n input values x 1	ena inaputa mUlyoM kA eka seta eksa 1
NP16 a set	eka seta
PP19 of n input values x 1	n inaputa ke mUlya x 1
NP21 n input values x 1	n inaputa mUlyoM x 1
NP22 n input values	n inaputa mUlya
NNS25 values	mUlyoM
PP26 x 1	eksa 1
SYM27 x	eksa
NP28 1	1
,30 ,	,
S31 .	..

----
00275	.	 		
274	274
FRAG1 .	..

----
00276	, x n and associate them with an output y .	 , x n Ora unheM eka Autaputa y ke sAWa saMbaxXa 		
275	275
UCP1 , x n and associate them with an output y .	, eksa ena Ora unheM eka Autaputa ke sAWa jodZawe hEM
NP2 ,	,
,3 ,	,
NP4 x n	eksa ena
CC7 and	Ora
VP8 associate them with an output y	unheM Autaputa ke sAWa jodZeM
VP8_LWG associate	sahayogI
NP10 them	unheM
PP12 with an output y	eka Autaputa ke sAWa vAI
NP14 an output y	eka Autaputa vAI

----
00277	These models would learn a set of weights w 1 , . .	 ina moYdaloM vajana w 1 , kA eka seta sIKanA hogA .		
276	276
S1 These models would learn a set of weights w 1 , . .	ye moYdala vajana kA eka seta sIKeMge dablyU 1,..
S2 These models would learn a set of weights w 1	ye moYdala dablyU 1 vajana kA seta sIKeMge
NP3 These models	ye moYdala
NNS5 models	moYdala
VP6_LWG would learn	sIKeMge
NP10 a set of weights	vajana kA eka seta
NP11 a set	eka seta
PP14 of weights	vajana kA
NP16 weights	vajana
NNS17 weights	vajana
PP18 w 1	dablyU 1
NP20 1	1
,22 ,	,
S23 .	..

----
00278	.	 		
277	277
FRAG1 .	..

----
00279	, w n and compute their output f ( x, w ) =	 w n Ora unake Autaputa f ( x , w ) =		
278	278
NP1 , w n and compute their output f ( x , w ) =	, w n Ora unake Autaputa f (x, w ) kA gaNanA kareM =
FRAG2 , w n and compute their output f ( x , w ) =	, w n Ora unake Autaputa f (x, w ) kA gaNanA kareM =
,3 ,	,
PP4 w n and compute their output f ( x , w )	w n Ora unake Autaputa f (x, w ) kA kaMpyUtara kareM
NP6 n and compute their output f ( x , w )	n Ora unake Autaputa f (x, w ) kA kanavarta kareM
NP7 n and compute	ena Ora kaMpyUtara
CC9 and	Ora
NN8 n	n
NN10 compute	gaNanA
NP11 their output f	unakA Autaputa f
NML13 output f	Autaputa f
PRN16 ( x , w )	(x, w )
NP18 x	eksa
SYM19 x	eksa
,20 ,	,
NP21 w	dablyU
,24 =	=

----
00280	x 1 w 1	 1 x 1 w 1		
279	279
FRAG1 x 1 w 1	eksa 1 dablyU 1
NP2 x 1	eksa 1
SYM3 x	eksa
NP5 w 1	dablyU 1

----
0028	Several articial intelligence projects have sought to hard-code knowledge about the world in formal languages.	 kaI kqwrima AsUcanA pariyojanAoM ne OpacArika BARAoM meM viSva ke bAre meM jFAna ko kaTora banAne kA prayAsa kiyA hE 		
27	27
S1 Several articial intelligence projects have sought to hard - code knowledge about the world in formal languages .	kaI kqwrima buxXimawwA pariyojanAoM ne kadZI mehanawa kI hE- OpacArika BARAoM meM xuniyA ke bAre meM koda jFAna kI mAMga kI hE
NP2 Several articial intelligence projects	kaI kqwrima buxXimawwA pariyojanAeM
NNS6 projects	pariyojanAeM
VP7_LWG have sought	mAMgI hE mAMga
PP11 to hard - code knowledge	kadZI mehanawa karane ke lie- koda jFAna
NP13 hard - code knowledge	kaTina - koda jFAna
NML14 hard - code	kaTina - koda
PP19 about the world in formal languages	OpacArika BARAoM meM xuniyA ke bAre meM
NP21 the world in formal languages	OpacArika BARAoM meM xuniyA
NP22 the world	xuniyA
PP25 in formal languages	OpacArika BARAoM meM
NP27 formal languages	OpacArika BARAeM
NNS29 languages	BARAeM

----
00281	+  + x n	 +  + x n		
280	280
FRAG1 +    + x n	+    + x n
NP2 +	+
SYM3 +	+
NP4    + x n	   + x n
NP5    +	   +
SYM7 	
SYM8 	
SYM9 +	+
SYM10 x	eksa
NP11 n	n

----
00282	w n .	 w n .		
281	281
NP1 w n .	dablyU ena

----
00283	This rst wave of neural networks research was known as cybernetics, as illustrated in gure 1.7.	 waMwrikA netavarka anusaMXAna kI isa pahalI lahara ko sAibaranetiksa ke nAma se jAnA jAwA WA , jEsA ki saMKyA 1 . 7 meM xarSAyA gayA hE 		
282	282
S1 This rst wave of neural networks research was known as cybernetics , as illustrated in gure 1.7 .	waMwrikA netavarka anusaMXAna kI yaha pahalI lahara sAibaranetika ke rUpa meM jAnI jAwI WI, jEsA ki AMkadZA 1.7 meM xiKAyA gayA WA
NP2 This rst wave of neural networks research	waMwrikA netavarka SoXa kI yaha pahalI lahara
NP3 This rst wave	yaha pahalI lahara
PP7 of neural networks research	waMwrikA netavarka SoXa
NP9 neural networks research	waMwrikA netavarka SoXa
NML10 neural networks	waMwrikA netavarka
NNS12 networks	netavarka
VP14_LWG was known	jAnA jAwA WA
PP18 as cybernetics	sAibaranetika ke rUpa meM
NP20 cybernetics	sAibaranetika
NNS21 cybernetics	sAibaranetika
,22 ,	,
SBAR23 as illustrated in gure 1.7	jEsA ki AMkadZe 1.7 meM xiKAyA gayA hE
S25 illustrated in gure 1.7	AMkadZe 1.7 meM xiKAyA gayA prawIka
VP26_LWG illustrated	xiKAyA gayA ciwra
PP28 in gure 1.7	AMkadZe 1.7 meM
NP30 gure 1.7	AMkadZA 1.7

----
00284	The McCulloch-Pitts neuron (McCulloch and Pitts, 1943) was an early model of brain function.	 mEkakloca - pitsa nyUroYna ( McCulloch and Pitts , 1943 ) maswiRka kArya kA eka prAraMBika moYdala WA .		
283	283
S1 The McCulloch - Pitts neuron ( McCulloch and Pitts , 1943 ) was an early model of brain function .	mEkkulloca- pitsa nyUrona (mEkkulloca Ora pitsa, 1943) maswiRka samAroha kA prAraMBika moYdala WA
NP2 The McCulloch - Pitts neuron ( McCulloch and Pitts , 1943 )	mEkakulloca- pitsa nyUrona (mEkakulloca Ora pitsa, 1943)
NML4 McCulloch - Pitts	mEkkulloca- pitsa
PRN9 ( McCulloch and Pitts , 1943 )	(mEkakulloca Ora pitsa, 1943)
NP11 McCulloch and Pitts	mEkkulloca Ora pitsa
CC13 and	Ora
NNP12 McCulloch	mEkakulaloca
NNP14 Pitts	pitsa
,15 ,	,
NP16 1943	1943
VP19_LWG was	WA
NP21 an early model of brain function	maswiRka samAroha kA prAraMBika moYdala
NP22 an early model	eka prAraMBika moYdala
PP26 of brain function	maswiRka samAroha kI
NP28 brain function	brena PaMkSana

----
00285	This linear model could recognize two dierent categories of inputs by testing whether f ( x, w ) is positive or negative.	 yaha rEKika moYdala inaputa kI xo alaga alaga SreNiyoM ko pahacAna sakawA hE parIkRaNa xvArA ki kyA ePa ( x , w ) sakArAwmaka hE yA nakArAwmaka hE .		
284	284
S1 This linear model could recognize two dierent categories of inputs by testing whether f ( x , w ) is positive or negative .	yaha lAinara moYdala parIkRaNa karake inaputoM kI xo alaga-alaga SreNiyoM ko pahacAna sakawA hE ki kyA ePa (eksa, dablyU) sakArAwmaka yA nakArAwmaka hE
NP2 This linear model	yaha lAinara moYdala
VP6_LWG could recognize	pahacAna sakawe We
NP10 two dierent categories of inputs	inaputa kI xo alaga-alaga SreNiyAM
NP11 two dierent categories	xo alaga-alaga SreNiyAM
NNS14 categories	SreNiyoM
PP15 of inputs	inaputoM kI
NP17 inputs	inaputa
NNS18 inputs	inaputa
PP19 by testing whether f ( x , w ) is positive or negative	parIkRaNa karane se kyA ePa (x, w) sakArAwmaka yA nakArAwmaka
S21 testing whether f ( x , w ) is positive or negative	parIkRaNa cAhe ePa (x, w) sakArAwmaka ho yA nakArAwmaka
VP22_LWG testing	parIkRaNa
SBAR24 whether f ( x , w ) is positive or negative	cAhe ePa (x, w) sakArAwmaka ho yA nakArAwmaka
S26 f ( x , w ) is positive or negative	ePa (x, w) sakArAwmaka yA nakArAwmaka hE
NP27 f ( x , w )	ePa (eksa, dablyU)
,31 ,	,
VP34_LWG is	hE
ADJP36 positive or negative	sakArAwmaka yA nakArAwmaka
CC38 or	yA
JJ37 positive	sakArAwmaka
JJ39 negative	nakArAwmaka

----
00286	Of course, for the model to correspond to the desired denition of the categories, the weights needed to be set correctly.	 niSciwa rUpa se , moYdala ke lie SreNiyoM kI icCiwa pariBARA ke anurUpa hone ke lie , BAra ko sahI DaMga se seta karane kI AvaSyakawA howI hE .		
285	285
S1 Of course , for the model to correspond to the desired denition of the categories , the weights needed to be set correctly .	beSaka, SreNiyoM kI vAMCiwa pariBARA kA anuvAxa karane ke lie moYdala ke lie sahI se nirXAriwa kie jAne kI AvaSyakawA hE
ADVP2 Of course	beSaka
,5 ,	,
PP6 for the model to correspond to the desired denition of the categories	SreNiyoM kI vAMCiwa pariBARA kA anuvAxa karane ke lie moYdala ke lie
NP8 the model to correspond to the desired denition of the categories	SreNiyoM kI vAMCiwa pariBARA kA anuvAxa karane vAlA moYdala
S11 to correspond to the desired denition of the categories	SreNiyoM kI vAMCiwa pariBARA kA anuvAxa karane ke lie
VP12_LWG to correspond	saMvAxa karane ke lie
PP16 to the desired denition of the categories	SreNiyoM kI vAMCiwa pariBARA ke lie
NP18 the desired denition of the categories	SreNiyoM kI vAMCiwa pariBARA
NP19 the desired denition	vAMCiwa pariBARA
PP23 of the categories	SreNiyoM kI
NP25 the categories	SreNiyoM
NNS27 categories	SreNiyoM
,28 ,	,
NP29 the weights	vajana
NNS31 weights	vajana
VP32_LWG needed	jarUrawa
S34 to be set correctly	sahI seta honA hE
VP35_LWG to be set correctly	sahI seta honA hE
ADVP41 correctly	sahI warIke se

----
00287	These weights could be set by the human operator.	 ina vajana mAnava oYparetara xvArA seta kiyA jA sakawA hE .		
286	286
S1 These weights could be set by the human operator .	ye vajana mAnava saMcAlaka xvArA nirXAriwa kie jA sakawe hEM
NP2 These weights	ye vajana
NNS4 weights	vajana
VP5_LWG could be set	seta ho sakawe hEM
PP11 by the human operator	mAnava saMcAlaka xvArA
NP13 the human operator	mAnava saMcAlaka

----
00288	In the 1950s, the perceptron (Rosenblatt, 1958, 1962) became the rst model that could learn the weights that dened the categories given examples of inputs from each category.	 1950 ke xaSaka meM , parseptoYna (Rosenblatt , 1958 , 1962 ) pahalA moYdala bana gayA jo prawyeka SreNI se inaputa ke uxAharaNa xie gae vargoM ko pariBARiwa karane vAle BAra ko sIKa sakawA WA .		
287	287
S1 In the 1950s , the perceptron ( Rosenblatt , 1958 , 1962 ) became the rst model that could learn the weights that dened the categories given examples of inputs from each category .	1950 ke xaSaka meM, avacewana (rosebleta, 1958, 1962) pahalA moYdala bana gayA, jisameM vaha vajana sIKa sakawA WA jisane prawyeka SreNI se inaputa ke uxAhara
PP2 In the 1950s	1950 ke xaSaka meM
NP4 the 1950s	1950 ke xaSaka meM
NNS6 1950s	1950 ke xaSaka meM
,7 ,	,
NP8 the perceptron ( Rosenblatt , 1958 , 1962 )	avacewana (rosenableta, 1958, 1962)
NP9 the perceptron	avaXAraNA
PRN12 ( Rosenblatt , 1958 , 1962 )	(rosenableta, 1958, 1962)
NP14 Rosenblatt	rosenableta
,16 ,	,
NP17 1958 , 1962	1958, 1962
,19 ,	,
VP22_LWG became	bana gayA
NP24 the rst model that could learn the weights that dened the categories given examples of inputs from each category	pahalA moYdala jo vajana sIKa sakawA hE jisane prawyeka SreNI se inaputa ke uxAharaNa xie gae SreNiyoM ko pariBARiwa kiyA
NP25 the rst model	pahalA moYdala
SBAR29 that could learn the weights that dened the categories given examples of inputs from each category	vaha vajana sIKa sakawA hE jisane prawyeka SreNI se inaputa ke uxAharaNa xie gae SreNiyoM ko pariBARiwa kiyA
WHNP30 that	vaha
S32 could learn the weights that dened the categories given examples of inputs from each category	vo vajana sIKa sakawe hEM jinhoMne prawyeka SreNI se inaputa ke uxAharaNa xie gae SreNiyoM ko pariBARiwa kiyA
VP33_LWG could learn	sIKa sakawe We
NP37 the weights that dened the categories given examples of inputs from each category	jina vajanoM ne prawyeka SreNI se inaputa ke uxAharaNa xie gae SreNiyoM ko pariBARiwa kiyA
NP38 the weights	vajana
NNS40 weights	vajana
SBAR41 that dened the categories given examples of inputs from each category	jisameM prawyeka SreNI se inaputa ke uxAharaNa xie gae SreNiyoM ko pariBARiwa kiyA gayA
WHNP42 that	vaha
S44 dened the categories given examples of inputs from each category	prawyeka SreNI se inaputa ke uxAharaNa xie gae SreNiyoM ko pariBARiwa
VP45_LWG dened	pariBARiwa
NP47 the categories	SreNiyoM
NNS49 categories	SreNiyoM
PP50 given examples of inputs from each category	prawyeka SreNI se inaputa ke uxAharaNa xie
NP52 examples of inputs from each category	prawyeka SreNI se inaputa ke uxAharaNa
NP53 examples	uxAharaNa
NNS54 examples	uxAharaNa
PP55 of inputs from each category	prawyeka SreNI se inaputa
NP57 inputs from each category	prawyeka SreNI se inaputa
NP58 inputs	inaputa
NNS59 inputs	inaputa
PP60 from each category	prawyeka SreNI se
NP62 each category	prawyeka SreNI

----
00289	The adaptive linear element (ADALINE), which dates from about the same time, simply returned the value of f	 anukUlI rEKika wawva ( ADALINE ) , jo lagaBaga usI samaya se prAraMBa huA , kevala f kA mUlya lOtA xiyA .		
288	288
S1 The adaptive linear element ( ADALINE ) , which dates from about the same time , simply returned the value of f	anukUla lAinara wawva (edIelAina) jo lagaBaga usI samaya se wArIKeM, basa ePa kI kImawa vApasa kara xiyA
NP2 The adaptive linear element ( ADALINE ) , which dates from about the same time ,	anukUla lAinara wawva (edIelAina) jo lagaBaga usI samaya se wArIKeM raKawA hE,
NP3 The adaptive linear element ( ADALINE )	anukUla parivarwana wawva (edIlAina)
NP4 The adaptive linear element	anukUla parivarwana wawva
NP10 ADALINE	edalAina
,13 ,	,
SBAR14 which dates from about the same time	jisakI wArIKa lagaBaga usI samaya se
WHNP15 which	jo ki
S17 dates from about the same time	lagaBaga usI samaya se wiWi
VP18_LWG dates	wArIKeM
PP20 from about the same time	lagaBaga usI samaya se
PP22 about the same time	usI samaya ke bAre meM
NP24 the same time	vahI samaya
,28 ,	,
VP29_LWG simply returned	basa lOta AyA
ADVP30 simply	basa iwanA hI
NP33 the value of f	f kI kImawa
NP34 the value	mUlya
PP37 of f	ePa kA
NP39 f	f

----
00290	( x ) itself to predict a real number (Widrow and Ho, 1960) and could also learn to predict these numbers from data.	 ( x ) svayaM eka vAswavika saMKyA kI BaviRyavANI karane ke lie ( Widrow Ora Hoff , 1960 ) Ora BI detA se ina saMKyAoM kI BaviRyavANI karane ke lie sIKa sakawA hE .		
289	289
S1 ( x ) itself to predict a real number ( Widrow and Ho , 1960 ) and could also learn to predict these numbers from data .	(x) Kuxa eka vAswavika saMKyA kI BaviRyavANI karane ke lie (vidro Ora hoYPa, 1960) Ora ina saMKyAoM kI AMkadZe se BaviRyavANI karanA BI sIKa
LST2 ( x )	(x)
NP6 itself	hI Kuxa
VP8_LWG predict could also learn	BaviRyavANI BI sIKa sakawI WI

----
0029	A computer can reason automatically about statements in these formal languages using logical inference rules.	 eka kaMpyUtara ina OpacArika BARAoM meM wArkika anumAna niyamoM kA upayoga karawe hue bayAnoM ke bAre meM svacAliwa rUpa se kAraNa ho sakawA hE .		
28	28
S1 A computer can reason automatically about statements in these formal languages using logical inference rules .	eka kaMpyUtara wArkika anupAwa niyamoM kA upayoga karake ina OpacArika BARAoM meM bayAnoM ke bAre meM svacAliwa wOra para warka xe sakawA hE
NP2 A computer	eka kaMpyUtara
VP5_LWG can reason automatically	svacAliwa wOra para kara sakawe hEM kAraNa
ADVP9 automatically	svacAliwa rUpa se
PP11 about statements in these formal languages using logical inference rules	wArkika anupAwa niyamoM kA upayoga kara ina OpacArika BARAoM meM bayAna ke bAre meM
NP13 statements in these formal languages using logical inference rules	wArkika anupAwa niyamoM kA upayoga kara ina OpacArika BARAoM meM bayAna
NP14 statements	bayAna
NNS15 statements	bayAna
PP16 in these formal languages using logical inference rules	ina OpacArika BARAoM meM wArkika anupAwa niyamoM kA upayoga kara
NP18 these formal languages using logical inference rules	wArkika anupAwa niyamoM kA upayoga karawe hue ye OpacArika BARAeM
NP19 these formal languages	ye OpacArika BARAeM
NNS22 languages	BARAeM
VP23_LWG using	iswemAla karanA
NP25 logical inference rules	wArkika anupAwa ke niyama
NNS28 rules	niyama

----
00291	These simple learning algorithms greatly aected the modern landscape of ma- chine learning.	 ina sarala SikRaNa elgorixama ne mA - cInI sIKane ke AXunika parixqSya ko bahuwa praBAviwa kiyA 		
290	290
S1 These simple learning algorithms greatly aected the modern landscape of ma - chine learning .	ina sAXAraNa sIKa elgorixama ne mA- cIna sIKane ke AXunika parixqSya ko kAPI praBAviwa kiyA
NP2 These simple learning algorithms	ye sAXAraNa sIKa elgorixama
NNS6 algorithms	elgorixama
ADVP7 greatly	bahuwa kuCa
VP9_LWG aected	praBAviwa
NP11 the modern landscape of ma - chine learning	mA- cIna kA AXunika parixqSya
NP12 the modern landscape	AXunika parixqSya
PP16 of ma - chine learning	mA- cIna sIKa rahe hEM
NP18 ma - chine learning	mA- cIna sIKa rahe hEM
NML19 ma - chine	mA- cIna

----
00292	The training algorithm used to adapt the weights of the ADALINE	 praSikRaNa elgoriWma edIeelaAI ke vajana ko anukUliwa karane ke lie iswemAla kiyA		
291	291
S1 The training algorithm used to adapt the weights of the ADALINE	edIelAina ke vajana ko anukUla banAwA WA praSikRaNa elgorixama
NP2 The training algorithm	praSikRaNa elgorixama
VP6_LWG used	iswemAla
S8 to adapt the weights of the ADALINE	edIelAina ke vajana ko anukUla banAne ke lie
VP9_LWG to adapt	anukUlana karane ke lie
NP13 the weights of the ADALINE	edalAina kA vajana
NP14 the weights	vajana
NNS16 weights	vajana
PP17 of the ADALINE	edalAina kA
NP19 the ADALINE	edalAina

----
00293	was a special case of an algorithm called stochastic gradient descent .	 eka elgoriWma kA eka viSeRa mAmalA WA jise stochastic DAla avawaraNa kahawe hEM 		
292	292
S1 was a special case of an algorithm called stochastic gradient descent .	eka Elgorixama kA eka viSeRa mAmalA WA jise stoYcastika gredeja vaMSa kahA jAwA hE
VP2_LWG was	WA
NP4 a special case of an algorithm called stochastic gradient descent	stoYcastika gawiSIla vaMSa nAmaka elgorixama kA eka KAsa mAmalA
NP5 a special case	eka KAsa mAmalA
PP9 of an algorithm called stochastic gradient descent	stoYcastika gawiSIla vaMSa nAmaka eka elgorixama kA
NP11 an algorithm called stochastic gradient descent	stoYcastika gawiSIla vaMSa nAmaka eka elgorixama
NP12 an algorithm	eka elgorixama
VP15_LWG called	bulAyA gayA
NP17 stochastic gradient descent	stoYcastika gawiSIla vaMSa

----
00294	Slightly modied versions of the stochastic gradient descent algorithm remain the dominant training algorithms for deep learning models today.	 Stochastic gredieMta avawaraNa elgoriWma ke halke saMSoXiwa saMskaraNa Aja gahare sIKane ke moYdaloM ke lie pramuKa praSikRaNa elgorixama bane hue hEM 		
293	293
S1 Slightly modied versions of the stochastic gradient descent algorithm remain the dominant training algorithms for deep learning models today .	stoYcastika gredeja deMganA elgorixama ke WodZe saMSoXiwa saMskaraNa Aja gahare sIKane ke moYdaloM ke lie pramuKa praSikRaNa elgorixama bane hue
NP2 Slightly modied versions of the stochastic gradient descent algorithm	stoYcastika gawiSIla vaMSa elgorixama ke WodZe saMSoXiwa saMskaraNa
NP3 Slightly modied versions	WodZA saMSoXiwa saMskaraNa
ADJP4 Slightly modied	WodZA saMSoXiwa
NNS7 versions	saMskaraNa
PP8 of the stochastic gradient descent algorithm	stoYcastika gawiSIla vaMSa elgorixama kA
NP10 the stochastic gradient descent algorithm	stoYcastika gawiSIla vaMSa elgorixama
VP16_LWG remain	bane raheM
NP18 the dominant training algorithms	pramuKa praSikRaNa elgorixama
NNS22 algorithms	elgorixama
PP23 for deep learning models	gaharI sIKa ke moYdaloM ke lie
NP25 deep learning models	gaharI sIKa ke moYdala
NML26 deep learning	gaharI sIKa
NNS29 models	moYdala
NP-TMP30 today	Aja

----
00295	Models based on the f ( x, w ) used by the perceptron and ADALINE are called linear models .	 ePa ( x , w ) para AXAriwa moYdaloM ko parseptoYna Ora edIelaAIena xvArA prayoga kiyA jAwA hE , rEKika moYdala kahA jAwA hE .		
294	294
S1 Models based on the f ( x , w ) used by the perceptron and ADALINE are called linear models .	moYdaloM para AXAriwa ePa (eksa, dablyU) para iswemAla hone vAle Ora edAlAina ko lAinara moYdala kahA jAwA hE
S2 Models based on the f ( x , w ) used by the perceptron	avaXAraNA xvArA upayoga kie jAne vAle ePa (eksa, dablyU) para AXAriwa moYdala
NP3 Models based on the f ( x , w )	ePa (eksa, dablyU) para AXAriwa moYdala
NP4 Models	moYdala
NNS5 Models	moYdala
VP6_LWG based	AXAriwa
PP8 on the f	ePa para
NP10 the f	ePa
PRN13 ( x , w )	(x, w )
NP15 x	eksa
SYM16 x	eksa
,17 ,	,
NP18 w	dablyU
VP21_LWG used	iswemAla
PP23 by the perceptron	avaXAraNA xvArA
NP25 the perceptron	avaXAraNA
CC28 and	Ora
S29 ADALINE are called linear models	edIelAina ko lAinara moYdala kahA jAwA hE
NP30 ADALINE	edalAina
VP32_LWG are called	kahawe hEM
NP36 linear models	lAinara moYdala
NNS38 models	moYdala

----
00296	These models remain some of the most widely used machine learning models, though in many cases they are trained in dierent ways than the original models were trained.	 ye moYdala kuCa sarvAXika vyApaka rUpa se prayukwa maSIna larniMga moYdala bane hue hEM , hAlAMki kaI mAmaloM meM inheM mUla moYdaloM ko praSikRiwa kie jAne kI wulanA meM viBinna warIkoM se praSikRiwa kiyA jAwA hE .		
295	295
S1 These models remain some of the most widely used machine learning models , though in many cases they are trained in dierent ways than the original models were trained .	ye moYdala sabase aXika iswemAla hone vAle maSIna sIKane vAle kuCa moYdala bane hue hEM, hAlAMki kaI mAmaloM meM unheM mUla moYdaloM kI wulanA meM
NP2 These models	ye moYdala
NNS4 models	moYdala
VP5_LWG remain	bane raheM
NP7 some of the most widely used machine learning models	sabase vyApaka iswemAla hone vAle maSIna sIKane vAle kuCa moYdala
NP8 some	kuCa
PP10 of the most widely used machine learning models	sabase vyApaka iswemAla hone vAle maSIna sIKane vAle moYdaloM kI
NP12 the most widely used machine learning models	sabase vyApaka iswemAla hone vAle maSIna sIKane vAle moYdala
ADJP14 most widely used	sabase aXika iswemAla howA hE
NML18 machine learning	maSIna sIKanA
NNS21 models	moYdala
,22 ,	,
SBAR23 though in many cases they are trained in dierent ways than the original models were trained	hAlAMki kaI mAmaloM meM unheM mUla moYdaloM kI wulanA meM alaga waraha se praSikRiwa kiyA jAwA hE
S25 in many cases they are trained in dierent ways than the original models were trained	kaI mAmaloM meM unheM mUla moYdaloM kI wulanA meM alaga waraha se praSikRiwa kiyA jAwA hE
PP26 in many cases	kaI mAmaloM meM
NP28 many cases	kaI mAmale
NNS30 cases	mAmale
NP31 they	ve
VP33_LWG are trained	praSikRiwa hEM
PP37 in dierent ways	alaga-alaga warIkoM se
NP39 dierent ways	alaga-alaga warIke
NNS41 ways	warIke
SBAR42 than the original models were trained	mUla moYdaloM kI wulanA meM praSikRiwa
S44 the original models were trained	mUla moYdaloM ko kiyA gayA praSikRaNa
NP45 the original models	mUla moYdala
NNS48 models	moYdala
VP49_LWG were trained	praSikRiwa We

----
00297	Linear models have many limitations.	 reKIya mAdaloM kI aneka sImAeM howI hEM 		
296	296
S1 Linear models have many limitations .	lAinara moYdala kI kaI sImA howI hE
NP2 Linear models	lAinara moYdala
NNS4 models	moYdala
VP5_LWG have	pAsa hE
NP7 many limitations	kaI sImAeM
NNS9 limitations	sImAeM

----
00298	Most famously, they cannot learn the XOR function, where f ([0 , 1] , w )	 aXikAMSa prasixXa , ve sIKa nahIM sakawe samAroha , jahAM f ( X0 ) , X0 , X0 , X0 , X0 , X0		
297	297
S1 Most famously , they can not learn the XOR function , where f ( [ 0 , 1 ] , w )	sabase prasixXa rUpa se ve eksaoAra PaMkSana nahIM sIKa sakawe, jahAM ePa (0, 1 ], dablyU )
ADVP2 Most famously	sabase prasixXa
,5 ,	,
NP6 they	ve
VP8_LWG can not learn	nahIM sIKa sakawe
NP13 the XOR function	eksaoAra PaMkSana
,17 ,	,
SBAR18 where f	jahAM ePa
WHADVP19 where	kahAM
S21 f	f
VP22_LWG f	f
PRN24 ( [ 0 , 1 ] , w )	( 0, 1], w )
NP27 0 , 1	0, 1
,29 ,	,
,32 ,	,
NP33 w	dablyU

----
00299	= 1 and f (	 = 1 Ora f (		
298	298
NP1 = 1 and f (	= 1 Ora ePa (
NML3 1 and f	1 Ora ePa
NML4 1	1
CC6 and	Ora
NML7 f	f

----
00300	[1 , 0] , w )	 [1 ] , 0 ] , 0 ]		
299	299
PRN1 [ 1 , 0 ] , w )	[ 1, 0], w )
NP3 1	1
,5 ,	,
NP6 0 ]	0 ]
,9 ,	,
NP10 w	dablyU

----
0030	This is known as the knowledge base approach to articial intelligence.	 ise kqwrima buxXi ke jFAna kA AXAra xqRtikoNa kahA jAwA hE 		
29	29
S1 This is known as the knowledge base approach to articial intelligence .	yaha kqwrima buxXimawwA ke jFAna AXAra xqRtikoNa ke rUpa meM jAnA jAwA hE
NP2 This	yaha
VP4_LWG is known	jAnA jAwA hE
PP8 as the knowledge base approach	jFAna AXAra xqRtikoNa ke rUpa meM
NP10 the knowledge base approach	jFAna AXAra xqRtikoNa
PP15 to articial intelligence	kqwrima buxXimawwA ko
NP17 articial intelligence	kqwrima buxXimawwA

----
00301	= 1 but f (	 = 1 lekina f (		
300	300
NP1 = 1 but f (	= 1 lekina f (
NML3 1 but f	1 lekina ePa
NML4 1	1
CC6 but	lekina
NML7 f	f

----
00302	[1 , 1] , w )	 [ 1 ] , 1 ] , 1 ) , 1 ) , 1 )		
301	301
PRN1 [ 1 , 1 ] , w )	[ 1, 1], w )
NP3 1	1
,5 ,	,
NP6 1 ]	1]
,9 ,	,
NP10 w	dablyU

----
00303	= 0 and f ([0 , 0] , w ) = 0.	 = 0 Ora f ( JukanA0 , JukanA , laharAnA , laharAnA ) = 0		
302	302
FRAG1 = 0 and f ( [ 0 , 0 ] , w ) = 0 .	= 0 Ora f (0, 0 ], w ) = 0.
NP2 = 0 and f ( [ 0 , 0 ] , w )	= 0 Ora f( [0, 0], w )
NP3 = 0 and f ( [ 0 , 0 ]	= 0 Ora f (] 0, 0]
NP4 = 0	= 0
SYM5 =	=
CC7 and	Ora
NP8 f ( [ 0 , 0 ]	f ( 0, 0]
PRN10 ( [ 0 , 0 ]	( 0, 0]
NP13 0 , 0	0, 0
,15 ,	,
,18 ,	,
NP19 w	dablyU
,22 =	=
NP23 0	0

----
00304	Critics who observed these aws in linear models caused a backlash against biologically inspired learning in general (Minsky and Papert, 1969).	 rEKika moYdaloM meM ina KAmiyoM ko xeKane vAle AlocakoM ne sAmAnya rUpa se jEvika rUpa se preriwa sIKane ke KilAPa bEkalESa kiyA ( Minsky Ora pepareta , 1969 )		
303	303
S1 Critics who observed these aws in linear models caused a backlash against biologically inspired learning in general ( Minsky and Papert , 1969 ) .	jina AlocakoM ne lAinara moYdala meM ina KAmiyoM kA avalokana kiyA, unase vEjFAnika rUpa se preriwa sIKa (miMskI Ora peparata, 1969) meM bEkalESa
NP2 Critics who observed these aws in linear models	ina KAmiyoM ko lAinara moYdaloM meM xeKane vAle Alocaka
NP3 Critics	Alocaka
NNS4 Critics	Alocaka
SBAR5 who observed these aws in linear models	jinhoMne lAinara moYdala meM ina KAmiyoM kA kiyA avalokana
WHNP6 who	kOna
S8 observed these aws in linear models	lAinara moYdala meM xeKe ye KAmiyAM
VP9_LWG observed	kiyA gayA avalokana
NP11 these aws	ye KAmiyAM
NNS13 aws	KAmiyAM
PP14 in linear models	lAinara moYdala meM
NP16 linear models	lAinara moYdala
NNS18 models	moYdala
VP19_LWG caused	kAraNa
NP21 a backlash against biologically inspired learning	jEvika rUpa se preriwa sIKa ke KilAPa bEkaleSa
NP22 a backlash	eka bEkaleSa
PP25 against biologically inspired learning	vEjFAnika rUpa se preriwa sIKa ke KilAPa
NP27 biologically inspired learning	jEvika rUpa se preriwa sIKa
ADJP28 biologically inspired	AnuvaMSika rUpa se preriwa
PP32 in general	sAmAnya wOra para
NP34 general	sAmAnya
PRN36 ( Minsky and Papert , 1969 )	(miMskI Ora peparata, 1969)
NP38 Minsky and Papert	miniskI Ora peparata
CC40 and	Ora
NNP39 Minsky	miniskI
NNP41 Papert	kAgaja
,42 ,	,
NP43 1969	1969

----
00305	This was the rst major dip in the popularity of neural networks.	 yaha waMwrika netavarka kI lokapriyawA kI pahalI badZI dubakI WI 		
304	304
S1 This was the rst major dip in the popularity of neural networks .	waMwrikA netavarka kI lokapriyawA meM yaha pahalI badZI dubakI WI
NP2 This	yaha
VP4_LWG was	WA
NP6 the rst major dip in the popularity of neural networks	waMwrikA netavarka kI lokapriyawA meM pahalI badZI dubakI
NP7 the rst major dip	pahalI badZI dubakI
PP12 in the popularity of neural networks	waMwrikA netavarka kI lokapriyawA meM
NP14 the popularity of neural networks	waMwrikA netavarka kI lokapriyawA
NP15 the popularity	lokapriyawA
PP18 of neural networks	waMwrikA netavarka ke
NP20 neural networks	waMwrikA netavarka
NNS22 networks	netavarka

----
00306	14 CHAPTER 1.	 14 CHAPTER 1 .		
305	305
FRAG1 14 CHAPTER 1 .	14 cEptara 1.
NP2 14	14
NP4 CHAPTER 1	cEptara 1

----
00307	INTRODUCTION Today, neuroscience is regarded as an important source of inspiration for deep learning researchers, but it is no longer the predominant guide for the eld.	 INTRODUCTER Aja , waMwrikA vijFAna gaharI sIKane ke SoXakarwAoM ke lie preraNA kA eka mahawvapUrNa srowa mAnA jAwA hE , lekina yaha aba kRewra ke lie pramuKa gAida nahIM hE .		
306	306
S1 INTRODUCTION Today , neuroscience is regarded as an important source of inspiration for deep learning researchers , but it is no longer the predominant guide for the eld .	uxGAtana Aja, gahana sIKa anusaMXAnakarwAoM ke lie preraNA ke mahawvapUrNa srowa ke rUpa meM nyUrosAiMsa ko mAnA jAwA hE, lekina yaha aba kRewra ke lie pramuKa gAida nahIM
S2 INTRODUCTION Today , neuroscience is regarded as an important source of inspiration for deep learning researchers	uxGAtana Aja, gahana sIKa anusaMXAnakarwAoM ke lie preraNA ke mahawvapUrNa srowa ke rUpa meM mAnA jAwA hE nyUrosAiMsa
NP-TMP3 INTRODUCTION Today	paricaya Aja
,6 ,	,
NP7 neuroscience	nyUrosAiMsa
VP9_LWG is regarded	mAnA jAwA hE
PP13 as an important source of inspiration	preraNA ke mahawvapUrNa srowa ke rUpa meM
NP15 an important source of inspiration	preraNA kA mahawvapUrNa srowa
NP16 an important source	eka mahawvapUrNa srowa
PP20 of inspiration	preraNA kI
NP22 inspiration	preraNA
PP24 for deep learning researchers	gahana sIKa anusaMXAnakarwAoM ke lie
NP26 deep learning researchers	gahana sIKa anusaMXAnakarwAoM
NML27 deep learning	gaharI sIKa
NNS30 researchers	SoXakarwAoM
,31 ,	,
CC32 but	lekina
S33 it is no longer the predominant guide for the eld	yaha aba mExAna ke lie pramuKa gAida nahIM hE
NP34 it	yaha
VP36_LWG is no	nahIM hE
ADVP38 no longer	aba nahIM
NP41 the predominant guide for the eld	mExAna ke lie pramuKa gAida
NP42 the predominant guide	pramuKa gAida
PP46 for the eld	Kewa ke lie
NP48 the eld	Kewa

----
00308	The main reason for the diminished role of neuroscience in deep learning research today is that we simply do not have enough information about the brain to use it as a guide.	 Aja gahana aXyayana anusaMXAna meM waMwrikA vijFAna kI kama BUmikA kA muKya kAraNa yaha hE ki hamAre pAsa maswiRka ke bAre meM paryApwa jAnakArI nahIM hE ki hama ise eka gAida ke rUpa meM iswemAla kara sakeM 		
307	307
S1 The main reason for the diminished role of neuroscience in deep learning research today is that we simply do not have enough information about the brain to use it as a guide .	gahana sIKa anusaMXAna meM nyUrosAiMsa kI kamI kI muKya vajaha Aja yaha hE ki hamAre pAsa sirPa eka gAida ke rUpa meM isakA upayoga karane ke lie maswiRka ke bAre
NP2 The main reason for the diminished role of neuroscience in deep learning research	gahana sIKa anusaMXAna meM nyUrosAiMsa kI kamI kI muKya vajaha
NP3 The main reason	muKya kAraNa
PP7 for the diminished role of neuroscience in deep learning research	gahana sIKa anusaMXAna meM nyUrosAiMsa kI kamI kI BUmikA ke lie
NP9 the diminished role of neuroscience in deep learning research	gahana sIKa anusaMXAna meM nyUrosAiMsa kI kamI kI BUmikA
NP10 the diminished role	kama huI BUmikA
PP14 of neuroscience in deep learning research	gahana sIKa anusaMXAna meM nyUrosAiMsa kA
NP16 neuroscience in deep learning research	gahana sIKa anusaMXAna meM nyUrosAiMsa
NP17 neuroscience	nyUrosAiMsa
PP19 in deep learning research	gahana sIKa anusaMXAna meM
NP21 deep learning research	gahana sIKa anusaMXAna
NML22 deep learning	gaharI sIKa
NP-TMP26 today	Aja
VP28_LWG is	hE
SBAR30 that we simply do not have enough information about the brain to use it as a guide	hamAre pAsa basa eka gAida ke rUpa meM ise upayoga karane ke lie maswiRka ke bAre meM paryApwa jAnakArI nahIM hE
S32 we simply do not have enough information about the brain to use it as a guide	hamAre pAsa basa eka gAida ke rUpa meM ise upayoga karane ke lie maswiRka ke bAre meM paryApwa jAnakArI nahIM hE
NP33 we	hama
ADVP35 simply	basa iwanA hI
VP37_LWG do not have	na hoM
NP42 enough information	paryApwa jAnakArI
PP45 about the brain	maswiRka ke bAre meM
NP47 the brain	maswiRka
S50 to use it as a guide	ise gAida ke rUpa meM iswemAla karane ke lie
VP51_LWG to use	iswemAla ke lie
NP55 it	yaha
PP57 as a guide	eka gAida ke rUpa meM
NP59 a guide	eka gAida

----
00309	To obtain a deep understanding of the actual algorithms used by the brain, we would need to be able to monitor the activity of (at the very least) thousands of interconnected neurons simultaneously.	 maswiRka xvArA prayoga kie jAne vAle vAswavika elgorixama kI gaharI samaJa prApwa karane ke lie , hameM eka sAWa ( kama se kama hajAroM paraspara saMbaxXa nyUroYnsa kI gawiviXi kI nigarAnI karane meM sakRama honA hogA 		
308	308
S1 To obtain a deep understanding of the actual algorithms used by the brain , we would need to be able to monitor the activity of ( at the very least ) thousands of interconnected neurons simultaneously .	maswiRka xvArA upayoga kie gae vAswavika elgorixama kI gaharI samaJa prApwa karane ke lie hameM (bahuwa kama se kama) hajAroM iMtarakanekteda nyUronsa kI gawiviXi
S2 To obtain a deep understanding of the actual algorithms used by the brain	maswiRka xvArA upayoga kie gae vAswavika elgorixama kI gaharI samaJa pAne ke lie
VP3_LWG To obtain	prApwa karane ke lie
NP7 a deep understanding of the actual algorithms used by the brain	maswiRka xvArA upayoga kie gae vAswavika elgorixama kI gaharI samaJa
NP8 a deep understanding	gaharI samaJa
PP12 of the actual algorithms used by the brain	maswiRka xvArA upayoga kie gae vAswavika elgorixama kA
NP14 the actual algorithms used by the brain	maswiRka xvArA iswemAla kie gae vAswavika elgorixama
NP15 the actual algorithms	vAswavika elgorixama
NNS18 algorithms	elgorixama
VP19_LWG used	iswemAla
PP21 by the brain	maswiRka se
NP23 the brain	maswiRka
,26 ,	,
NP27 we	hama
VP29_LWG would need	jarUrawa hogI
S33 to be able to monitor the activity of ( at the very least ) thousands of interconnected neurons simultaneously	(bahuwa kama se kama) hajAroM iMtarakanekteda nyUronsa kI gawiviXi kI nigarAnI karane meM sakRama honA cAhie
VP34_LWG to be simultaneously	eka sAWa honA hE
ADJP38 able to monitor the activity of ( at the very least ) thousands of interconnected neurons	(bahuwa kama se kama) hajAroM iMtarakanekteda nyUrona kI gawiviXi kI nigarAnI kara sakawe hEM
S40 to monitor the activity of ( at the very least ) thousands of interconnected neurons	(bahuwa kama se kama) hajAroM iMtarakanekteda nyUrona kI gawiviXi para najara raKane ke lie
VP41_LWG to monitor the	nigarAnI karane ke lie
ADVP45 the activity of	kI gawiviXi
NP46 the activity	gawiviXi
PP51 at the very least	bahuwa kama se kama
NP53 the very least	bahuwa kama se kama
ADJP55 very least	bahuwa kama se kama
NP59 thousands of interconnected neurons	hajAroM iMtarakanekteda nyUrona
NP60 thousands	hajAroM
NNS61 thousands	hajAroM
PP62 of interconnected neurons	iMtarakanekteda nyUrona kI
NP64 interconnected neurons	iMtarakanekteda nyUrona
NNS66 neurons	nyUrona
ADVP67 simultaneously	eka sAWa

----
00310	Because we are not able to do this, we are far from understanding even some of the most simple and well-studied parts of the brain (Olshausen and Field, 2005).	 kyoMki hama EsA nahIM kara pA rahe hEM , hama maswiRka ke kuCa sabase sarala Ora acCI waraha se jFAwa BAgoM ko BI samaJane se xUra hEM ( olaSasana Ora PIlda , 2005 )		
309	309
S1 Because we are not able to do this , we are far from understanding even some of the most simple and well - studied parts of the brain ( Olshausen and Field , 2005 ) .	kyoMki hama EsA karane meM sakRama nahIM hEM, hama maswiRka ke kuCa sabase sarala Ora acCI waraha se paDZe gae kuCa hissoM ko BI samaJane se bahuwa xUra
SBAR2 Because we are not able to do this	kyoMki hama EsA nahIM kara pA rahe hEM
S4 we are not able to do this	EsA nahIM kara pA rahe hEM hama
NP5 we	hama
VP7_LWG are not	nahIM hEM
ADJP10 able to do this	EsA kara pA rahe hEM
S12 to do this	EsA karane ke lie
VP13_LWG to do	karane ke lie
NP17 this	yaha
,19 ,	,
NP20 we	hama
VP22_LWG are far	bahuwa xUra hEM
ADVP24 far from understanding even	samaJa se BI xUra
PP26 from understanding even	samaJa se BI
NP28 understanding even	samaJaxArI BI
NP31 some of the most simple and well - studied parts of the brain	kuCa sabase sarala Ora acCI waraha se- maswiRka ke kuCa hissoM kA aXyayana
NP32 some	kuCa
PP34 of the most simple and well - studied parts of the brain	sabase sarala Ora acCI waraha se- maswiRka ke aXyayaniwa hissoM kA
NP36 the most simple and well - studied parts of the brain	sabase sarala Ora acCI waraha se- maswiRka ke kuCa hissoM kA aXyayana
NP37 the most simple and well - studied parts	sabase sarala Ora acCI waraha se paDZe-liKe hissoM
ADJP39 most simple and well - studied	sabase sarala Ora acCI waraha se paDZA-
ADJP40 most simple	sabase sarala
CC43 and	Ora
ADJP44 well - studied	acCI waraha se paDZA-
NNS48 parts	BAgoM
PP49 of the brain	maswiRka kA
NP51 the brain	maswiRka
PRN54 ( Olshausen and Field , 2005 )	(olahasana Ora PIlda, 2005)
NP56 Olshausen and Field	oYlahasana Ora PIlda
CC58 and	Ora
NNP57 Olshausen	oYlaSuAna
NNP59 Field	PIlda
,60 ,	,
NP61 2005	2005

----
0031	None of these projects has led to a major success.	 inameM se kisI BI pariyojanA ko badZI saPalawA nahIM milI hE 		
30	30
S1 None of these projects has led to a major success .	inameM se kisI ne BI badZI saPalawA nahIM hAsila kI hE
NP2 None of these projects	inameM se koI BI projekta nahIM
NP3 None	koI nahIM
PP5 of these projects	ina pariyojanAoM kI
NP7 these projects	ye pariyojanAeM
NNS9 projects	pariyojanAeM
VP10_LWG has led	newqwva kiyA hE
PP14 to a major success	eka badZI saPalawA ke lie
NP16 a major success	eka badZI saPalawA

----
00311	Neuroscience has given us a reason to hope that a single deep learning algorithm can solve many dierent tasks.	 waMwrikA vijFAna hameM ummIxa karane ke lie eka kAraNa xiyA hE ki eka ekala gaharI sIKane elgoriWma kaI alaga alaga kAryoM ko hala kara sakawe hEM .		
310	310
S1 Neuroscience has given us a reason to hope that a single deep learning algorithm can solve many dierent tasks .	nyUrosAiMsa ne hameM ummIxa kA kAraNa bawAyA hE ki eka hI gaharI sIKa kA elgorixama kaI alaga-alaga kAryoM ko hala kara sakawA hE
NP2 Neuroscience	nyUrosAiMsa
VP4_LWG has given	xe cuke hEM
NP8 us	hameM
NP10 a reason to hope that a single deep learning algorithm can solve many dierent tasks	ummIxa kI vajaha hE ki eka hI gaharI sIKa kA elgorixama kaI alaga-alaga kAryoM kA samAXAna kara sakawA hE
S13 to hope that a single deep learning algorithm can solve many dierent tasks	ummIxa karane ke lie ki eka BI gaharI sIKa kA elgorixama kaI alaga-alaga kAryoM kA samAXAna kara sakawA hE
VP14_LWG to hope	ummIxa karane ke lie
SBAR18 that a single deep learning algorithm can solve many dierent tasks	ki eka hI gaharI sIKa kA elgorixama kaI alaga-alaga kAryoM kA samAXAna kara sakawA hE
S20 a single deep learning algorithm can solve many dierent tasks	eka hI gaharI sIKa kA elgorixama kaI alaga-alaga kAryoM kA samAXAna kara sakawA hE
NP21 a single deep learning algorithm	eka hI gaharI sIKa elgorixama
NML24 deep learning	gaharI sIKa
VP28_LWG can solve	hala kara sakawe hEM
NP32 many dierent tasks	kaI alaga-alaga kArya
NNS35 tasks	kAryoM

----
00312	Neuroscientists have found that ferrets can learn to see with the auditory processing region of their brain if their brains are rewired to send visual signals to that area (Von Melchner et al., 2000).	 waMwrikA vijFAniyoM ne pAyA hE ki nqtsa apane maswiRka ke SravaNa saMsAXana kRewra ke sAWa sAMKya sIKa sakawe hEM yaxi unake maswiRka ko usa kRewra meM xqSya saMkewa Bejane ke lie punarAvarwiwa kiyA jAwA hE ( vAna melcara eta ala 2000 ) .		
311	311
S1 Neuroscientists have found that ferrets can learn to  see  with the auditory processing region of their brain if their brains are rewired to send visual signals to that area ( Von Melchner et al. , 2000 ) .	nyUro vEjFAnikoM ne pAyA hE ki agara unake ximAga ke oYditorarI prosesiMga kRewra ke sAWa unake maswiRka ko Pira se BejA jAwA hE wo unake ximAga ko usa kRewra (vo
NP2 Neuroscientists	nyUro vEjFAnika
NNS3 Neuroscientists	nyUro vEjFAnika
VP4_LWG have found	pAyA hE
SBAR8 that ferrets can learn to  see  with the auditory processing region of their brain if their brains are rewired to send visual signals to that area	yaxi unake maswiRka ke oYditorarI prosesiMga kRewra ke sAWa unake maswiRka ke sAWa xeKeM sIKa sakawe hEM yaxi unake ximAga ko usa kRewra meM xqSya saMka
S10 ferrets can learn to  see  with the auditory processing region of their brain if their brains are rewired to send visual signals to that area	Peretsa apane maswiRka ke oYditorarI prosesiMga kRewra ke sAWa xeKeM sIKa sakawe hEM agara unake ximAga ko usa kRewra meM xqSya saMkewa Bejane ke lie Pira
NP11 ferrets	Peretsa
NNS12 ferrets	Peretsa
VP13_LWG can learn	sIKa sakawe hEM
S17 to  see  with the auditory processing region of their brain	unake maswiRka ke oYditoritI prosesiMga kRewra ke sAWa xeKeM
VP18_LWG to see	xeKane ke lie
PP24 with the auditory processing region of their brain	unake maswiRka ke oYditoritI prosesiMga kRewra ke sAWa
NP26 the auditory processing region of their brain	unake maswiRka kA oYditarI prosesiMga kRewra
NP27 the auditory processing region	oYditarI prasaMskaraNa kRewra
PP32 of their brain	unake maswiRka kI
NP34 their brain	unakA maswiRka
SBAR37 if their brains are rewired to send visual signals to that area	agara unake ximAga ko usa kRewra ko xqSya saMkewa Bejane ke lie Pira se Beja xiyA jAwA hE
S39 their brains are rewired to send visual signals to that area	usa kRewra ko xqSya saMkewa Bejane ke lie unake ximAga ko Pira se jArI kiyA jAwA hE
NP40 their brains	unake ximAgZa
NNS42 brains	ximAgZa
VP43_LWG are	hEM
ADJP45 rewired to send visual signals to that area	usa kRewra ko xqSya saMkewa Bejane ke lie kiyA gayA punarmilana
S47 to send visual signals to that area	usa kRewra ko xqSya saMkewa Bejane ke lie
VP48_LWG to send	Bejane ke lie
NP52 visual signals	xqSya saMkewa
NNS54 signals	saMkewa
PP55 to that area	usa kRewra ko
NP57 that area	vaha ilAkA
PRN60 ( Von Melchner et al. , 2000 )	(vona melacEna eta ala, 2000)
NP62 Von Melchner et al. , 2000	voYna melcanara eta ala, 2000
NP63 Von Melchner et al.	voYna melcanara eta ala
NP64 Von Melchner	voYna melcanara
ADVP67 et al.	eta ala
,70 ,	,
NP71 2000	2000

----
00313	This suggests that much of the mammalian brain might use a single algorithm to solve most of the dierent tasks that the brain solves.	 yaha suJAva hE ki swanaXArI maswiRka ke bahuwa sAre viBinna kAryoM hE ki maswiRka ko hala karawA hE ke aXikAMSa ko hala karane ke lie eka ekala elgoriWma kA upayoga kara sakawA hE .		
312	312
S1 This suggests that much of the mammalian brain might use a single algorithm to solve most of the dierent tasks that the brain solves .	isase pawA calawA hE ki jyAxAwara alaga-alaga kAryoM ko hala karane ke lie swanaXArI maswiRka eka hI elgorixama kA iswemAla kara sakawA hE
NP2 This	yaha
VP4_LWG suggests	suJAva
SBAR6 that much of the mammalian brain might use a single algorithm to solve most of the dierent tasks that the brain solves	swana hala karane vAle aXikAMSa viBinna kAryoM ko hala karane ke lie swana kA bahuwa sArA swana ekala elgorixama kA upayoga kara sakawA hE
S8 much of the mammalian brain might use a single algorithm to solve most of the dierent tasks that the brain solves	swana hala karane vAle aXikAMSa alaga-alaga kAryoM ke samAXAna ke lie bahuwa sArA swana ekala elgorixama kA upayoga kara sakawA hE
NP9 much of the mammalian brain	swanapAyI maswiRka kA bahuwa kuCa
NP10 much	bahuwa kuCa
PP12 of the mammalian brain	swanapAyI maswiRka kA
NP14 the mammalian brain	swanapAyI maswiRka
VP18_LWG might use	iswemAla ho sakawA hE
NP22 a single algorithm	eka hI elgorixama
S26 to solve most of the dierent tasks that the brain solves	jyAxAwara viBinna kAryoM ko hala karane ke lie jo maswiRka hala karawA hE
VP27_LWG to solve	hala karane ke lie
NP31 most of the dierent tasks	jyAxAwara alaga-alaga kArya
NP32 most	sabase jyAxA
PP34 of the dierent tasks	alaga-alaga kAryoM kA
NP36 the dierent tasks	alaga-alaga kArya
NNS39 tasks	kAryoM
SBAR40 that the brain solves	ki maswiRka hala karawA hE
S42 the brain solves	maswiRka hala karawA hE
NP43 the brain	maswiRka
VP46_LWG solves	hala karawA hE

----
00314	Before this hypothesis, machine learning research was more fragmented, with dierent communities of researchers studying natural language processing, vision, motion planning and speech recognition.	 isa parikalpanA se pahale , maSIna aXigama anusaMXAna aXika KaMdiwa WA , SoXakarwAoM ke viBinna samuxAyoM ke sAWa jo prAkqwika BARA saMsAXana , xqRti , gawi yojanA Ora BARaNa mAnyawA kA aXyayana kara rahe We .		
313	313
S1 Before this hypothesis , machine learning research was more fragmented , with dierent communities of researchers studying natural language processing , vision , motion planning and speech recognition .	isa parikalpanA se pahale, maSIna sIKa anusaMXAna aXika viBAjiwa WA, jisameM prAkqwika BARA prasaMskaraNa, xqRti, moSana plAniMga Ora BARaNa mAnyawA kA aXyayana
PP2 Before this hypothesis	isa parikalpanA se pahale
NP4 this hypothesis	yaha parikalpanA
,7 ,	,
NP8 machine learning research	maSIna sIKa anusaMXAna
NML9 machine learning	maSIna sIKanA
VP13_LWG was	WA
ADJP15 more fragmented	aXika vikqwa
,18 ,	,
PP19 with dierent communities of researchers studying natural language processing , vision , motion planning and speech recognition	prAkqwika BARA prasaMskaraNa, xqRti, moSana plAniMga Ora BARaNa mAnyawA prApwa karane vAle anusaMXAnakarwAoM ke viBinna samuxAyoM ke sAWa
NP21 dierent communities of researchers studying natural language processing , vision , motion planning and speech recognition	prAkqwika BARA prasaMskaraNa, xqRti, moSana plAniMga Ora BARaNa mAnyawA kA aXyayana karane vAle anusaMXAnakarwAoM ke viBinna samuxAya
NP22 dierent communities	alaga-alaga samuxAya
NNS24 communities	samuxAyoM
PP25 of researchers studying natural language processing , vision , motion planning and speech recognition	prAkqwika BARA prasaMskaraNa, xqRti, moSana plAniMga Ora BARaNa mAnyawA kA aXyayana karane vAle anusaMXAnakarwAoM
NP27 researchers studying natural language processing , vision , motion planning and speech recognition	prAkqwika BARA prasaMskaraNa, xqRti, moSana plAniMga Ora BARaNa mAnyawA kA aXyayana karane vAle anusaMXAnakarwA
NP28 researchers	SoXakarwAoM
NNS29 researchers	SoXakarwAoM
VP30_LWG studying	paDZAI
NP32 natural language processing , vision , motion planning and speech recognition	prAkqwika BARA prasaMskaraNa, xqRti, gawi yojanA Ora BARaNa mAnyawA
NML33 natural language processing , vision , motion planning and speech	prAkqwika BARA prasaMskaraNa, xqRti, gawi yojanA Ora BARaNa
NML34 natural language processing	prAkqwika BARA prasaMskaraNa
,38 ,	,
NML39 vision	xqRti
,41 ,	,
NML42 motion planning	moSana plAniMga
CC45 and	Ora
NML46 speech	BARaNa

----
00315	Today, these application communities are still separate, but it is common for deep learning research groups to study many or even all these application areas simultaneously.	 Aja , ina anuprayoga samuxAyoM aBI BI alaga hEM , lekina gahare sIKane anusaMXAna samUhoM ke lie eka sAWa kaI yA yahAM waka ki ina saBI anuprayoga kRewroM kA aXyayana karanA Ama bAwa hE .		
314	314
S1 Today , these application communities are still separate , but it is common for deep learning research groups to study many or even all these application areas simultaneously .	Aja yaha Avexana samuxAya aBI BI alaga hEM, lekina gahana sIKa anusaMXAna samUhoM ke lie ina saBI Avexana kRewroM kA eka sAWa aXyayana karanA Ama bAwa hE
S2 Today , these application communities are still separate	Aja aba BI alaga hEM ye Avexana samuxAya
NP-TMP3 Today	Aja
,5 ,	,
NP6 these application communities	ye Avexana samuxAya
NNS9 communities	samuxAyoM
VP10_LWG are still	aBI BI hEM
ADVP12 still	aBI BI
ADJP14 separate	alaga
,16 ,	,
CC17 but	lekina
S18 it is common for deep learning research groups to study many or even all these application areas simultaneously	gahana sIKa anusaMXAna samUhoM ke lie yaha eka sAWa kaI yA ina saBI Avexana kRewroM kA aXyayana karanA Ama hE
NP19 it	yaha
VP21_LWG is	hE
ADJP23 common for deep learning research groups	gahana sIKa anusaMXAna samUhoM ke lie Ama
PP25 for deep learning research groups	gahana sIKa anusaMXAna samUhoM ke lie
NP27 deep learning research groups	gahana sIKa anusaMXAna samUha
NML29 learning research	sIKanA SoXa
NNS32 groups	samUha
S33 to study many or even all these application areas simultaneously	kaI yA yahAM waka ki ina saBI Avexana kRewroM kA eka sAWa aXyayana karane ke lie
VP34_LWG to study simultaneously	eka sAWa paDZAI karane ke lie
NP38 many or even all these application areas	kaI yA yahAM waka ki ye saBI Avexana kRewra
UCP39 many or even all these application	kaI yA yahAM waka ki ye saBI Avexana
ADJP40 many	kaI
CC42 or	yA
NP43 even all these application	yahAM waka ki ye saBI Avexana
NNS48 areas	ilAke
ADVP49 simultaneously	eka sAWa

----
00316	We are able to draw some rough guidelines from neuroscience.	 hama waMwrikA vijFAna se kuCa rUKe xiSA nirxeSa KIMcane meM sakRama hEM 		
315	315
S1 We are able to draw some rough guidelines from neuroscience .	hama nyUrosAiMsa se kuCa kaTina xiSA-nirxeSa nikAlane meM sakRama hEM
NP2 We	hama
VP4_LWG are	hEM
ADJP6 able to draw some rough guidelines from neuroscience	nyUrosAiMsa se kuCa kaTina xiSA-nirxeSa nikAlane meM sakRama
S8 to draw some rough guidelines from neuroscience	nyUrosAiMsa se kuCa kaTina xiSA-nirxeSa nikAlane ke lie
VP9_LWG to draw	AkarRiwa karane ke lie
NP13 some rough guidelines	kuCa kaTina xiSA-nirxeSa
NNS16 guidelines	xiSA-nirxeSa
PP17 from neuroscience	nyUrosAiMsa se
NP19 neuroscience	nyUrosAiMsa

----
00317	The basic idea of having many computational units that become intelligent only via their interactions with each other is inspired by the brain.	 kaI kampyUteSanala ikAiyoM ke hone kA mUla vicAra jo kevala eka xUsare ke sAWa unakI bAwacIwa ke mAXyama se buxXimAna bana jAwe hEM , maswiRka se preriwa howA hE .		
316	316
S1 The basic idea of having many computational units that become intelligent only via their interactions with each other is inspired by the brain .	kaI kampyUteSanala yUnitsa hone kA mUla vicAra jo kevala eka xUsare ke sAWa apanI bAwacIwa ke mAXyama se buxXimAna ho jAwe hEM, vaha maswiRka se preriwa howA
NP2 The basic idea of having many computational units that become intelligent only via their interactions with each other	kaI kampyUteSanala yUnita hone kA mUla vicAra jo eka xUsare ke sAWa apanI bAwacIwa ke mAXyama se hI buxXimAna ho jAwe hEM
NP3 The basic idea	buniyAxI vicAra
PP7 of having many computational units that become intelligent only via their interactions with each other	kaI kampyUteSanala yUnitsa hone kI jo eka xUsare ke sAWa apanI bAwacIwa ke mAXyama se hI buxXimAna ho jAwI hEM
S9 having many computational units that become intelligent only via their interactions with each other	kaI kampyUteSanala yUnitsa honA jo eka xUsare ke sAWa apanI bAwacIwa ke mAXyama se hI buxXimAna ho jAwI hEM
VP10_LWG having	ho rahA hE
NP12 many computational units that become intelligent only via their interactions with each other	kaI kampyUteSanala yUnita jo eka xUsare ke sAWa apanI bAwacIwa ke mAXyama se hI buxXimAna ho jAwI hEM
NP13 many computational units	kaI kaMpyUteSanala yUnita
NNS16 units	ikAiyAM
SBAR17 that become intelligent only via their interactions with each other	jo eka xUsare ke sAWa apanI bAwacIwa ke mAXyama se hI buxXimAna ho jAwA hE
WHNP18 that	vaha
S20 become intelligent only via their interactions with each other	eka xUsare ke sAWa apanI bAwacIwa ke mAXyama se hI ho jAwe hEM buxXimAna
VP21_LWG become	bana gae
ADJP23 intelligent only via their interactions	apanI bAwacIwa ke mAXyama se hI buxXimAna
PP25 only via their interactions	unakI bAwacIwa ke mAXyama se hI
ADVP26 only	kevala
NP29 their interactions	unakI bAwacIwa
NNS31 interactions	bAwacIwa
PP32 with each other	eka xUsare ke sAWa
NP34 each other	eka xUsare ko
VP37_LWG is inspired	preriwa hE
PP41 by the brain	maswiRka se
NP43 the brain	maswiRka

----
00318	The neocognitron (Fukushima, 1980) introduced a powerful model architecture for processing images that was inspired by the structure of the mammalian visual system and later became the basis for the modern convolutional network (LeCun et al., 1998b), as we will see in section 9.10.	 navonmeRa ( Fukushima , 1980 ) ne swanaXArI xqSya praNAlI kI saMracanA se preriwa CaviyoM ke prasaMskaraNa ke lie eka SakwiSAlI moYdala vAswukalA kI SuruAwa kI Ora bAxa meM AXunika saMyugmana netavarka ( LeCun et al . 9 , 1998 ) kA AXAra banA 		
317	317
S1 The neocognitron ( Fukushima , 1980 ) introduced a powerful model architecture for processing images that was inspired by the structure of the mammalian visual system and later became the basis for the modern convolutional network ( LeCun et al. , 1998b ) , as we will see in section 9.10 .	nyUkokEMtrana (PukuSimA, 1980) ne prasaMskaraNa kI CaviyoM ke lie eka SakwiSAlI moYdala vAswukalA peSa kiyA, jo mammelina xqSya praNAlI ke
NP2 The neocognitron ( Fukushima , 1980 )	nyUkoMkSanara (PukuSimA, 1980)
,7 ,	,
VP10_LWG introduced later became	bAxa meM peSa kiyA gayA peSa
NP13 a powerful model architecture	eka SakwiSAlI moYdala vAswukalA
PP18 for processing images that was inspired by the structure of the mammalian visual system	swana prasaMskaraNa ke lie jo swanapAyI xqSya praNAlI ke saMracanA se preriwa We
NP20 processing images that was inspired by the structure of the mammalian visual system	swana prasaMskaraNa jo swanaXArI xqSya praNAlI ke saMracanA se preriwa We
NP21 processing images	CaviyoM kA prasaMskaraNa
NNS23 images	CaviyAM
SBAR24 that was inspired by the structure of the mammalian visual system	yaha mammeliyA xqSya praNAlI ke DAMce se preriwa WA
WHNP25 that	vaha
S27 was inspired by the structure of the mammalian visual system	swanaXArI xqSya praNAlI ke DAMce se preriwa We
VP28_LWG was inspired	preriwa WA
PP32 by the structure of the mammalian visual system	swanaXArI xqSya praNAlI kI saMracanA se
NP34 the structure of the mammalian visual system	swanaXArI xqSya praNAlI kI saMracanA
NP35 the structure	saMracanA
PP38 of the mammalian visual system	swanaXArI xqSya praNAlI kA
NP40 the mammalian visual system	swanaXArI xqSya praNAlI
CC45 and	Ora
VP11 introduced a powerful model architecture for processing images that was inspired by the structure of the mammalian visual system	peSa kI gaI CaviyoM ko prasaMskaraNa ke lie eka SakwiSAlI moYdala vAswukalA peSa kI gaI jo mammeliyA xqSya praNAlI ke DAMce se preriwa WI
VP46 later became the basis for the modern convolutional network ( LeCun et al. , 1998b ) , as we will see in section 9.10	bAxa meM AXunika saMviliyana netavarka (lekana eta ala., 1998b) ke lie AXAra bana gayA, kyoMki hama XArA 9.20 meM xeKeMge
ADVP47 later	bAxa meM
NP50 the basis	AXAra
PP53 for the modern convolutional network ( LeCun et al. , 1998b )	AXunika saMviliyana netavarka ke lie (lekana eta ala., 1998b)
NP55 the modern convolutional network ( LeCun et al. , 1998b )	AXunika parivarwanaSIla netavarka (lekana eta ala., 1998b)
PRN60 ( LeCun et al. , 1998b )	(lekana eta ala, 1998b)
NP62 LeCun et al. , 1998b	lekana eta ala., 1998b
NP63 LeCun et al.	lekana eta ala
NP64 LeCun	lekana
ADVP66 et al.	eta ala
,69 ,	,
NP70 1998b	1998b
NNS71 1998b	1998b
,73 ,	,
SBAR74 as we will see in section 9.10	jEsA ki hama XArA 9.20 meM xeKeMge
S76 we will see in section 9.10	hama XArA 9.20 meM xeKeMge
NP77 we	hama
VP79_LWG will see	xeKeMge
PP83 in section 9.10	XArA 9.20 meM
NP85 section 9.10	XArA 9.20

----
00319	Most neural networks today are based on a model neuron called the rectied linear unit .	 aXikAMSa waMwrikA netavarka Aja eka moYdala nyUroYna para AXAriwa hEM , jise saMSoXiwa rEKika ikAI kahA jAwA hE 		
318	318
S1 Most neural networks today are based on a model neuron called the rectied linear unit .	jyAxAwara waMwrikA netavarka Aja eka moYdala nyUrona para AXAriwa hEM jise reguleta lAinara yUnita kahA jAwA hE
NP2 Most neural networks	aXikAMSa waMwrikA netavarka
NNS5 networks	netavarka
NP-TMP6 today	Aja
VP8_LWG are based	AXAriwa hEM
PP12 on a model neuron called the rectied linear unit	reksda lAinara yUnita nAmaka moYdala nyUrona para
NP14 a model neuron called the rectied linear unit	reguleta lAinara yUnita nAmaka eka moYdala nyUrona
NP15 a model neuron	eka moYdala nyUrona
VP19_LWG called	bulAyA gayA
NP21 the rectied linear unit	saMSoXiwa lAinara yUnita

----
00320	The original cognitron (Fukushima, 1975) introduced a more complicated version that was highly inspired by our knowledge of brain function.	 mUla cognitron ( Fukushima , 1975 ) ne eka aXika jatila saMskaraNa kI SuruAwa kI jo hamAre maswiRka samAroha ke jFAna se awyaXika preriwa WA .		
319	319
S1 The original cognitron ( Fukushima , 1975 ) introduced a more complicated version that was highly inspired by our knowledge of brain function .	mUla saMjFAna (PukuSimA, 1975) ne eka Ora aXika jatila saMskaraNa peSa kiyA jo hamAre maswiRka samAroha ke hamAre jFAna se bahuwa preriwa WA
NP2 The original cognitron ( Fukushima , 1975 )	mUla saMjFAna (PukuSimA, 1975)
PRN6 ( Fukushima , 1975 )	(PukuSimA, 1975)
NP8 Fukushima , 1975	PukuSimA, 1975
,10 ,	,
VP13_LWG introduced	peSa kiyA gayA
NP15 a more complicated version that was highly inspired by our knowledge of brain function	eka Ora jatila saMskaraNa jo hamAre maswiRka samAroha ke hamAre jFAna se behaxa preriwa WA
NP16 a more complicated version	eka Ora jatila saMskaraNa
ADJP18 more complicated	aXika jatila
SBAR22 that was highly inspired by our knowledge of brain function	yaha hamAre maswiRka samAroha ke hamAre jFAna se bahuwa preriwa WA
WHNP23 that	vaha
S25 was highly inspired by our knowledge of brain function	maswiRka samAroha ke hamAre jFAna se behaxa preriwa WA
VP26_LWG was highly inspired	behaxa preraNAxAyI We
ADVP28 highly	awyaXika
PP32 by our knowledge of brain function	maswiRka samAroha ke hamAre jFAna se
NP34 our knowledge of brain function	maswiRka samAroha kA hamArA jFAna
NP35 our knowledge	hamArA jFAna
PP38 of brain function	maswiRka samAroha kI
NP40 brain function	brena PaMkSana

----
0032	One of the most famous such projects is Cyc (Lenat and Guha, 1989).	 EsI sabase prasixXa pariyojanAoM meM se eka hE sAika (Lenat Ora Guha , 1989 )		
31	31
S1 One of the most famous such projects is Cyc ( Lenat and Guha , 1989 ) .	EsI sabase prasixXa pariyojanAoM meM se eka sAika (lInawa Ora guhA, 1989) hE
NP2 One of the most famous such projects	EsI sabase prasixXa pariyojanAoM meM se eka
NP3 One	eka
PP5 of the most famous such projects	EsI sabase prasixXa pariyojanAoM kI
NP7 the most famous such projects	EsI sabase maSahUra pariyojanAeM
ADJP9 most famous	sabase prasixXa
NNS13 projects	pariyojanAeM
VP14_LWG is	hE
ADJP16 Cyc	sAika
PRN18 ( Lenat and Guha , 1989 )	(lenawa Ora guhA, 1989)
NP20 Lenat and Guha	lInawa Ora guhA
CC22 and	Ora
NNP21 Lenat	lInawa
NNP23 Guha	guhA
,24 ,	,
NP25 1989	1989

----
00321	The simplied modern version was developed incorporating ideas from many viewpoints, with Nair and Hinton (2010) and Glorot et al.	 saralIkqwa AXunika saMskaraNa kaI xqRtikoNoM se vicAroM ko samAhiwa karawe hue vikasiwa kiyA gayA WA , jisameM nAyara Ora hAinatana (2010 ) Ora glorota eta ala .		
320	320
S1 The simplied modern version was developed incorporating ideas from many viewpoints , with Nair and Hinton ( 2010 ) and Glorot et al. .	sarala AXunika saMskaraNa ko kaI xqRtikoNoM se vicAroM ko SAmila karawe hue vikasiwa kiyA gayA WA, jisameM nAyara Ora hiMtana ( 2010) Ora glorota eta ala
NP2 The simplied modern version	sarala AXunika saMskaraNa
VP7_LWG was developed Glorot et	vikasiwa kiyA gayA glorota eta
S12 incorporating ideas from many viewpoints , with Nair and Hinton	kaI xqRtikoNoM se vicAroM ko SAmila karanA, nAyara Ora hiMtana ke sAWa
VP13_LWG incorporating	SAmila
NP15 ideas	vicAra
NNS16 ideas	vicAra
PP17 from many viewpoints	kaI xqRtikoNoM se
NP19 many viewpoints	kaI xqRtikoNa
NNS21 viewpoints	xqRtikoNa
,22 ,	,
PP23 with Nair and Hinton	nAyara Ora hiMtana ke sAWa
NP25 Nair and Hinton	nAyara Ora hiMtana
CC27 and	Ora
NNP26 Nair	nAyara
NNP28 Hinton	hiMtana
PRN29 ( 2010 )	( 2010)
NP31 2010	2010
CC34 and	Ora
VP10 developed incorporating ideas from many viewpoints , with Nair and Hinton ( 2010 )	kaI xqRtikoNoM se vicAroM ko SAmila karawe hue vikasiwa kiyA gayA, nAyara Ora hiMtana ( 2010)
VP35 Glorot et al.	glorota eta ala
ADVP37 et al.	eta ala

----
00322	(2011a) citing neuroscience as an inuence, and Jarrett et al.	 (2011 ) ne waMwrikA vijFAna ko eka praBAva ke rUpa meM uxXqwa kiyA , Ora jeretta eta ala .		
321	321
FRAG1 ( 2011a ) citing neuroscience as an inuence , and Jarrett et al. .	( 2011) nyUrosAiMsa ko eka praBAva ke rUpa meM uxXqwa karawe hue, Ora jarAta eta ala
NP2 ( 2011a )	( 2011 e)
VP6_LWG citing neuroscience	nyUrosAiMsa kA havAlA xewe hue
ADVP8 neuroscience	nyUrosAiMsa
PP10 as an inuence , and Jarrett et al.	eka praBAva ke rUpa meM, Ora jarAta eta ala
NP12 an inuence , and Jarrett et al.	eka praBAva, Ora jarAta eta ala
NP13 an inuence	eka praBAva
,16 ,	,
CC17 and	Ora
NP18 Jarrett	jarAta
ADVP20 et al.	eta ala

----
00323	(2009) citing more engineering- oriented inuences.	 (2009 - aXika iMjIniyarI unmuKa praBAvoM kA havAlA xewe hue 		
322	322
FRAG1 ( 2009 ) citing more engineering - oriented inuences .	( 2009) aXika iMjIniyariMga-ugra praBAvoM kA havAlA xewe hue
NP3 2009	2009
S6 citing more engineering - oriented inuences	aXika iMjIniyariMga kA havAlA xewe hue-ugra praBAva
VP7_LWG citing	havAlA xewe hue
NP9 more engineering - oriented inuences	aXika iMjIniyariMga- orieMteda praBAva
ADJP11 engineering - oriented	iMjIniyariMga- orieMteda
NP12 engineering	iMjIniyariMga
NNS16 inuences	praBAva

----
00324	While neuroscience is an important source of inspiration, it need not be taken as a rigid guide.	 jabaki waMwrikA vijFAna preraNA kA eka mahawvapUrNa srowa hE , yaha eka kaTora gAida ke rUpa meM lene kI jarUrawa nahIM hE .		
323	323
S1 While neuroscience is an important source of inspiration , it need not be taken as a rigid guide .	jahAM nyUrosAiMsa preraNA kA mahawvapUrNa srowa hE, vahIM ise eka riglAida gAida ke rUpa meM nahIM liyA jAnA cAhie
SBAR2 While neuroscience is an important source of inspiration	jabaki nyUrosAiMsa preraNA kA mahawvapUrNa srowa hE
S4 neuroscience is an important source of inspiration	manovijFAna preraNA kA mahawvapUrNa srowa hE
NP5 neuroscience	nyUrosAiMsa
VP7_LWG is	hE
NP9 an important source of inspiration	preraNA kA mahawvapUrNa srowa
NP10 an important source	eka mahawvapUrNa srowa
PP14 of inspiration	preraNA kI
NP16 inspiration	preraNA
,18 ,	,
NP19 it	yaha
VP21_LWG need not be taken	nahIM lene kI jarUrawa
PP28 as a rigid guide	eka majabUwa gAida ke rUpa meM
NP30 a rigid guide	eka gaTajodZa gAida

----
00325	We know that actual neurons compute very dierent functions than modern rectied linear units, but greater neural realism has not yet led to an improvement in machine learning performance.	 hama jAnawe hEM ki vAswavika nyUroYnsa AXunika saMSoXiwa rEKika ikAiyoM kI wulanA meM bahuwa alaga kAryoM kI parikalana karawe hEM , lekina aXika waMwrikA yaWArWavAxa aBI waka maSIna sIKane ke praxarSana meM suXAra kA newqwva nahIM kiyA hE .		
324	324
S1 We know that actual neurons compute very dierent functions than modern rectied linear units , but greater neural realism has not yet led to an improvement in machine learning performance .	hama jAnawe hEM ki vAswavika nyUrona AXunika saMSoXiwa laGu ikAiyoM kI wulanA meM bahuwa alaga kAryoM kA gaNanA karawe hEM, lekina aXika waMwrikA yaWArWa
S2 We know that actual neurons compute very dierent functions than modern rectied linear units	hama jAnawe hEM ki vAswavika nyUrona AXunika suXAriwa lAinara ikAiyoM kI wulanA meM bahuwa alaga kAryoM kA gaNanA karawe hEM
NP3 We	hama
VP5_LWG know	jAnie
SBAR7 that actual neurons compute very dierent functions than modern rectied linear units	vaha vAswavika nyUrona AXunika suXAriwa lAinara ikAiyoM kI wulanA meM bahuwa alaga kAryoM kA gaNanA karawe hEM
S9 actual neurons compute very dierent functions than modern rectied linear units	vAswavika nyUrona AXunika saMSoXiwa laGu ikAiyoM kI wulanA meM bahuwa alaga kAryoM kA gaNanA karawe hEM
NP10 actual neurons	vAswavika nyUrona
NNS12 neurons	nyUrona
VP13_LWG compute	gaNanA
NP15 very dierent functions	bahuwa alaga-alaga kArya
ADJP16 very dierent	bahuwa alaga
NNS19 functions	samAroha
PP20 than modern rectied linear units	AXunika suXAriwa lAinara ikAiyoM kI wulanA meM
NP22 modern rectied linear units	AXunika suXArI huI lAinara yUnita
NNS26 units	ikAiyAM
,27 ,	,
CC28 but	lekina
S29 greater neural realism has not yet led to an improvement in machine learning performance	jyAxA waMwrikA yaWArWavAxa ne aBI waka maSIna sIKane ke praxarSana meM suXAra nahIM kiyA hE
NP30 greater neural realism	jyAxA waMwrikA yaWArWavAxa
VP34_LWG has not yet led	aBI waka nahIM bane hEM newqwva
ADVP37 yet	aBI waka
PP41 to an improvement in machine learning performance	maSIna larniMga praxarSana meM suXAra ke lie
NP43 an improvement in machine learning performance	maSIna larniMga praxarSana meM suXAra
NP44 an improvement	eka suXAra
PP47 in machine learning performance	maSIna larniMga praxarSana meM
NP49 machine learning performance	maSIna sIKa kA praxarSana
NML50 machine learning	maSIna sIKanA

----
00326	Also, while neuroscience has successfully inspired several neural network architectures, we do not yet know enough about biological learning for neuroscience to oer much guidance for the learning algorithms we use to train these architectures.	 isake alAvA , jabaki waMwrikA vijFAna saPalawApUrvaka kaI waMwrikA netavarka vAswukalA ko preriwa kiyA hE , hama aBI waka waMwrikA vijFAna ke lie jEvika sIKane ke bAre meM paryApwa pawA nahIM hE sIKane elgorixama ke lie bahuwa mArgaxarSana kI peSakaSa karane ke lie hama ina vAswukalAoM ko praSikRiwa karane ke lie upayoga karawe hEM .		
325	325
S1 Also , while neuroscience has successfully inspired several neural network architectures , we do not yet know enough about biological learning for neuroscience to oer much guidance for the learning algorithms we use to train these architectures .	sAWa hI, jahAM nyUrosAiMsa ne kaI waMwrikA netavarka vAswukalAoM ko saPalawApUrvaka preriwa kiyA hE, vahIM hameM aBI waka nyUrosAiMsa ke lie jEvika sI
ADVP2 Also	sAWa hI
,4 ,	,
SBAR5 while neuroscience has successfully inspired several neural network architectures	jabaki nyUrosAiMsa ne kaI waMwrikA netavarka vAswukalAoM ko saPalawApUrvaka preriwa kiyA hE
S7 neuroscience has successfully inspired several neural network architectures	nyUrosAiMsa ne kaI waMwrikA netavarka vAswukalAoM ko saPalawApUrvaka preriwa kiyA hE
NP8 neuroscience	nyUrosAiMsa
VP10_LWG has successfully inspired	saPalawApUrvaka preriwa kiyA hE
ADVP12 successfully	saPalawApUrvaka
NP16 several neural network architectures	kaI waMwrikA netavarka vAswukalAja
NML18 neural network	waMwrikA netavarka
NNS21 architectures	vAswukAroM
,22 ,	,
NP23 we	hama
VP25_LWG do not yet know enough	aBI waka paryApwa nahIM pawA
ADVP28 yet	aBI waka
ADVP32 enough about biological learning	jEvika sIKa ke bAre meM paryApwa
PP34 about biological learning	jEvika sIKa ke bAre meM
NP36 biological learning	jEvika sIKa
PP39 for neuroscience	nyUrosAiMsa ke lie
NP41 neuroscience	nyUrosAiMsa
S43 to oer much guidance for the learning algorithms we use to train these architectures	ina vAswukalAoM ko praSikRiwa karane ke lie hama jiwane SikRaNa elgorixama kA upayoga karawe hEM, usake lie bahuwa aXika mArgaxarSana
VP44_LWG to oer	peSakaSa ke lie
NP48 much guidance	bahuwa mArgaxarSana
PP51 for the learning algorithms we use to train these architectures	sIKane ke elgorixama ke lie hama ina vAswukalAoM ko praSikRiwa karane ke lie upayoga karawe hEM
NP53 the learning algorithms we use to train these architectures	ina vAswukAroM ko praSikRiwa karane ke lie hama jina sIKiMga elgorixama kA upayoga karawe hEM
NP54 the learning algorithms	sIKane ke elgorixama
NNS57 algorithms	elgorixama
SBAR58 we use to train these architectures	ina vAswukAroM ko praSikRiwa karane ke lie hama upayoga karawe hEM
S59 we use to train these architectures	ina vAswukAroM ko praSikRiwa karane ke lie hama upayoga karawe hEM
NP60 we	hama
VP62_LWG use	upayoga
S64 to train these architectures	ina vAswukAroM ko praSikRiwa karane ke lie
VP65_LWG to train	treniMga ke lie
NP69 these architectures	ye vAswukalA
NNS71 architectures	vAswukAroM

----
00327	15 CHAPTER 1.	 15 CHAPTER 1 .		
326	326
FRAG1 15 CHAPTER 1 .	15 cEptara 1.
NP2 15	15
NP4 CHAPTER 1	cEptara 1

----
00328	INTRODUCTION Media accounts often emphasize the similarity of deep learning to the brain.	 INTRODUCTION mIdiyA KAwoM aksara maswiRka ke lie gahare SikRaNa kI samAnawA para jora xewI hE 		
327	327
S1 INTRODUCTION Media accounts often emphasize the similarity of deep learning to the brain .	paricaya mIdiyA KAwe aksara maswiRka se gaharI sIKa kI samAnawA para jora xewe hEM
NP2 INTRODUCTION Media accounts	paricaya mIdiyA KAwe
NML3 INTRODUCTION Media	paricaya mIdiyA
NNS6 accounts	KAwe
ADVP7 often	aksara
VP9_LWG emphasize	jora
NP11 the similarity of deep learning	gaharI sIKa kI samAnawA
NP12 the similarity	samAnawA
PP15 of deep learning	gaharI sIKa kA
NP17 deep learning	gaharI sIKa
PP20 to the brain	maswiRka ko
NP22 the brain	maswiRka

----
00329	While it is true that deep learning researchers are more likely to cite the brain as an inuence than researchers working in other machine learning elds, such as kernel machines or Bayesian statistics, one should not view deep learning as an attempt to simulate the brain.	 jabaki yaha saca hE ki gahana aXyayana karane vAle SoXakarwAoM ke maswiRka ko anya maSIna sIKane ke kRewroM jEse karnela maSIna yA bEsISiyana sAMKyikI meM kAma karane vAle SoXakarwAoM kI wulanA meM aXika praBAva ke rUpa meM uxXqwa karane kI saMBAvanA howI hE , vyakwi ko gahana aXyayana ko maswiRka anukaraNa karane ke prayAsa ke rUpa meM nahIM xeKanA cAhie 		
328	328
S1 While it is true that deep learning researchers are more likely to cite the brain as an inuence than researchers working in other machine learning elds , such as kernel machines or Bayesian statistics , one should not view deep learning as an attempt to simulate the brain .	jabaki yaha sawya hE ki gaharI sIKa anusaMXAnakarwAoM ko maswiRka ko anya maSIna sIKane ke kRewroM meM kAma karane vAle anusaMXAnakarwAoM kI wulanA meM praBAva ke rUpa
SBAR2 While it is true that deep learning researchers are more likely to cite the brain as an inuence than researchers working in other machine learning elds , such as kernel machines or Bayesian statistics	jabaki yaha sawya hE ki gaharI sIKa anusaMXAnakarwAoM ko maswiRka ko anya maSIna sIKane ke kRewroM meM kAma karane vAle anusaMXAnakarwAoM kI wulanA meM praBAva ke rUpa
S4 it is true that deep learning researchers are more likely to cite the brain as an inuence than researchers working in other machine learning elds , such as kernel machines or Bayesian statistics	yaha sawya hE ki gaharI sIKa anusaMXAnakarwAoM ko maswiRka ko anya maSIna sIKane ke kRewroM meM kAma karane vAle anusaMXAnakarwAoM kI wulanA meM praBAva ke rUpa meM uxXa
NP5 it	yaha
VP7_LWG is	hE
ADJP9 true	saca hE
SBAR11 that deep learning researchers are more likely to cite the brain as an inuence than researchers working in other machine learning elds , such as kernel machines or Bayesian statistics	yaha gaharI sIKa anusaMXAnakarwAoM ko maswiRka ko anya maSIna sIKane ke kRewroM meM kAma karane vAle anusaMXAnakarwAoM kI wulanA meM praBAva ke rUpa meM uxXqwa karane kI
S13 deep learning researchers are more likely to cite the brain as an inuence than researchers working in other machine learning elds , such as kernel machines or Bayesian statistics	gaharI sIKa anusaMXAnakarwAoM ko maswiRka ko anya maSIna sIKane ke kRewroM meM kAma karane vAle anusaMXAnakarwAoM kI wulanA meM praBAva ke rUpa meM uxXqwa karane kI aXika saM
NP14 deep learning researchers	gahana sIKa anusaMXAnakarwAoM
NML15 deep learning	gaharI sIKa
NNS18 researchers	SoXakarwAoM
VP19_LWG are	hEM
ADJP21 more likely to cite the brain as an inuence than researchers working in other machine learning elds , such as kernel machines or Bayesian statistics	anya maSIna sIKane ke kRewroM meM kAma karane vAle anusaMXAnakarwAoM kI wulanA meM maswiRka ko praBAva ke rUpa meM uxXqwa karane kI aXika saMBAvanA, jEse ki guTalI
S24 to cite the brain as an inuence than researchers working in other machine learning elds , such as kernel machines or Bayesian statistics	maswiRka ko anya maSIna sIKane ke kRewroM meM kAma karane vAle anusaMXAnakarwAoM kI wulanA meM praBAva ke rUpa meM uxXqwa karane ke lie, jEse ki guTalI maSI
VP25_LWG to cite	uxXqwa karane ke lie
NP29 the brain as an inuence	eka praBAva ke rUpa meM maswiRka
NP30 the brain	maswiRka
PP33 as an inuence	praBAva ke rUpa meM
NP35 an inuence	eka praBAva
PP38 than researchers working in other machine learning elds , such as kernel machines or Bayesian statistics	anya maSIna sIKane ke kRewroM meM kAma karane vAle anusaMXAnakarwAoM kI wulanA meM, jEse ki guTalI maSInoM yA beyasiyana AMkadZe
NP40 researchers working in other machine learning elds , such as kernel machines or Bayesian statistics	anya maSIna sIKane ke kRewroM meM kAma karane vAle anusaMXAnakarwA, jEse ki guTalIxAra maSIneM yA beyasiyana AMkadZe
NP41 researchers working in other machine learning elds	anya maSIna sIKane ke kRewroM meM kAma kara rahe anusaMXAnakarwA
NP42 researchers	SoXakarwAoM
NNS43 researchers	SoXakarwAoM
VP44_LWG working	kAma kara rahA hE
PP46 in other machine learning elds	anya maSIna sIKane ke kRewroM meM
NP48 other machine learning elds	anya maSIna sIKane ke kRewra
NML50 machine learning	maSIna sIKanA
NNS53 elds	KewoM
,54 ,	,
PP55 such as kernel machines or Bayesian statistics	jEse ki guTalI maSIna yA bEsiyAna AMkadZe
NP58 kernel machines or Bayesian statistics	guTalI maSInoM yA bEsiyAna AMkadZe
NP59 kernel machines	guTalI maSIneM
NNS61 machines	maSIneM
CC62 or	yA
NP63 Bayesian statistics	bIyaSciyana AMkadZe
NNS65 statistics	AMkadZe
,66 ,	,
NP67 one	eka
VP69_LWG should not view	nahIM xeKanA cAhie najara
NP74 deep learning	gaharI sIKa
PP77 as an attempt	eka koSiSa ke rUpa meM
NP79 an attempt	eka koSiSa
S82 to simulate the brain	maswiRka kA anukaraNa karane ke lie
VP83_LWG to simulate	kalpanA karane ke lie
NP87 the brain	maswiRka

----
00330	Modern deep learning draws inspiration from many elds, especially applied math fundamentals like linear algebra, probability, information theory, and numerical optimization.	 AXunika gahana SikRaNa kaI kRewroM , viSeRa rUpa se lAgU gaNiwa buniyAxI jEse rEKika bIjagaNiwa , saMBAvanA , sUcanA sixXAMwa , Ora saMKyAwmaka anukUlana se preraNA grahaNa karawA hE .		
329	329
S1 Modern deep learning draws inspiration from many elds , especially applied math fundamentals like linear algebra , probability , information theory , and numerical optimization .	AXunika gaharI sIKa kaI kRewroM se preraNA KIMcawI hE, KAsakara gaNiwa kattarapaMWI jEse lAinara bIjagaNiwa, saMBAvanA, sUcanA sixXAMwa, Ora aMka
NP2 Modern deep learning	AXunika gaharI sIKa
VP6_LWG draws especially applied	KIMcawA hE viSeRa rUpa se lAgU
NP9 inspiration	preraNA
PP11 from many elds	kaI kRewroM se
NP13 many elds	kaI Kewa
NNS15 elds	KewoM
,16 ,	,
ADVP17 especially	KAsakara
NP21 math fundamentals	gaNiwa kattarapaMWI
NNS23 fundamentals	mOlika
PP24 like linear algebra , probability , information theory , and numerical optimization	jEse lAinara bIjagaNiwa, saMBAvanA, sUcanA sixXAMwa, Ora aMkagaNiwa parivarwana
NP26 linear algebra , probability , information theory , and numerical optimization	laGu aMkagaNiwa, saMBAvanA, sUcanA sixXAMwa, Ora aMkagaNiwa parivarwana
NP27 linear algebra	laGugaNiwa buxXijIvI
,30 ,	,
NP31 probability	saMBAvanA
,33 ,	,
NP34 information theory	jAnakArI sixXAMwa
,37 ,	,
CC38 and	Ora
NP39 numerical optimization	aMkagaNiwa parivarwana

----
0033	Cyc is an inference engine and a database of statements in a language called CycL.	 sAika eka anumAna iMjana hE Ora eka sAita ( CycL ) nAmaka BARA meM kaWanoM kA detAbesa hE .		
32	32
S1 Cyc is an inference engine and a database of statements in a language called CycL .	sAika eka iMPekSana iMjana hE Ora sIAIsIela nAmaka BARA meM bayAna kA detAbesa hE
S2 Cyc is an inference engine	sAika eka iMPekSana iMjana hE
NP3 Cyc	sAika
VP5_LWG is	hE
NP7 an inference engine	eka iMPekSana iMjana
CC11 and	Ora
S12 a database of statements in a language called CycL	sIAIsIela nAmaka BARA meM bayAnoM kA detAbesa
NP13 a database of statements in a language	eka BARA meM bayAnoM kA detAbesa
NP14 a database	eka detAbesa
PP17 of statements in a language	eka BARA meM bayAna
NP19 statements in a language	eka BARA meM bayAna
NP20 statements	bayAna
NNS21 statements	bayAna
PP22 in a language	eka BARA meM
NP24 a language	eka BARA
VP27_LWG called	bulAyA gayA
NP29 CycL	sAikaela

----
00331	While some deep learning researchers cite neuroscience as an important source of inspiration, others are not concerned with neuroscience at all.	 jabaki kuCa gaharI sIKane ke SoXakarwA waMwrikA vijFAna ko preraNA ke eka mahawvapUrNa srowa ke rUpa meM uxXqwa karawe hEM , xUsaroM ko bilkula BI waMwrikA vijFAna se saMbaMXiwa nahIM hEM .		
330	330
S1 While some deep learning researchers cite neuroscience as an important source of inspiration , others are not concerned with neuroscience at all .	jahAM kuCa gaharI sIKa anusaMXAnakarwA nyUrosAiMsa ko preraNA kA mahawvapUrNa srowa bawAwe hEM, vahIM xUsare nyUrosAiMsa se bilkula ciMwiwa nahIM hEM
SBAR2 While some deep learning researchers cite neuroscience as an important source of inspiration	jabaki kuCa gahana sIKa anusaMXAnakarwA nyUrosAiMsa ko preraNA ke mahawvapUrNa srowa ke rUpa meM uxXqwa karawe hEM
S4 some deep learning researchers cite neuroscience as an important source of inspiration	kuCa gaharI sIKa anusaMXAnakarwAoM ne nyUrosAiMsa ko preraNA ke mahawvapUrNa srowa ke rUpa meM uxXqwa kiyA
NP5 some deep learning researchers	kuCa gaharI sIKa anusaMXAnakarwAoM
NML7 deep learning	gaharI sIKa
NNS10 researchers	SoXakarwAoM
VP11_LWG cite	uxXqwa
NP13 neuroscience	nyUrosAiMsa
PP15 as an important source of inspiration	preraNA ke mahawvapUrNa srowa ke rUpa meM
NP17 an important source of inspiration	preraNA kA mahawvapUrNa srowa
NP18 an important source	eka mahawvapUrNa srowa
PP22 of inspiration	preraNA kI
NP24 inspiration	preraNA
,26 ,	,
NP27 others	xUsare
NNS28 others	xUsare
VP29_LWG are not concerned	ciMwiwa nahIM hEM
PP34 with neuroscience	nyUrosAiMsa ke sAWa
NP36 neuroscience	nyUrosAiMsa
PP38 at all	bilkula hI
NP40 all	saBI

----
00332	It is worth noting that the eort to understand how the brain works on an algorithmic level is alive and well.	 gOrawalaba hE ki yaha samaJane kA prayAsa ki maswiRka elgoriWama swara para kEse kAma karawA hE , jIviwa Ora acCI waraha se hE 		
331	331
S1 It is worth noting that the eort to understand how the brain works on an algorithmic level is alive and well .	gOrawalaba hE ki yaha samaJane kI koSiSa hE ki eka elgorixamika swara para maswiRka kEse kAma karawA hE vaha jIviwa Ora acCI waraha se jIviwa hE
NP2 It	yaha
VP4_LWG is	hE
ADJP6 worth	lAyaka
S8 noting that the eort to understand how the brain works on an algorithmic level is alive and well	gOrawalaba hE ki yaha samaJane kI koSiSa hE ki eka elgorixamika swara para maswiRka kEse kAma karawA hE vaha jIviwa Ora acCI waraha se
VP9_LWG noting	gOra karanA
SBAR11 that the eort to understand how the brain works on an algorithmic level is alive and well	yaha samaJane kI koSiSa hE ki eka elgorixamika swara para maswiRka kEse kAma karawA hE vaha jIviwa Ora acCI waraha se
S13 the eort to understand how the brain works on an algorithmic level is alive and well	yaha samaJane kI koSiSa hE ki eka elgorixamika swara para maswiRka kEse kAma karawA hE vaha jIviwa Ora acCI waraha se
NP14 the eort to understand how the brain works on an algorithmic level	yaha samaJane kI koSiSa ki kEse eka elgorixamika swara para maswiRka kAma karawA hE
S17 to understand how the brain works on an algorithmic level	yaha samaJane ke lie ki eka elgorixamika swara para maswiRka kEse kAma karawA hE
VP18_LWG to understand	samaJane ke lie
SBAR22 how the brain works on an algorithmic level	kEse eka elgorixamika swara para kAma karawA hE maswiRka
WHADVP23 how	kEse
S25 the brain works on an algorithmic level	maswiRka eka elgorixamika swara para kAma karawA hE
NP26 the brain	maswiRka
VP29_LWG works	kAma karawA hE
PP31 on an algorithmic level	eka elgorixamika swara para
NP33 an algorithmic level	eka elgorixamika swara
VP37_LWG is	hE
ADJP39 alive and well	jiMxA Ora acCI waraha se
CC41 and	Ora
JJ40 alive	jiMxA
JJ42 well	acCI waraha se

----
00333	This endeavor is primarily known as computational neuroscience and is a separate eld of study from deep learning.	 isa prayAsa ko muKya rUpa se " kampyUteSanala waMwrikA vijFAna " ke rUpa meM jAnA jAwA hE Ora yaha gahana SikRaNa se aXyayana kA eka alaga kRewra hE 		
332	332
S1 This endeavor is primarily known as  computational neuroscience  and is a separate eld of study from deep learning .	yaha prayAsa muKya rUpa se kampyUteSanala nyUrosAiMsa ke nAma se jAnA jAwA hE Ora yaha gaharI sIKa se aXyayana kA eka alaga kRewra hE
NP2 This endeavor	yaha koSiSa
VP5_LWG is primarily known is	muKya rUpa se jAnA jAwA hE
ADVP8 primarily	muKya rUpa se
PP12 as  computational neuroscience 	jEsA ki kampyUteSanala nyUrosAiMsa
NP14  computational neuroscience 	kampyUteSanala nyUrosAiMsa
CC19 and	Ora
VP6 is primarily known as  computational neuroscience 	muKya rUpa se kampyUteSanala nyUrosAiMsa ke rUpa meM jAnA jAwA hE
VP20 is a separate eld of study from deep learning	gaharI sIKa se aXyayana kA alaga kRewra hE
NP22 a separate eld of study from deep learning	gaharI sIKa se aXyayana kA alaga kRewra
NP23 a separate eld	eka alaga kRewra
PP27 of study from deep learning	gaharI sIKa se paDZAI kI
NP29 study from deep learning	gaharI sIKa se paDZAI
NP30 study	aXyayana
PP32 from deep learning	gaharI sIKa se
NP34 deep learning	gaharI sIKa

----
00334	It is common for researchers to move back and forth between both elds.	 SoXakarwAoM ke lie yaha Ama hE ki ve xonoM kRewroM ke bIca Age - pICe GUmeM 		
333	333
S1 It is common for researchers to move back and forth between both elds .	anusaMXAnakarwAoM ke lie xonoM kRewroM ke bIca pICe Ora pICe hatakara Age baDZanA Ama bAwa hE
NP2 It	yaha
VP4_LWG is	hE
ADJP6 common for researchers	anusaMXAnakarwAoM ke lie Ama
PP8 for researchers	SoXakarwAoM ke lie
NP10 researchers	SoXakarwAoM
NNS11 researchers	SoXakarwAoM
S12 to move back and forth between both elds	xonoM KewoM ke bIca pICe Ora pICe hatane ke lie
VP13_LWG to move back	vApasa le jAne ke lie
ADVP17 back and forth	pICe Ora pICe
CC19 and	Ora
RB18 back	vApasa
RB20 forth	Age
PP21 between both elds	xonoM kRewroM ke bIca
NP23 both elds	xonoM Kewa
NNS25 elds	KewoM

----
00335	The eld of deep learning is primarily concerned with how to build computer systems that are able to successfully solve tasks requiring intelligence, while the eld of computational neuroscience is primarily concerned with building more accurate models of how the brain actually works.	 gahana aXyayana kA kRewra muKya rUpa se kaMpyUtara sistama kA nirmANa karane se saMbaMXiwa hE jo buxXi kI AvaSyakawA vAle kAryoM ko saPalawApUrvaka hala karane meM sakRama hE , jabaki kampyUteSanala waMwrikA vijFAna kA kRewra muKya rUpa se maswiRka vAswava meM kEse kAma karawA hE ke aXika satIka moYdala ke nirmANa se saMbaMXiwa hE .		
334	334
S1 The eld of deep learning is primarily concerned with how to build computer systems that are able to successfully solve tasks requiring intelligence , while the eld of computational neuroscience is primarily concerned with building more accurate models of how the brain actually works .	gaharI sIKa kA kRewra muKya rUpa se yaha ciMwiwa hE ki kampyUtara sistama kEse banAyA jA sakawA hE jo buxXi kI AvaSyakawA vAle kAryoM ko saPalawApUrvaka hala karane meM sakRa
NP2 The eld of deep learning	gaharI sIKa kA kRewra
NP3 The eld	Kewa
PP6 of deep learning	gaharI sIKa kA
NP8 deep learning	gaharI sIKa
VP11_LWG is	hE
ADJP13 primarily concerned with how to build computer systems that are able to successfully solve tasks requiring intelligence	muKya rUpa se kampyUtara sistama kEse banAyA jAe, jo buxXi kI AvaSyakawA vAle kAryoM ko saPalawApUrvaka hala karane meM sakRama hEM
PP16 with how to build computer systems that are able to successfully solve tasks requiring intelligence	kampyUtara sistama kEse banAyA jAe jo buxXi kI AvaSyakawA vAle kAryoM ko saPalawApUrvaka hala kara pAwe hEM
SBAR18 how to build computer systems that are able to successfully solve tasks requiring intelligence	kampyUtara sistama kEse banAe jAeM jo buxXi kI AvaSyakawA vAle kAryoM ko saPalawApUrvaka hala kara pAwe hEM
WHADVP19 how	kEse
S21 to build computer systems that are able to successfully solve tasks requiring intelligence	kampyUtara sistama banAne ke lie jo buxXi kI AvaSyakawA vAle kAryoM ko saPalawApUrvaka hala kara pAwe hEM
VP22_LWG to build	banAne ke lie
NP26 computer systems that are able to successfully solve tasks requiring intelligence	kampyUtara sistama jo buxXi kI AvaSyakawA vAle kAryoM ko saPalawApUrvaka hala kara pAwe hEM
NP27 computer systems	kampyUtara sistama
NNS29 systems	sistama
SBAR30 that are able to successfully solve tasks requiring intelligence	jo buxXi kI AvaSyakawA raKane vAle kAryoM ko saPalawApUrvaka hala kara pA rahe hEM
WHNP31 that	vaha
S33 are able to successfully solve tasks requiring intelligence	buxXi kI AvaSyakawA vAle kAryoM ko saPalawApUrvaka hala kara pA rahe hEM
VP34_LWG are	hEM
ADJP36 able to successfully solve tasks requiring intelligence	buxXi kI AvaSyakawA vAle kAryoM ko saPalawApUrvaka hala karane meM sakRama
S38 to successfully solve tasks requiring intelligence	buxXi kI AvaSyakawA vAle kAryoM ko saPalawApUrvaka hala karane ke lie
VP39_LWG to successfully solve	saPalawApUrvaka hala karane ke lie
ADVP41 successfully	saPalawApUrvaka
NP45 tasks	kAryoM
NNS46 tasks	kAryoM
S47 requiring intelligence	buxXimawwA kI AvaSyakawA
VP48_LWG requiring	AvaSyakawA
NP50 intelligence	buxXimawwA
,52 ,	,
SBAR53 while the eld of computational neuroscience is primarily concerned with building more accurate models of how the brain actually works	jabaki kaMpyUtara nyUrosAiMsa kA kRewra muKya rUpa se aXika satIka moYdala banAne se ciMwiwa hE ki kEse vAswava meM maswiRka kAma karawA hE
S55 the eld of computational neuroscience is primarily concerned with building more accurate models of how the brain actually works	kaMpyUtara nyUrosAiMsa kA kRewra muKya rUpa se aXika satIka moYdala banAne se ciMwiwa hE ki vAswava meM maswiRka kEse kAma karawA hE
NP56 the eld of computational neuroscience	kaMpyUtara nyUrosAiMsa kA kRewra
NP57 the eld	Kewa
PP60 of computational neuroscience	kaMpyUtara nyUrosAiMsa kA
NP62 computational neuroscience	kaMpyUtara nyUrosAiMsa
VP65_LWG is	hE
ADJP67 primarily concerned with building more accurate models of how the brain actually works	muKya rUpa se maswiRka vAswava meM kEse kAma karawA hE isake aXika satIka moYdala banAne se saMbaMXiwa
PP70 with building more accurate models of how the brain actually works	Ora aXika satIka moYdala banAne ke sAWa maswiRka vAswava meM kEse kAma karawA hE
S72 building more accurate models of how the brain actually works	vAswava meM kEse kAma karawA hE maswiRka ke aXika satIka moYdala banAnA
VP73_LWG building	bildiMga
NP75 more accurate models of how the brain actually works	maswiRka vAswava meM kEse kAma karawA hE isake aXika satIka moYdala
NP76 more accurate models	aXika satIka moYdala
ADJP77 more accurate	aXika satIka
NNS80 models	moYdala
PP81 of how the brain actually works	vAswava meM maswiRka kEse kAma karawA hE
SBAR83 how the brain actually works	vAswava meM maswiRka kEse kAma karawA hE
WHADVP84 how	kEse
S86 the brain actually works	maswiRka vAswava meM kAma karawA hE
NP87 the brain	maswiRka
ADVP90 actually	vAswava meM
VP92_LWG works	kAma karawA hE

----
00336	In the 1980s, the second wave of neural network research emerged in great part via a movement called connectionism , or parallel distributed process-	 1980 ke xaSaka meM , waMwrikA netavarka anusaMXAna kI xUsarI lahara kAPI BAga meM kanekSanavAxa , yA samAnAMwara viwariwa prakriyA - nAmaka AMxolana ke mAXyama se uBarI .		
335	335
S1 In the 1980s , the second wave of neural network research emerged in great part via a movement called connectionism , or parallel distributed process -	1980 ke xaSaka meM kanekSanavAxa, yA samAnAMwara viwariwa prakriyA nAmaka AMxolana ke mAXyama se waMwrikA netavarka anusaMXAna kI xUsarI lahara badZe hisse meM uBarI -
PP2 In the 1980s	1980 ke xaSaka meM
NP4 the 1980s	1980 ke xaSaka
NNS6 1980s	1980 ke xaSaka meM
,7 ,	,
NP8 the second wave of neural network research	waMwrikA netavarka anusaMXAna kI xUsarI lahara
NP9 the second wave	xUsarI lahara
PP13 of neural network research	waMwrikA netavarka anusaMXAna kI
NP15 neural network research	waMwrikA netavarka anusaMXAna
VP19_LWG emerged	uBare
PP21 in great part via a movement called connectionism , or parallel distributed process	kanekSanavAxa nAmaka AMxolana ke mAXyama se badZe hisse meM, yA samAnAMwara viwariwa prakriyA
NP23 great part via a movement called connectionism , or parallel distributed process	kanekSanavAxa nAmaka AMxolana ke mAXyama se mahAna BAga, yA samAnAMwara viwariwa prakriyA
NP24 great part	SAnaxAra hissA
PP27 via a movement called connectionism , or parallel distributed process	kanekSanavAxa nAmaka AMxolana ke mAXyama se, yA samAnAMwara viwariwa prakriyA
NP29 a movement called connectionism , or parallel distributed process	kanekSanavAxa nAmaka AMxolana, yA samAnAMwara viwariwa prakriyA
NP30 a movement called connectionism	kanekSanavAxa nAmaka AMxolana
NP31 a movement	eka AMxolana
VP34_LWG called	bulAyA gayA
NP36 connectionism	kanekSanavAxa
,38 ,	,
CC39 or	yA
NP40 parallel distributed process	samAnAMwara viwariwa prakriyA
NP41 parallel	samAnAMwara
VP43_LWG distributed	viwariwa
NP45 process	prakriyA

----
00337	ing (Rumelhart et al., 1986c;	 ala		
336	336
S1 ing ( Rumelhart et al. , 1986c ;	liMga (rumAlahArta eta ala., 1986c ;
VP2_LWG ing	liMga
NP5 Rumelhart et al.	Rumelhart eta ala
NP6 Rumelhart	rUmAlahArta
ADVP8 et al.	eta ala
,11 ,	,
S12 1986c	1986 ke xaSaka meM
ADJP13 1986c	1986 ke xaSaka meM

----
00338	McClelland et al., 1995).	 mEkklalEMda eta ala , 1995		
337	337
NP1 McClelland et al. , 1995 ) .	mEkalelEMda eta ala, 1995 )
NP2 McClelland et al.	mEkalelEMda eta ala
NP3 McClelland	mEkalelEMda
ADVP5 et al.	eta ala
,8 ,	,
NP9 1995	1995

----
00339	Connectionism arose in the context of cognitive science.	 saMjFAnAwmaka vijFAna ke saMxarBa meM kanekSanavAxa kA uxaya huA .		
338	338
S1 Connectionism arose in the context of cognitive science .	saMjFAnAwmaka vijFAna ke saMxarBa meM kanekSanavAxa uTA
NP2 Connectionism	kanekSanavAxa
VP4_LWG arose	uTA
PP6 in the context of cognitive science	saMjFAnAwmaka vijFAna ke saMxarBa meM
NP8 the context of cognitive science	saMjFAnAwmaka vijFAna kA saMxarBa
NP9 the context	saMxarBa
PP12 of cognitive science	saMjFAnAwmaka vijFAna kA
NP14 cognitive science	saMjFAnAwmaka vijFAna

----
00340	Cognitive science is an interdisciplinary approach to understanding the mind, combining multiple dierent levels of analysis.	 saMjFAnAwmaka vijFAna mana ko samaJane ke lie eka aMwarviRayaka xqRtikoNa hE , viSleRaNa ke kaI alaga swaroM ko saMyojiwa .		
339	339
S1 Cognitive science is an interdisciplinary approach to understanding the mind , combining multiple dierent levels of analysis .	saMjFAnAwmaka vijFAna mana ko samaJane ke lie eka aMwarrARtrIya xqRtikoNa hE, jo viSleRaNa ke kaI alaga-alaga swara ko saMyojiwa karawA hE
NP2 Cognitive science	saMjFAnAwmaka vijFAna
VP5_LWG is	hE
NP7 an interdisciplinary approach to understanding the mind , combining multiple dierent levels of analysis	mana ko samaJane ke lie eka aMwarrARtrIya xqRtikoNa, viSleRaNa ke kaI alaga-alaga swara ko saMyukwa
NP8 an interdisciplinary approach	eka aMwarrARtrIya xqRtikoNa
PP12 to understanding the mind , combining multiple dierent levels of analysis	mana ko samaJane ke lie, viSleRaNa ke kaI alaga-alaga swara ko saMyukwa
S14 understanding the mind , combining multiple dierent levels of analysis	mana ko samaJanA, viSleRaNa ke kaI alaga-alaga swara ko jodZanA
VP15_LWG understanding combining	samaJa meM AnA-jAnA
NP18 the mind	mana
,21 ,	,
NP24 multiple dierent levels of analysis	viSleRaNa ke kaI alaga-alaga swara
NP25 multiple dierent levels	kaI alaga-alaga swara
NNS28 levels	swara
PP29 of analysis	viSleRaNa kA
NP31 analysis	viSleRaNa

----
0034	These statements are entered by a sta of human supervisors.	 ina bayAnoM ko mAnava paryavekRakoM ke eka stAPa xvArA xarja kiyA jAwA hE .		
33	33
S1 These statements are entered by a sta of human supervisors .	ina bayAnoM ko mAnava paryavekRakoM ke eka stAPa ne praveSa kiyA hE
NP2 These statements	ye bayAna
NNS4 statements	bayAna
VP5_LWG are entered	praveSa kara rahe hEM
PP9 by a sta of human supervisors	mAnava paryavekRakoM ke stAPa xvArA
NP11 a sta of human supervisors	mAnava paryavekRakoM kA stAPa
NP12 a sta	eka karmacArI
PP15 of human supervisors	mAnava paryavekRakoM kI
NP17 human supervisors	mAnava paryavekRaka
NNS19 supervisors	paryavekRakoM

----
00341	During the early 1980s, most cognitive scientists studied models of symbolic reasoning.	 1980 ke xaSaka ke prAraMBa meM , aXikAMSa saMjFAnAwmaka vEjFAnikoM ne prawIkAwmaka warka ke moYdaloM kA aXyayana kiyA .		
340	340
S1 During the early 1980s , most cognitive scientists studied models of symbolic reasoning .	1980 ke xaSaka ke xOrAna, aXikAMSa saMjFAnAwmaka vEjFAnikoM ne sAMkewika warka ke moYdala kA aXyayana kiyA
PP2 During the early 1980s	1980 ke xaSaka ke xOrAna
NP4 the early 1980s	1980 ke xaSaka kI SuruAwa meM
NNS7 1980s	1980 ke xaSaka meM
,8 ,	,
NP9 most cognitive scientists	sabase saMjFAnAwmaka vEjFAnika
ADJP10 most cognitive	sabase saMjFAnAwmaka
NNS13 scientists	vEjFAnikoM
VP14_LWG studied	paDZA-liKA
NP16 models of symbolic reasoning	prawIkAwmaka warka ke moYdala
NP17 models	moYdala
NNS18 models	moYdala
PP19 of symbolic reasoning	prawIkAwmaka warka kA
NP21 symbolic reasoning	prawIkAwmaka warka

----
00342	Despite their popularity, symbolic models were dicult to explain in terms of how the brain could actually implement them using neurons.	 unakI lokapriyawA ke bAvajUxa , prawIkAwmaka moYdaloM ko isa saMxarBa meM samaJAnA muSkila WA ki kEse maswiRka vAswava meM nyUroYnsa kA upayoga kara unheM lAgU kara sakawA hE .		
341	341
S1 Despite their popularity , symbolic models were dicult to explain in terms of how the brain could actually implement them using neurons .	unakI lokapriyawA ke bAvajUxa, prawIkAwmaka moYdaloM ko yaha samaJanA muSkila WA ki vAswava meM maswiRka ne inheM nyUrona kA upayoga karake kEse lAgU kara sakawA hE
PP2 Despite their popularity	unakI lokapriyawA ke bAvajUxa
NP4 their popularity	unakI lokapriyawA
,7 ,	,
NP8 symbolic models	prawIkAwmaka moYdala
NNS10 models	moYdala
VP11_LWG were	We
ADJP13 dicult to explain in terms of how the brain could actually implement them using neurons	isa bAwa ke lihAja se samaJAnA muSkila hE ki vAswava meM maswiRka ne inheM nyUrona kA upayoga karake kEse lAgU kiyA
S15 to explain in terms of how the brain could actually implement them using neurons	isa bAwa ke lihAja se samaJAne ke lie ki maswiRka vAswava meM inheM nyUrona kA upayoga karake kEse lAgU kara sakawA hE
VP16_LWG to explain	samaJAne ke lie
PP20 in terms of how the brain could actually implement them using neurons	isa lihAja se maswiRka vAswava meM inheM nyUrona kA upayoga karake kEse lAgU kara sakawA hE
NP22 terms of how the brain could actually implement them using neurons	SarwoM ke anusAra maswiRka vAswava meM inheM nyUrona kA upayoga karake kEse lAgU kara sakawA hE
NP23 terms	SarwoM
NNS24 terms	SarwoM
PP25 of how the brain could actually implement them using neurons	maswiRka vAswava meM inheM nyUrona kA upayoga karake kEse lAgU kara sakawA WA
SBAR27 how the brain could actually implement them using neurons	maswiRka vAswava meM inheM nyUrona kA upayoga karake kEse lAgU kara sakawA WA
WHADVP28 how	kEse
S30 the brain could actually implement them using neurons	maswiRka vAswava meM inheM nyUrona kA upayoga karake lAgU kara sakawA WA
NP31 the brain	maswiRka
VP34_LWG could actually implement	vAswava meM lAgU kara sakawe We
ADVP36 actually	vAswava meM
S40 them using neurons	unheM nyUrona kA iswemAla kara
NP41 them	unheM
VP43_LWG using	iswemAla karanA
NP45 neurons	nyUrona
NNS46 neurons	nyUrona

----
00343	The connectionists began to study models of cognition that could actually be grounded in neural implementations (Touretzky and Minton, 1985), reviving many ideas dating back to the work of psychologist Donald Hebb in the 1940s (Hebb, 1949).	 kanekSanistoM ne saMjFAna ke una moYdaloM kA aXyayana karanA SurU kiyA jo vAswava meM waMwrikA kAryAnvayanoM meM AXAriwa ho sakawe We ( Touretzky Ora miMtana , 1985 ) , jisameM 1940 ke xaSaka meM manovijFAnI donAlda heba ke kArya se saMbaMXiwa kaI vicAroM ko punaH prApwa kiyA gayA WA .		
342	342
S1 The connectionists began to study models of cognition that could actually be grounded in neural implementations ( Touretzky and Minton , 1985 ) , reviving many ideas dating back to the work of psychologist Donald Hebb in the 1940s ( Hebb , 1949 ) .	kanekSanavAxiyoM ne mAnyawA ke moYdaloM kA aXyayana karanA SurU kiyA jo vAswava meM waMwrikA kriyAnvayana (tyUretjZakI Ora miMtana, 1985) meM jame hue
NP2 The connectionists	kanekSanavAxiyoM
NNS4 connectionists	kanekSanavAxiyoM
VP5_LWG began	SurU huI SuruAwa
S7 to study models of cognition that could actually be grounded in neural implementations ( Touretzky and Minton , 1985 )	jFApana ke moYdaloM kA aXyayana karane ke lie jinheM vAswava meM waMwrikA kriyAnvayana meM grAuMda kiyA jA sakawA hE (toretjZakI Ora miMtana, 1985)
VP8_LWG to study	aXyayana karane ke lie
NP12 models of cognition that could actually be grounded in neural implementations ( Touretzky and Minton , 1985 )	mAnyawA ke moYdala jinheM vAswava meM waMwrikA kriyAnvayana meM grAuMda kiyA jA sakawA hE (toretjZakI Ora miMtana, 1985)
NP13 models	moYdala
NNS14 models	moYdala
PP15 of cognition that could actually be grounded in neural implementations ( Touretzky and Minton , 1985 )	jFApana kI jo vAswava meM waMwrikA kriyAnvayana meM grAuMda kI jA sakawI hE (toretjZakI Ora miMtana, 1985)
NP17 cognition that could actually be grounded in neural implementations ( Touretzky and Minton , 1985 )	mAnyawA jise vAswava meM waMwrikA kriyAnvayana meM grAuMda kiyA jA sakawA hE (toretjZakI Ora miMtana, 1985)
NP18 cognition	mAnyawA
SBAR20 that could actually be grounded in neural implementations ( Touretzky and Minton , 1985 )	yaha vAswava meM waMwrikA kriyAnvayana meM grAuMda kiyA jA sakawA hE (toretjZakI Ora miMtana, 1985)
WHNP21 that	vaha
S23 could actually be grounded in neural implementations ( Touretzky and Minton , 1985 )	vAswava meM waMwrikA kriyAnvayana (toretjZakI Ora miMtana, 1985) meM grAuMda kiyA jA sakawA hE
VP24_LWG could actually be grounded	vAswava meM kiyA jA sakawA hE grAuMda
ADVP26 actually	vAswava meM
PP32 in neural implementations	waMwrikA kriyAnvayana meM
NP34 neural implementations	waMwrikA lAgU honA
NNS36 implementations	kriyAnvayana
PRN37 ( Touretzky and Minton , 1985 )	(toretjZakI Ora miMtana, 1985)
NP39 Touretzky and Minton	tUretjZakI Ora miMtana
CC41 and	Ora
NNP40 Touretzky	tUretjZakI
NNP42 Minton	minatana
,43 ,	,
NP44 1985	1985
,47 ,	,
S48 reviving many ideas dating back to the work of psychologist Donald Hebb in the 1940s ( Hebb , 1949 )	kaI vicAroM ko punarjIviwa karanA 1940 ke xaSaka meM manovEjFAnika donAlda hebba ke kAma para vApasa detiMga karanA (hebba, 1949)
VP49_LWG reviving	punarjIviwa
NP51 many ideas	kaI vicAra
NNS53 ideas	vicAra
S54 dating back to the work of psychologist Donald Hebb in the 1940s ( Hebb , 1949 )	1940 ke xaSaka meM manovEjFAnika donAlda hebba ke kAma para vApasa detiMga karanA (hebba, 1949)
VP55_LWG dating back	detiMga bEka
ADVP57 back to the work of psychologist Donald Hebb	manovEjFAnika donAlda hebba ke kAma para vApasa
PP59 to the work of psychologist Donald Hebb	manovEjFAnika donAlda hebba ke kAma ke lie
NP61 the work of psychologist Donald Hebb	manovEjFAnika donAlda hebba kA kAma
NP62 the work	kAmakAja
PP65 of psychologist Donald Hebb	manovEjFAnika donAlda hebba kA
NP67 psychologist Donald Hebb	manovEjFAnika donAlda hebba
PP71 in the 1940s	1940 ke xaSaka meM
NP73 the 1940s	1940 ke xaSaka meM
NNS75 1940s	1940 ke xaSaka meM
PRN76 ( Hebb , 1949 )	(hebba, 1949)
NP78 Hebb , 1949	hebba, 1949
,80 ,	,

----
00344	The central idea in connectionism is that a large number of simple computational units can achieve intelligent behavior when networked together.	 kanekSanavAxa meM keMxrIya vicAra yaha hE ki sarala kampyUteSanala ikAiyoM kI eka badZI saMKyA eka sAWa netavarka hone para buxXimAna vyavahAra prApwa kara sakawe hEM .		
343	343
S1 The central idea in connectionism is that a large number of simple computational units can achieve intelligent behavior when networked together .	saMbaMXavAxa meM keMxrIya vicAra yaha hE ki eka sAWa netavarka karane para badZI saMKyA meM sAXAraNa kaMpyUtara ikAiyAM buxXimAna vyavahAra hAsila kara sakawI hEM
NP2 The central idea in connectionism	saMbaMXavAxa meM keMxrIya vicAra
NP3 The central idea	keMxrIya vicAra
PP7 in connectionism	saMbaMXavAxa meM
NP9 connectionism	kanekSanavAxa
VP11_LWG is	hE
SBAR13 that a large number of simple computational units can achieve intelligent behavior when networked together	ki eka sAWa netavarka karane para badZI saMKyA meM sAXAraNa kaMpyUtareSanala yUnitsa buxXimAna vyavahAra hAsila kara sakawI hEM
S15 a large number of simple computational units can achieve intelligent behavior when networked together	eka sAWa netavarka karane para badZI saMKyA meM sAXAraNa kaMpyUtareSanala yUnitsa buxXimAna vyavahAra hAsila kara sakawI hEM
NP16 a large number of simple computational units	badZI saMKyA meM sAXAraNa kaMpyUtareSanala yUnita
NP17 a large number	badZI saMKyA
PP21 of simple computational units	sAXAraNa kaMpyUteSanala yUnitoM kI
NP23 simple computational units	sAXAraNa kaMpyUteSanala yUnita
NNS26 units	ikAiyAM
VP27_LWG can achieve	hAsila kara sakawe hEM
NP31 intelligent behavior	buxXimAna vyavahAra
SBAR34 when networked together	jaba eka sAWa netavarka
WHADVP35 when	kaba
S37 networked together	eka sAWa netavarka
VP38_LWG networked together	eka sAWa netavarka
ADVP40 together	eka sAWa

----
00345	This insight applies equally to neurons in biological nervous systems as it does to hidden units in computational models.	 yaha aMwarxqRti jEvika waMwrikA praNAliyoM meM nyUroYnsa ke lie samAna rUpa se lAgU howA hE , jEsA ki yaha kampyUteSanala moYdala meM CipA ikAiyoM ke lie karawA hE .		
344	344
S1 This insight applies equally to neurons in biological nervous systems as it does to hidden units in computational models .	yaha aMwarjFAna jEvika waMwrikA waMwra meM nyUronsa ke barAbara lAgU howA hE kyoMki yaha gaNiwIya moYdaloM meM CipI huI ikAiyoM ko karawA hE
NP2 This insight	yaha aMwarjFAna
VP5_LWG applies	lAgU howA hE
SBAR7 equally to neurons in biological nervous systems as it does to hidden units in computational models	jEvika waMwrikA waMwra meM nyUrona ke barAbara rUpa se kyoMki yaha gaNiwIya moYdaloM meM ikAiyoM ko CipAne ke lie karawA hE
ADVP8 equally to neurons in biological nervous systems	jEvika waMwrikA waMwra meM nyUrona ke barAbara
PP10 to neurons in biological nervous systems	jEvika waMwrikA waMwra meM nyUronarsa ko
NP12 neurons in biological nervous systems	jEvika waMwrikA waMwra meM nyUrona
NP13 neurons	nyUrona
NNS14 neurons	nyUrona
PP15 in biological nervous systems	jEvika waMwrikA waMwra meM
NP17 biological nervous systems	jEvika waMwrikA waMwra
NNS20 systems	sistama
S22 it does to hidden units in computational models	yaha kaMpyUtoriyala moYdala meM ikAiyoM ko CipAnA karawA hE
NP23 it	yaha
VP25_LWG does	karawA hE
PP27 to hidden units in computational models	kaMpyUteSanala moYdala meM CipI huI ikAiyoM ko
NP29 hidden units in computational models	kampyUteSanala moYdala meM CipI huI ikAiyAM
NP30 hidden units	CipI huI ikAiyAM
NNS32 units	ikAiyAM
PP33 in computational models	kampyUteSanala moYdala meM
NP35 computational models	kampyUteSanala moYdala
NNS37 models	moYdala

----
00346	Several key concepts arose during the connectionism movement of the 1980s that remain central to todays deep learning.	 1980 ke xaSaka ke kanekSanavAxa AMxolana ke xOrAna kaI pramuKa avaXAraNAeM uwpanna huIM jo Aja ke gahana aXyayana kA keMxra banI huI hEM 		
345	345
S1 Several key concepts arose during the connectionism movement of the 1980s that remain central to today s deep learning .	1980 ke xaSaka ke kanekSanavAxa AMxolana ke xOrAna kaI pramuKa avaXArAez uBarIM jo Aja kI gaharI sIKa ke lie keMxrIya banI huI hEM
NP2 Several key concepts	kaI pramuKa avaXArAeM
NNS5 concepts	avaXAraNAez
VP6_LWG arose	uTA
PP8 during the connectionism movement of the 1980s that remain central to today s deep learning	1980 ke xaSaka ke saMbaxXawA AMxolana ke xOrAna jo Aja kI gaharI sIKa ke lie keMxrIya rahawe hEM
NP10 the connectionism movement of the 1980s that remain central to today s deep learning	1980 ke xaSaka ke kanekSana kA AMxolana jo Aja kI gaharI sIKa ke lie keMxrIya rahawA hE
NP11 the connectionism movement	kanekSana kA AMxolana
PP15 of the 1980s that remain central to today s deep learning	1980 ke xaSaka ke jo Aja kI gaharI sIKa ke lie keMxrIya rahe
NP17 the 1980s that remain central to today s deep learning	1980 ke xaSaka jo Aja kI gaharI sIKa ke lie keMxrIya rahawe hEM
NP18 the 1980s	1980 ke xaSaka
NNS20 1980s	1980 ke xaSaka meM
SBAR21 that remain central to today s deep learning	jo Aja kI gaharI sIKa ke lie keMxrIya rahawA hE
WHNP22 that	vaha
S24 remain central to today s deep learning	Aja kI gaharI sIKa ke lie keMxrIya raheM
VP25_LWG remain	bane raheM
NP27 central to today s deep learning	Aja kI gaharI sIKa ke lie keMxrIya
ADJP28 central to today s	Aja ke lie keMxrIya
PP30 to today s	Aja ke lie
NP32 today s	Aja kA xina

----
00347	One of these concepts is that of distributed representation (Hinton et al., 1986).	 ina avaXAraNAoM meM se eka hE viwariwa prawiniXiwva ( hEnatana eta ala , 1986 ) .		
346	346
S1 One of these concepts is that of distributed representation ( Hinton et al. , 1986 ) .	inameM se eka avaXAraNA viwariwa prawiniXiwva (hiMtana eta ala, 1986) kI hE
NP2 One of these concepts	inameM se eka avaXAraNA
NP3 One	eka
PP5 of these concepts	ina avaXArAoM kI
NP7 these concepts	ye avaXArAeM
NNS9 concepts	avaXAraNAez
VP10_LWG is	hE
NP12 that of distributed representation ( Hinton et al. , 1986 )	viwariwa prawiniXiwva (hiMtana eta ala, 1986)
NP13 that	vaha
PP15 of distributed representation ( Hinton et al. , 1986 )	viwariwa prawiniXiwva (hiMtana eta ala, 1986)
NP17 distributed representation ( Hinton et al. , 1986 )	viwariwa prawiniXiwva (hiMtana eta ala, 1986)
PRN20 ( Hinton et al. , 1986 )	(hiMtana eta ala, 1986)
NP22 Hinton et al. , 1986	hiMtana eta ala., 1986
NP23 Hinton et al.	hiMtana eta ala
NP24 Hinton	hiMtana
ADVP26 et al.	eta ala
,29 ,	,
NP30 1986	1986

----
00348	This is the idea that each input to a system should be represented by many features, and each feature should be involved in the representation of many possible inputs.	 yaha eka vicAra hE ki eka sistama ke prawyeka inaputa ko kaI viSeRawAoM xvArA prawiniXiwva kiyA jAnA cAhie , Ora prawyeka viSeRawA ko kaI saMBava AxAnoM ke prawiniXiwva meM SAmila kiyA jAnA cAhie .		
347	347
S1 This is the idea that each input to a system should be represented by many features , and each feature should be involved in the representation of many possible inputs .	yaha vicAra hE ki kisI sistama ke prawyeka inaputa kA prawiniXiwva kaI PIcarsa xvArA kiyA jAnA cAhie, Ora prawyeka PIcara kaI saMBAviwa inaputa ke prawiniXiwva meM
S2 This is the idea that each input to a system should be represented by many features	yaha vicAra hE ki kisI sistama ke prawyeka inaputa kA kaI PIcarsa kA prawiniXiwva honA cAhie
NP3 This	yaha
VP5_LWG is	hE
NP7 the idea	vicAra
SBAR10 that each input to a system should be represented by many features	ki kisI sistama ko prawyeka inaputa kA kaI PIcarsa kA prawiniXiwva honA cAhie
S12 each input to a system should be represented by many features	kisI sistama ko prawyeka inaputa kA kaI PIcarsa kA prawiniXiwva honA cAhie
NP13 each input to a system	eka sistama ko prawyeka inaputa
NP14 each input	prawyeka inaputa
PP17 to a system	eka praNAlI ko
NP19 a system	eka praNAlI
VP22_LWG should be represented	prawiniXiwva honA cAhie
PP28 by many features	kaI PIcarsa se
NP30 many features	kaI PIcarsa
NNS32 features	PIcarsa
,33 ,	,
CC34 and	Ora
S35 each feature should be involved in the representation of many possible inputs	kaI saMBAviwa inaputa ke prawiniXiwva meM SAmila honA cAhie prawyeka suviXA
NP36 each feature	prawyeka viSeRawA
VP39_LWG should be involved	SAmila honA cAhie
PP45 in the representation of many possible inputs	kaI saMBAviwa inaputa ke prawiniXiwva meM
NP47 the representation of many possible inputs	kaI saMBAviwa inaputa kA prawiniXiwva
NP48 the representation	prawiniXiwva
PP51 of many possible inputs	kaI saMBAviwa inaputoM kI
NP53 many possible inputs	kaI saMBAviwa inaputa
NNS56 inputs	inaputa

----
00349	For example, suppose we have a vision system that can recognize 16 CHAPTER 1.	 uxAharaNa ke lie , mAna lIjie ki hamAre pAsa eka xqRti praNAlI hE jo 16 CHAPTER 1 ko pahacAna sakawI hE 		
348	348
FRAG1 For example , suppose we have a vision system that can recognize 16 CHAPTER 1 .	uxAharaNa ke lie, mAna lIjie ki hamAre pAsa eka xqRti praNAlI hE jo 16 aXyAya 1 ko pahacAna sakawI hE
S2 For example , suppose we have a vision system that can recognize 16	uxAharaNa ke lie, mAna lIjie ki hamAre pAsa eka xqRti praNAlI hE jo 16 ko pahacAna sakawI hE
PP3 For example	uxAharaNa ke lie
NP5 example	uxAharaNa
,7 ,	,
VP8_LWG suppose	mAna lIjie
SBAR10 we have a vision system that can recognize 16	hamAre pAsa eka xqRti praNAlI hE jo 16 ko pahacAna sakawI hE
S11 we have a vision system that can recognize 16	hamAre pAsa eka xqRti praNAlI hE jo 16 ko pahacAna sakawI hE
NP12 we	hama
VP14_LWG have	pAsa hE
NP16 a vision system that can recognize 16	eka xqRti praNAlI jo 16 ko pahacAna sakawI hE
NP17 a vision system	eka xqRti praNAlI
SBAR21 that can recognize 16	jo 16 ko pahacAna sakawA hE
WHNP22 that	vaha
S24 can recognize 16	16 ko pahacAna sakawe hEM
VP25_LWG can recognize	pahacAna sakawe hEM
NP29 16	16
NP31 CHAPTER 1	cEptara 1

----
00350	INTRODUCTION cars, trucks, and birds, and these objects can each be red, green, or blue.	 INTRODUCING kAroM , trakoM , Ora pakRiyoM , Ora ina vaswuoM prawyeka lAla , hare , yA nIle ho sakawA hE		
349	349
S1 INTRODUCTION cars , trucks , and birds , and these objects can each be red , green , or blue .	paricaya kAra, traka, Ora pakRI, Ora ye vaswueM prawyeka lAla, hare, yA nIlI ho sakawI hEM
S2 INTRODUCTION cars , trucks , and birds	paricaya kAreM, traka, Ora pakRI
VP3_LWG INTRODUCTION	paricaya
NP5 cars , trucks , and birds	kAreM, traka Ora pakRI
NNS6 cars	kAreM
,7 ,	,
NNS8 trucks	traka
,9 ,	,
CC10 and	Ora
NNS11 birds	pakRI
,12 ,	,
CC13 and	Ora
S14 these objects can each be red , green , or blue	ye vaswueM prawyeka lAla, hare, yA nIlI ho sakawI hEM
NP15 these objects	ye vaswueM
NNS17 objects	vaswueM
VP18_LWG can be	ho sakawA hE
ADJP23 red , green , or blue	lAla, hare, yA nIlA
,25 ,	,
,27 ,	,
CC28 or	yA
JJ29 blue	nIlA

----
0035	It is an unwieldy process.	 yaha eka aparicCinna prakriyA hE 		
34	34
S1 It is an unwieldy process .	yaha eka anAvaSyaka prakriyA hE
NP2 It	yaha
VP4_LWG is	hE
NP6 an unwieldy process	eka avivekI prakriyA

----
00351	One way of representing these inputs would be to have a separate neuron or hidden unit that activates for each of the nine possible combinations: red truck, red car, red bird, green truck, and so on.	 ina AxAnoM kA prawiniXiwva karane kA eka warIkA yaha hogA ki eka alaga nyUroYna yA CupI ikAI ho jo nO saMBava saMyojanoM meM se prawyeka ke lie sakriya ho : lAla traka , lAla kAra , lAla pakRI , hare traka , Axi .		
350	350
S1 One way of representing these inputs would be to have a separate neuron or hidden unit that activates for each of the nine possible combinations : red truck , red car , red bird , green truck , and so on .	ina inaputa kA prawiniXiwva karane kA eka warIkA eka alaga nyUrona yA CipI huI yUnita hogA jo nO saMBAviwa saMyojanoM meM se prawyeka ke lie sakriya howI hE H lAla
NP2 One way of representing these inputs	ina inaputa kA prawiniXiwva karane kA eka warIkA
NP3 One way	eka waraha se
PP6 of representing these inputs	ina inaputa kA prawiniXiwva karane kI
S8 representing these inputs	ina inaputa kA prawiniXiwva karawe hue
VP9_LWG representing	prawiniXiwva karawe hue
NP11 these inputs	ye inaputa
NNS13 inputs	inaputa
VP14_LWG would be	howA.
S18 to have a separate neuron or hidden unit that activates for each of the nine possible combinations : red truck , red car , red bird , green truck , and so on	eka alaga nyUrona yA CipI huI yUnita honA jo nO saMBAviwa saMyojanoM meM se prawyeka ke lie sakriya howA hE H lAla traka, lAla kAra, lAla pakRI, harI
VP19_LWG to have	karane ke lie
NP23 a separate neuron or hidden unit that activates for each of the nine possible combinations : red truck , red car , red bird , green truck , and so on	eka alaga nyUrona yA CipI huI yUnita jo nO saMBAviwa saMyojanoM meM se prawyeka ke lie sakriya howI hE H lAla traka, lAla kAra, lAla pakRI, harI traka
NP24 a separate neuron or hidden unit	eka alaga nyUrona yA CipI huI yUnita
NP25 a separate neuron	eka alaga nyUrona
CC29 or	yA
NP30 hidden unit	CipI huI yUnita
SBAR33 that activates for each of the nine possible combinations : red truck , red car , red bird , green truck , and so on	yaha nO saMBAviwa saMyojanoM meM se prawyeka ke lie sakriya howA hE H lAla traka, lAla kAra, lAla pakRI, harI traka, Ora isa prakAra
WHNP34 that	vaha
S36 activates for each of the nine possible combinations : red truck , red car , red bird , green truck , and so on	nO saMBAviwa saMyojanoM meM se prawyeka ke lie sakriya howA hEH lAla traka, lAla kAra, lAla pakRI, hare raMga kA traka, Ora isa prakAra
VP37_LWG activates	sakriya
PP39 for each of the nine possible combinations : red truck , red car , red bird , green truck , and so on	nO saMBAviwa saMyojanoM meM se prawyeka ke lie lAla traka, lAla kAra, lAla pakRI, hare raMga kA traka, Ora isa prakAra
NP41 each of the nine possible combinations : red truck , red car , red bird , green truck , and so on	nO saMBAviwa saMyojanoM meM se prawyekaH lAla traka, lAla kAra, lAla pakRI, hare raMga kA traka, Ora isI waraha
NP42 each of the nine possible combinations	nO saMBAviwa saMyojanoM meM se prawyeka
NP43 each	prawyeka
PP45 of the nine possible combinations	nO saMBAviwa saMyojanoM kI
NP47 the nine possible combinations	nO saMBAviwa saMyojana
NNS51 combinations	saMyojana
UCP53 red truck , red car , red bird , green truck , and so on	lAla traka, lAla kAra, lAla pakRI, hare raMga kA traka, Ora isI waraha se
NML54 red truck	lAla traka
,57 ,	,
NML58 red car	lAla kAra
,61 ,	,
NML62 red bird	lAla pakRI
,65 ,	,
NML66 green truck	hare traka
,69 ,	,
CC70 and	Ora
ADJP71 so on	Ese para

----
00352	This requires nine dierent neurons, and each neuron must independently learn the concept of color and object identity.	 isake lie nO alaga - alaga nyUroYnsa kI AvaSyakawA howI hE , Ora prawyeka nyUroYna ko svawaMwra rUpa se raMga Ora vaswu pahacAna kI avaXAraNA sIKanA hogA .		
351	351
S1 This requires nine dierent neurons , and each neuron must independently learn the concept of color and object identity .	isameM nO alaga-alaga nyUrona kI AvaSyakawA howI hE, Ora prawyeka nyUrona ko svawaMwra rUpa se raMga Ora vaswu pahacAna kI avaXAraNA sIKanI cAhie
S2 This requires nine dierent neurons	isameM nO alaga-alaga nyUrona kI AvaSyakawA howI hE
NP3 This	yaha
VP5_LWG requires	jarUrawa hE
NP7 nine dierent neurons	nO alaga-alaga nyUrona
NNS10 neurons	nyUrona
,11 ,	,
CC12 and	Ora
S13 each neuron must independently learn the concept of color and object identity	prawyeka nyUrona ko svawaMwra rUpa se raMga Ora vaswu pahacAna kI avaXAraNA sIKanI cAhie
NP14 each neuron	prawyeka nyUrona
VP17_LWG must independently learn object	svawaMwra rUpa se sIKanA cAhie vaswueM
ADVP19 independently	svawaMwra rUpa se
NP24 the concept of color	raMga kI avaXAraNA
NP25 the concept	avaXAraNA
PP28 of color	raMga kA
NP30 color	raMga
CC32 and	Ora
VP22 learn the concept of color	jAnie raMga kI avaXAraNA
VP33 object identity	vaswu pahacAna
NP35 identity	pahacAna

----
00353	One way to improve on this situation is to use a distributed representation, with three neurons describing the color and three neurons describing the object identity.	 isa sWiwi meM suXAra karane kA eka warIkA hE eka viwariwa prawiniXiwva kA upayoga karanA , jisameM wIna nyUroYnsa raMga kA varNana karawe hEM Ora wIna nyUroYnsa vaswu pahacAna kA varNana karawe hEM .		
352	352
S1 One way to improve on this situation is to use a distributed representation , with three neurons describing the color and three neurons describing the object identity .	isa sWiwi para suXAra kA eka warIkA viwariwa prawiniXiwva kA upayoga karanA hE, jisameM raMga kA varNana karane vAle wIna nyUro Ora vaswu pahacAna kA ulleKa karawe hue
NP2 One way to improve on this situation	isa sWiwi para suXAra kA eka warIkA
NP3 One way	eka waraha se
SBAR6 to improve on this situation	isa sWiwi ko behawara banAne ke lie
S7 to improve on this situation	isa sWiwi ko behawara banAne ke lie
VP8_LWG to improve	suXAra karane ke lie
PP12 on this situation	isa sWiwi para
NP14 this situation	yaha sWiwi
VP17_LWG is	hE
S19 to use a distributed representation , with three neurons describing the color and three neurons describing the object identity	eka viwariwa prawiniXiwva kA upayoga karane ke lie, raMga kA varNana karane vAle wIna nyUro Ora vaswu pahacAna ke bAre meM bawAne vAle wIna nyUro ke sAWa
VP20_LWG to use	iswemAla ke lie
NP24 a distributed representation	eka viwariwa prawiniXiwva
,28 ,	,
PP29 with three neurons describing the color and three neurons describing the object identity	wIna nyUrona ke sAWa raMga Ora vaswu pahacAna ke bAre meM bawAe gae wIna nyUrona
NP31 three neurons describing the color and three neurons describing the object identity	wIna nyUrona xvArA raMga Ora vaswu pahacAna kA ulleKa karawe hue wIna nyUrona
NP32 three neurons	wIna nyUrona
NNS34 neurons	nyUrona
VP35_LWG describing	bawAwe hue
NP37 the color and three neurons	raMga Ora wIna nyUrona
NP38 the color	raMga
CC41 and	Ora
NP42 three neurons	wIna nyUrona
NNS44 neurons	nyUrona
S45 describing the object identity	vaswu pahacAna kA jikra karawe hue
VP46_LWG describing	bawAwe hue
NP48 the object identity	vaswu pahacAna

----
00354	This requires only six neurons total instead of nine, and the neuron describing redness is able to learn about redness from images of cars, trucks and birds, not just from images of one specic category of objects.	 isake lie nO ke bajAya kevala Caha nyUroYnsa kula kI AvaSyakawA howI hE , Ora lAlimA kA varNana karane vAle nyUroYna kAroM , trakoM Ora pakRiyoM kI CaviyoM se lAlimA ke bAre meM sIKane meM sakRama hE , na kevala vaswuoM kI eka viSiRta SreNI kI CaviyoM se .		
353	353
S1 This requires only six neurons total instead of nine , and the neuron describing redness is able to learn about redness from images of cars , trucks and birds , not just from images of one specic category of objects .	isameM nO ke bajAya kevala Caha nyUrona kula kI AvaSyakawA howI hE, Ora lAlapana kA ulleKa karane vAlA nyUrona kAra, traka Ora pakRiyoM kI CaviyoM se lAla
S2 This requires only six neurons total instead of nine	isameM nO ke bajAya sirPa Caha nyUrona kula kI AvaSyakawA howI hE
NP3 This	yaha
VP5_LWG requires	jarUrawa hE
NP7 only six neurons total instead of nine	nO kI jagaha sirPa Caha nyUrona kula
NP8 only six neurons total	kevala Caha nyUrona kula
QP9 only six	kevala Caha
NNS12 neurons	nyUrona
PP14 instead of nine	nO kI jagaha
NP17 nine	nO
,19 ,	,
CC20 and	Ora
S21 the neuron describing redness is able to learn about redness from images of cars , trucks and birds , not just from images of one specic category of objects	lAlapana kA ulleKa karane vAlA nyUrona kAra, traka Ora pakRiyoM kI CaviyoM se lAlapana ke bAre meM sIKa sakawA hE, sirPa vaswuoM kI eka viSiRta SreNI kI Cava
NP22 the neuron describing redness	lAlapana kA ulleKa karawe hue nyUrona
NP23 the neuron	nyUrona
VP26_LWG describing	bawAwe hue
NP28 redness	lAlapana
VP30_LWG is	hE
ADJP32 able to learn about redness from images of cars , trucks and birds , not just from images of one specic category of objects	kAra, traka Ora pakRiyoM kI CaviyoM se lAlapana ke bAre meM sIKa sakawe hEM, sirPa vaswuoM kI eka viSiRta SreNI kI CaviyoM se nahIM
S34 to learn about redness from images of cars , trucks and birds , not just from images of one specic category of objects	kAra, traka Ora pakRiyoM kI CaviyoM se lAlapana ke bAre meM sIKane ke lie sirPa vaswuoM kI eka viSiRta SreNI kI CaviyoM se nahIM
VP35_LWG to learn	sIKane ke lie
PP39 about redness	lAlapana ke bAre meM
NP41 redness	lAlapana
PP43 from images of cars , trucks and birds , not just from images of one specic category of objects	kAroM kI CaviyoM se, traka Ora pakRI, sirPa vaswuoM kI eka viSiRta SreNI kI CaviyoM se nahIM
PP44 from images of cars , trucks and birds	kAroM kI CaviyoM se, traka Ora pakRI
NP46 images of cars , trucks and birds	kAroM kI CaviyAM, traka Ora pakRI
NP47 images	CaviyAM
NNS48 images	CaviyAM
PP49 of cars , trucks and birds	kAra, traka Ora pakRI
NP51 cars , trucks and birds	kAreM, traka Ora pakRI
NNS52 cars	kAreM
,53 ,	,
NNS54 trucks	traka
CC55 and	Ora
NNS56 birds	pakRI
,57 ,	,
CONJP58 not just	sirPa nahIM
PP61 from images of one specic category of objects	vaswuoM kI eka viSiRta SreNI kI CaviyoM se
NP63 images of one specic category of objects	vaswuoM kI eka viSiRta SreNI kI CaviyAM
NP64 images	CaviyAM
NNS65 images	CaviyAM
PP66 of one specic category of objects	vaswuoM kI eka viSiRta SreNI kI
NP68 one specic category of objects	vaswuoM kI eka viSiRta SreNI
NP69 one specic category	eka viSiRta SreNI
PP73 of objects	vaswuoM kA
NP75 objects	vaswueM
NNS76 objects	vaswueM

----
00355	The concept of distributed representation is central to this book and is described in greater detail in chapter 15.	 viwariwa prawiniXiwva kI avaXAraNA isa puswaka ke lie keMxrIya hE Ora aXyAya 15 meM aXika viswAra meM varNiwa hE .		
354	354
S1 The concept of distributed representation is central to this book and is described in greater detail in chapter 15 .	viwariwa prawiniXiwva kI avaXAraNA isa puswaka ke lie keMxrIya hE Ora ise aXyAya 15 meM aXika viswAra se varNiwa kiyA gayA hE
NP2 The concept of distributed representation	viwariwa prawiniXiwva kI avaXAraNA
NP3 The concept	avaXAraNA
PP6 of distributed representation	viwariwa prawiniXiwva kA
NP8 distributed representation	viwariwa prawiniXiwva
VP11_LWG is is described	bawAyA jAwA hE
ADJP14 central to this book	isa puswaka ke lie keMxrIya
PP16 to this book	isa kiwAba ko
NP18 this book	yaha puswaka
CC21 and	Ora
VP12 is central to this book	isa puswaka ke lie keMxrIya hE
VP22 is described in greater detail in chapter 15	aXyAya 15 meM aXika viswAra se bawAyA gayA hE
PP26 in greater detail in chapter 15	aXyAya 15 meM aXika viswAra se
NP28 greater detail in chapter 15	aXyAya 15 meM aXika viswAra
NP29 greater detail	jyAxA badZA viswAra
PP32 in chapter 15	aXyAya 15 meM
NP34 chapter 15	aXyAya 15

----
00356	Another major accomplishment of the connectionist movement was the suc- cessful use of back-propagation to train deep neural networks with internal repre- sentations and the popularization of the back-propagation algorithm (Rumelhart et al., 1986a; LeCun, 1987).	 kanekSanista AMxolana kI eka anya pramuKa upalabXi WI , AMwarika uxvegoM se yukwa gahana waMwrikA netavarkoM ko praSikRiwa karane ke lie bEkuMTa prayoga Ora bEkuMTa kalana viXi kA lokapriyIkaraNa ( ArumalahArta eta ala , 1987 , lesana , 1986 ) 		
355	355
S1 Another major accomplishment of the connectionist movement was the suc - cessful use of back - propagation to train deep neural networks with internal repre - sentations and the popularization of the back - propagation algorithm ( Rumelhart et al. , 1986a ; LeCun , 1987 ) .	kanekSanavAxI AMxolana kI eka Ora badZI upalabXi WI- pICe kA upayoga - AMwarika pUrvavarwana ke sAWa gahare waMwrikA netavarka ko praSikRiwa karane ke lie pracAra
NP2 Another major accomplishment of the connectionist movement	kanekSanavAxI AMxolana kI eka Ora badZI upalabXi
NP3 Another major accomplishment	eka Ora badZI upalabXi
PP7 of the connectionist movement	kanekSanavAxI AMxolana kA
NP9 the connectionist movement	kanekSanavAxI AMxolana
VP13_LWG was	WA
NP15 the suc - cessful use of back - propagation	cUso - pICe kA lAparavAhI se upayoga - pracAra
NP16 the suc - cessful use	cUso- sAvaXAnI se upayoga
NML18 suc - cessful	cUso- sAvaXAna
PP23 of back - propagation	pICe kI - pracAra
NP25 back - propagation	vApasa- pracAra-prasAra
S29 to train deep neural networks with internal repre - sentations and the popularization of the back - propagation algorithm	AMwarika pUrvavarwana ke sAWa gahare waMwrikA netavarka ko praSikRiwa karane ke lie  Beje jAne Ora pICe ke lokapriyakaraNa - pracAra elgorixama
VP30_LWG to train	treniMga ke lie
NP34 deep neural networks with internal repre - sentations and the popularization of the back - propagation algorithm	AMwarika pUrvavarwana ke sAWa gahare waMwrikA netavarka Ora pICe ke lokapriyakaraNa ke sAWa - pracAra elgorixama
NP35 deep neural networks with internal repre - sentations	AMwarika pUrvavarwana ke sAWa gahare waMwrikA netavarka- Beje gae
NP36 deep neural networks	gahare waMwrikA netavarka
NNS39 networks	netavarka
PP40 with internal repre - sentations	AMwarika pUrvAgraha ke sAWa- Beje gae AxeSa
NP42 internal repre - sentations	AMwarika pUrvAgraha- Beje gae AxeSa
NNS46 sentations	Beje gae Bawwe
CC47 and	Ora
NP48 the popularization of the back - propagation algorithm	pICe kA lokapriyakaraNa- pracAra elgorixama
NP49 the popularization	lokapriyawA
PP52 of the back - propagation algorithm	pICe kI - pracAra elgorixama
NP54 the back - propagation algorithm	pICe - pracAra elgorixama
NML56 back - propagation	vApasa- pracAra-prasAra
PRN61 ( Rumelhart et al. , 1986a ; LeCun , 1987 )	(rumAlahArta eta ala., 1986 ; lekana, 1987 )
FRAG63 Rumelhart et al. , 1986a	rUmAlahArta eta ala., 1986a
NP64 Rumelhart et al.	Rumelhart eta ala
ADVP66 et al.	eta ala
,69 ,	,
NP70 1986a	1986a
FRAG73 LeCun , 1987	lekana, 1987
NP74 LeCun	lekana
,76 ,	,
NP77 1987	1987

----
00357	This algorithm has waxed and waned in popularity but, as of this writing, is the dominant approach to training deep models.	 isa elgoriWma ne lokapriyawA meM vqxXi kI hE Ora kamI AI hE , lekina isa leKana ke rUpa meM , gahare moYdaloM ke praSikRaNa kA pramuKa xqRtikoNa hE 		
356	356
S1 This algorithm has waxed and waned in popularity but , as of this writing , is the dominant approach to training deep models .	isa elgorixama ne lokapriyawA meM moma Ora vicaliwa kiyA hE lekina, isa leKana ke rUpa meM, gahare moYdaloM ko praSikRiwa karane kA pramuKa xqRtikoNa hE
NP2 This algorithm	yaha elgorixama
VP5_LWG has waxed waned as is	jEsA hE vEsA hI ladZaKadZA gayA hE jEsA ki
CC10 and	Ora
VBN9 waxed	momabawwI
VBN11 waned	jAma kara xiyA gayA
PP12 in popularity	lokapriyawA meM
NP14 popularity	lokapriyawA
CC16 but	lekina
VP6 has waxed and waned in popularity	lokapriyawA meM moma Ora laharAyA hE
,17 ,	,
ADVP18 as of this writing	isa leKana ke rUpa meM
PP20 of this writing	isa leKana kA
NP22 this writing	yaha leKana
,25 ,	,
NP28 the dominant approach to training deep models	gahare moYdaloM ko praSikRiwa karane kA pramuKa xqRtikoNa
NP29 the dominant approach	pramuKa xqRtikoNa
PP33 to training deep models	gahare moYdaloM ko praSikRiwa karane ke lie
NP35 training deep models	praSikRaNa gahare moYdala
NNS38 models	moYdala

----
00358	During the 1990s, researchers made important advances in modeling sequences with neural networks.	 1990 ke xaSaka ke xOrAna , SoXakarwAoM ne waMwrikA netavarka ke sAWa moYdaliMga anukrama meM mahawvapUrNa pragawi kI .		
357	357
S1 During the 1990s , researchers made important advances in modeling sequences with neural networks .	1990 ke xaSaka ke xOrAna SoXakarwAoM ne waMwrikA netavarka ke sAWa moYdaliMga sIkveMsa meM ahama agrima pragawi kI
PP2 During the 1990s	1990 ke xaSaka ke xOrAna
NP4 the 1990s	1990 ke xaSaka
NNS6 1990s	1990 ke xaSaka
,7 ,	,
NP8 researchers	SoXakarwAoM
NNS9 researchers	SoXakarwAoM
VP10_LWG made	banAyA gayA
NP12 important advances	mahawvapUrNa agrima
NNS14 advances	edavAMsa
PP15 in modeling sequences	moYdaliMga sIkveMsa meM
NP17 modeling sequences	moYdaliMga sIkveMsa
NNS19 sequences	xqSya
PP20 with neural networks	waMwrikA netavarka ke sAWa
NP22 neural networks	waMwrikA netavarka
NNS24 networks	netavarka

----
00359	Hochreiter (1991) and Bengio et al.	 hoYkareitara ( 1991 ) Ora benjiyo eta ala .		
358	358
NP1 Hochreiter ( 1991 ) and Bengio et al. .	hocaretara ( 1991) Ora beMgiyo eta ala..
NP2 Hochreiter ( 1991 )	hocaretara ( 1991)
PRN4 ( 1991 )	( 1991)
NP6 1991	1991
CC9 and	Ora
NP10 Bengio et al.	beMjiyo eta ala
NP11 Bengio	beMgIo
ADVP13 et al.	eta ala

----
00360	(1994) identied some of the fundamental mathematical diculties in modeling long sequences, described in section 10.7.	 (1994 I . ) ne laMbe anukramoM ke moYdaliMga meM kuCa mUlaBUwa gaNiwIya kaTinAiyoM kI pahacAna kI , jisakA varNana XArA 10 . 7 meM kiyA gayA hE 		
359	359
S1 ( 1994 ) identied some of the fundamental mathematical diculties in modeling long sequences , described in section 10.7 .	( 1994) ne XArA 10.7 meM varNiwa kuCa mOlika gaNiwIya kaTinAiyoM kI pahacAna kI
NP2 ( 1994 )	( 1994)
VP6_LWG identied	pahacAna
NP8 some of the fundamental mathematical diculties	kuCa mOlika gaNiwIya kaTinAiyoM
NP9 some	kuCa
PP11 of the fundamental mathematical diculties	mOlika gaNiwIya kaTinAiyoM kA
NP13 the fundamental mathematical diculties	mOlika gaNiwIya kaTinAiyAM
NNS17 diculties	muSkileM
PP18 in modeling long sequences , described in section 10.7	moYdaliMga laMbe krama meM, XArA 10.7 meM varNiwa
NP20 modeling long sequences , described in section 10.7	moYdaliMga laMbe kramAMka, XArA 10.7 meM varNiwa
NP21 modeling long sequences	moYdaliMga laMbe sIkveMsa
NNS24 sequences	xqSya
,25 ,	,
VP26_LWG described	varNiwa
PP28 in section 10.7	XArA 10.7 meM
NP30 section 10.7	XArA 10.7

----
0036	People struggle to devise formal rules with enough complexity to accurately describe the world.	 loga xuniyA kA sahI varNana karane ke lie paryApwa jatilawA ke sAWa OpacArika niyama banAne ke lie saMGarRa karawe hEM .		
35	35
S1 People struggle to devise formal rules with enough complexity to accurately describe the world .	loga viSva ko satIka rUpa se varNana karane ke lie paryApwa jatilawA ke sAWa OpacArika niyama banAne ke lie saMGarRa karawe hEM
NP2 People	logoM
NNS3 People	logoM
VP4_LWG struggle	saMGarRa
S6 to devise formal rules with enough complexity to accurately describe the world	xuniyA kA satIka varNana karane ke lie paryApwa jatilawA ke sAWa OpacArika niyama banAne ke lie
VP7_LWG to devise	wEyAra karane ke lie
NP11 formal rules	OpacArika niyama
NNS13 rules	niyama
PP14 with enough complexity	paryApwa jatilawA ke sAWa
NP16 enough complexity	paryApwa jatilawA
S19 to accurately describe the world	xuniyA kA satIka varNana karane ke lie
VP20_LWG to accurately describe	satIka rUpa se varNana karane ke lie
ADVP22 accurately	satIka warIke se
NP26 the world	xuniyA

----
00361	Hochreiter and Schmidhuber (1997) introduced the long short-term memory (LSTM) network to resolve some of these diculties.	 hoPrItara Ora SammIxabara (1997 ) ne inameM se kuCa kaTinAiyoM ko xUra karane ke lie laMbI alpakAlika smqwi ( elaesatIema netavarka ) kI SurUAwa kI 		
360	360
S1 Hochreiter and Schmidhuber ( 1997 ) introduced the long short - term memory ( LSTM ) network to resolve some of these diculties .	hovarAitara Ora smiWahabara ( 1997) ne inameM se kuCa kaTinAiyoM ke samAXAna ke lie laMbe Cote- Sabxa kI smqwi (elaesatIema) netavarka ko peSa kiyA
NP2 Hochreiter and Schmidhuber ( 1997 )	hovarAitara Ora smiWahabara ( 1997)
CC4 and	Ora
NNP3 Hochreiter	hoYvaretiveta
NNP5 Schmidhuber	smiGababara
PRN6 ( 1997 )	( 1997)
NP8 1997	1997
VP11_LWG introduced	peSa kiyA gayA
NP13 the long short - term memory ( LSTM ) network	laMbI CotI- Sabxa kI smqwi (elaesatIema) netavarka
NML16 short - term memory ( LSTM )	laGu - Sabxa kI smqwi (elaesatIema)
NML17 short - term	laGu - Sabxa
PRN22 ( LSTM )	(elaesatIema)
NP24 LSTM	elaesatIema
S28 to resolve some of these diculties	inameM se kuCa kaTinAiyoM kA samAXAna karane ke lie
VP29_LWG to resolve	hala karane ke lie
NP33 some of these diculties	inameM se kuCa kaTinAiyAM
NP34 some	kuCa
PP36 of these diculties	ina kaTinAiyoM kI
NP38 these diculties	ye muSkileM
NNS40 diculties	muSkileM

----
00362	Today, the LSTM is widely used for many sequence modeling tasks, including many natural language processing tasks at Google.	 Aja , LSTM vyApaka rUpa se kaI anukrama moYdaliMga kAryoM ke lie prayoga kiyA jAwA hE , jisameM gUgala para kaI prAkqwika BARA saMsAXana kArya SAmila hEM .		
361	361
S1 Today , the LSTM is widely used for many sequence modeling tasks , including many natural language processing tasks at Google .	Aja elaesatIema kA iswemAla kaI sIkveMsa moYdaliMga kAryoM ke lie vyApaka rUpa se kiyA jAwA hE, jisameM gUgala meM kaI prAkqwika BARA prasaMskaraNa kArya SAmila
NP-TMP2 Today	Aja
,4 ,	,
NP5 the LSTM	elaesatIema
VP8_LWG is widely used	vyApaka rUpa se iswemAla howA hE
ADVP10 widely	vyApaka rUpa se
PP14 for many sequence modeling tasks , including many natural language processing tasks at Google	gUgala meM kaI prAkqwika BARA prasaMskaraNa kAryoM sahiwa kaI sIkveMsa moYdaliMga kAryoM ke lie
NP16 many sequence modeling tasks , including many natural language processing tasks at Google	gUgala meM kaI prAkqwika BARA prasaMskaraNa kAryoM sahiwa kaI anukrama moYdaliMga kArya,
NP17 many sequence modeling tasks	kaI sIkveMsa moYdaliMga kArya
ADJP19 sequence modeling	sIkveMsa moYdaliMga
NNS22 tasks	kAryoM
,23 ,	,
PP24 including many natural language processing tasks at Google	gUgala meM kaI prAkqwika BARA prasaMskaraNa kArya sahiwa
NP26 many natural language processing tasks at Google	gUgala para kaI prAkqwika BARA prasaMskaraNa kArya
NP27 many natural language processing tasks	kaI prAkqwika BARA prasaMskaraNa kArya
NML29 natural language processing	prAkqwika BARA prasaMskaraNa
NNS33 tasks	kAryoM
PP34 at Google	gUgala para
NP36 Google	gUgala

----
00363	The second wave of neural networks research lasted until the mid-1990s.	 waMwrikA netavarka anusaMXAna kI xUsarI lahara 1990 xaSaka ke maXya waka calI 		
362	362
S1 The second wave of neural networks research lasted until the mid-1990s .	waMwrikA netavarka anusaMXAna kI xUsarI lahara 1990 ke maXya waka calI
NP2 The second wave of neural networks research	waMwrikA netavarka SoXa kI xUsarI lahara
NP3 The second wave	xUsarI lahara
PP7 of neural networks research	waMwrikA netavarka SoXa
NP9 neural networks research	waMwrikA netavarka SoXa
NML10 neural networks	waMwrikA netavarka
NNS12 networks	netavarka
VP14_LWG lasted	calA gayA
PP16 until the mid-1990s	1990 ke maXya waka
NP18 the mid-1990s	1990 ke maXya meM
NNS20 mid-1990s	1990 ke maXya meM

----
00364	Ven- tures based on neural networks and other AI technologies began to make unrealisti- cally ambitious claims while seeking investments.	 waMwrikA netavarka waWA anya eAI prOxyogikiyoM para AXAriwa vena - tyUnsa ne niveSa kI mAMga karawe samaya avAswavika waWA awi mahawvAkAMkRI xAve karane SurU kara xie 		
363	363
S1 Ven - tures based on neural networks and other AI technologies began to make unrealisti - cally ambitious claims while seeking investments .	vena- waMwrikA netavarka Ora anya eAI prOxyogikAoM para AXAriwa tyUrsa anAravAxI banAne lage- kaWiwa mahawvAkAMkRI xAvoM ko niveSa kI mAMga karawe hue
NP2 Ven - tures based on neural networks and other AI technologies	vena- waMwrikA netavarka Ora anya eAI wakanIkoM para AXAriwa tyUrsa
NP3 Ven - tures	vena- tyUmara
NNS6 tures	tyUmara
PP7 based on neural networks and other AI technologies	waMwrikA netavarka Ora anya eAI wakanIkoM para AXAriwa
PP9 on neural networks and other AI technologies	waMwrikA netavarka Ora anya eAI wakanIkoM para
NP11 neural networks and other AI technologies	waMwrikA netavarka Ora anya eAI wakanIkoM
NP12 neural networks	waMwrikA netavarka
NNS14 networks	netavarka
CC15 and	Ora
NP16 other AI technologies	anya eAI wakanIkoM
NNS19 technologies	wakanIkoM
VP20_LWG began	SurU huI SuruAwa
S22 to make unrealisti - cally ambitious claims while seeking investments	anAravAxI banAne ke lie - niveSa kI mAMga karawe hue kaWiwa mahawvAkAMkRI xAvexAra
VP23_LWG to make	banAne ke lie
NP27 unrealisti - cally ambitious claims	anAravAxI- koYla karane vAle mahawvAkAMkRI xAve
ADJP28 unrealisti - cally	anArawI- koYla
NNS33 claims	xAve
PP34 while seeking investments	niveSa kI mAMga karawe hue
S36 seeking investments	niveSa kI mAMga
VP37_LWG seeking	mAMga rahA WA
NP39 investments	niveSa
NNS40 investments	niveSa

----
00365	When AI research did not fulll these unreasonable expectations, investors were disappointed.	 jaba eAI anusaMXAna ina anuciwa ummIxoM ko pUrA nahIM kiyA , niveSakoM nirASa We .		
364	364
S1 When AI research did not fulll these unreasonable expectations , investors were disappointed .	eAI anusaMXAna ne jaba ina anuciwa apekRAoM ko pUrA nahIM kiyA wo niveSaka nirASa ho gae
SBAR2 When AI research did not fulll these unreasonable expectations	jaba eAI anusaMXAna ne ina anuciwa apekRAoM ko pUrA nahIM kiyA
WHADVP3 When	kaba
S5 AI research did not fulll these unreasonable expectations	eAI anusaMXAna ne ina anuciwa apekRAoM ko pUrA nahIM kiyA
NP6 AI research	eAI anusaMXAna
VP9_LWG did not fulll	pUrI nahIM kI WI pUrI
NP14 these unreasonable expectations	ye anuciwa ummIxeM
NNS17 expectations	ummIxeM
,18 ,	,
NP19 investors	niveSaka
NNS20 investors	niveSaka
VP21_LWG were	We
ADJP23 disappointed	nirASa

----
00366	Simultaneously, other elds of machine learning made advances.	 isake sAWa hI maSInI SikRA ke anya kRewroM ne BI pragawi kI 		
365	365
S1 Simultaneously , other elds of machine learning made advances .	sAWa hI maSIna sIKane ke anya kRewroM ne Age baDZawa banAI
ADVP2 Simultaneously	sAWa hI
,4 ,	,
NP5 other elds of machine learning	maSIna sIKane ke anya kRewra
NP6 other elds	xUsare Kewa
NNS8 elds	KewoM
PP9 of machine learning	maSIna sIKane kI
NP11 machine learning	maSIna sIKanA
VP14_LWG made	banAyA gayA
NP16 advances	edavAMsa
NNS17 advances	edavAMsa

----
00367	Kernel machines (Boser et al., 1992; Cortes and Vapnik, 1995; Schlkopf et al., 1999) and graphical models (Jor- dan, 1998) both achieved good results on many important tasks.	 karnela maSIneM ( 1992 mahawvapUrNa kora Ora vepeM Ora 1995 ) & # 44 ; esa ema esa & # 44 ; 19999 & # 44 ; esa ema esa & # 44 ; grAPikala moYdala ( 1998 ) & # 44 ; jina kAryoM se kaI acCe pariNAma prApwa hue 		
366	366
S1 Kernel machines ( Boser et al. , 1992 ; Cortes and Vapnik , 1995 ; Schlkopf et al. , 1999 ) and graphical models ( Jor - dan , 1998 ) both achieved good results on many important tasks .	kernala maSInoM (bosara eta ala., 1992 ; korta Ora vAparanika, 1995 ;SolekoYPa eta ala., 1999 ) Ora grAPika moYdala (jora-
NP2 Kernel machines ( Boser et al. , 1992 ; Cortes and Vapnik , 1995 ; Schlkopf et al. , 1999 ) and graphical models ( Jor - dan , 1998 )	kernala maSInoM (bosara eta ala., 1992 ; korta Ora vAparanika, 1995 ;SolekoYPa eta ala., 1999 ) Ora grAPika moYdala (jora-
NP3 Kernel machines ( Boser et al. , 1992 ; Cortes and Vapnik , 1995 ; Schlkopf et al. , 1999 )	kernala maSInoM (bosara eta ala., 1992 ; korta Ora vAparanika, 1995 ; SoYkalakoPa eta ala., 1999 )
NNS5 machines	maSIneM
PRN6 ( Boser et al. , 1992 ; Cortes and Vapnik , 1995 ; Schlkopf et al. , 1999 )	(bosara eta ala., 1992 ; korta Ora vAparanika, 1995 ; SElakoPa eta ala., 1999 )
FRAG8 Boser et al. , 1992	bosara eta ala., 1992
NP9 Boser et al.	bosara eta ala
ADVP11 et al.	eta ala
,14 ,	,
NP15 1992	1992
FRAG18 Cortes and Vapnik , 1995	korta Ora vApnika, 1995
NP19 Cortes and Vapnik	korta Ora vAparanika
CC21 and	Ora
NNP20 Cortes	kortisa
NNP22 Vapnik	vAparanika
,23 ,	,
NP24 1995	1995
FRAG27 Schlkopf et al. , 1999	SElakoPa eta ala, 1999
NP28 Schlkopf et al.	SElakoPa eta ala
ADVP30 et al.	eta ala
,33 ,	,
NP34 1999	1999
CC37 and	Ora
NP38 graphical models ( Jor - dan , 1998 )	grAPika moYdala (jora-xAna, 1998)
NNS40 models	moYdala
PRN41 ( Jor - dan , 1998 )	(jora- xAna, 1998)
NP43 Jor - dan	jora- xAna
,47 ,	,
NP48 1998	1998
VP52_LWG achieved	hAsila
NP54 good results	acCe pariNAma
NNS56 results	pariNAma
PP57 on many important tasks	kaI ahama kAryoM para
NP59 many important tasks	kaI ahama kArya
NNS62 tasks	kAryoM

----
00368	These two factors led to a decline in the popularity of neural networks that lasted until 2007.	 ina xo kAraNoM se waMwrikA netavarka kI lokapriyawA meM girAvata AI jo 2007 waka calI 		
367	367
S1 These two factors led to a decline in the popularity of neural networks that lasted until 2007 .	ina xonoM kArakoM kI vajaha se waMwrikA netavarka kI lokapriyawA meM girAvata AI jo 2007 waka calI
NP2 These two factors	ye xo kAraka
NNS5 factors	kAraKAne
VP6_LWG led	newqwva
PP8 to a decline	girAvata ke lie
NP10 a decline	eka girAvata
PP13 in the popularity of neural networks that lasted until 2007	2007 waka cale waMwrikA netavarka kI lokapriyawA meM
NP15 the popularity of neural networks that lasted until 2007	waMwrikA netavarka kI lokapriyawA jo 2007 waka calI
NP16 the popularity	lokapriyawA
PP19 of neural networks that lasted until 2007	waMwrikA netavarkoM kI jo 2007 waka calI
NP21 neural networks that lasted until 2007	waMwrikA netavarka jo 2007 waka cale
NP22 neural networks	waMwrikA netavarka
NNS24 networks	netavarka
SBAR25 that lasted until 2007	vaha 2007 waka calI WI
WHNP26 that	vaha
S28 lasted until 2007	2007 waka calI WI
VP29_LWG lasted	calA gayA
PP31 until 2007	2007 waka
NP33 2007	2007

----
00369	During this time, neural networks continued to obtain impressive performance on some tasks (	 isa xOrAna , waMwrikA netavarka kuCa kAryoM para praBAvaSAlI praxarSana prApwa karane ke lie jArI raKA (		
368	368
S1 During this time , neural networks continued to obtain impressive performance on some tasks (	isa xOrAna kuCa kAryoM para waMwrikA netavarka praBAvaSAlI praxarSana prApwa karawe rahe (
PP2 During this time	isa xOrAna
NP4 this time	isa bAra
,7 ,	,
NP8 neural networks	waMwrikA netavarka
NNS10 networks	netavarka
VP11_LWG continued	jArI
S13 to obtain impressive performance on some tasks (	kuCa kAryoM para praBAvaSAlI praxarSana prApwa karane ke lie (
VP14_LWG to obtain	prApwa karane ke lie
NP18 impressive performance	praBAvaSAlI praxarSana
PP21 on some tasks (	kuCa kAryoM para (
NP23 some tasks (	kuCa kArya (
NNS25 tasks	kAryoM

----
00370	LeCun et al.	 lekuna ewa ala		
369	369
NP1 LeCun et al. .	lekana eta ala.
NP2 LeCun	lekana
ADVP4 et al.	eta ala

----
0037	For example, Cyc failed to understand a story about a person named Fred shaving in the morning (Linde, 1992).	 uxAharaNa ke lie , Cyc subaha Preda Seva nAma ke eka vyakwi ke bAre meM eka kahAnI samaJane meM viPala rahA ( Linde , 1992 )		
36	36
S1 For example , Cyc failed to understand a story about a person named Fred shaving in the morning ( Linde , 1992 ) .	uxAharaNa ke lie, sAika subaha (liMde, 1992) meM Preda SeviMga nAmaka vyakwi ke bAre meM eka kahAnI samaJane meM viPala rahA
PP2 For example	uxAharaNa ke lie
NP4 example	uxAharaNa
,6 ,	,
NP7 Cyc	sAika
VP9_LWG failed	asaPala
S11 to understand a story about a person named Fred shaving in the morning ( Linde , 1992 )	subaha meM Preda SeviMga nAmaka vyakwi ke bAre meM eka kahAnI samaJane ke lie (liMde, 1992)
VP12_LWG to understand	samaJane ke lie
NP16 a story	eka kahAnI
PP19 about a person named Fred shaving in the morning ( Linde , 1992 )	subaha (liMde, 1992) meM Preda SeviMga nAmaka vyakwi ke bAre meM
NP21 a person named Fred shaving in the morning ( Linde , 1992 )	subaha meM Preda SeviMga nAmaka vyakwi (liMde, 1992)
NP22 a person	eka vyakwi
VP25_LWG named	nAmajaxa
NP27 Fred shaving in the morning	subaha Preda SeviMga
NP28 Fred shaving	Preda SeviMga
PP31 in the morning	subaha-subaha
NP33 the morning	subaha
PRN36 ( Linde , 1992 )	(liMde, 1992)
NP38 Linde , 1992	liMde, 1992
,40 ,	,

----
00371	, 1998b; Bengio et al., 2001).	 1998 , benajIra Butto hawyAkAMda , 2001		
370	370
FRAG1 , 1998b ; Bengio et al. , 2001 ) .	, 1998b ; beMgiyo eta ala, 2001 )
NP2 , 1998b	, 1998b
,3 ,	,
FRAG6 Bengio et al. , 2001	beMgiyo eta ala, 2001
NP7 Bengio et al.	beMjiyo eta ala
ADVP9 et al.	eta ala
,12 ,	,
NP13 2001	2001

----
00372	The Canadian Institute for Advanced Research (CIFAR) helped to keep neural networks research alive via its Neural Computation and Adaptive Perception (NCAP) research initiative.	 kanAdA iMstItyUta PoYra edavAMsda risarca ( sIAIePaeAra ) ne waMwrikA netavarka anusaMXAna ko apane grAmINa kaMpyUtiMga Ora anukUlI saMkalpanA ( enasIepI anusaMXAna pahala ) ke mAXyama se jIviwa raKane meM maxaxa kI		
371	371
S1 The Canadian Institute for Advanced Research ( CIFAR ) helped to keep neural networks research alive via its Neural Computation and Adaptive Perception ( NCAP ) research initiative .	kanAdAI iMstItyUta PoYra edavAMsa risarca (sIAIePaeAra) ne apanI waMwrikA prawiRTA Ora anukUla XAraNA (enakEpa) SoXa pahala ke jarie waMwrikA
NP2 The Canadian Institute for Advanced Research ( CIFAR )	kanAdAI iMstItyUta PoYra edavAMsa risarca (sIAIePaeAra)
NP3 The Canadian Institute	kanAdAI saMsWAna
PP7 for Advanced Research ( CIFAR )	edavAMsa risarca (sIAIePaeAra) ke lie
NP9 Advanced Research ( CIFAR )	edavAMsa risarca (sIAIePaeAra)
VP15_LWG helped	maxaxa kI
S17 to keep neural networks research alive via its Neural Computation and Adaptive Perception ( NCAP ) research initiative	apanI waMwrikA prawiRTA Ora anukUla XAraNA (NCAP) anusaMXAna pahala ke mAXyama se waMwrikA netavarka anusaMXAna ko jiMxA raKane ke lie
VP18_LWG to keep	raKane ke lie
NP22 neural networks research alive via its Neural Computation and Adaptive Perception ( NCAP ) research initiative	waMwrikA netavarka apanI waMwrikA prawiRTA Ora anukUla XAraNA (NCAP) anusaMXAna pahala ke mAXyama se jiMxA anusaMXAna
NML23 neural networks	waMwrikA netavarka
NNS25 networks	netavarka
ADJP27 alive via its Neural Computation and Adaptive Perception ( NCAP )	jiMxA isakI waMwrikA prawiRTA Ora anukUla XAraNA (enakEpa) ke mAXyama se jIviwa
PP29 via its Neural Computation and Adaptive Perception ( NCAP )	isake waMwrikA prawiRTA Ora anukUla XAraNA (NCAP) ke mAXyama se
NP31 its Neural Computation and Adaptive Perception ( NCAP )	isakI waMwrikA prawiRTA Ora anukUla XAraNA (NCAP)
NP32 its Neural Computation	isakI waMwrikA prawiRTA
CC36 and	Ora
NP37 Adaptive Perception ( NCAP )	anukUla XAraNA (enakEpa)

----
00373	This program united machine learning research groups led by Georey Hinton at University of Toronto, Yoshua Bengio at University of Montreal, and Yann LeCun at New York University.	 isa kAryakrama saMyukwa maSIna SikRaNa anusaMXAna samUhoM toraMto viSvavixyAlaya meM jioPrI hintana ke newqwva meM , moYntatriyala viSvavixyAlaya meM yoSu beMjiyo , Ora nyUyoYrka viSvavixyAlaya meM yAna lIkuna		
372	372
S1 This program united machine learning research groups led by Georey Hinton at University of Toronto , Yoshua Bengio at University of Montreal , and Yann LeCun at New York University .	yaha kAryakrama toraMto viSvavixyAlaya, moYntriyala yUnivarsitI oYPa moYntriyala meM jePrI hiMtana ke newqwva meM ekajuta maSIna sIKa anusaMXAna samUha, moYntri
NP2 This program	yaha kAryakrama
VP5_LWG united	ekajuta hokara
SBAR7 machine learning research groups led by Georey Hinton at University of Toronto , Yoshua Bengio at University of Montreal , and Yann LeCun at New York University	toraMto viSvavixyAlaya, moYntriyala yUnivarsitI meM jiPrI hiMtana ke newqwva meM maSIna sIKa anusaMXAna samUha, moYntriyala yUnivarsitI meM joSuA beMjiyo
S8 machine learning research groups led by Georey Hinton at University of Toronto , Yoshua Bengio at University of Montreal , and Yann LeCun at New York University	toraMto viSvavixyAlaya, moYntriyala yUnivarsitI meM jiPrI hiMtana ke newqwva meM maSIna sIKa anusaMXAna samUha, moYntriyala yUnivarsitI meM joSuA beMjiyo
NP9 machine learning research groups	maSIna sIKa anusaMXAna samUha
NML10 machine learning	maSIna sIKanA
NNS14 groups	samUha
VP15_LWG led	newqwva
PP17 by Georey Hinton	jePrI hiMtana xvArA
NP19 Georey Hinton	jePrI hiMtana
PP22 at University of Toronto , Yoshua Bengio at University of Montreal , and Yann LeCun at New York University	toraMto viSvavixyAlaya meM, moYntriyala yUnivarsitI meM joSuA beMgiyo, Ora nyUyoYrka yUnivarsitI meM yena lekana
NP24 University of Toronto , Yoshua Bengio at University of Montreal , and Yann LeCun at New York University	toraMto viSvavixyAlaya, moYntriyala yUnivarsitI meM joSuA beMgiyo, Ora nyUyoYrka yUnivarsitI meM yena lekana
NP25 University of Toronto	toraMto viSvavixyAlaya
NP26 University	yUnivarsitI
PP28 of Toronto	toraMto kA
NP30 Toronto	toraMto
,32 ,	,
NP33 Yoshua Bengio at University of Montreal	moYntriyala yUnivarsitI meM yoSuA beMgiyo
NP34 Yoshua Bengio	yoSuA beMgiyo
PP37 at University of Montreal	moYntriyala yUnivarsitI meM
NP39 University of Montreal	moYntriyala yUnivarsitI
NP40 University	yUnivarsitI
PP42 of Montreal	moYntriyala ke
NP44 Montreal	moYntriyala
,46 ,	,
CC47 and	Ora
NP48 Yann LeCun at New York University	yAnana lekana nyUyoYrka yUnivarsitI meM
NP49 Yann LeCun	yAMnI leUna
PP52 at New York University	nyUyoYrka yUnivarsitI meM
NP54 New York University	nyUyoYrka yUnivarsitI
NML55 New York	nyUyoYrka

----
00374	The multidisciplinary CIFAR NCAP research initiative 17 CHAPTER 1.	 bahusAMskqwika sIAIePaeAra enasIepI anusaMXAna pahala 17 CHAPTER 1 .		
373	373
FRAG1 The multidisciplinary CIFAR NCAP research initiative 17 CHAPTER 1 .	viBAjanakArI sIAIePaeAra enakEpa anusaMXAna pahala 17 cEptara 1
NP2 The multidisciplinary CIFAR NCAP	viBAjanakArI sIAIePaeAra enakEpa
NP7 research initiative 17	anusaMXAna kI pahala 17
NP11 CHAPTER 1	cEptara 1

----
00375	INTRODUCTION also included neuroscientists and experts in human and computer vision.	 INTRODUCING meM waMwrikA vijFAnI Ora mAnava Ora kaMpyUtara xqRti ke viSeRajFa BI SAmila We 		
374	374
S1 INTRODUCTION also included neuroscientists and experts in human and computer vision .	paricaya meM mAnava Ora kaMpyUtara xqRti meM nyUro vEjFAnikoM Ora viSeRajFoM ko BI SAmila kiyA gayA
NP2 INTRODUCTION	paricaya
ADVP4 also	sAWa hI
VP6_LWG included	SAmila
NP8 neuroscientists and experts	nyUro vEjFAnika Ora viSeRajFa
NNS9 neuroscientists	nyUro vEjFAnika
CC10 and	Ora
NNS11 experts	viSeRajFoM
PP12 in human and computer vision	mAnava Ora kaMpyUtara xqRti meM
NP14 human and computer vision	mAnava Ora kaMpyUtara xqRti
UCP15 human and computer	iMsAna Ora kaMpyUtara
CC17 and	Ora
JJ16 human	mAnava
NN18 computer	kaMpyUtara

----
00376	At this point, deep networks were generally believed to be very dicult to train.	 isa biMxu para , gahare netavarka Ama wOra para mAnA jAwA WA ki praSikRaNa xenA bahuwa kaTina howA hE .		
375	375
S1 At this point , deep networks were generally believed to be very dicult to train .	isa biMxu para gahare netavarka ko AmawOra para praSikRiwa karanA bahuwa muSkila mAnA jAwA WA
PP2 At this point	isa biMxu para
NP4 this point	yaha biMxu
,7 ,	,
NP8 deep networks	gahare netavarka
NNS10 networks	netavarka
VP11_LWG were generally believed	AmawOra para mAnA jAwA WA
ADVP13 generally	Ama wOra para
S17 to be very dicult to train	treniMga karanA bahuwa muSkila hogA
VP18_LWG to be	honA cAhie
ADJP22 very dicult to train	treniMga karanA bahuwa muSkila
S25 to train	treniMga ke lie
VP26_LWG to train	treniMga ke lie

----
00377	We now know that algorithms that have existed since the 1980s work quite well, but this was not apparent circa 2006.	 aba hama jAnawe hEM ki elgorixama ki 1980 ke xaSaka ke bAxa se aswiwva meM hE kAPI acCI waraha se kAma karawe hEM , lekina yaha spaRta rUpa se circa 2006 nahIM WA .		
376	376
S1 We now know that algorithms that have existed since the 1980s work quite well , but this was not apparent circa 2006 .	aba hama jAnawe hEM ki elgorixama jo 1980 ke kAma ke bAxa se acCI waraha se mOjUxa hEM, lekina yaha spaRta nahIM WA sarkasa 2006
S2 We now know that algorithms that have existed since the 1980s work quite well	aba hama jAnawe hEM ki elgorixama jo 1980 ke xaSaka ke kAma ke bAxa se mOjUxa hEM, kAPI acCI waraha se
NP3 We	hama
ADVP5 now	aba
VP7_LWG know	jAnie
SBAR9 that algorithms that have existed since the 1980s work quite well	vaha elgorixama jo 1980 ke xaSaka ke kAma ke bAxa se mOjUxa hEM, kAPI acCI waraha se
S11 algorithms that have existed since the 1980s work quite well	elgorixama jo 1980 ke xaSaka ke kAma ke bAxa se mOjUxa hEM, kAPI acCI waraha se
NP12 algorithms that have existed since the 1980s	elgorixama jo 1980 ke xaSaka se mOjUxa hEM
NP13 algorithms	elgorixama
NNS14 algorithms	elgorixama
SBAR15 that have existed since the 1980s	vaha 1980 ke xaSaka se mOjUxa hE
WHNP16 that	vaha
S18 have existed since the 1980s	1980 ke xaSaka se mOjUxa hE
VP19_LWG have existed	mOjUxa hE
PP23 since the 1980s	1980 ke xaSaka se
NP25 the 1980s	1980 ke xaSaka
NNS27 1980s	1980 ke xaSaka meM
VP28_LWG work quite	kAma kAPI
ADVP30 quite well	kAPI acCI waraha se
,33 ,	,
CC34 but	lekina
S35 this was not apparent circa 2006	yaha spaRta nahIM WA sarkasa 2006
NP36 this	yaha
VP38_LWG was not	nahIM WA
ADJP41 apparent circa 2006	spaRta sarkasa 2006
PP43 circa 2006	sarkasa 2006
NP45 2006	2006

----
00378	The issue is perhaps simply that these algorithms were too computationally costly to allow much experimentation with the hardware available at the time.	 muxxA SAyaxa sirPa yaha hE ki ina elgorixama bahuwa kampyUteSanala rUpa se mahaMgA WA usa samaya upalabXa hArdaveyara ke sAWa bahuwa prayoga kI anumawi xene ke lie .		
377	377
S1 The issue is perhaps simply that these algorithms were too computationally costly to allow much experimentation with the hardware available at the time .	muxxA SAyaxa yaha hE ki ye elgorixama usa samaya upalabXa hArdaveyara ke sAWa jyAxA prayoga karane kI anumawi xene ke lie bahuwa mahaMgA We
NP2 The issue	muxxA
VP5_LWG is perhaps simply	SAyaxa basa iwanA hI
ADVP7 perhaps	SAyaxa
ADVP9 simply	basa iwanA hI
SBAR11 that these algorithms were too computationally costly to allow much experimentation with the hardware available at the time	yaha elgorixama usa samaya upalabXa hArdaveyara ke sAWa jyAxA prayoga karane kI anumawi xene ke lie bahuwa mahaMgA We
S13 these algorithms were too computationally costly to allow much experimentation with the hardware available at the time	yaha elgorixama usa samaya upalabXa hArdaveyara ke sAWa bahuwa aXika prayoga kI anumawi xene ke lie bahuwa mahaMgA We
NP14 these algorithms	ye elgorixama
NNS16 algorithms	elgorixama
VP17_LWG were too computationally	bahuwa xakRawA se We
ADVP19 too	sAWa hI
ADVP21 computationally	prawiRTApUrvaka
ADJP23 costly	mahaMgAI
S25 to allow much experimentation with the hardware available at the time	usa samaya upalabXa hArdaveyara ke sAWa jyAxA prayoga anumawi xene ke lie
VP26_LWG to allow	anumawi xene ke lie
NP30 much experimentation	bahuwa prayoga
PP33 with the hardware available at the time	usa samaya upalabXa hArdaveyara ke sAWa
NP35 the hardware available at the time	usa samaya upalabXa hArdaveyara
NP36 the hardware	hArdaveyara
ADJP39 available at the time	usa samaya upalabXa
PP41 at the time	usa samaya
NP43 the time	samaya

----
00379	The third wave of neural networks research began with a breakthrough in 2006.	 waMwrikA netavarka anusaMXAna kI wIsarI lahara 2006 meM eka wodZa ke sAWa SurU huI 		
378	378
S1 The third wave of neural networks research began with a breakthrough in 2006 .	waMwrikA netavarka anusaMXAna kI wIsarI lahara 2006 meM eka saPalawA ke sAWa SurU huI
NP2 The third wave of neural networks research	waMwrikA netavarka SoXa kI wIsarI lahara
NP3 The third wave	wIsarI lahara
PP7 of neural networks research	waMwrikA netavarka SoXa
NP9 neural networks research	waMwrikA netavarka SoXa
NML10 neural networks	waMwrikA netavarka
NNS12 networks	netavarka
VP14_LWG began	SurU huI SuruAwa
PP16 with a breakthrough in 2006	2006 meM milI saPalawA ke sAWa
NP18 a breakthrough in 2006	2006 meM eka saPalawA
NP19 a breakthrough	eka saPalawA
PP22 in 2006	2006 meM
NP24 2006	2006

----
00380	Georey Hinton showed that a kind of neural network called a deep belief network could be eciently trained using a strategy called greedy layer-wise pretraining (Hinton et al., 2006), which we describe in more detail in section 15.1.	 jioPrI hAinatana ne xiKAyA ki eka prakAra kA waMwrikA netavarka jise gaharA viSvAsa netavarka kahA jAwA hE , lAlacI parawa - vAra prItriniMga ( hEnatana eta ala 2006 ) nAmaka raNanIwi kA upayoga karake kuSalawApUrvaka praSikRiwa kiyA jA sakawA hE , jisakA hama KaMda 151 meM aXika viswAra se varNana karawe hEM .		
379	379
S1 Georey Hinton showed that a kind of neural network called a deep belief network could be eciently trained using a strategy called greedy layer-wise pretraining ( Hinton et al. , 2006 ) , which we describe in more detail in section 15.1 .	jePrI hiMtana ne xiKAyA ki gaharI mAnyawA netavarka nAmaka eka waraha kA waMwrikA netavarka lAlacI parawavAra praSikRaNa ( hiMtana eta ala, 2006) nAmaka raNanIwi kA
NP2 Georey Hinton	jePrI hiMtana
VP5_LWG showed	xiKAyA gayA
SBAR7 that a kind of neural network called a deep belief network could be eciently trained using a strategy called greedy layer-wise pretraining ( Hinton et al. , 2006 ) , which we describe in more detail in section 15.1	gaharI mAnyawA prApwa netavarka nAmaka eka prakAra kA waMwrikA netavarka lAlacI parawavAra praSikRaNa (hiMtana eta ala, 2006) nAmaka raNanIwi kA upayoga karake kuSalawA se
S9 a kind of neural network called a deep belief network could be eciently trained using a strategy called greedy layer-wise pretraining ( Hinton et al. , 2006 ) , which we describe in more detail in section 15.1	gaharI mAnyawA prApwa netavarka nAmaka eka prakAra kA waMwrikA netavarka kuSalawA se praSikRiwa kiyA jA sakawA hE jise hama XArA 15.1 meM aXika viswAra se bawAwe hEM
NP10 a kind of neural network	eka waraha kA waMwrikA netavarka
NP11 a kind	eka waraha kA
PP14 of neural network	waMwrikA netavarka kA
NP16 neural network	waMwrikA netavarka
VP19_LWG called	bulAyA gayA
SBAR21 a deep belief network could be eciently trained using a strategy called greedy layer-wise pretraining ( Hinton et al. , 2006 ) , which we describe in more detail in section 15.1	eka gaharA mAnyawA netavarka lAlacI parawavAra praSikRaNa (hiMtana eta ala, 2006) nAmaka raNanIwi kA upayoga karake kuSalawApUrvaka praSikRiwa kiyA jA sakawA hE, jisakA
S22 a deep belief network could be eciently trained using a strategy called greedy layer-wise pretraining ( Hinton et al. , 2006 ) , which we describe in more detail in section 15.1	eka gaharA mAnyawA netavarka lAlacI parawavAra praSikRaNa (hiMtana eta ala, 2006) nAmaka raNanIwi kA upayoga karake kuSalawApUrvaka praSikRiwa kiyA jA sakawA hE, jisakA
NP23 a deep belief network	gaharA viSvAsa netavarka
VP28_LWG could be eciently trained	kuSala warIke se praSikRiwa ho sakawe hEM
ADVP32 eciently	kuSalawA se
S36 using a strategy called greedy layer-wise pretraining ( Hinton et al. , 2006 ) , which we describe in more detail in section 15.1	lAlacI parawa vAle praSikRaNa (hiMtana eta ala, 2006) nAmaka raNanIwi kA upayoga karawe hue, jisakA hama XArA 15.1 meM aXika viswAra se varNana karawe hEM
VP37_LWG using	iswemAla karanA
NP39 a strategy called greedy layer-wise pretraining ( Hinton et al. , 2006 ) , which we describe in more detail in section 15.1	lAlacI parawa vAle praSikRaNa (hiMtana eta ala, 2006) nAmaka eka raNanIwi jisakA hama XArA 15.1 meM aXika viswAra se varNana karawe hEM
NP40 a strategy	eka raNanIwi
VP43_LWG called	bulAyA gayA
NP45 greedy layer-wise pretraining ( Hinton et al. , 2006 ) , which we describe in more detail in section 15.1	lAlacI parawavAra praSikRaNa (hiMtana eta ala, 2006) jisakA hama XArA 15.1 meM aXika viswAra se varNana karawe hEM
NP46 greedy layer-wise pretraining ( Hinton et al. , 2006 )	lAlacI parawavAra praSikRaNa (hiMtana eta ala., 2006)
PRN50 ( Hinton et al. , 2006 )	(hiMtana eta ala, 2006)
NP52 Hinton et al. , 2006	hiMtana eta ala., 2006
NP53 Hinton et al.	hiMtana eta ala
NP54 Hinton	hiMtana
ADVP56 et al.	eta ala
,59 ,	,
NP60 2006	2006
,63 ,	,
SBAR64 which we describe in more detail in section 15.1	jisakA hama XArA 15.1 meM aXika viswAra se bawAwe hEM
WHNP65 which	jo ki
S67 we describe in more detail in section 15.1	hama XArA 15.1 meM aXika viswAra se varNana karawe hEM
NP68 we	hama
VP70_LWG describe	varNana
PP72 in more detail in section 15.1	XArA 15.1 meM aXika viswAra se
NP74 more detail in section 15.1	XArA 15.1 meM aXika vivaraNa
NP75 more detail	aXika viswAra
PP78 in section 15.1	XArA 15.1 meM
NP80 section 15.1	XArA 15.1
NNS82 15.1	15.1

----
0038	Its inference engine detected an inconsistency in the story: it knew that people do not have electrical parts, but because Fred was holding an electric razor, it believed the entity FredWhileShaving contained electrical parts.	 isake anumAna iMjana ne kahAnI meM eka asaMgawi kA pawA lagAyAH yaha jAnawA WA ki logoM ke pAsa bijalI ke purje nahIM hEM , lekina cUMki Preda eka bijalI kA rejZara lagA rahA WA , isalie yaha viSvAsa karawA WA ki yaha ikAI PZedavAhilaSiMga meM vixyuwa purje We .		
37	37
S1 Its inference engine detected an inconsistency in the story : it knew that people do not have electrical parts , but because Fred was holding an electric razor , it believed the entity  FredWhileShaving  contained electrical parts .	isake aMwarjFAna iMjana ne kahAnI meM eka asaMwoRa kA pawA lagAyA H yaha jAnawA WA ki logoM ke pAsa bijalI ke hisse nahIM hEM, lekina kyoMki Preda eka ilekta
S2 Its inference engine detected an inconsistency in the story	isake iMPekSana iMjana ne kahAnI meM eka asaMwoRa kA pawA lagAyA
NP3 Its inference engine	isakI iMPekSana iMjana
VP7_LWG detected	pawA calA
NP9 an inconsistency	eka asaMwoRa
PP12 in the story	kahAnI meM
NP14 the story	kahAnI
S18 it knew that people do not have electrical parts , but because Fred was holding an electric razor , it believed the entity  FredWhileShaving  contained electrical parts	yaha jAnawA WA ki logoM ke pAsa bijalI ke aMga nahIM hEM, lekina kyoMki Preda eka ilektrika rejara pakadZe hue We, isalie yaha mAnawA WA ki ikAI 
S19 it knew that people do not have electrical parts	yaha jAnawA WA ki logoM ke pAsa bijalI ke aMga nahIM hEM
NP20 it	yaha
VP22_LWG knew	jAnawe We
SBAR24 that people do not have electrical parts	ki logoM ke pAsa bijalI ke hisse nahIM hEM
S26 people do not have electrical parts	logoM ke pAsa ilektrikala pArtsa nahIM hEM
NP27 people	logoM
NNS28 people	logoM
VP29_LWG do not have	na hoM
NP34 electrical parts	bijalI ke hisse
NNS36 parts	BAgoM
,37 ,	,
CC38 but	lekina
S39 because Fred was holding an electric razor , it believed the entity  FredWhileShaving  contained electrical parts	kyoMki Preda eka ilektrika rejara pakadZe hue We, isakA mAnanA WA ki ikAI PredaviSiMga SeviMga meM bijalI ke aMga SAmila We
SBAR40 because Fred was holding an electric razor	kyoMki Preda eka ilektrika rejara pakadZe hue We
S42 Fred was holding an electric razor	Preda eka ilektrika rejara pakadZe hue We
NP43 Fred	Preda
VP45_LWG was holding	pakadZe hue We
NP49 an electric razor	eka bijalI rejara
,53 ,	,
NP54 it	yaha
VP56_LWG believed	mAnA jAwA
SBAR58 the entity  FredWhileShaving  contained electrical parts	ikAI PredaviWiMga SeviMga meM bijalI ke aMga SAmila
S59 the entity  FredWhileShaving  contained electrical parts	ikAI PredaviWiMga SeviMga meM bijalI ke aMga SAmila
NP60 the entity  FredWhileShaving 	ikAI PredaviSiMgsaviMga
VP66_LWG contained	SAmila
NP68 electrical parts	bijalI ke hisse
NNS70 parts	BAgoM

----
00381	The other CIFAR-aliated research groups quickly showed that the same strategy could be used to train many other kinds of deep networks (Bengio et al., 2007; Ranzato et al., 2007a) and systematically helped to improve generalization on test examples.	 anya saMbaxXa anusaMXAna SIGrawA se usI raNanIwi kA prayoga kiyA jA sakawA hE ( CIFAR ) usI raNanIwi kA prayoga kiyA jA sakawA hE ( CIFAR ) usI raNanIwi kA prayoga kiyA jA sakawA hE ( CIFAR ) usI raNanIwi kA prayoga kiyA jA sakawA hE ( CIFAR ) usI praNAlI kA prayoga kiyA jA sakawA hE , jisa praNAlI kA prayoga kiyA gayA WA 		
380	380
S1 The other CIFAR - aliated research groups quickly showed that the same strategy could be used to train many other kinds of deep networks ( Bengio et al. , 2007 ; Ranzato et al. , 2007a ) and systematically helped to improve generalization on test examples .	xUsare sIAIePaeAra- saMbaxXa anusaMXAna samUhoM ne jalxI se yaha xiKAyA ki kaI anya prakAra ke gahare netavarka (beMgiyo eta ala, 2007 ; raNa
NP2 The other CIFAR - aliated research groups	anya CIFAR- saMbaxXa anusaMXAna samUha
ADJP4 other CIFAR - aliated	anya CIFAR- saMbaxXa
NP5 other CIFAR	anya CIFAR
NNS11 groups	samUha
ADVP12 quickly	jalxI se
VP14_LWG showed	xiKAyA gayA
SBAR16 that the same strategy could be used to train many other kinds of deep networks ( Bengio et al. , 2007 ; Ranzato et al. , 2007a ) and systematically helped to improve generalization on test examples	yaha BI raNanIwi kA iswemAla kaI anya prakAra ke gahare netavarka (beMgiyo eta ala, 2007 ; raNajAwo eta ala., 2007a) ko praSikRiwa karane ke
S18 the same strategy could be used to train many other kinds of deep networks ( Bengio et al. , 2007 ; Ranzato et al. , 2007a ) and systematically helped to improve generalization on test examples	isI raNanIwi kA iswemAla kaI anya prakAra ke gahare netavarka (beMgiyo eta ala, 2007 ; raNajAwo eta ala., 2007a) ko praSikRiwa karane ke lie
NP19 the same strategy	vahI raNanIwi
VP23_LWG could be used systematically helped	vyavasWiwa rUpa se iswemAla kiyA jA sakawA hE maxaxa
S30 to train many other kinds of deep networks	kaI anya prakAra ke gahare netavarka ko praSikRiwa karane ke lie
VP31_LWG to train	treniMga ke lie
NP35 many other kinds of deep networks	kaI anya prakAra ke gahare netavarka
NP36 many other kinds	kaI anya waraha ke
NNS39 kinds	waraha-waraha kI waraha
PP40 of deep networks	gahare netavarkoM kI
NP42 deep networks	gahare netavarka
NNS44 networks	netavarka
PRN45 ( Bengio et al. , 2007 ; Ranzato et al. , 2007a )	(beMgiyo eta ala., 2007 ; raNajAwo eta ala., 2007a)
FRAG47 Bengio et al. , 2007	beMgiyo eta ala, 2007
NP48 Bengio et al.	beMjiyo eta ala
ADVP50 et al.	eta ala
,53 ,	,
NP54 2007	2007
FRAG57 Ranzato et al. , 2007a	raNajAwo eta ala., 2007 e
NP58 Ranzato et al.	raNajAwo eta ala
ADVP60 et al.	eta ala
,63 ,	,
NP64 2007a	2007 e.
CC67 and	Ora
VP28 used to train many other kinds of deep networks ( Bengio et al. , 2007 ; Ranzato et al. , 2007a )	kaI anya prakAra ke gahare netavarka (beMgiyo eta ala., 2007 ; raNajAwo eta ala., 2007a)
ADVP68 systematically	vyavasWiwa rUpa se
S72 to improve generalization on test examples	parIkRaNa uxAharaNoM para sAmAnyakaraNa meM suXAra lAne ke lie
VP73_LWG to improve	suXAra karane ke lie
NP77 generalization	AmIkaraNa
PP79 on test examples	parIkRaNa uxAharaNoM para
NP81 test examples	parIkRaNa uxAharaNa
NNS83 examples	uxAharaNa

----
00382	This wave of neural networks research popularized the use of the term deep learning to emphasize that researchers were now able to train deeper neural networks than had been possible before, and to focus attention on the theoretical importance of depth (Bengio and LeCun, 2007; Delalleau and Bengio, 2011; Pascanu et al., 2014a; Montufar et al., 2014).	 waMwrikA netavarka anusaMXAna kI isa lahara ne xIpa aXigama Sabxa ke prayoga ko lokapriya banAyA wAki SoXakarwA aba gahare waMwrikA netavarka ko praSikRiwa karane meM sakRama ho sake jo isase pahale saMBava WA , Ora isake lie mahawva kI ora XyAna xiyA gayA WA , jo sana 2011 kI gaharAI ( bEMgiyota ) , elkyo Pokasa , elkyuela 2014		
381	381
S1 This wave of neural networks research popularized the use of the term  deep learning  to emphasize that researchers were now able to train deeper neural networks than had been possible before , and to focus attention on the theoretical importance of depth ( Bengio and LeCun , 2007 ; Delalleau and Bengio , 2011 ; Pascanu et al. , 2014a ; Montufar et al. , 2014 ) .	waMwrikA netavarka anusaMXAna kI isa lahara ne isa bAwa para bala xiyA ki SoXakarwA aba gaharI sIKa Sabxa ke upayoga ko baDZAvA xene meM sakRama We, Ora gaharAI
NP2 This wave of neural networks research	waMwrikA netavarka SoXa kI yaha lahara
NP3 This wave	yaha lahara
PP6 of neural networks research	waMwrikA netavarka SoXa
NP8 neural networks research	waMwrikA netavarka SoXa
NML9 neural networks	waMwrikA netavarka
NNS11 networks	netavarka
VP13_LWG popularized	poYpyularAijda
NP15 the use of the term  deep learning 	gaharI sIKa Sabxa kA upayoga
NP16 the use	upayoga
PP19 of the term  deep learning 	gaharI sIKa Sabxa kA
NP21 the term  deep learning 	Sabxa gaharI sIKa
NML25 deep learning	gaharI sIKa
S29 to emphasize that researchers were now able to train deeper neural networks than had been possible before , and to focus attention on the theoretical importance of depth	yaha jora xene ke lie ki SoXakarwA aba pahale kI wulanA meM gahare waMwrikA netavarka ko praSikRiwa karane meM sakRama We, Ora gaharAI ke sExXAMwika maha
VP30_LWG to emphasize	jora xene ke lie
SBAR34 that researchers were now able to train deeper neural networks than had been possible before , and to focus attention on the theoretical importance of depth	yaha SoXakarwA aba pahale se saMBava hone kI wulanA meM gahare waMwrikA netavarka ko praSikRiwa karane meM sakRama We, Ora gaharAI ke sExXAMwika mahawva para
S36 researchers were now able to train deeper neural networks than had been possible before , and to focus attention on the theoretical importance of depth	anusaMXAnakarwA aba pahale kI wulanA meM gahare waMwrikA netavarka ko praSikRiwa karane meM sakRama We, Ora gaharAI ke sExXAMwika mahawva para XyAna keMxriwa kara
NP37 researchers	SoXakarwAoM
NNS38 researchers	SoXakarwAoM
VP39_LWG were now	aba We
ADVP41 now	aba
ADJP43 able to train deeper neural networks than had been possible before , and to focus attention on the theoretical importance of depth	pahale kI wulanA meM gahare waMwrikA netavarka ko praSikRiwa karane meM sakRama, Ora gaharAI ke sExXAMwika mahawva para XyAna keMxriwa karane ke lie
S45 to train deeper neural networks than had been possible before , and to focus attention on the theoretical importance of depth	pahale kI wulanA meM gahare waMwrikA netavarka ko praSikRiwa karane ke lie, Ora gaharAI ke sExXAMwika mahawva para XyAna keMxriwa karanA
VP46_LWG to train to focus	XyAna keMxriwa karane ke lie praSikRaNa
NP51 deeper neural networks	gaharA waMwrikA netavarka
NNS54 networks	netavarka
SBAR55 than had been possible before	pahale se jyAxA saMBava WA
S57 had been possible before	pahale BI saMBava rahA WA
VP58_LWG had been before	pahale BI We
ADJP62 possible	saMBava
ADVP64 before	isase pahale
,66 ,	,
CC67 and	Ora
VP68 to focus attention on the theoretical importance of depth	gaharAI ke sExXAMwika mahawva para XyAna xenA
NP72 attention	XyAna
PP74 on the theoretical importance of depth	gaharAI ke sExXAMwika mahawva para
NP76 the theoretical importance of depth	gaharAI kA sExXAMwika mahawva
NP77 the theoretical importance	sExXAMwika mahawva
PP81 of depth	gaharAI kI
NP83 depth	gaharAI
PRN85 ( Bengio and LeCun , 2007 ; Delalleau and Bengio , 2011 ; Pascanu et al. , 2014a ; Montufar et al. , 2014 )	(beMgiyo Ora lekana, 2007 ; delAlU Ora beMgiyo, 2011 ; pAsakanU eta ala., 2014 ; moMtUPara eta ala., 2014 )
FRAG87 Bengio and LeCun , 2007	beMgo Ora lekana, 2007
NP88 Bengio and LeCun	beMgiyo Ora lekana
CC90 and	Ora
NNP89 Bengio	beMgIo
NNP91 LeCun	lekana
,92 ,	,
NP93 2007	2007
FRAG96 Delalleau and Bengio , 2011	delelU Ora biMgo, 2011
NP97 Delalleau and Bengio	delelU Ora beMgiyo
CC99 and	Ora
NNP98 Delalleau	delOu
NNP100 Bengio	beMgIo
,101 ,	,
NP102 2011	2011
FRAG105 Pascanu et al. , 2014a	pAsakanU eta ala., 2014a
NP106 Pascanu et al.	pAsakanU eta ala
ADVP108 et al.	eta ala
,111 ,	,
NP112 2014a	2014 e.
FRAG115 Montufar et al. , 2014	moMtUPara eta ala., 2014
NP116 Montufar et al.	moMtUPara eta ala
ADVP118 et al.	eta ala
,121 ,	,
NP122 2014	2014

----
00383	At this time, deep neural networks outperformed competing AI systems based on other machine learning technologies as well as hand-designed functionality.	 isa samaya , gahana waMwrikA netavarka anya maSIna sIKane kI prOxyogikiyoM Ora sAWa hI hAWa para nirBara kAryakRamawA para AXAriwa eAI praNAliyoM ko bAhara kI ora saMkewa xiyA .		
382	382
S1 At this time , deep neural networks outperformed competing AI systems based on other machine learning technologies as well as hand - designed functionality .	isa samaya gahare waMwrikA netavarka ne anya maSIna sIKane kI wakanIkoM ke AXAra para prawisparXA karane vAle eAI sistama ko behawarIna banAyA Ora sAWa hI
PP2 At this time	isa samaya
NP4 this time	isa bAra
,7 ,	,
NP8 deep neural networks	gahare waMwrikA netavarka
NNS11 networks	netavarka
VP12_LWG outperformed	behawarIna rahI
NP14 competing AI systems	prawisparXA eAI sistama
NNS17 systems	sistama
PP18 based on other machine learning technologies as well as hand - designed functionality	anya maSIna sIKane kI wakanIkoM para AXAriwa Ora sAWa hI hAWa - dijAina kI gaI suviXA
PP20 on other machine learning technologies as well as hand - designed functionality	anya maSIna sIKane kI wakanIkoM para sAWa hI hAWa - dijAina kI gaI suviXA
NP22 other machine learning technologies as well as hand - designed functionality	anya maSIna sIKane kI wakanIkoM ke sAWa-sAWa hAWa - dijAina kI gaI suviXA
NP23 other machine learning technologies	anya maSIna sIKane kI wakanIka
NP24 other machine	anya maSIna
VP27_LWG learning	sIKanA
NP29 technologies	wakanIkoM
NNS30 technologies	wakanIkoM
CONJP31 as well as	sAWa hI
NP35 hand - designed functionality	hAWa- dijAina kI gaI suviXA
ADJP36 hand - designed	hAWa- dijAina

----
00384	This third wave of popularity of neural networks continues to the time of this writing, though the focus of deep learning research has changed dramatically within the time of this wave.	 waMwrikA netavarka kI lokapriyawA kI yaha wIsarI lahara isa leKana ke samaya waka jArI hE , hAlAMki gahare SikRaNa anusaMXAna kA XyAna isa lahara ke samaya ke BIwara nAtakIya rUpa se baxala gayA hE .		
383	383
S1 This third wave of popularity of neural networks continues to the time of this writing , though the focus of deep learning research has changed dramatically within the time of this wave .	isa leKana ke samaya meM waMwrikA netavarka kI lokapriyawA kI yaha wIsarI lahara jArI hE, hAlAMki isa lahara ke samaya meM gaharI sIKa anusaMXAna kA XyAna nAtakIya
NP2 This third wave of popularity of neural networks	waMwrikA netavarka kI lokapriyawA kI yaha wIsarI lahara
NP3 This third wave	yaha wIsarI lahara
PP7 of popularity of neural networks	waMwrikA netavarka kI lokapriyawA kA
NP9 popularity of neural networks	waMwrikA netavarka kI lokapriyawA
NP10 popularity	lokapriyawA
PP12 of neural networks	waMwrikA netavarka ke
NP14 neural networks	waMwrikA netavarka
NNS16 networks	netavarka
VP17_LWG continues	jArI
PP19 to the time of this writing	isa leKana ke samaya waka
NP21 the time of this writing	isa leKana kA samaya
NP22 the time	samaya
PP25 of this writing	isa leKana kA
NP27 this writing	yaha leKana
,30 ,	,
SBAR31 though the focus of deep learning research has changed dramatically within the time of this wave	hAlAMki isa lahara ke samaya meM gaharI sIKa anusaMXAna kA XyAna nAtakIya baxala gayA hE
S33 the focus of deep learning research has changed dramatically within the time of this wave	isa lahara ke samaya meM gaharI sIKa anusaMXAna kA XyAna nAtakIya baxala gayA hE
NP34 the focus of deep learning research	gahana sIKa anusaMXAna kA XyAna
NP35 the focus	XyAna
PP38 of deep learning research	gahana sIKa anusaMXAna kI
NP40 deep learning research	gahana sIKa anusaMXAna
NML41 deep learning	gaharI sIKa
VP45_LWG has changed dramatically	nAtakIya DaMga se baxala gayA hE
ADVP49 dramatically	nAtakIya DaMga se
PP51 within the time of this wave	isa lahara ke samaya ke BIwara
NP53 the time of this wave	isa lahara kA samaya
NP54 the time	samaya
PP57 of this wave	isa lahara kI
NP59 this wave	yaha lahara

----
00385	The third wave began with a focus on new unsupervised learning techniques and the ability of deep models to generalize well from small datasets, but today there is more interest in much older supervised learning algorithms and the ability of deep models to leverage large labeled datasets.	 wIsarI lahara kI SuruAwa naI anupayukwa aXigama wakanIkoM Ora Cote detAsetoM se acCI waraha sAmAnya karane ke lie gahare moYdaloM kI kRamawA para XyAna keMxriwa karane ke sAWa huI , lekina Aja bahuwa purAne paryavekRiwa aXigama elgorixama meM aXika ruci hE Ora badZe lebala vAle detAseta karane ke lie gahare moYdaloM kI kRamawA hE .		
384	384
S1 The third wave began with a focus on new unsupervised learning techniques and the ability of deep models to generalize well from small datasets , but today there is more interest in much older supervised learning algorithms and the ability of deep models to leverage large labeled datasets .	wIsarI lahara kI SuruAwa naI aprawyASiwa SikRaNa wakanIkoM para XyAna xekara huI Ora Cote-Cote AMkadZe se acCI waraha se vyApaka hone kI kRama
S2 The third wave began with a focus on new unsupervised learning techniques and the ability of deep models to generalize well from small datasets	wIsarI lahara kI SuruAwa naI aprawyASiwa SikRaNa wakanIkoM para XyAna xene ke sAWa huI Ora Cote-Cote AMkadZe se acCI waraha se vEcArika kara
NP3 The third wave	wIsarI lahara
VP7_LWG began	SurU huI SuruAwa
PP9 with a focus on new unsupervised learning techniques and the ability of deep models	naI aprawyASiwa SikRaNa wakanIkoM Ora gahare moYdala kI kRamawA para XyAna xene ke sAWa
NP11 a focus on new unsupervised learning techniques and the ability of deep models	naI aprawyASiwa SikRaNa wakanIkoM para XyAna Ora gahare moYdala kI kRamawA para XyAna
NP12 a focus on new unsupervised learning techniques	naI aprawyASiwa sIKa wakanIkoM para XyAna
NP13 a focus	eka XyAna
PP16 on new unsupervised learning techniques	naI aprawyASiwa sIKa wakanIkoM para
NP18 new unsupervised learning techniques	naI aprawyASiwa sIKa wakanIkI
NNS22 techniques	wakanIkoM
CC23 and	Ora
NP24 the ability of deep models	gahare moYdala kI kRamawA
NP25 the ability	kRamawA
PP28 of deep models	gahare moYdala ke
NP30 deep models	gahare moYdala
NNS32 models	moYdala
S33 to generalize well from small datasets	Cote-Cote detAbeta se acCI waraha se sAmAnya karane ke lie
VP34_LWG to generalize well	acCI waraha se sAmAnyakaraNa karane ke lie
ADVP38 well	acCI waraha se
PP40 from small datasets	Cote-Cote detAbeta se
NP42 small datasets	Cote-Cote detAbeta
NNS44 datasets	detAbesa
,45 ,	,
CC46 but	lekina
S47 today there is more interest in much older supervised learning algorithms and the ability of deep models to leverage large labeled datasets	Aja bahuwa purAne nirIkRaNiwa SikRaNa elgorixama meM aXika xilacaspI hE Ora badZe lebala vAle detAbesa ko lIvareja karane ke lie gahare moYdala kI kRama
NP-TMP48 today	Aja
NP50 there	vahAM
VP52_LWG is	hE
NP54 more interest in much older supervised learning algorithms and the ability of deep models	bahuwa purAne nirIkRaNiwa SikRaNa elgorixama Ora gahare moYdala kI kRamawA meM aXika xilacaspI
NP55 more interest in much older supervised learning algorithms	bahuwa purAne meM aXika xilacaspI sIKane vAle elgorixama meM
NP56 more interest	aXika byAja
PP59 in much older supervised learning algorithms	bahuwa purAne meM nirIkRaNiwa sIKa elgorixama
NP61 much older supervised learning algorithms	bahuwa purAne ne sIKa elgorixama kI nigarAnI kI
ADJP62 much older	bahuwa badZA
NNS67 algorithms	elgorixama
CC68 and	Ora
NP69 the ability of deep models	gahare moYdala kI kRamawA
NP70 the ability	kRamawA
PP73 of deep models	gahare moYdala ke
NP75 deep models	gahare moYdala
NNS77 models	moYdala
S78 to leverage large labeled datasets	badZe lebala vAle detAbesa ko lIvareja karane ke lie
VP79_LWG to leverage	lABa uTAne ke lie
NP83 large labeled datasets	badZe lebala kie gae detAbesa
NNS86 datasets	detAbesa

----
00386	1.2.2	 1 . 2		
385	385
FRAG1 1.2.2	Apply
NP2 1.2.2	Apply

----
00387	Increasing Dataset Sizes	 baDZawe dAtAseta AkAra		
386	386
FRAG1 Increasing Dataset Sizes	baDZawI detAseta ke AkAra
NP2 Increasing Dataset	baDZawI detAseta
NP5 Sizes	AkAra

----
00388	One may wonder why deep learning has only recently become recognized as a crucial technology even though the rst experiments with articial neural networks were conducted in the 1950s.	 AScarya ho sakawA hE ki hAla hI meM gahana aXyayana ko eka mahawvapUrNa prOxyogikI ke rUpa meM mAnyawA kyoM xI gaI hE , hAlAMki kqwrima waMwrikA netavarkoM ke sAWa praWama prayoga 1950 ke xaSaka meM kie gae We 		
387	387
S1 One may wonder why deep learning has only recently become recognized as a crucial technology even though the rst experiments with articial neural networks were conducted in the 1950s .	eka AScarya ho sakawA hE ki gaharI sIKa kevala hAla hI meM mahawvapUrNa wakanIka ke rUpa meM mAnya ho gaI hE, Bale hI 1950 ke xaSaka meM kqwrima waMwrikA netavarka
NP2 One	eka
VP4_LWG may wonder	AScarya ho sakawA hE
SBAR8 why deep learning has only recently become recognized as a crucial technology even though the rst experiments with articial neural networks were conducted in the 1950s	kyoM gaharI sIKa hAla hI meM mahawvapUrNa wakanIka ke rUpa meM mAnya ho gaI hE Bale hI 1950 ke xaSaka meM kqwrima waMwrikA netavarka ke sAWa pahale prayoga kie ga
WHADVP9 why	kyoM
S11 deep learning has only recently become recognized as a crucial technology even though the rst experiments with articial neural networks were conducted in the 1950s	gaharI sIKa hAla hI meM mahawvapUrNa wakanIka ke rUpa meM mAnya ho gaI hE Bale hI 1950 ke xaSaka meM kqwrima waMwrikA netavarka ke sAWa pahale prayoga kie gae
NP12 deep learning	gaharI sIKa
VP15_LWG has only recently become recognized	hAla hI meM mAnyawA prApwa huI hE
ADVP17 only	kevala
ADVP20 recently	hAla hI meM
PP25 as a crucial technology	eka mahawvapUrNa wakanIka ke rUpa meM
NP27 a crucial technology	eka mahawvapUrNa wakanIka
SBAR31 even though the rst experiments with articial neural networks were conducted in the 1950s	Bale hI 1950 ke xaSaka meM kqwrima waMwrikA netavarka ke sAWa pahale prayoga kie gae
S34 the rst experiments with articial neural networks were conducted in the 1950s	kqwrima waMwrikA netavarka ke sAWa pahale prayoga 1950 ke xaSaka meM kie gae
NP35 the rst experiments with articial neural networks	kqwrima waMwrikA netavarka ke sAWa pahalA prayoga
NP36 the rst experiments	pahale prayoga
NNS39 experiments	prayoga
PP40 with articial neural networks	kqwrima waMwrikA netavarka ke sAWa
NP42 articial neural networks	kqwrima waMwrikA netavarka
NNS45 networks	netavarka
VP46_LWG were conducted	Ayojiwa kiyA gayA
PP50 in the 1950s	1950 ke xaSaka meM
NP52 the 1950s	1950 ke xaSaka meM
NNS54 1950s	1950 ke xaSaka meM

----
00389	Deep learning has been successfully used in commercial applications since the 1990s but was often regarded as being more of an art than a technology and something that only an expert could use, until recently.	 gahana aXyayana saPalawApUrvaka 1990 ke xaSaka ke bAxa se vANijyika anuprayogoM meM iswemAla kiyA gayA hE , lekina aksara eka prOxyogikI Ora kuCa hE ki kevala eka viSeRajFa kA upayoga kara sakawA hE kI wulanA meM eka kalA ke aXika mAnA jAwA WA , hAla hI waka .		
388	388
S1 Deep learning has been successfully used in commercial applications since the 1990s but was often regarded as being more of an art than a technology and something that only an expert could use , until recently .	1990 ke xaSaka se vANijyika AvexanoM meM gaharI sIKa kA saPala iswemAla howA rahA hE lekina aksara eka wakanIka kI wulanA meM aXika kalA Ora kuCa EsA jisakA upayoga sirPa
NP2 Deep learning	gaharI sIKa
VP5_LWG has been successfully used was often regarded	saPalawApUrvaka iswemAla kiyA jAwA hE aksara mAnA jAwA WA
ADVP10 successfully	saPalawApUrvaka
PP14 in commercial applications	vANijyika AvexanoM meM
NP16 commercial applications	vANijyika Avexana
NNS18 applications	Avexana
PP19 since the 1990s	1990 ke xaSaka se
NP21 the 1990s	1990 ke xaSaka
NNS23 1990s	1990 ke xaSaka
CC24 but	lekina
VP6 has been successfully used in commercial applications since the 1990s	1990 ke xaSaka se vANijyika AvexanoM meM saPalawApUrvaka iswemAla kiyA gayA hE
VP25 was often regarded as being more of an art than a technology and something that only an expert could use , until recently	aksara eka wakanIka kI wulanA meM eka kalA se aXika hone Ora kuCa EsA jisakA upayoga kevala eka viSeRajFa kara sakawA hE, hAla waka
ADVP28 often	aksara
PP31 as being more of an art than a technology and something that only an expert could use	eka wakanIka kI wulanA meM eka kalA se aXika hone Ora kuCa EsA jisakA upayoga kevala eka viSeRajFa kara sakawA hE
S33 being more of an art than a technology and something that only an expert could use	eka wakanIka kI wulanA meM eka kalA se aXika honA Ora kuCa EsA jisakA upayoga sirPa eka viSeRajFa kara sakawA hE
VP34_LWG being	hone ke nAwe
ADJP36 more of an art than a technology and something that only an expert could use	eka wakanIka kI wulanA meM eka kalA kA aXika Ora kuCa EsA jisakA upayoga kevala eka viSeRajFa kara sakawA hE
ADJP37 more of an art than a technology and something	eka wakanIka se jyAxA kalA Ora kuCa Ora
PP39 of an art than a technology and something	eka wakanIka kI wulanA meM eka kalA Ora kuCa Ora
NP41 an art than a technology and something	eka wakanIka kI wulanA meM eka kalA Ora kuCa Ora
NP42 an art	eka kalA
PP45 than a technology and something	eka wakanIka kI wulanA meM Ora kuCa Ora
NP47 a technology and something	eka wakanIka Ora kuCa
CC50 and	Ora
NN49 technology	wakanIka
NN51 something	kuCa
SBAR52 that only an expert could use	ki sirPa eka viSeRajFa iswemAla kara sakawA hE
S54 only an expert could use	kevala eka viSeRajFa iswemAla kara sakawA WA
NP55 only an expert	kevala eka viSeRajFa
VP59_LWG could use	iswemAla kara sakawe hEM
,63 ,	,
SBAR64 until recently	hAla hI meM waka
FRAG66 recently	hAla hI meM
ADVP67 recently	hAla hI meM

----
00390	It is true that some skill is required to get good performance from a deep learning algorithm.	 yaha saca hE ki eka gaharI sIKane elgoriWma se acCA praxarSana prApwa karane ke lie kuCa kOSala kI AvaSyakawA hE .		
389	389
S1 It is true that some skill is required to get good performance from a deep learning algorithm .	yaha sawya hE ki gaharI sIKa elgorixama se acCA praxarSana pAne ke lie kuCa kOSala kI AvaSyakawA howI hE
NP2 It	yaha
VP4_LWG is	hE
ADJP6 true	saca hE
SBAR8 that some skill is required to get good performance from a deep learning algorithm	ki gaharI sIKa elgorixama se acCA praxarSana pAne ke lie kuCa kOSala kI jarUrawa hE
S10 some skill is required to get good performance from a deep learning algorithm	gaharI sIKa elgorixama se acCA praxarSana pAne ke lie kuCa kOSala kI AvaSyakawA
NP11 some skill	kuCa kOSala
VP14_LWG is required	jarUrawa hE
S18 to get good performance from a deep learning algorithm	gaharI sIKa elgorixama se acCA praxarSana pAne ke lie
VP19_LWG to get	prApwa karane ke lie
NP23 good performance	acCA praxarSana
PP26 from a deep learning algorithm	eka gaharI sIKa elgorixama se
NP28 a deep learning algorithm	eka gaharI sIKa elgorixama
NML30 deep learning	gaharI sIKa

----
0039	It therefore asked whether Fred was still a person while he was shaving.	 isalie yaha pUCA gayA ki kyA Preda aBI BI eka vyakwi WA jabaki vaha Seva kara rahA WA .		
38	38
S1 It therefore asked whether Fred was still a person while he was shaving .	isalie pUCA gayA ki kyA Preda aBI BI eka vyakwi WA jaba vaha SeviMga kara rahA WA
NP2 It	yaha
ADVP4 therefore	isalie
VP6_LWG asked	pUCA
SBAR8 whether Fred was still a person while he was shaving	cAhe Preda aBI BI SeviMga kara rahe We, jabaki vaha SeviMga kara rahe We
S10 Fred was still a person while he was shaving	Preda aBI BI eka vyakwi WA jaba vaha SeviMga kara rahA WA
NP11 Fred	Preda
VP13_LWG was still	aBI BI WA
ADVP15 still	aBI BI
NP17 a person	eka vyakwi
SBAR20 while he was shaving	jabaki vo SeviMga kara rahe We
S22 he was shaving	vo SeviMga kara rahe We
NP23 he	vaha
VP25_LWG was	WA
NP27 shaving	SeviMga

----
00391	Fortunately, the amount of skill required reduces as the amount of training data increases.	 sOBAgya se , AvaSyaka kOSala kI mAwrA kama ho jAwI hE kyoMki praSikRaNa detA kI mAwrA baDZa jAwI hE .		
390	390
S1 Fortunately , the amount of skill required reduces as the amount of training data increases .	sOBAgya se praSikRaNa AMkadZe kI rASi baDZane ke sAWa hI kOSala kI rASi kama ho jAwI hE
ADVP2 Fortunately	sOBAgya se
,4 ,	,
NP5 the amount of skill required	kOSala kI rASi AvaSyaka
NP6 the amount	rASi
PP9 of skill required	kOSala kI AvaSyakawA
NP11 skill required	kOSala AvaSyaka
NP12 skill	kOSala
VP14_LWG required	AvaSyaka
VP16_LWG reduces	kama howA hE
SBAR18 as the amount of training data increases	jEse praSikRaNa ke AMkadZoM kI mAwrA baDZI
S20 the amount of training data increases	praSikRaNa AMkadZe kI mAwrA baDZI
NP21 the amount of training data	praSikRaNa AMkadZe kI rASi
NP22 the amount	rASi
PP25 of training data	praSikRaNa ke AMkadZe
NP27 training data	praSikRaNa kA AMkadZA
NNS29 data	detA
VP30_LWG increases	baDZawA jAwA hE

----
00392	The learning algorithms reaching human performance on complex tasks today are nearly identical to the learning algorithms that struggled to solve toy problems in the 1980s, though the models we train with these algorithms have 18 CHAPTER 1.	 Aja jatila kAryoM para mAnava praxarSana waka pahuMcane vAlI aXigama elgorixama , 1980 ke xaSaka meM KilOnA samasyAoM ko hala karane ke lie saMGarRa karane vAlI aXigama elgorixama ke lagaBaga samAna hE , hAlAMki ina elgorixama ke sAWa hama praSikRiwa moYdaloM meM 18 CHAPTER 1 hE .		
391	391
S1 The learning algorithms reaching human performance on complex tasks today are nearly identical to the learning algorithms that struggled to solve toy problems in the 1980s , though the models we train with these algorithms have 18 CHAPTER 1 .	jatila kAryoM para Aja mAnavIya praxarSana waka pahuMcane vAle sIKa elgorixama usa sIKa elgorixama se lagaBaga samAna hEM jo 1980 ke xaSaka meM KilOne kI sama
NP2 The learning algorithms reaching human performance on complex tasks today	jatila kAryoM para mAnavIya praxarSana waka pahuMca rahe sIKa elgorixama Aja
NP3 The learning algorithms	sIKane vAle elgorixama
NNS6 algorithms	elgorixama
VP7_LWG reaching	pahuMca rahA hE
NP9 human performance	mAnavIya praxarSana
PP12 on complex tasks	jatila kAryoM para
NP14 complex tasks	jatila kArya
NNS16 tasks	kAryoM
NP-TMP17 today	Aja
VP19_LWG are	hEM
ADJP21 nearly identical to the learning algorithms that struggled to solve toy problems in the 1980s , though the models we train with these algorithms have 18 CHAPTER 1	1980 ke xaSaka meM KilOnoM kI samasyAoM ke samAXAna ke lie saMGarRa karane vAle sIKa elgorixama se lagaBaga samAna, hAlAMki hama ina elgorixama ke sAWa praSikRa
PP24 to the learning algorithms that struggled to solve toy problems in the 1980s , though the models we train with these algorithms have 18 CHAPTER 1	1980 ke xaSaka meM KilOnoM kI samasyAoM ke samAXAna ke lie saMGarRa karane vAle sIKa elgorixama ko, hAlAMki hama ina elgorixama ke sAWa praSikRiwa karawe hEM 18
NP26 the learning algorithms that struggled to solve toy problems in the 1980s , though the models we train with these algorithms have 18 CHAPTER 1	1980 ke xaSaka meM KilOnoM kI samasyAoM ke samAXAna ke lie saMGarRa karane vAle sIKa elgorixama, hAlAMki hama ina elgorixama ke sAWa praSikRiwa karane vAle ma
NP27 the learning algorithms	sIKane ke elgorixama
NNS30 algorithms	elgorixama
SBAR31 that struggled to solve toy problems in the 1980s , though the models we train with these algorithms have 18 CHAPTER 1	vaha 1980 ke xaSaka meM KilOnoM kI samasyAoM ke samAXAna ke lie saMGarRa karawA WA, hAlAMki hama ina elgorixama ke sAWa praSikRiwa moYdaloM meM 18 cEptara 1
WHNP32 that	vaha
S34 struggled to solve toy problems in the 1980s , though the models we train with these algorithms have 18 CHAPTER 1	1980 ke xaSaka meM KilOnoM kI samasyAoM ke samAXAna ke lie saMGarRa karanA, hAlAMki hama ina elgorixama ke sAWa praSikRiwa moYdaloM meM 18 aXyAya 1
VP35_LWG struggled	saMGarRa kiyA
S37 to solve toy problems in the 1980s , though the models we train with these algorithms have 18 CHAPTER 1	1980 ke xaSaka meM KilOnoM kI samasyAoM ke samAXAna ke lie, hAlAMki hama ina elgorixama ke sAWa praSikRiwa moYdaloM meM 18 aXyAya 1 hE
VP38_LWG to solve	hala karane ke lie
NP42 toy problems	KilOnoM kI samasyA
NNS44 problems	samasyAeM
PP45 in the 1980s	1980 ke xaSaka meM
NP47 the 1980s	1980 ke xaSaka
NNS49 1980s	1980 ke xaSaka meM
,50 ,	,
SBAR51 though the models we train with these algorithms have 18 CHAPTER 1	hAlAMki ina elgorixama ke sAWa hama jina moYdaloM ko praSikRiwa karawe hEM unameM 18 cEptara 1 hE
S53 the models we train with these algorithms have 18 CHAPTER 1	ina elgorixama ke sAWa hama jina moYdaloM ko praSikRiwa karawe hEM unameM 18 ceptara 1 hE
NP54 the models we train with these algorithms	ina elgorixama ke sAWa hama jina moYdaloM ko praSikRiwa karawe hEM
NP55 the models	moYdala
NNS57 models	moYdala
SBAR58 we train with these algorithms	hama ina elgorixama ke sAWa praSikRaNa
S59 we train with these algorithms	hama ina elgorixama ke sAWa praSikRaNa
NP60 we	hama
VP62_LWG train	trena
PP64 with these algorithms	ina elgorixama ke sAWa
NP66 these algorithms	ye elgorixama
NNS68 algorithms	elgorixama
VP69_LWG have	pAsa hE
NP71 18 CHAPTER 1	18 cEptara 1
NP72 18 CHAPTER	18 cEptara
NP75 1	1

----
00393	INTRODUCTION undergone changes that simplify the training of very deep architectures.	 INTRODUCTION meM Ese baxalAva kie gae jo bahuwa gahare vAswuSilpoM ke praSikRaNa ko sarala banAwe hEM 		
392	392
S1 INTRODUCTION undergone changes that simplify the training of very deep architectures .	paricaya meM Ese baxalAva hue jo bahuwa gaharI vAswukalAoM ke praSikRaNa ko AsAna banAwe hEM
NP2 INTRODUCTION undergone	paricaya calA gayA
VP5_LWG changes	parivarwana
SBAR7 that simplify the training of very deep architectures	yaha bahuwa gaharI vAswukAroM ke praSikRaNa ko AsAna banAwA hE
WHNP8 that	vaha
S10 simplify the training of very deep architectures	bahuwa gaharI vAswukAroM ke praSikRaNa ko saralIkqwa kareM
VP11_LWG simplify	saralIkqwa kareM
NP13 the training of very deep architectures	bahuwa gaharI vAswukAroM kA praSikRaNa
NP14 the training	praSikRaNa
PP17 of very deep architectures	bahuwa gaharI vAswukAroM kI
NP19 very deep architectures	bahuwa gaharI vAswueM
ADJP20 very deep	bahuwa gaharA
NNS23 architectures	vAswukAroM

----
00394	The most important new development is that today we can provide these algorithms with the resources they need to succeed.	 sabase mahawvapUrNa nayA vikAsa hE ki Aja hama ina elgorixama ve saPala hone kI jarUrawa saMsAXanoM ke sAWa praxAna kara sakawe hEM .		
393	393
S1 The most important new development is that today we can provide these algorithms with the resources they need to succeed .	sabase mahawvapUrNa nayA vikAsa yaha hE ki Aja hama ina elgorixama ko una saMsAXanoM ke sAWa upalabXa karavA sakawe hEM, jinakI unheM saPala hone kI jarUrawa hE
NP2 The most important new development	sabase mahawvapUrNa nayA vikAsa
ADJP4 most important	sabase mahawvapUrNa
VP9_LWG is	hE
SBAR11 that today we can provide these algorithms with the resources they need to succeed	ki Aja hama ina elgorixama ko una saMsAXanoM ke sAWa upalabXa karavA sakawe hEM jinheM saPala hone kI jarUrawa hE
S13 today we can provide these algorithms with the resources they need to succeed	Aja hama ina elgorixama ko una saMsAXanoM ke sAWa upalabXa karavA sakawe hEM jinheM saPala hone kI jarUrawa hE
NP-TMP14 today	Aja
NP16 we	hama
VP18_LWG can provide	upalabXa karavA sakawe hEM
NP22 these algorithms	ye elgorixama
NNS24 algorithms	elgorixama
PP25 with the resources they need to succeed	saMsAXanoM ke sAWa unheM saPala hone kI jarUrawa hE
NP27 the resources they need to succeed	jina saMsAXanoM kI unheM saPalawA kI jarUrawa hE
NP28 the resources	saMsAXana
NNS30 resources	saMsAXana
SBAR31 they need to succeed	unheM saPala hone kI jarUrawa hE
S32 they need to succeed	unheM saPala hone kI jarUrawa hE
NP33 they	ve
VP35_LWG need	jarUrawa
S37 to succeed	saPala hone ke lie
VP38_LWG to succeed	saPala hone ke lie

----
00395	Figure 1.8 shows how the size of benchmark datasets has expanded remarkably over time.	 ciwra 1 . 8 se pawA calawA hE ki kEse beMcamArka detAseta ke AkAra meM samaya ke sAWa ulleKanIya viswAra huA hE .		
394	394
S1 Figure 1.8 shows how the size of benchmark datasets has expanded remarkably over time .	AMkadZA 1.8 se pawA calawA hE ki beMcamArka detAbetsa ke AkAra kA samaya ke sAWa ulleKanIya rUpa se viswAra kEse huA hE
NP2 Figure 1.8	AMkadZA 1.8
VP5_LWG shows	xiKAwA hE So
SBAR7 how the size of benchmark datasets has expanded remarkably over time	kEse samaya ke sAWa beMcamArka detAbetsa kA AkAra ulleKanIya viswAra huA hE
WHADVP8 how	kEse
S10 the size of benchmark datasets has expanded remarkably over time	beMcamArka detAbetsa ke AkAra kA samaya ke sAWa ulleKanIya viswAra huA hE
NP11 the size of benchmark datasets	beMcamArka detAbetsa kA AkAra
NP12 the size	AkAra
PP15 of benchmark datasets	beMcamArka detAbetsa kA
NP17 benchmark datasets	beMcamArka detAbesa
NNS19 datasets	detAbesa
VP20_LWG has expanded remarkably	ulleKanIya viswAra kiyA hE
ADVP24 remarkably	ulleKanIya
PP26 over time	samaya ke sAWa
NP28 time	samaya

----
00396	This trend is driven by the increasing digitization of society.	 yaha pravqwwi samAja ke baDZawe dijitAijeSana se preriwa hE 		
395	395
S1 This trend is driven by the increasing digitization of society .	samAja ke baDZawe dijitalIkaraNa se isa pravqwwi ko preriwa kiyA jAwA hE
NP2 This trend	yaha ruJAna
VP5_LWG is driven	calAyA gayA hE
PP9 by the increasing digitization of society	samAja ke baDZawe dijitalIkaraNa se
NP11 the increasing digitization of society	samAja kA baDZawA dijitalAijeSana
NP12 the increasing digitization	baDZawI dijitalAijeSana
PP16 of society	samAja kI
NP18 society	samAja

----
00397	As more and more of our activities take place on computers, more and more of what we do is recorded.	 jiwanI aXika se aXika hamArI gawiviXiyAM kaMpyUtara para howI hEM , uwanI hI aXika se aXika hama jo karawe hEM , xarja kI jAwI hEM 		
396	396
S1 As more and more of our activities take place on computers , more and more of what we do is recorded .	hamAre xvArA jiwanI aXika se aXika gawiviXiyAM kaMpyUtaroM para howI hEM, usase aXika se aXika hama jo karawe hEM vaha rikoYrda kiyA jAwA hE
SBAR2 As more and more of our activities take place on computers	hamAre jyAxA se jyAxA gawiviXiyAM kaMpyUtaroM para howI hEM
S4 more and more of our activities take place on computers	hamAre jyAxA se jyAxA gawiviXiyAM kaMpyUtaroM para howI hEM
NP5 more and more of our activities	hamAre jyAxA se jyAxA gawiviXiyAM
NP6 more and more	jyAxA se jyAxA
CC8 and	Ora
JJR7 more	jyAxA
JJR9 more	jyAxA
PP10 of our activities	hamArI gawiviXiyoM kA
NP12 our activities	hamArI gawiviXiyAM
NNS14 activities	gawiviXiyAM
VP15_LWG take	le lo
NP17 place	jagaha
PP19 on computers	kaMpyUtaroM para
NP21 computers	kaMpyUtara
NNS22 computers	kaMpyUtara
,23 ,	,
NP24 more and more of what we do	hama jo karawe hEM usase jyAxA se jyAxA
NP25 more and more	jyAxA se jyAxA
CC27 and	Ora
JJR26 more	jyAxA
JJR28 more	jyAxA
PP29 of what we do	hama kyA karawe hEM
SBAR31 what we do	hama kyA karawe hEM
WHNP32 what	kyA
S34 we do	hama karawe hEM
NP35 we	hama
VP37_LWG do	karawe hEM
VP39_LWG is recorded	rikoYrda hE

----
00398	As our computers are increasingly networked together, it becomes easier to centralize these records and curate them into a dataset appropriate for machine learning applications.	 cUMki hamAre kaMpyUtara lagAwAra eka sAWa netavarka kara rahe hEM , isalie ina aBileKoM ko keMxrIkqwa karanA Ora unheM maSIna sIKane ke anuprayogoM ke lie upayukwa detAseta meM TIka karanA AsAna ho jAwA hE 		
397	397
S1 As our computers are increasingly networked together , it becomes easier to centralize these records and curate them into a dataset appropriate for machine learning applications .	jEsA ki hamAre kaMpyUtara wejI se eka sAWa netavarka banAe jAwe hEM, yaha ina rikoYrda ko keMxriwa karanA Ora unheM maSIna sIKane ke AvexanoM ke
SBAR2 As our computers are increasingly networked together	jEsA ki hamAre kaMpyUtara wejI se eka sAWa netavarka kara rahe hEM
S4 our computers are increasingly networked together	hamAre kaMpyUtara wejI se eka sAWa netavarka
NP5 our computers	hamAre kaMpyUtara
NNS7 computers	kaMpyUtara
VP8_LWG are together	eka sAWa hEM
ADJP10 increasingly networked	wejI se netavarka
ADVP13 together	eka sAWa
,15 ,	,
NP16 it	yaha
VP18_LWG becomes	bana jAwA hE
ADJP20 easier to centralize these records and curate them into a dataset appropriate for machine learning applications	ina rikoYrda ko keMxriwa karanA AsAna hE Ora unheM maSIna sIKane ke AvexanoM ke lie upayukwa detAbesa meM katArA
ADJP21 easier	AsAna
S23 to centralize these records and curate them into a dataset appropriate for machine learning applications	ina rikoYrda ko keMxriwa karane ke lie Ora unheM maSIna sIKane ke AvexanoM ke lie upayukwa detAbesa meM pariBARiwa karane ke lie
VP24_LWG to centralize curate	kyUreta ko keMxriwa karane ke lie
NP29 these records	ye rikoYrda
NNS31 records	rikoYrda
CC32 and	Ora
VP27 centralize these records	ina rikoYrdoM ko keMxriwa kareM
VP33 curate them into a dataset appropriate for machine learning applications	maSIna larniMga eplikeSana ke lie unheM upayukwa detAbeta meM kate kareM
NP35 them	unheM
PP37 into a dataset appropriate for machine learning applications	maSIna sIKane ke AvexanoM ke lie upayukwa dAtAbeta meM
NP39 a dataset appropriate for machine learning applications	maSIna sIKane ke AvexanoM ke lie upayukwa eka dAtAbeta
NP40 a dataset	eka detAbesa
ADJP43 appropriate for machine learning applications	maSIna sIKane ke AvexanoM ke lie uciwa
PP45 for machine learning applications	maSIna sIKane ke AvexanoM ke lie
NP47 machine learning applications	maSIna sIKane ke Avexana
NML48 machine learning	maSIna sIKanA
NNS51 applications	Avexana

----
00399	The age of Big Data 1900 1950 1985 2000 2015 Year 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 Dataset size (number examples)	 biga detA 1900 1950 2000 2015 varRa 10 0 10 1 10 2 10 10 3 10 4 10 10 10 10 10 6 7 8 9 detAseta AkAra ( asaMKya uxAharaNa		
398	398
NP1 The age of  Big Data  1900195019852000 2015 Year 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 Dataset size ( number examples )	biga detA 2100 1950 1985 2000 varRa kI umra 10 0 10 1 10 2 10 4 10 5 10 6 10 7 10 8 10 9 detAseta AkAra (naMbara uxAharaNa)
NP2 The age	umra
PP5 of  Big Data  1900195019852000	biga detA 2100 1950 1985 2000
NP7  Big Data  1900195019852000	badZA detA 2100 1950 1985 2000
NML8  Big Data 	badZA detA
NP-TMP14 2015 Year	2015 varRa
NP-TMP17 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 Dataset size	10 0 10 1 10 1 10 4 10 5 10 6 10 7 10 8 10 9 detAseta AkAra
NP18 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9	10 0 10 1 10 2 10 3 10 4 10 6 10 7 10 7 10 8 10 9
NP39 Dataset size	detAseta kA AkAra
NP43 number examples	naMbara uxAharaNa
NNS45 examples	uxAharaNa

----
00400	Iris MNIST	 AIrisa emanEta		
399	399
NP1 Iris MNIST	Airisa manista

----
0040	The diculties faced by systems relying on hard-coded knowledge suggest that AI systems need the ability to acquire their own knowledge, by extracting 2 CHAPTER 1.	 kaTora jFAna para BarosA karane vAlI praNAliyoM ke sAmane Ane vAlI kaTinAiyoM se pawA calawA hE ki eAiesa praNAliyoM ke lie apane svayaM ke jFAna ko prApwa karane kI kRamawA kI jarUrawa hE , 2 aXyAya 1 kA niRkarRa nikAlane ke xvArA .		
39	39
S1 The diculties faced by systems relying on hard - coded knowledge suggest that AI systems need the ability to acquire their own knowledge , by extracting 2 CHAPTER 1 .	kaTina - kodiwa jFAna para nirBara praNAliyoM ke sAmane Ane vAlI kaTinAiyoM se pawA calawA hE ki eAI praNAliyoM ko apanA jFAna hAsila karane kI kRamawA kI AvaSyakawA
NP2 The diculties faced by systems relying on hard - coded knowledge	sistama se ho rahI muSkileM kadZI mehanawa se - kodiwa jFAna
NP3 The diculties	muSkileM
NNS5 diculties	muSkileM
VP6_LWG faced	sAmanA kiyA
PP8 by systems relying on hard - coded knowledge	sistama kadZI mehanawa para nirBara - kodiwa jFAna
NP10 systems relying on hard - coded knowledge	kadZI mehanawa para nirBara sistama - kodiwa jFAna
NP11 systems	sistama
NNS12 systems	sistama
VP13_LWG relying	JUTa bolanA
PP15 on hard - coded knowledge	kadZI mehanawa para- kodiwa jFAna
NP17 hard - coded knowledge	kaTina - kodiwa jFAna
NML18 hard - coded	kaTina - kodiwa
VP23_LWG suggest	suJAva
SBAR25 that AI systems need the ability to acquire their own knowledge , by extracting 2 CHAPTER 1	eAI praNAliyoM ko apanA jFAna hAsila karane kI kRamawA kI AvaSyakawA howI hE, 2 aXyAya 1 ko nikAlakara
S27 AI systems need the ability to acquire their own knowledge , by extracting 2 CHAPTER 1	eAI praNAliyoM ko apanA jFAna hAsila karane kI kRamawA kI AvaSyakawA howI hE, 2 aXyAya 1 ko nikAlakara
NP28 AI systems	eAI sistama
NNS30 systems	sistama
VP31_LWG need	jarUrawa
NP33 the ability to acquire their own knowledge	apanA jFAna hAsila karane kI kRamawA
S36 to acquire their own knowledge	apanA jFAna hAsila karane ke lie
VP37_LWG to acquire	aXigrahaNa ke lie
NP41 their own knowledge	unakA apanA jFAna
,45 ,	,
PP46 by extracting 2 CHAPTER 1	2 cEptara 1 nikAlakara
S48 extracting 2 CHAPTER 1	2 cEptara 1 nikAlanA
VP49_LWG extracting	nikAlanA
NP51 2 CHAPTER 1	2 cEptara 1
NML53 CHAPTER 1	cEptara 1

----
00401	Public SVHN ImageNet	 sArvajanika SVHN CaviNet		
400	400
NP1 Public SVHN ImageNet	sArvajanika esavIecaena imejaneta

----
00402	CIFAR-10	 CIFAR - 10		
401	401
FRAG1 CIFAR - 10	CIFAR- 10
NP2 CIFAR	CIFAR
NP5 10	10

----
00403	ImageNet10k ILSVRC 2014 Sports-1	 imejanata10k ILSVRC 2014 sportsa - 1		
402	402
FRAG1 ImageNet10k ILSVRC 2014 Sports - 1	ImageNET10k IRgoraC 2014 Kela - 1
NP2 ImageNet10k ILSVRC	AidiyAneta10ke AIelaesavIArasI
NP5 2014	2014
NP7 Sports	Kela-Kela
NP10 1	1

----
00404	M Rotated T vs. C T vs. G vs. F Criminals Canadian Hansard WMT Figure 1.8:
###	
403	403
FRAG1 M Rotated T vs. C T vs. G vs. F Criminals Canadian Hansard WMT Figure 1.8 :	ema roteteda tI banAma sI tI banAma jI banAma ePa aparAXI kanAdAI haMsArda dablyUematI kA AMkadZA 1.8
NP2 M Rotated T	ema roteteda tI
PP6 vs. C T vs. G vs. F Criminals Canadian Hansard WMT	banAma sI tI banAma jI banAma ePa aparAXI kanAdAI haMsArda dablyUematI
NP8 C T vs. G vs. F Criminals Canadian Hansard WMT	sI tI banAma jI banAma ePa aparAXI kanAdAI haMsArda dablyUematI
NML9 C T vs. G vs. F Criminals Canadian	sI tI banAma jI banAma ePa aparAXI kanAdAI
CC12 vs.	banAma
NNP11 T	tI
NNP13 G	jI.
CC14 vs.	banAma
NNP15 F	ePa
NP20 Figure 1.8	AMkadZA 1.8

----
00405	Increasing dataset size over time.	 samaya ke sAWa baDZawA detAseta AkAra .		
404	404
NP1 Increasing dataset size over time .	samaya ke sAWa mOjUxA detAbesa kA AkAra baDZa rahA hE
NP2 Increasing dataset size	baDZawI detAbesa kA AkAra
PP6 over time	samaya ke sAWa
NP8 time	samaya

----
00406	In the early 1900s, statisticians studied datasets using hundreds or thousands of manually compiled measurements (Garson, 1900; Gosset, 1908; Anderson, 1935; Fisher, 1936).	 mEnyuala rUpa se saMkaliwa AMkadZoM ke sEkadZoM ke upayoga kA prayoga karawe hue sAMKyikIvixoM ne 1900 ke AraMBika AMkadZoM kA aXyayana kiyA ( gasarasana yA 1900 ke prAraMBika AMkadZe mApane vAle hajAroM gosEta 1908 , mawsya eMdarasana , 1935		
405	405
S1 In the early 1900s , statisticians studied datasets using hundreds or thousands of manually compiled measurements ( Garson , 1900 ; Gosset , 1908 ; Anderson , 1935 ; Fisher , 1936 ) .	prAraMBika 1950 ke xaSaka meM, sAMKyika logoM ne sEkadZoM yA hajAroM kA prayoga karake detAbesa kA aXyayana kiyA jo mEnuala rUpa se saMkaliwa mApa (gArsana,
PP2 In the early 1900s	prAraMBika 1950 ke xaSaka meM
NP4 the early 1900s	prAraMBika 1400 ke xaSaka kI SuruAwa
NNS7 1900s	1400 ke xaSaka
,8 ,	,
NP9 statisticians	sAMKyikAoM
NNS10 statisticians	sAMKyikAoM
VP11_LWG studied	paDZA-liKA
NP13 datasets	detAbesa
NNS14 datasets	detAbesa
S15 using hundreds or thousands of manually compiled measurements ( Garson , 1900 ; Gosset , 1908 ; Anderson , 1935 ; Fisher , 1936 )	sEkadZoM yA hajAroM kA upayoga karake mEnuala rUpa se saMkaliwa mApa (gArsana, 2100; goseta,; eMdarasana, 1936 ; PiSara, 1936 ; PiSara, 1936 )
VP16_LWG using	iswemAla karanA
NP18 hundreds or thousands of manually compiled measurements ( Garson , 1900 ; Gosset , 1908 ; Anderson , 1935 ; Fisher , 1936 )	sEkadZoM yA hajAroM mEnuala rUpa se saMkaliwa mApa (gArsana, 2100 ; goseta, 1907 ; eMdarasana, 1936 ; PiSara, 1936 ; PiSara, 1936 )
NP19 hundreds or thousands	sEkadZoM yA hajAroM
QP20 hundreds or thousands	sEkadZoM yA hajAroM
NNS21 hundreds	sEkadZoM
CC22 or	yA
NNS23 thousands	hajAroM
PP24 of manually compiled measurements ( Garson , 1900 ; Gosset , 1908 ; Anderson , 1935 ; Fisher , 1936 )	mEnuala rUpa se saMkaliwa mApa (gArsana, 2100 ; goseta, 1907 ; eMdarasana, 1936 ; PiSara, 1936 )
NP26 manually compiled measurements ( Garson , 1900 ; Gosset , 1908 ; Anderson , 1935 ; Fisher , 1936 )	mEnuala rUpa se saMkaliwa mApa (gArsana, 2100 ; goseta, 1907 ; eMdarasana, 1936 ; PiSara, 1936 )
ADJP27 manually compiled	mEnuala rUpa se saMkaliwa
NNS30 measurements	mApa
PRN31 ( Garson , 1900 ; Gosset , 1908 ; Anderson , 1935 ; Fisher , 1936 )	(gArsana, 1800 ; goseta, 1907 ; eMdarasana, 1936 ; PiSara, 1936 )
NP33 Garson	gArsana
,35 ,	,
NP36 1900	1400 ke xOrAna
NP39 Gosset	godaseta
,41 ,	,
NP42 1908	1928
NP45 Anderson	eMdarasana
,47 ,	,
NP48 1935	1936
NP51 Fisher	PiSara
,53 ,	,
NP54 1936	1936

----
00407	In the 1950s through the 1980s, the pioneers of biologically inspired machine learning often worked with small synthetic datasets, such as low-resolution bitmaps of letters, that were designed to incur low computational cost and demonstrate that neural networks were able to learn specic kinds of functions (Widrow and Ho, 1960; Rumelhart et al., 1986b).	 1950 ke xaSaka meM , jEvika rUpa se preriwa maSInI SikRaNa ke agraxUwa aksara Cote saMSleRiwa detAseta , jEse nimna saMSleRiwa akRaroM ke bitamEpa ke sAWa kAma karawe We , jo nimna saMgaNanAwmaka lAgawa Ora 1960 ke xaSaka ko praxarSiwa karawe We ki waMwrikA waMwra viSiRta prakAra ke kArya sIKane meM sakRama We (		
406	406
S1 In the 1950s through the 1980s , the pioneers of biologically inspired machine learning often worked with small synthetic datasets , such as low - resolution bitmaps of letters , that were designed to incur low computational cost and demonstrate that neural networks were able to learn specic kinds of functions ( Widrow and Ho , 1960 ; Rumelhart et al. , 1986b ) .	1980 ke xaSaka ke xOra meM jEvika rUpa se preriwa maSIna sIKane vAle pahalavAna aksara Cote siMWetika detAbetsa ke sAWa kAma karawe We, jEse ki pawroM ke kama
PP2 In the 1950s through the 1980s	1950 ke xaSaka meM 1980 ke xaSaka ke xOra meM
NP4 the 1950s through the 1980s	1950 ke xaSaka ke xaSaka meM 1980 ke xaSaka ke xOra
NP5 the 1950s	1950 ke xaSaka meM
NNS7 1950s	1950 ke xaSaka meM
PP8 through the 1980s	1980 ke xaSaka ke xOrAna
NP10 the 1980s	1980 ke xaSaka
NNS12 1980s	1980 ke xaSaka meM
,13 ,	,
NP14 the pioneers of biologically inspired machine learning	AnuvaMSika rUpa se preriwa maSIna sIKane ke agraNI
NP15 the pioneers	agraNI
NNS17 pioneers	agraNI
PP18 of biologically inspired machine learning	jEvika rUpa se preriwa maSIna sIKane kI
NP20 biologically inspired machine learning	AnuvaMSika rUpa se preriwa maSIna sIKa rahI
ADJP21 biologically inspired	AnuvaMSika rUpa se preriwa
ADVP26 often	aksara
VP28_LWG worked	kAma kiyA
PP30 with small synthetic datasets , such as low - resolution bitmaps of letters , that were designed to incur low computational cost and demonstrate that neural networks were able to learn specic kinds of functions ( Widrow and Ho , 1960 ; Rumelhart et al. , 1986b )	Cote-Cote siMWetika detAbesa ke sAWa, jEse ki lo-rijolyUSana bitamEksa, jinheM kama kaMpyUtiSanala koYsta Karca karane ke lie dijAina kiyA
NP32 small synthetic datasets , such as low - resolution bitmaps of letters , that were designed to incur low computational cost and demonstrate that neural networks were able to learn specic kinds of functions ( Widrow and Ho , 1960 ; Rumelhart et al. , 1986b )	Cote-Cote siMWetika detAbesa, jEse ki lo-rijolyUSana bitamEksa, jinheM kama kaMpyUtiSanala koYsta Karca karane ke lie dijAina kiyA gayA WA
NP33 small synthetic datasets	Cote-Cote siMWetika detAbeta
NNS36 datasets	detAbesa
,37 ,	,
PP38 such as low - resolution bitmaps of letters , that were designed to incur low computational cost and demonstrate that neural networks were able to learn specic kinds of functions ( Widrow and Ho , 1960 ; Rumelhart et al. , 1986b )	jEse ki kama - saMkalpiwa pawroM ke saMkalpa bitakule, jinheM kama kaMpyUtarika lAgawa Karca karane ke lie dijZAina kiyA gayA WA Ora yaha praxarSiwa kiyA ga
NP41 low - resolution bitmaps of letters , that were designed to incur low computational cost and demonstrate that neural networks were able to learn specic kinds of functions ( Widrow and Ho , 1960 ; Rumelhart et al. , 1986b )	kama - saMkalpiwa pawroM ke saMkalpa bitakule, jinheM kama kaMpyUtarika lAgawa Karca karane ke lie dijZAina kiyA gayA WA Ora yaha praxarSiwa kiyA gayA ki waMwra
NP42 low - resolution bitmaps of letters	kama - pawroM kA saMkalpa bitanakSa
NP43 low - resolution bitmaps	kama - rijolyUSana bitamEksa
NML44 low - resolution	kama - saMkalpa
NNS48 bitmaps	bitamEksa
PP49 of letters	akRaroM kI
NP51 letters	akRara
NNS52 letters	akRara
,53 ,	,
SBAR54 that were designed to incur low computational cost and demonstrate that neural networks were able to learn specic kinds of functions ( Widrow and Ho , 1960 ; Rumelhart et al. , 1986b )	yaha kama kaMpyUtarika lAgawa Karca karane ke lie dijAina kiyA gayA WA Ora yaha praxarSana karawA WA ki waMwrikA netavarka viSiRta prakAra ke kArya (vidro eMda hoY
WHNP55 that	vaha
S57 were designed to incur low computational cost and demonstrate that neural networks were able to learn specic kinds of functions ( Widrow and Ho , 1960 ; Rumelhart et al. , 1986b )	kama kaMpyUtarika lAgawa Karca karane ke lie dijAina kiyA gayA Ora yaha praxarSiwa kiyA gayA ki waMwrikA netavarka viSiRta prakAra ke kAryoM (vidro eMda hoY
VP58_LWG were designed	We dijAina
S62 to incur low computational cost and demonstrate that neural networks were able to learn specic kinds of functions	kama kaMpyUteSanala koYsta Karca karane ke lie Ora praxarSiwa karane ke lie ki waMwrikA netavarka viSiRta prakAra ke kArya sIKa sakawe We
VP63_LWG to incur demonstrate	praxarSana karane ke lie
NP68 low computational cost	kama kampyUteSanala lAgawa
CC72 and	Ora
VP66 incur low computational cost	kama kaMpyUtiSanala lAgawa Karca
VP73 demonstrate that neural networks were able to learn specic kinds of functions	praxarSana kareM ki waMwrikA netavarka viSiRta prakAra ke kArya sIKa sakawe We
SBAR75 that neural networks were able to learn specic kinds of functions	usa waMwrikA netavarka ko sIKane meM sakRama We viSiRta prakAra ke kArya
S77 neural networks were able to learn specic kinds of functions	viSiRta prakAra ke kArya sIKa sakawe We waMwrikA netavarka
NP78 neural networks	waMwrikA netavarka
NNS80 networks	netavarka
VP81_LWG were	We
ADJP83 able to learn specic kinds of functions	viSiRta prakAra ke kArya sIKane meM sakRama
S85 to learn specic kinds of functions	viSiRta prakAra ke kArya sIKane ke lie
VP86_LWG to learn	sIKane ke lie
NP90 specic kinds of functions	viSiRta prakAra ke kArya
NP91 specic kinds	viSiRta prakAra ke
NNS93 kinds	waraha-waraha kI waraha
PP94 of functions	kAryoM kA
NP96 functions	samAroha
NNS97 functions	samAroha
PRN98 ( Widrow and Ho , 1960 ; Rumelhart et al. , 1986b )	(vidro eMda hoYPa, 1960 ; rumelahArta eta ala., 1986b)
NP100 Widrow and Ho	vidro Ora hoYPa
CC102 and	Ora
NNP101 Widrow	vixro
NNP103 Ho	hoYPa
,104 ,	,
NP105 1960	1960
FRAG108 Rumelhart et al. , 1986b	rUmAlahArta eta ala., 1986b
NP109 Rumelhart et al.	Rumelhart eta ala
ADVP111 et al.	eta ala
,114 ,	,
NP115 1986b	1986b

----
00408	In the 1980s and 1990s, machine learning became more statistical and began to leverage larger datasets containing tens of thousands of examples, such as the MNIST dataset (shown in gure 1.9) of scans of handwritten numbers (LeCun et al., 1998b).	 varRa 1980 Ora 1990 ke xaSaka meM maSInI aXigama aXika sAMKyikIya bana gayA Ora hajAroM uxAharaNoM ke xasiyoM se yukwa badZe detAsetoM ko baDZAnA SurU kiyA , jEse ki MNIST detAseta ( ginawI meM haswaliKiwa saMKyA ke 19 ke AMkadZe ) , 1998		
407	407
S1 In the 1980s and 1990s , machine learning became more statistical and began to leverage larger datasets containing tens of thousands of examples , such as the MNIST dataset ( shown in gure 1.9 ) of scans of handwritten numbers ( LeCun et al. , 1998b ) .	1980 ke xaSaka Ora 1990 ke xaSaka meM maSIna sIKanA aXika sAMKyikIya ho gayA Ora hajAroM uxAharaNoM sahiwa badZe detAbetsa lene kI SuruAwa kI, jEse ki
PP2 In the 1980s and 1990s	1980 ke xaSaka Ora 1990 ke xaSaka meM
NP4 the 1980s and 1990s	1980 ke xaSaka Ora 1990 ke xaSaka
NNS6 1980s	1980 ke xaSaka meM
CC7 and	Ora
NNS8 1990s	1990 ke xaSaka
,9 ,	,
NP10 machine learning	maSIna sIKanA
VP13_LWG became began	banI WI SuruAwa
ADJP16 more statistical	aXika sAMKyikIya
CC19 and	Ora
VP14 became more statistical	aXika sAMKyikIya
VP20 began to leverage larger datasets containing tens of thousands of examples , such as the MNIST dataset ( shown in gure 1.9 ) of scans of handwritten numbers ( LeCun et al. , 1998b )	xasiyoM hajAroM uxAharaNa sahiwa badZe-badZe detAbeta lene kI SuruAwa kI, jEse ki haswaliKiwa saMKyAoM ke skEnoM ke emaenaesatIesa detAbeta (
S22 to leverage larger datasets containing tens of thousands of examples , such as the MNIST dataset ( shown in gure 1.9 ) of scans of handwritten numbers ( LeCun et al. , 1998b )	xasiyoM hajAra uxAharaNa sahiwa badZe-badZe detAbesa lIvareja karane ke lie, jEse ki haswaliKiwa saMKyAoM ke skEna (lekana eta ala, 1998b)
VP23_LWG to leverage	lABa uTAne ke lie
NP27 larger datasets	badZe detAbeta
NNS29 datasets	detAbesa
S30 containing tens of thousands of examples , such as the MNIST dataset ( shown in gure 1.9 ) of scans of handwritten numbers ( LeCun et al. , 1998b )	jinameM xasiyoM hajAra uxAharaNa hEM, jEse ki manista kA detAbesa (haswaliKiwa saMKyAoM ke skEnoM ke AMkadZe 19 meM xiKAyA gayA) (lekana eta ala,
VP31_LWG containing	jisameM
NP33 tens of thousands of examples , such as the MNIST dataset ( shown in gure 1.9 ) of scans of handwritten numbers ( LeCun et al. , 1998b )	hajAroM uxAharaNa jEse, jEse manista dAtAtek ta (haswaliKiwa saMKyA ke skEna (lekana eta ala, 1998 bI)
NP34 tens of thousands	hajAroM kI saMKyA
QP35 tens of thousands	hajAroM kI saMKyA
NNS36 tens	xasiyoM
NNS38 thousands	hajAroM
PP39 of examples	uxAharaNoM kI
NP41 examples	uxAharaNa
NNS42 examples	uxAharaNa
,43 ,	,
PP44 such as the MNIST dataset ( shown in gure 1.9 ) of scans of handwritten numbers ( LeCun et al. , 1998b )	jEse ki manista detAbesa (haswaliKiwa saMKyA ke skEna (lekana eta ala., 1998b)
NP47 the MNIST dataset ( shown in gure 1.9 ) of scans of handwritten numbers ( LeCun et al. , 1998b )	manista dAtAbesa (haswaliKiwa saMKyA ke skEnoM ke AMkadZe 19 meM xiKAyA gayA) (lekana eta ala., 1998b)
NP48 the MNIST dataset ( shown in gure 1.9 )	manista kA detAbesa (AMkadZA 2.1 meM xiKAyA gayA)
NP49 the MNIST dataset	manista kA detAbesa
VP54_LWG shown	xiKAyA gayA
PP56 in gure 1.9	AMkadZe 1.4 meM
NP58 gure 1.9	AMkadZA 1.4
PP62 of scans of handwritten numbers	hAWa liKiwa naMbaroM ke skEna kA
NP64 scans of handwritten numbers	hAWa liKiwa naMbaroM ke skEna
NP65 scans	skEna
NNS66 scans	skEna
PP67 of handwritten numbers	hAWa liKiwa naMbaroM kI
NP69 handwritten numbers	haswaliKiwa naMbara
NNS71 numbers	naMbara
PRN72 ( LeCun et al. , 1998b )	(lekana eta ala, 1998b)
NP74 LeCun et al. , 1998b	lekana eta ala., 1998b
NP75 LeCun et al.	lekana eta ala
NP76 LeCun	lekana
ADVP78 et al.	eta ala
,81 ,	,
NP82 1998b	1998b
NNS83 1998b	1998b

----
00409	In the rst decade of the 2000s, more sophisticated datasets of this same size, such as the CIFAR-10 dataset (Krizhevsky and Hinton, 2009), continued to be produced.	 2000 ke xaSaka ke pahale xaSaka meM , isa AkAra ke aXika pariRkqwa detAseta , jEse CIFAR - 10 detAseta ( Krizhevsky Ora Hinton , 2009 ) kA uwpAxana jArI rahA .		
408	408
S1 In the rst decade of the 2000s , more sophisticated datasets of this same size , such as the CIFAR - 10 dataset ( Krizhevsky and Hinton , 2009 ) , continued to be produced .	2000 ke pahale xaSaka meM isI AkAra ke aXika pariRkqwa detAbesa, jEse sIAIePaeAra- 10 detAbesa (kriJveskI Ora hiMtana, 2009) kA uwpAxana howA
PP2 In the rst decade of the 2000s	2000 ke pahale xaSaka meM
NP4 the rst decade of the 2000s	2000 ke pahale xaSaka
NP5 the rst decade	pahalA xaSaka
PP9 of the 2000s	2000 ke xaSaka ke
NP11 the 2000s	2000 ke xaSaka
,14 ,	,
NP15 more sophisticated datasets of this same size , such as the CIFAR - 10 dataset ( Krizhevsky and Hinton , 2009 ) ,	isI AkAra ke aXika pariRkqwa detAbesa, jEse ki sIAIePaeAra- 10 detAbesa (kriJveskI Ora hiMtana, 2009),
NP16 more sophisticated datasets	aXika pariRkqwa detAbesa
ADJP17 more sophisticated	aXika pariRkqwa
NNS20 datasets	detAbesa
PP21 of this same size , such as the CIFAR - 10 dataset ( Krizhevsky and Hinton , 2009 ) ,	isI AkAra kI, jEse sIAIePaeAra- 10 detAbesa (kriJveskI Ora hiMtana, 2009),
NP23 this same size , such as the CIFAR - 10 dataset ( Krizhevsky and Hinton , 2009 ) ,	isI AkAra, jEse sIAIePaeAra- 10 detAbesa (kriJveskI Ora hiMtana, 2009),
NP24 this same size	isI AkAra kA
,28 ,	,
PP29 such as the CIFAR - 10 dataset ( Krizhevsky and Hinton , 2009 )	jEse sIAIePaeAra- 10 detAbesa (kriJveskI Ora hiMtana, 2009)
NP32 the CIFAR - 10 dataset ( Krizhevsky and Hinton , 2009 )	sIAIePaeAra- 10 detAbesa (kriJveskI Ora hiMtana, 2009)
NML34 CIFAR - 10	CIFAR- 10
PRN39 ( Krizhevsky and Hinton , 2009 )	(kriJevaskI Ora hiMtana, 2009)
NP41 Krizhevsky and Hinton	kriJevaskI Ora hiMtana
CC43 and	Ora
NNP42 Krizhevsky	kriJevskI
NNP44 Hinton	hiMtana
,45 ,	,
NP46 2009	2009
,49 ,	,
VP50_LWG continued	jArI
S52 to be produced	uwpAxana honA hE
VP53_LWG to be produced	uwpAxana honA hE

----
00410	Toward the end of that decade and throughout the rst half of the 2010s, signicantly larger datasets, containing hundreds of thousands to tens of millions of examples, completely changed what was possible with deep learning.	 usa xaSaka ke aMwa meM Ora 2010 ke xaSaka ke pUrvArXa meM , ulleKanIya rUpa se badZe detAseta , jisameM sEkadZoM hajAroM se xasiyoM lAKa uxAharaNa We , pUrI waraha se baxala gayA jo gaharI sIKane ke sAWa saMBava WA .		
409	409
S1 Toward the end of that decade and throughout the rst half of the 2010s , signicantly larger datasets , containing hundreds of thousands to tens of millions of examples , completely changed what was possible with deep learning .	usa xaSaka ke aMwa kI ora Ora 2010 ke xaSaka ke pUre pahale hAPa meM, kAPI badZe detAbetsa, jinameM sEkadZoM hajAroM se lekara lAKoM uxAharaNa SAmila hEM,
PP2 Toward the end of that decade and throughout the rst half of the 2010s	usa xaSaka ke aMwa kI ora Ora 2010 ke pUre pahale hAPa kI ora
PP3 Toward the end of that decade	usa xaSaka ke aMwa kI ora
NP5 the end of that decade	usa xaSaka kA aMwa
NP6 the end	aMwa
PP9 of that decade	usa xaSaka kI
NP11 that decade	vaha xaSaka
CC14 and	Ora
PP15 throughout the rst half of the 2010s	2010 ke pUre pahale hAPa meM
NP17 the rst half of the 2010s	2010 kA pahalA AXA hissA
NP18 the rst half	pahalA hAPa
PP22 of the 2010s	2010 ke xaSaka ke
NP24 the 2010s	2010 ke xaSaka
,27 ,	,
NP28 signicantly larger datasets , containing hundreds of thousands to tens of millions of examples ,	mahawvapUrNa rUpa se badZe detAbesa, jinameM sEkadZoM hajAra se lekara lAKoM uxAharaNa SAmila hEM,
NP29 signicantly larger datasets	KAsa wOra para badZA detAbeta
ADJP30 signicantly larger	kAPI badZA
NNS33 datasets	detAbesa
,34 ,	,
VP35_LWG containing	jisameM
NP37 hundreds of thousands	sEkadZoM hajAroM
QP38 hundreds of thousands	sEkadZoM hajAroM
NNS39 hundreds	sEkadZoM
NNS41 thousands	hajAroM
PP42 to tens of millions of examples	lAKoM uxAharaNoM waka
NP44 tens of millions of examples	lAKoM uxAharaNa
NP45 tens of millions	lAKoM kI saMKyA
QP46 tens of millions	lAKoM kI saMKyA
NNS47 tens	xasiyoM
NNS49 millions	lAKoM
PP50 of examples	uxAharaNoM kI
NP52 examples	uxAharaNa
NNS53 examples	uxAharaNa
,54 ,	,
ADVP55 completely	pUrI waraha se
VP57_LWG changed	baxala gayA
SBAR59 what was possible with deep learning	gaharI sIKa se kyA huA saMBava
WHNP60 what	kyA
S62 was possible with deep learning	gaharI sIKa se saMBava WA
VP63_LWG was	WA
ADJP65 possible with deep learning	gaharI sIKa se saMBava
PP67 with deep learning	gaharI sIKa ke sAWa
NP69 deep learning	gaharI sIKa

----
0041	INTRODUCTION patterns from raw data.	 kacce detA se INTRODUCING pEtarna		
40	40
S1 INTRODUCTION patterns from raw data .	kacce detA se paricaya pEtarna
VP2_LWG INTRODUCTION	paricaya
NP4 patterns	pEtarna
NNS5 patterns	pEtarna
PP6 from raw data	kacce AMkadZoM se
NP8 raw data	kaccA detA
NNS10 data	detA

----
00411	These datasets included the public Street View House Numbers dataset (Netzer et al., 2011), various versions of the ImageNet dataset (Deng et al., 2009, 2010a; Russakovsky et al., 2014a), and the Sports-1	 ina AkadoM meM sArvajanika strIta vyU detAsetsa ( nIwjZara eta ala , viBinna imeja neneta ala , 2009 , 2010 , rUsokovska ala sportsa Ora 2014 saMskaraNoM kI saMKyAeM SAmila WIM 		
410	410
S1 These datasets included the public Street View House Numbers dataset ( Netzer et al. , 2011 ) , various versions of the ImageNet dataset ( Deng et al. , 2009 , 2010a ; Russakovsky et al. , 2014a ) , and the Sports - 1	ina detAbetoM meM sArvajanika strIta vyU hAusa naMbara detAbesa (nejara eta ala, 2011), viBinna saMskaraNoM ke AidiyAneta detAbesa (deMga eta ala
NP2 These datasets	ye detAbetsa
NNS4 datasets	detAbesa
VP5_LWG included	SAmila
NP7 the public Street View House Numbers dataset ( Netzer et al. , 2011 ) , various versions of the ImageNet dataset ( Deng et al. , 2009 , 2010a ; Russakovsky et al. , 2014a ) , and the Sports - 1	pablika strIta vyU hAusa naMbara detAbesa (nejara eta ala., 2011), viBinna saMskaraNoM ke AidiyAneta detAbesa (deMga eta ala., 2009, 2010
NP8 the public Street View House Numbers dataset ( Netzer et al. , 2011 )	pablika strIta vyU hAusa naMbara detAbeta (nejara eta ala, 2011)
NP9 the public Street View House Numbers dataset	pablika strIta vyU hAusa naMbara detAbeta
NML12 Street View House	strIta vyU hAusa
NNS16 Numbers	naMbara
NP19 Netzer et al. , 2011 )	nejara eta ala, 2011 )
NP20 Netzer	nejara
ADVP22 et al.	eta ala
PRN25 , 2011 )	, 2011 )
,26 ,	,
NP27 2011	2011
,30 ,	,
NP31 various versions of the ImageNet dataset ( Deng et al. , 2009 , 2010a ; Russakovsky et al. , 2014a )	AidiyAneta detAbesa ke viBinna saMskaraNa (xeMga eta ala., 2009, 2010 e ; rUsAkovaskI eta ala., 2014a )
NP32 various versions	viBinna saMskaraNa
NNS34 versions	saMskaraNa
PP35 of the ImageNet dataset ( Deng et al. , 2009 , 2010a ; Russakovsky et al. , 2014a )	AidiyAneta detAbesa (deMga eta ala., 2009, 2010 e ; rUsAkovaskI eta ala., 2014a )
NP37 the ImageNet dataset ( Deng et al. , 2009 , 2010a ; Russakovsky et al. , 2014a )	AidiyAneta detAbesa (deMga eta ala., 2009, 2010 e ; rUsAkovaskI eta ala., 2014a )
PRN41 ( Deng et al. , 2009 , 2010a ; Russakovsky et al. , 2014a )	(xeMga eta ala, 2009, 2010 e ; rUsAkovaskI eta ala, 2014a )
FRAG43 Deng et al. , 2009 , 2010a	xeMga eta ala., 2009, 2010 e
NP44 Deng et al.	xeMga eta ala
ADVP46 et al.	eta ala
,49 ,	,
NP50 2009 , 2010a	2009, 2010 e
,52 ,	,
FRAG55 Russakovsky et al. , 2014a	rUsAkovaskI eta ala, 2014a
NP56 Russakovsky et al.	rUsAkovaskI eta ala
ADVP58 et al.	eta ala
,61 ,	,
NP62 2014a	2014 e.
,65 ,	,
CC66 and	Ora
NP67 the Sports - 1	Kela- 1
NML69 Sports - 1	Kela- 1

----
00412	M dataset	 M dAtAseta		
411	411
NP1 M dataset	ema detAbesa

----
00413	(Karpathy et al., 2014).	 ( sahAnuBUwi eta ala , 2014 )		
412	412
FRAG1 ( Karpathy et al. , 2014 ) .	(karapaWI eta ala., 2014 )
NP3 Karpathy et al.	kArpEWI eta ala
ADVP5 et al.	eta ala
,8 ,	,
NP9 2014	2014

----
00414	At the top of the graph, we see that datasets of translated sentences, such as IBMs dataset constructed from the Canadian Hansard (Brown et al., 1990) and the WMT 2014 English to French dataset (Schwenk, 2014), are typically far ahead of other dataset sizes.	 grAPa ke SIrRa para , hama xeKawe hEM ki anuvAxiwa vAkyoM ke detAseta , jEse ki kanAdA ke haMsArda ( Brown eta ala , 1990 ) Ora WMT 2014 aMgrejI se PreMca detAseta ( Schwenk 2014 ) Ama wOra para anya detAseta se kAPI Age hEM .		
413	413
S1 At the top of the graph , we see that datasets of translated sentences , such as IBM s dataset constructed from the Canadian Hansard ( Brown et al. , 1990 ) and the WMT 2014 English to French dataset ( Schwenk , 2014 ) , are typically far ahead of other dataset sizes .	grAPa ke SIrRa para, hama xeKawe hEM ki kanAdA ke haMsArda ( brAuna eta ala, 1990) se nirmiwa AIbIema kA detAbesa Ora dablyUematI 2014 anya detAbesa ke
PP2 At the top of the graph	grAPa ke SIrRa para
NP4 the top of the graph	grAPa ke SIrRa
NP5 the top	SIrRa
PP8 of the graph	grAPa kA
NP10 the graph	grAPa
,13 ,	,
NP14 we	hama
VP16_LWG see	xeKeM
SBAR18 that datasets of translated sentences , such as IBM s dataset constructed from the Canadian Hansard ( Brown et al. , 1990 ) and the WMT 2014 English to French dataset ( Schwenk , 2014 ) , are typically far ahead of other dataset sizes	kanAdA ke haMsArda ( brAuna eta ala, 1990 ) Ora dablyUematI 2014 ke anya detAbesa ke AMkadZe, AmawOra para xUsare detAbesa ke AkAra se Age hEM
S20 datasets of translated sentences , such as IBM s dataset constructed from the Canadian Hansard ( Brown et al. , 1990 ) and the WMT 2014 English to French dataset ( Schwenk , 2014 ) , are typically far ahead of other dataset sizes	kanAdAI haMsArda ( brAuna eta ala, 1990) se nirmiwa AIbIema kA detAbesa Ora dablyUematI 2014 aMgrejI se PrAMsIsI detAbesa (svIka, 2014) Amawa
NP21 datasets of translated sentences , such as IBM s dataset constructed from the Canadian Hansard ( Brown et al. , 1990 ) and the WMT 2014 English to French dataset ( Schwenk , 2014 )	kanAdAI haMsArda ( brAuna eta ala, 1990 ) Ora dablyUematI 2014 ko PrAMsIsI detAbesa (svIka, 2014) ke detAbesa jEse AIbIema kA detAte
NP22 datasets	detAbesa
NNS23 datasets	detAbesa
PP24 of translated sentences , such as IBM s dataset constructed from the Canadian Hansard ( Brown et al. , 1990 ) and the WMT 2014 English to French dataset ( Schwenk , 2014 )	kanAdAI haMsArda ( brAuna eta ala, 1990 ) Ora dablyUematI 2014 PrAMsIsI detAbesa (svIka, 2014) jEse AIbIema kA detAbesa, jEse ki
NP26 translated sentences , such as IBM s dataset constructed from the Canadian Hansard ( Brown et al. , 1990 ) and the WMT 2014 English to French dataset ( Schwenk , 2014 )	sajA, jEse ki AIbIema kA detAbesa kanAdA ke haMsArda ( brAuna eta ala, 1990 ) Ora dablyUematI 2014 PrAMsIsI detAbesa (svIka, 2014 )
NP27 translated sentences	anuvAxiwa vAkya
NNS29 sentences	vAkya
,30 ,	,
PP31 such as IBM s dataset constructed from the Canadian Hansard ( Brown et al. , 1990 ) and the WMT 2014 English to French dataset ( Schwenk , 2014 )	jEse ki AIbIema kA detAbesa kanAdA ke haMsArda ( brAuna eta ala, 1990) Ora dablyUematI 2014 PrAMsIsI detAbesa (svIka, 2014) se banAyA gayA
NP34 IBM s dataset constructed from the Canadian Hansard ( Brown et al. , 1990 ) and the WMT 2014 English to French dataset ( Schwenk , 2014 )	AIbIema kA yaha detAbesa kanAdA ke haMsArda ( brAuna eta ala, 1990) Ora dablyUematI 2014 PrAMsIsI detAbesa (svIka, 2014) se banAyA gayA hE
NP35 IBM s dataset	AIbIema kA detAbesa
VP39_LWG constructed	nirmANa kiyA gayA
PP41 from the Canadian Hansard ( Brown et al. , 1990 ) and the WMT 2014 English	kanAdAI haMsArda ( brAuna eta ala., 1990) Ora dablyUematI 2014 aMgrejI
NP43 the Canadian Hansard ( Brown et al. , 1990 ) and the WMT 2014 English	kanAdAI haMsArda ( brAuna eta ala., 1990) Ora dablyUematI 2014 aMgrejI
NP44 the Canadian Hansard ( Brown et al. , 1990 )	kanAdAI haMsArda ( brAuna eta ala, 1990)
PRN48 ( Brown et al. , 1990 )	( brAuna eta ala, 1990)
NP50 Brown et al. , 1990	brAuna eta ala., 1990
NP51 Brown et al.	brAuna eta ala
NP52 Brown	brAuna
ADVP54 et al.	eta ala
,57 ,	,
NP58 1990	1990
CC61 and	Ora
NP62 the WMT 2014 English	dablyUematI 2014 aMgrejI
PP67 to French dataset	PrAMsIsI detAbesa ko
NP69 French dataset	PrAMsIsI detAbesa
PRN72 ( Schwenk , 2014 )	(Svenake, 2014)
NP74 Schwenk	sviPaMka
,76 ,	,
NP77 2014	2014
,80 ,	,
VP81_LWG are typically far	AmawOra para xUra hEM
ADVP83 typically	AmawOra para
ADVP85 far ahead of other dataset sizes	xUsare detAbesa ke AkAra se bahuwa Age
ADVP86 far ahead	bahuwa Age
PP89 of other dataset sizes	anya detAbesa ke AkAra
NP91 other dataset sizes	anya detAbesa ke AkAra
NNS94 sizes	AkAra

----
00415	19 CHAPTER 1.	 19 CHAPTER 1 .		
414	414
FRAG1 19 CHAPTER 1 .	19 cEptara 1.
NP2 19	19
NP4 CHAPTER 1	cEptara 1

----
00416	INTRODUCTION Figure 1.9:	 INTRODUCTION ciwra 1 . 9 :		
415	415
FRAG1 INTRODUCTION Figure 1.9 :	paricaya AMkadZA 1.4 H
NP2 INTRODUCTION	paricaya
NP4 Figure 1.9	AMkadZA 1.4

----
00417	Example inputs from the MNIST dataset.	 MNIST detAseta se uxAharaNa inaputa		
416	416
S1 Example inputs from the MNIST dataset .	manista detAbesa se uxAharaNa inaputa
VP2_LWG Example	uxAharaNa
NP4 inputs	inaputa
NNS5 inputs	inaputa
PP6 from the MNIST dataset	manista detAbesa se
NP8 the MNIST dataset	manista kA detAbesa

----
00418	The NIST stands for National Institute of Standards and Technology, the agency that originally collected this data.	 NIST rARtrIya mAnaka Ora prOxyogikI saMsWAna , ejeMsI ke lie hE jo mUla rUpa se isa detA ko ekawra kiyA .		
417	417
S1 The  NIST  stands for National Institute of Standards and Technology , the agency that originally collected this data .	enaAIesatI neSanala iMstItyUta oYPa stEMdarda eMda teknoloYjI ke lie KadZA hE, jisa ejeMsI ne mUla rUpa se isa detA ko ekawriwa kiyA WA
NP2 The  NIST 	 NIST
VP7_LWG stands	KadZe hokara KadZe ho jAwe hEM
PP9 for National Institute of Standards and Technology , the agency that originally collected this data	neSanala iMstItyUta oYPa stEMdarda eMda teknoloYjI ke lie, ejeMsI jisane mUla rUpa se isa detA ko ekawriwa kiyA
NP11 National Institute of Standards and Technology , the agency that originally collected this data	neSanala iMstItyUta oYPa stEMdarda eMda teknoloYjI, ejeMsI jisane mUla rUpa se isa detA ko ekawriwa kiyA
NP12 National Institute of Standards and Technology	neSanala iMstItyUta oYPa stEMdarda eMda teknoloYjI
NP13 National Institute	rARtrIya saMsWAna
PP16 of Standards and Technology	mAnakoM Ora prOxyogikI ke
NP18 Standards and Technology	mAnakoM Ora prOxyogikI
CC20 and	Ora
NNPS19 Standards	mAnakoM
NNP21 Technology	wakanIka
,22 ,	,
NP23 the agency that originally collected this data	ejeMsI jisane mUla rUpa se isa detA ko ekawriwa kiyA
NP24 the agency	ejeMsI
SBAR27 that originally collected this data	jisane mUla rUpa se isa detA ko ekawriwa kiyA
WHNP28 that	vaha
S30 originally collected this data	mUla rUpa se ikatTA kiyA gayA ye detA
ADVP31 originally	mUla rUpa se
VP33_LWG collected	ekawriwa
NP35 this data	yaha AMkadZA
NNS37 data	detA

----
00419	The M stands for modied, since the data has been preprocessed for easier use with machine learning algorithms.	 M moYdeda ke lie KadZA hE , kyoMki detA maSIna sIKane elgorixama ke sAWa AsAna upayoga ke lie pUrvaprakramiwa kiyA gayA hE .		
418	418
S1 The  M  stands for  modied ,  since the data has been preprocessed for easier use with machine learning algorithms .	ema saMSoXiwa, ke lie KadZA hE, kyoMki maSIna sIKa elgorixama ke sAWa AsAna upayoga ke lie detA ko pUrvanirXAriwa kiyA gayA hE
NP2 The  M 	ema
VP7_LWG stands	KadZe hokara KadZe ho jAwe hEM
PP9 for  modied ,  since the data has been preprocessed for easier use with machine learning algorithms	"saMSoXiwa,  cUMki maSIna sIKa elgorixama ke sAWa AsAna upayoga ke lie AMkadZe kA prawyAropa kiyA gayA hE
NP11  modied ,  since the data has been preprocessed for easier use with machine learning algorithms	saMSoXiwa,  cUMki maSIna sIKane ke elgorixama ke sAWa AsAna upayoga ke lie AMkadZe kA prawyAropa kiyA gayA hE
NP13 modied	saMSoXiwa
,15 ,	,
SBAR17 since the data has been preprocessed for easier use with machine learning algorithms	cUMki maSIna sIKane ke elgorixama ke sAWa AsAna iswemAla ke lie detA ko kiyA gayA hE pahale se
S19 the data has been preprocessed for easier use with machine learning algorithms	maSIna larniMga elgorixama ke sAWa AsAna iswemAla ke lie dAtA ko kiyA gayA hE pahale se
NP20 the data	detA
NNS22 data	detA
VP23_LWG has been preprocessed	pahale se hI ho cukA hE
PP29 for easier use with machine learning algorithms	maSIna sIKane ke elgorixama ke sAWa AsAna upayoga ke lie
NP31 easier use with machine learning algorithms	maSIna sIKane ke elgorixama ke sAWa AsAna upayoga
NP32 easier use	AsAna iswemAla
PP35 with machine learning algorithms	maSIna sIKane ke elgorixama ke sAWa
NP37 machine learning algorithms	maSIna sIKa rahe elgorixama
NML38 machine learning	maSIna sIKanA
NNS41 algorithms	elgorixama

----
00420	The MNIST dataset consists of scans of handwritten digits and associated labels describing which digit 09 is contained in each image.	 MNIST detAseta meM haswaliKiwa aMkoM Ora saMbaxXa lebaloM ke skEna howe hEM , jinameM yaha varNana howA hE ki prawyeka Cavi meM kOna sA aMka 0 - 9 nihiwa hE 		
419	419
S1 The MNIST dataset consists of scans of handwritten digits and associated labels describing which digit 0  9 is contained in each image .	manista dAtAbesa meM hAWa liKiwa dijita ke skEna howe hEM Ora isase judZe lebaloM meM bawAyA gayA hE ki prawyeka Cavi meM kisa dijita 0- 9 samAhiwa hE
NP2 The MNIST dataset	manista kA detAbesa
VP6_LWG consists	SAmila
PP8 of scans of handwritten digits and associated labels describing which digit 0  9 is contained in each image	haswaliKiwa dijitoM ke skEna Ora isase judZe lebaloM kA jikra karawe hue bawAyA gayA hE ki prawyeka Cavi meM kisa dijita 0- 9 samAhiwa hE
NP10 scans of handwritten digits and associated labels describing which digit 0  9 is contained in each image	haswaliKiwa dijitoM ke skEna Ora jodZe gae lebaloM kA varNana karawe hue bawAwe hEM ki prawyeka Cavi meM kisa dijita 0- 9 samAhiwa hE
NP11 scans of handwritten digits	hAWa liKiwa dijita ke skEna
NP12 scans	skEna
NNS13 scans	skEna
PP14 of handwritten digits	hAWa liKiwa aMkoM kI
NP16 handwritten digits	hAWa se liKiwa aMka
NNS18 digits	dijita
CC19 and	Ora
NP20 associated labels describing which digit 0  9 is contained in each image	prawyeka Cavi meM kisa aMkoM 0- 9 ko SAmila karawe hEM isakA ulleKa karane vAle saMbaMXiwa lebala
NP21 associated labels	judZe lebala
NNS23 labels	lebala
VP24_LWG describing	bawAwe hue
SBAR26 which digit 0  9 is contained in each image	prawyeka Cavi meM kisa aMkoM 0- 9 samAhiwa hE
WHNP27 which	jo ki
S29 digit 0  9 is contained in each image	prawyeka Cavi meM dijita 0- 9 hE samAhiwa
NP30 digit 0  9	dijita 0- 9
NP31 digit 0	dijita 0
PP34  9	 9
SYM35 	
NP36 9	9
VP38_LWG is contained	samAhiwa hE
PP42 in each image	prawyeka Cavi meM
NP44 each image	prawyeka Cavi

----
0042	This capability is known as machine learning .	 isa kRamawA ko maSInI sIKane ke nAma se jAnA jAwA hE 		
41	41
S1 This capability is known as machine learning .	yaha kRamawA maSIna sIKane ke rUpa meM jAnI jAwI hE
NP2 This capability	yaha kRamawA
VP5_LWG is known	jAnA jAwA hE
PP9 as machine learning	maSIna sIKane ke rUpa meM
NP11 machine learning	maSIna sIKanA

----
00421	This simple classication problem is one of the simplest and most widely used tests in deep learning research.	 isa sarala vargIkaraNa samasyA gaharA sIKane anusaMXAna meM sabase sarala Ora sabase vyApaka rUpa se iswemAla parIkRaNoM meM se eka hE .		
420	420
S1 This simple classication problem is one of the simplest and most widely used tests in deep learning research .	yaha sAXAraNa vargIkaraNa samasyA gahana sIKa anusaMXAna meM sabase sarala Ora sabase vyApaka rUpa se iswemAla hone vAle parIkRaNoM meM se eka hE
NP2 This simple classication problem	yaha sAXAraNa SreNI kI samasyA
VP7_LWG is	hE
NP9 one of the simplest and most widely used tests in deep learning research	dIpa larniMga anusaMXAna meM sabase sarala Ora sabase vyApaka iswemAla hone vAle parIkRaNoM meM se eka
NP10 one	eka
PP12 of the simplest and most widely used tests in deep learning research	dIpa larniMga anusaMXAna meM sabase sarala Ora sabase vyApaka iswemAla hone vAle parIkRaNoM kI
NP14 the simplest and most widely used tests in deep learning research	dIpa larniMga anusaMXAna meM sabase sarala Ora sabase vyApaka iswemAla hone vAle parIkRaNa
NP15 the simplest and most widely used tests	sabase sarala Ora sabase vyApaka iswemAla hone vAle parIkRaNa
ADJP17 simplest and most widely used	sabase sarala Ora sabase vyApaka iswemAla
ADJP18 simplest	sabase sarala
CC20 and	Ora
ADJP21 most widely used	sabase aXika iswemAla howA hE
NNS25 tests	parIkRaNa
PP26 in deep learning research	gahana sIKa anusaMXAna meM
NP28 deep learning research	gahana sIKa anusaMXAna
NML29 deep learning	gaharI sIKa

----
00422	It remains popular despite being quite easy for modern techniques to solve.	 yaha AXunika wakanIkoM ko hala karane ke lie kAPI AsAna hone ke bAvajUxa lokapriya rahawA hE .		
421	421
S1 It remains popular despite being quite easy for modern techniques to solve .	yaha hala karane ke lie AXunika wakanIkoM ke lie kAPI AsAna hone ke bAvajUxa lokapriya rahawA hE
NP2 It	yaha
VP4_LWG remains	bane hue hEM
ADJP6 popular despite being quite easy for modern techniques to solve	hala karane ke lie AXunika wakanIkoM ke lie kAPI AsAna hone ke bAvajUxa lokapriya
PP8 despite being quite easy for modern techniques to solve	hala karane ke lie AXunika wakanIkoM ke lie kAPI AsAna hone ke bAvajUxa
S10 being quite easy for modern techniques to solve	hala karane ke lie AXunika wakanIkoM ke lie kAPI AsAna ho rahA hE
VP11_LWG being	hone ke nAwe
ADJP13 quite easy for modern techniques to solve	hala karane ke lie AXunika wakanIkoM ke lie kAPI AsAna
ADJP14 quite easy for modern techniques	AXunika wakanIkoM ke lie kAPI AsAna
PP17 for modern techniques	AXunika wakanIkoM ke lie
NP19 modern techniques	AXunika wakanIkoM
NNS21 techniques	wakanIkoM
S22 to solve	hala karane ke lie
VP23_LWG to solve	hala karane ke lie

----
00423	Georey Hinton has described it as the drosophila of machine learning, meaning that it enables machine learning researchers to study their algorithms in controlled laboratory conditions, much as biologists often study fruit ies.	 jePrI hAinatana ne ise  maSInI SikRaNa kA derosoPilA kahA hE , jisakA arWa hE ki yaha maSInI SikRaNa SoXakarwAoM ko niyaMwriwa prayogaSAlA parisWiwiyoM meM unake elgorixama kA aXyayana karane meM sakRama banAwA hE , bahuwa kuCa jIvavijFAnI aksara Pala makKiyoM kA aXyayana karawe hEM .		
422	422
S1 Georey Hinton has described it as  the drosophila of machine learning ,  meaning that it enables machine learning researchers to study their algorithms in controlled laboratory conditions , much as biologists often study fruit ies .	jePrI hiMtana ne ise maSIna sIKane kA doroPiPA, bawAyA hE, arWAwa yaha anusaMXAnakarwAoM ko niyaMwriwa prayogaSAlA parisWiwiyoM meM apane elgorixama kA aXya
NP2 Georey Hinton	jePrI hiMtana
VP5_LWG has described	varNiwa kiyA hE
NP9 it	yaha
PP11 as  the drosophila of machine learning ,  meaning that it enables machine learning researchers to study their algorithms in controlled laboratory conditions , much as biologists often study fruit ies	jEsA ki maSIna sIKane kA doroPiPA, kA arWa hE ki yaha maSIna sIKane vAle anusaMXAnakarwAoM ko niyaMwriwa prayogaSAlA parisWiwiyoM meM apane elgorixama
NP13  the drosophila of machine learning ,  meaning that it enables machine learning researchers to study their algorithms in controlled laboratory conditions , much as biologists often study fruit ies	maSIna sIKane kA doroPiPA, arWa hE ki yaha maSIna sIKane vAle anusaMXAnakarwAoM ko niyaMwriwa prayogaSAlA kI parisWiwiyoM meM apane elgorixama kA aXyayana kara
NP14  the drosophila of machine learning , 	maSIna sIKane kA doroPilA,
NP16 the drosophila	doroPilA
PP19 of machine learning	maSIna sIKane kI
NP21 machine learning	maSIna sIKanA
,24 ,	,
VP26_LWG meaning	mawalaba
SBAR28 that it enables machine learning researchers to study their algorithms in controlled laboratory conditions , much as biologists often study fruit ies	yaha maSIna sIKane vAle anusaMXAnakarwAoM ko niyaMwriwa prayogaSAlA kI parisWiwiyoM meM apane elgorixama kA aXyayana karane meM sakRama banAwA hE, kyoMki jIvavijFAnI ak
S30 it enables machine learning researchers to study their algorithms in controlled laboratory conditions , much as biologists often study fruit ies	yaha maSIna sIKane vAle anusaMXAnakarwAoM ko niyaMwriwa prayogaSAlA kI parisWiwiyoM meM apane elgorixama kA aXyayana karane meM sakRama banAwA hE, kyoMki jIvavijFAnI ak
NP31 it	yaha
VP33_LWG enables	sakRama
NP35 machine learning researchers	maSIna sIKa anusaMXAnakarwAoM
NML36 machine learning	maSIna sIKanA
NNS39 researchers	SoXakarwAoM
S40 to study their algorithms in controlled laboratory conditions , much as biologists often study fruit ies	niyaMwriwa prayogaSAlA parisWiwiyoM meM apane elgorixama kA aXyayana karane ke lie, bahuwa aXika jIvavijFAnI aksara PaloM kI makKiyoM kA aXyayana karawe hEM
VP41_LWG to study much often study	bahuwa bAra paDZAI karane ke lie paDZAI
NP46 their algorithms	unake elgorixama
NNS48 algorithms	elgorixama
PP49 in controlled laboratory conditions	niyaMwriwa prayogaSAlA parisWiwiyoM meM
NP51 controlled laboratory conditions	niyaMwriwa prayogaSAlA kI sWiwi
NNS54 conditions	SarweM
,55 ,	,
ADVP56 much as biologists	bahuwa jEse bAyoloYjista
PP58 as biologists	jIvavijFAnI ke rUpa meM
NP60 biologists	bAyoloYjista
NNS61 biologists	bAyoloYjista
ADVP62 often	aksara
NP66 fruit ies	PaloM kI udZana
NNS68 ies	makKiyoM

----
00424	has made machine learning much easier because the key burden of statistical estimationgeneralizing well to new data after observing only a small amount of datahas been considerably lightened.	 maSInI SikRaNa ko kAPI AsAna banA xiyA hE kyoMki sAMKyikIya Akalana kA muKya boJa - kevala WodZI mAwrA meM detA xeKane ke bAxa nae detA ko acCI waraha se uwpanna karanA - kAPI halkA ho gayA hE 		
423	423
S1 has made machine learning much easier because the key burden of statistical estimation  generalizing well to new data after observing only a small amount of data  has been considerably lightened .	maSIna ko bahuwa AsAna banA xiyA hE kyoMki sAMKyikIya anumAna kA pramuKa boJa kevala eka CotI mAwrA meM detA kA avalokana karane ke bAxa nae detA ko acCa
VP2_LWG has made	banA cuke hEM
NP6 machine	maSIna
S8 learning much easier because the key burden of statistical estimation  generalizing well to new data after observing only a small amount of data  has been considerably lightened	bahuwa AsAna sIKanA kyoMki sAMKyikIya anumAna kA mahawvapUrNa boJa kevala eka CotI mAwrA meM detA xeKane ke bAxa nae detA ko acCI waraha se samaJAnA
VP9_LWG learning generalizing well	acCI waraha se sAmAnya jFAna sIKanA
ADJP12 much easier because the key burden of statistical estimation	bahuwa AsAna hE kyoMki sAMKyikIya anumAna kA pramuKa boJa
ADJP13 much easier	bahuwa AsAna
PP16 because the key burden of statistical estimation	kyoMki sAMKyikIya anumAna kA pramuKa boJa
NP18 the key burden of statistical estimation	sAMKyikIya anumAna kA pramuKa boJa
NP19 the key burden	pramuKa boJa
PP23 of statistical estimation	sAMKyikIya anumAna
NP25 statistical estimation	sAMKyikIya anumAna
ADVP31 well	acCI waraha se
PP33 to new data	nae detA ko
NP35 new data	nayA detA
NNS37 data	detA
PP38 after observing only a small amount of data  has been considerably lightened	kevala eka CotI sI mAwrA meM dAtA xeKane ke bAxa kAPI halkA ho gayA hE
S40 observing only a small amount of data  has been considerably lightened	kevala eka CotI sI mAwrA meM dAtA kA avalokana karanA kAPI halkA huA hE
VP41_LWG observing	xeKa rahA hUz
NP43 only a small amount of data  has been considerably lightened	kevala eka CotI sI mAwrA meM detA kAPI halkA huA hE
NP44 only a small amount	kevala eka CotI sI rASi
PP49 of data  has been considerably lightened	detA kAPI halkA kiyA gayA hE
S51 data  has been considerably lightened	detA kAPI halkA ho cukA hE
NP52 data	detA
NNS53 data	detA
VP55_LWG has been	rahA hE
ADJP59 considerably lightened	kAPI halkA

----
00425	As of 2016, a rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category and will match or 20 CHAPTER 1.	 ke rUpa meM 2016 , aMgUTe kA eka mote niyama hE ki eka paryavekRiwa gaharI aXigama elgoriWma AmawOra para lagaBaga 5 , 000 lebala uxAharaNa prawi varga ke sAWa svIkArya praxarSana prApwa karegA Ora mEca yA 20 aXyAya 1 .		
424	424
S1 As of 2016 , a rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category and will match or 20 CHAPTER 1 .	2016 ke rUpa meM aMgUTe kA eka motA niyama hE ki eka nirIkRaka gaharI sIKa elgorixama AmawOra para prawi SreNI ke lagaBaga 5,000 lebala vAle uxAharaNoM ke sAWa
PP2 As of 2016	2016 ke anusAra
PP4 of 2016	2016 kA
NP6 2016	2016
,8 ,	,
NP9 a rough rule of thumb	aMgUTe kA motA niyama
NP10 a rough rule	eka motA niyama
PP14 of thumb	aMgUTe kI
NP16 thumb	aMgUTI
VP18_LWG is	hE
SBAR20 that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category and will match or 20 CHAPTER 1	yaha ki eka nirIkRaka gaharA sIKa elgorixama AmawOra para prawi SreNI ke lagaBaga 5,000 lebala vAle uxAharaNoM ke sAWa svIkArya praxarSana hAsila karegA Ora vaha prawiyoga
S22 a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category and will match or 20 CHAPTER 1	eka nirIkRaka gaharA sIKa elgorixama AmawOra para prawi SreNI ke lagaBaga 5,000 lebala vAle uxAharaNoM ke sAWa svIkArya praxarSana hAsila karegA Ora yaha mEca yA 20 ca
NP23 a supervised deep learning algorithm	eka nirIkRaka gahana sIKa elgorixama
NML26 deep learning	gaharI sIKa
VP30_LWG will generally achieve will match	Ama wOra para hAsila kareMge mEca
ADVP33 generally	Ama wOra para
NP37 acceptable performance	svIkArya praxarSana
PP40 with around 5,000 labeled examples per category	lagaBaga 5,000 lebala vAle uxAharaNa prawi SreNI
NP42 around 5,000 labeled examples per category	prawi SreNI ke lagaBaga 5,000 lebala xie gae uxAharaNa
NP43 around 5,000 labeled examples	lagaBaga 5,000 lebala vAle uxAharaNa
QP44 around 5,000	karIba 5,000
NNS48 examples	uxAharaNa
PP49 per category	prawi SreNI
NP51 category	SreNI
CC53 and	Ora
VP31 will generally achieve acceptable performance with around 5,000 labeled examples per category	AmawOra para prawi SreNI ke lagaBaga 5,000 lebala vAle uxAharaNoM ke sAWa svIkArya praxarSana hAsila kareMge
VP54 will match or 20 CHAPTER 1	mEca hogA yA 20 ceptara 1
NP58 or 20 CHAPTER 1	yA 20 cEptara 1
QP59 or 20	yA 20

----
00426	INTRODUCTION exceed human performance when trained with a dataset containing at least 10 million labeled examples.	 INTRODUCTION mAnava praxarSana se aXika howA hE jaba eka detAseta ke sAWa praSikRiwa kiyA jAwA hE jisameM kama se kama 10 miliyana lebala kie gae uxAharaNa howe hEM .		
425	425
S1 INTRODUCTION exceed human performance when trained with a dataset containing at least 10 million labeled examples .	paricaya mAnava praxarSana se aXika hE jaba kama se kama 10 miliyana lebala vAle uxAharaNoM ke sAWa praSikRiwa kiyA gayA
NP2 INTRODUCTION	paricaya
VP4_LWG exceed	aXika se aXika
NP6 human performance	mAnavIya praxarSana
SBAR9 when trained with a dataset containing at least 10 million labeled examples	jaba kama se kama 10 miliyana lebala vAle uxAharaNoM se praSikRiwa
WHADVP10 when	kaba
S12 trained with a dataset containing at least 10 million labeled examples	kama se kama 10 miliyana lebala vAle uxAharaNoM ke sAWa praSikRiwa
VP13_LWG trained	praSikRiwa
PP15 with a dataset containing at least 10 million labeled examples	eka sUcI ke sAWa jisameM kama se kama 10 miliyana lebala vAle uxAharaNa SAmila hEM
NP17 a dataset containing at least 10 million labeled examples	kama se kama 10 miliyana lebala vAle uxAharaNa vAlA eka detAbesa
NP18 a dataset	eka detAbesa
VP21_LWG containing	jisameM
NP23 at least 10 million labeled examples	kama se kama 10 lAKa lebala vAle uxAharaNa
QP24 at least 10 million	kama se kama 10 karodZa
ADVP25 at least	kama se kama
NNS31 examples	uxAharaNa

----
00427	Working successfully with datasets smaller than this is an important research area, focusing in particular on how we can take advantage of large quantities of unlabeled examples, with unsupervised or semi-supervised learning.	 isase Cote detAseta ke sAWa saPalawApUrvaka kArya karanA eka mahawvapUrNa anusaMXAna kRewra hE , viSeRa rUpa se isa bAwa para XyAna keMxriwa karanA ki kEse hama aviSvasanIya uxAharaNoM kI badZI mAwrA kA lABa le sakawe hEM , anupayukwa yA arXa - prawIkRiwa aXigama ke sAWa .		
426	426
FRAG1 Working successfully with datasets smaller than this is an important research area , focusing in particular on how we can take advantage of large quantities of unlabeled examples , with unsupervised or semi-supervised learning .	isase Cote detAbesa ke sAWa saPalawApUrvaka kAma karanA eka mahawvapUrNa anusaMXAna kRewra hE, viSeRa rUpa se XyAna keMxriwa karanA hE ki hama anasulakRiwa yA arXa-arXa
VP2_LWG Working successfully focusing	kAmakAja saPalawApUrvaka keMxriwa
ADVP5 successfully	saPalawApUrvaka
PP7 with datasets smaller than this is an important research area	isase Cote detAbetsa ke sAWa eka mahawvapUrNa anusaMXAna kRewra hE
NP9 datasets smaller than this is an important research area	isase Cote detAbetsa eka mahawvapUrNa anusaMXAna kRewra
NP10 datasets	detAbesa
NNS11 datasets	detAbesa
ADJP12 smaller than this is an important research area	isase CotA eka mahawvapUrNa anusaMXAna kRewra hE
ADJP13 smaller	CotA
SBAR15 than this is an important research area	isase jyAxA mahawvapUrNa anusaMXAna kRewra hE
S17 this is an important research area	yaha eka mahawvapUrNa anusaMXAna kRewra hE
NP18 this	yaha
VP20_LWG is	hE
NP22 an important research area	eka mahawvapUrNa anusaMXAna kRewra
,27 ,	,
PP30 in particular on how we can take advantage of large quantities of unlabeled examples	viSeRa rUpa se avEXa uxAharaNoM kI badZI mAwrA kA PAyaxA kEse uTA sakawe hEM
NP32 particular on how we can take advantage of large quantities of unlabeled examples	viSeRa rUpa se avEXa uxAharaNoM kI badZI mAwrA kA PAyaxA kEse uTA sakawe hEM
NP33 particular on how we can take advantage	KAsakara hama kEse le sakawe hEM PAyaxA
ADJP34 particular on how we can take	KAsakara hama kEse le sakawe hEM
PP36 on how we can take	kEse le sakawe hEM hama
SBAR38 how we can take	hama kEse le sakawe hEM
WHADVP39 how	kEse
S41 we can take	hama le sakawe hEM
NP42 we	hama
VP44_LWG can take	le sakawe hEM
PP49 of large quantities of unlabeled examples	aprawyASiwa uxAharaNoM kI badZI mAwrA
NP51 large quantities of unlabeled examples	badZI mAwrA meM avEXa uxAharaNa
NP52 large quantities	badZI mAwrA meM
NNS54 quantities	mAwrA
PP55 of unlabeled examples	avEXa uxAharaNoM kA
NP57 unlabeled examples	avEXa uxAharaNa
NNS59 examples	uxAharaNa
,60 ,	,
PP61 with unsupervised or semi-supervised learning	aprawyASiwa yA arXa-arXanirmiwa sIKa ke sAWa
NP63 unsupervised or semi-supervised learning	aprawyASiwa yA arXa-arXanirmiwa sIKa
ADJP64 unsupervised or semi-supervised	aprawyASiwa yA arXanirmiwa
CC66 or	yA
JJ65 unsupervised	aprawyASiwa
JJ67 semi-supervised	arXanirmiwa

----
00428	1.2.3	 1 . 2 . 3		
427	427
FRAG1 1.2.3	1.2.3
NP2 1.2.3	1.2.3

----
00429	Increasing Model Sizes Another key reason that neural networks are wildly successful today after enjoying comparatively little success since the 1980s is that we have the computational resources to run much larger models today.	 baDZawI moYdala AkAra eka Ora pramuKa kAraNa hE ki waMwrikA netavarka Aja apekRAkqwa kama saPalawA kA AnaMxa lene ke bAxa bewahASA saPala rahe hEM 1980 ke xaSaka se hE ki hama Aja bahuwa badZe moYdala calAne ke lie kampyUteSanala saMsAXanoM hE .		
428	428
S1 Increasing Model Sizes Another key reason that neural networks are wildly successful today after enjoying comparatively little success since the 1980s is that we have the computational resources to run much larger models today .	baDZawA huA moYdala AkAra eka Ora pramuKa vajaha hE ki waMwrikA netavarka 1980 ke xaSaka se wulanAwmaka rUpa se kama saPalawA kA AnaMxa lene ke bAxa Aja kAPI saPala
NP2 Increasing Model	baDZawA huA moYdala
VP5_LWG Sizes	AkAra
NP7 Another key reason that neural networks are wildly successful	eka Ora pramuKa vajaha hE ki waMwrikA netavarka behaxa saPala hEM
NP8 Another key reason	eka Ora pramuKa kAraNa
SBAR12 that neural networks are wildly successful	vaha waMwrikA netavarka behaxa saPala hEM
S14 neural networks are wildly successful	waMwrikA netavarka behaxa saPala hEM
NP15 neural networks	waMwrikA netavarka
NNS17 networks	netavarka
VP18_LWG are	hEM
ADJP20 wildly successful	behaxa saPala
NP-TMP23 today	Aja
PP25 after enjoying comparatively little success since the 1980s is that we have the computational resources to run much larger models today	1980 ke xaSaka se wulanAwmaka rUpa se kama saPalawA kA AnaMxa lene ke bAxa yaha hE ki Aja hamAre pAsa bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana
S27 enjoying comparatively little success since the 1980s is that we have the computational resources to run much larger models today	1980 ke xaSaka se wulanAwmaka rUpa se kama saPalawA kA AnaMxa lenA yaha hE ki Aja hamAre pAsa bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana hEM
VP28_LWG enjoying	AnaMxa le rahA hE
NP30 comparatively little success	wulanAwmaka rUpa se kama saPalawA
ADJP31 comparatively little	wulanAwmaka rUpa se kama
SBAR35 since the 1980s is that we have the computational resources to run much larger models today	1980 ke xaSaka se yaha hE ki Aja hamAre pAsa bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana hEM
S37 the 1980s is that we have the computational resources to run much larger models today	1980 kA xOra yaha hE ki Aja hamAre pAsa bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana hEM
NP38 the 1980s	1980 ke xaSaka
NNS40 1980s	1980 ke xaSaka meM
VP41_LWG is	hE
SBAR43 that we have the computational resources to run much larger models today	hamAre pAsa Aja bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana hEM
S45 we have the computational resources to run much larger models today	hamAre pAsa Aja bahuwa badZe moYdala calAne ke lie kaMpyUteSanala saMsAXana hEM
NP46 we	hama
VP48_LWG have	pAsa hE
NP50 the computational resources to run much larger models today	bahuwa badZe moYdala calAne ke lie kampyUteSanala saMsAXana Aja
NNS53 resources	saMsAXana
S54 to run much larger models today	bahuwa badZe moYdala calAne ke lie Aja
VP55_LWG to run	xOdZane ke lie
NP59 much larger models	bahuwa badZe moYdala
ADJP60 much larger	bahuwa badZA
NNS63 models	moYdala
NP-TMP64 today	Aja

----
00430	One of the main insights of connection- ism is that animals become intelligent when many of their neurons work together.	 kanekSana - isma kI eka muKya aMwarxqRti yaha hE ki jAnavara buxXimAna ho jAwe hEM jaba unake kaI nyUroYnsa eka sAWa kAma karawe hEM .		
429	429
S1 One of the main insights of connection - ism is that animals become intelligent when many of their neurons work together .	kanekSana kI muKya aMwarxqRti meM se eka-eka yaha hE ki jAnavara buxXimAna ho jAwe hEM jaba unake kaI nyUro eka sAWa kAma karawe hEM
NP2 One of the main insights of connection - ism	kanekSana ke muKya aMwaroM meM se eka - islAma
NP3 One	eka
PP5 of the main insights of connection - ism	kanekSana ke muKya cOrAhoM kI -islAma
NP7 the main insights of connection - ism	kanekSana ke muKya aMwara  Xarma
NP8 the main insights	muKya aMwarxqRti
NNS11 insights	aMwarxqRti
PP12 of connection - ism	kanekSana kA-islAma
NP14 connection - ism	kanekSana-islAma
VP18_LWG is	hE
SBAR20 that animals become intelligent when many of their neurons work together	vaha jAnavara buxXimAna ho jAwe hEM jaba unake kaI nyUrona eka sAWa kAma karawe hEM
S22 animals become intelligent when many of their neurons work together	jAnavara buxXimAna ho jAwe hEM jaba unake kaI nyUrona eka sAWa kAma karawe hEM
NP23 animals	jAnavaroM
NNS24 animals	jAnavaroM
VP25_LWG become	bana gae
ADJP27 intelligent	buxXimAna
SBAR29 when many of their neurons work together	jaba unake kaI nyUrona eka sAWa kAma karawe hEM
WHADVP30 when	kaba
S32 many of their neurons work together	unake kaI nyUrona eka sAWa kAma karawe hEM
NP33 many of their neurons	unake kaI nyUrona
NP34 many	kaI
PP36 of their neurons	unake nyUrona kI
NP38 their neurons	unake nyUrona
NNS40 neurons	nyUrona
VP41_LWG work together	sAWa meM kAma karawe hEM
ADVP43 together	eka sAWa

----
0043	The introduction of machine learning enabled computers to tackle problems involving knowledge of the real world and make decisions that appear subjective.	 maSInI SikRaNa kI SuruAwa ne kaMpyUtaroM ko vAswavika xuniyA ke jFAna se saMbaMXiwa samasyAoM se nipatane Ora vyakwiparaka xiKane vAle nirNaya lene meM sakRama banAyA .		
42	42
S1 The introduction of machine learning enabled computers to tackle problems involving knowledge of the real world and make decisions that appear subjective .	maSIna sIKane ke paricaya se kaMpyUtara vAswavika xuniyA ke jFAna se judZI samasyAoM se nipatane ke lie sakRama howe hEM Ora nirNaya lewe hEM jo viRayAwmaka prawI
NP2 The introduction of machine learning	maSIna sIKane kA paricaya
NP3 The introduction	paricaya
PP6 of machine learning	maSIna sIKane kI
NP8 machine learning	maSIna sIKanA
VP11_LWG enabled	sakRama
NP13 computers	kaMpyUtara
NNS14 computers	kaMpyUtara
S15 to tackle problems involving knowledge of the real world and make decisions that appear subjective	vAswavika xuniyA ke jFAna se judZI samasyAoM se nipatane ke lie Ora nirNaya lene ke lie jo viRayAwmaka prawIwa howe hEM
VP16_LWG to tackle make	muSkila se nipatane ke lie
NP21 problems involving knowledge of the real world	vAswavika xuniyA ke jFAna se judZI samasyAeM
NP22 problems	samasyAeM
NNS23 problems	samasyAeM
PP24 involving knowledge of the real world	vAswavika xuniyA kA jFAna SAmila
NP26 knowledge of the real world	vAswavika xuniyA kA jFAna
NP27 knowledge	jFAna
PP29 of the real world	vAswavika xuniyA kI
NP31 the real world	asalI xuniyA
CC35 and	Ora
VP19 tackle problems involving knowledge of the real world	vAswavika xuniyA ke jFAna se judZI muSkileM
VP36 make decisions that appear subjective	Ese nirNaya kareM jo viRayAwmaka prawIwa hoM
NP38 decisions that appear subjective	jo PEsale viRayAwmaka xiKAI xewe hEM
NP39 decisions	PEsale
NNS40 decisions	PEsale
SBAR41 that appear subjective	vaha viRayAwmaka prawIwa howA hE
WHNP42 that	vaha
S44 appear subjective	viRayAwmaka xiKAI xeM
VP45_LWG appear	xiKAI xewe hEM
ADJP47 subjective	viRayAwmaka

----
00431	An individual neuron or small collection of neurons is not particularly useful.	 eka alaga nyUroYna yA nyUroYnsa ke Cote saMgraha viSeRa rUpa se upayogI nahIM hE .		
430	430
S1 An individual neuron or small collection of neurons is not particularly useful .	eka vyakwigawa nyUrona yA nyUrona kA CotA sA saMgraha viSeRa rUpa se upayogI nahIM hE
NP2 An individual neuron or small collection of neurons	eka vyakwigawa nyUrona yA nyUrona kA CotA sA saMgraha
NP3 An individual neuron	eka vyakwi nyUrona
CC7 or	yA
NP8 small collection of neurons	nyUrona kA CotA sA saMgraha
NP9 small collection	CotA saMgraha
PP12 of neurons	nyUrona kI
NP14 neurons	nyUrona
NNS15 neurons	nyUrona
VP16_LWG is not	nahIM hE
ADJP19 particularly useful	KAsakara upayogI

----
00432	Biological neurons are not especially densely connected.	 jEvika nyUroYnsa viSeRa rUpa se Gane judZe nahIM hEM .		
431	431
S1 Biological neurons are not especially densely connected .	jEvika nyUrona viSeRa rUpa se GanI waraha se judZe nahIM hEM
NP2 Biological neurons	jEvika nyUrona
NNS4 neurons	nyUrona
VP5_LWG are not especially	viSeRa rUpa se nahIM hEM
ADVP8 especially	KAsakara
ADJP10 densely connected	Gana se judZA huA

----
00433	As seen in gure 1.10, our machine learning models have had a number of connections per neuron within an order of magnitude of even mammalian brains for decades.	 jEsA ki aMka 1 . 10 meM xeKA gayA hE , hamAre maSIna sIKane ke moYdaloM meM prawi nyUroYna kaI kanekSana We xaSakoM ke lie BI swanaXArI maswiRka ke parimANa ke eka krama ke BIwara .		
432	432
S1 As seen in gure 1.10 , our machine learning models have had a number of connections per neuron within an order of magnitude of even mammalian brains for decades .	jEsA ki AMkadZA 1.10 meM xeKA gayA hE, hamAre maSIna sIKane vAle moYdaloM meM xaSakoM se yahAM waka ki mamme ke ximAga kI wIvrawA ke AxeSa ke
SBAR2 As seen in gure 1.10	jEsA ki AMkadZe 1.10 meM xeKA gayA
S4 seen in gure 1.10	AMkadZe 1.10 meM xeKA gayA
VP5_LWG seen	xeKA jAwA hE
PP7 in gure 1.10	AMkadZe 1.10 meM
NP9 gure 1.10	AMkadZA 1.10
,12 ,	,
NP13 our machine learning models	hamAre maSIna sIKane ke moYdala
NML15 machine learning	maSIna sIKanA
NNS18 models	moYdala
VP19_LWG have had	kara cuke hEM
NP23 a number of connections per neuron	prawi nyUrona ke kanekSanoM kI saMKyA
NP24 a number	eka naMbara
PP27 of connections per neuron	prawi nyUrona ke kanekSana
NP29 connections per neuron	prawi nyUrona ke kanekSana
NP30 connections	kanekSana
NNS31 connections	kanekSana
PP32 per neuron	prawi nyUrona
NP34 neuron	nyUrona
PP36 within an order of magnitude of even mammalian brains for decades	xaSakoM se yahAM waka ki mamme ke ximAga kI wIvrawA ke AxeSa ke BIwara
NP38 an order of magnitude of even mammalian brains for decades	xaSakoM se yahAM waka ki mamme ke ximAga kI wIvrawA kA AxeSa
NP39 an order	eka AxeSa
PP42 of magnitude of even mammalian brains for decades	xaSakoM se yahAM waka ki mamme ke ximAga kI wIvrawA
NP44 magnitude of even mammalian brains for decades	xaSakoM se yahAM waka ki mamme ke ximAga kI wIvrawA
NP45 magnitude	wIvrawA
PP47 of even mammalian brains for decades	xaSakoM se yahAM waka ki swanaXArI ximAga kI
NP49 even mammalian brains for decades	yahAM waka ki xaSakoM ke lie swanaXArI ximAga
NP50 even mammalian brains	yahAM waka ki swanaXArI ximAga BI
NNS53 brains	ximAgZa
PP54 for decades	xaSakoM se
NP56 decades	xaSakoM
NNS57 decades	xaSakoM

----
00434	In terms of the total number of neurons, neural networks have been astonishingly small until quite recently, as shown in gure 1.11.	 nyUroYnsa kI kula saMKyA ke saMxarBa meM , waMwrikA netavarka AScaryajanaka rUpa se kAPI hAla hI meM Cote rahe hEM , jEsA ki AMkadZA 1 . 11 meM xiKAyA gayA hE 		
433	433
S1 In terms of the total number of neurons , neural networks have been astonishingly small until quite recently , as shown in gure 1.11 .	nyUrona kI kula saMKyA ke mAmale meM, waMwrikA netavarka hAla hI meM kAPI Cote rahA hE, jEsA ki AMkadZe  1.10 meM xiKAyA gayA hE
PP2 In terms of the total number of neurons	nyUrona kI kula saMKyA ke mAmale meM
NP4 terms of the total number of neurons	nyUrona kI kula saMKyA kI SarweM
NP5 terms	SarwoM
NNS6 terms	SarwoM
PP7 of the total number of neurons	nyUrona kI kula saMKyA
NP9 the total number of neurons	nyUrona kI kula saMKyA
NP10 the total number	kula saMKyA
PP14 of neurons	nyUrona kI
NP16 neurons	nyUrona
NNS17 neurons	nyUrona
,18 ,	,
NP19 neural networks	waMwrikA netavarka
NNS21 networks	netavarka
VP22_LWG have been	kara cuke hEM
ADJP26 astonishingly small	AScaryajanaka rUpa se CotA
PP29 until quite recently , as shown in gure 1.11	hAla hI meM kAPI hAla waka, jEsA ki AMkadZe meM xiKAyA gayA hE
ADVP31 quite recently , as shown in gure 1.11	kAPI hAla hI meM, jEsA ki AMkadZe meM xiKAyA gayA hE
ADVP32 quite recently	kAPI hAla hI meM
,35 ,	,
SBAR36 as shown in gure 1.11	jEsA ki AMkadZe  1.10 meM xiKAyA gayA
S38 shown in gure 1.11	AMkadZe  1.10 meM xiKAyA gayA
VP39_LWG shown	xiKAyA gayA
PP41 in gure 1.11	AMkadZe  1.10
NP43 gure 1.11	AMkadZA  1.10

----
00435	Since the introduction of hidden units, articial neural networks have doubled in size roughly every 2.4 years.	 CupI ikAiyoM ke SurU hone ke bAxa se , kqwrima waMwrikA netavarka kA AkAra mote wOra para hara 2 . 4 varRa meM xogunA ho gayA hE 		
434	434
S1 Since the introduction of hidden units , articial neural networks have doubled in size roughly every 2.4 years .	CipI huI ikAiyoM ke paricaya ke bAxa se kqwrima waMwrikA netavarka hara 2.4 sAla meM badZe pEmAne para xogunA ho gayA hE
PP2 Since the introduction of hidden units	cUMki CipI huI ikAiyoM kA paricaya
NP4 the introduction of hidden units	CipI huI ikAiyoM kA paricaya
NP5 the introduction	paricaya
PP8 of hidden units	CipI huI ikAiyoM kI
NP10 hidden units	CipI huI ikAiyAM
NNS12 units	ikAiyAM
,13 ,	,
NP14 articial neural networks	kqwrima waMwrikA netavarka
NNS17 networks	netavarka
VP18_LWG have doubled	xogunA kara cuke hEM
PP22 in size	AkAra meM
NP24 size	AkAra
NP-TMP26 roughly every 2.4 years	mote wOra para hara 2.4 sAla
QP27 roughly every 2.4	mote wOra para hara 2.4
NNS31 years	sAla-ba-ba-ba-ba-ba-ba-ba.

----
00436	This growth is driven by faster computers with larger memory and by the availability of larger datasets.	 yaha vqxXi badZI smqwi vAle weja kaMpyUtaroM Ora badZe detAsetoM kI upalabXawA se preriwa hE 		
435	435
S1 This growth is driven by faster computers with larger memory and by the availability of larger datasets .	isa vqxXi ko badZI yAxaxASwa vAle Ora badZe detAbesa kI upalabXawA se preriwa kiyA jAwA hE
NP2 This growth	yaha vqxXi
VP5_LWG is driven	calAyA gayA hE
PP9 by faster computers with larger memory and by the availability of larger datasets	badZI yAxaxASwa vAle weja kaMpyUtara Ora badZe detAbesa kI upalabXawA se
PP10 by faster computers with larger memory	badZI memorI vAle weja kaMpyUtaroM xvArA
NP12 faster computers with larger memory	badZI memorI vAle weja kaMpyUtara
NP13 faster computers	weja kaMpyUtara
NNS15 computers	kaMpyUtara
PP16 with larger memory	badZI smqwi ke sAWa
NP18 larger memory	badZI yAxaxASwa
CC21 and	Ora
PP22 by the availability of larger datasets	badZe detAbeta kI upalabXawA se
NP24 the availability of larger datasets	badZe detAbeta kI upalabXawA
NP25 the availability	upalabXawA
PP28 of larger datasets	badZe detAbeta kA
NP30 larger datasets	badZe detAbeta
NNS32 datasets	detAbesa

----
00437	Larger networks are able to achieve higher accuracy on more complex tasks.	 badZe netavarka aXika jatila kAryoM para aXika satIkawA prApwa karane meM sakRama howe hEM .		
436	436
S1 Larger networks are able to achieve higher accuracy on more complex tasks .	badZe netavarka aXika jatila kAryoM para ucca satIkawA hAsila karane meM sakRama hEM
NP2 Larger networks	badZe netavarka
NNS4 networks	netavarka
VP5_LWG are	hEM
ADJP7 able to achieve higher accuracy on more complex tasks	aXika jatila kAryoM para ucca satIkawA hAsila karane meM sakRama
S9 to achieve higher accuracy on more complex tasks	aXika jatila kAryoM para ucca satIkawA hAsila karane ke lie
VP10_LWG to achieve	hAsila karane ke lie
NP14 higher accuracy	ucca satIkawA
PP17 on more complex tasks	aXika jatila kAryoM para
NP19 more complex tasks	aXika jatila kArya
ADJP20 more complex	aXika jatila
NNS23 tasks	kAryoM

----
00438	This trend looks set to continue for decades.	 yaha pravqwwi xaSakoM waka jArI rahane ke lie waya laga rahI hE 		
437	437
S1 This trend looks set to continue for decades .	yaha treMda xaSakoM waka jArI rahane ke lie nirXAriwa xiKawA hE
NP2 This trend	yaha ruJAna
VP5_LWG looks set	laga rahA hE seta
S9 to continue for decades	xaSakoM waka jArI rahanA
VP10_LWG to continue	jArI raKane ke lie
PP14 for decades	xaSakoM se
NP16 decades	xaSakoM
NNS17 decades	xaSakoM

----
00439	Unless new technologies enable faster scaling, articial neural networks will not have the same number of neurons as the human brain until at least the 2050s.	 jaba waka naI prOxyogikiyAM wejI se skeliMga ko sakRama nahIM karawI hEM , kqwrima waMwrikA netavarkoM meM kama se kama 2050 ke xaSaka waka mAnava maswiRka ke samAna nyUroYnsa kI saMKyA nahIM hogI .		
438	438
S1 Unless new technologies enable faster scaling , articial neural networks will not have the same number of neurons as the human brain until at least the 2050s .	jaba waka naI prOxyogikAeM wejI se skeliMga karane meM sakRama nahIM hoMgI, kqwrima waMwrikA netavarka meM kama se kama 2025 waka mAnava maswiRka ke samAna saMKyA
SBAR2 Unless new technologies enable faster scaling	jaba waka naI prOxyogikAeM weja skeliMga ko sakRama nahIM karawIM
S4 new technologies enable faster scaling	naI wakanIkoM se ho sakawI hE weja skeliMga
NP5 new technologies	naI wakanIkoM
NNS7 technologies	wakanIkoM
VP8_LWG enable	sakRama
NP10 faster scaling	weja skeliMga
,13 ,	,
NP14 articial neural networks	kqwrima waMwrikA netavarka
NNS17 networks	netavarka
VP18_LWG will not have	nahIM hogA
NP23 the same number of neurons as the human brain	mAnava maswiRka ke samAna saMKyA meM nyUrona
NP24 the same number	vahI saMKyA
PP28 of neurons as the human brain	mAnava maswiRka ke rUpa meM nyUrona kI
NP30 neurons as the human brain	mAnava maswiRka ke rUpa meM nyUrona
NP31 neurons	nyUrona
NNS32 neurons	nyUrona
PP33 as the human brain	mAnava maswiRka ke rUpa meM
NP35 the human brain	mAnava maswiRka
PP39 until at least the 2050s	kama se kama 2025 waka
NP41 at least the 2050s	kama se kama 2025 ke xaSaka meM
QP42 at least	kama se kama
ADVP43 at least	kama se kama

----
00440	Biological neurons may represent more complicated functions than current articial neurons, so biological neural networks may be even larger than this plot portrays.	 jEvika nyUroYnsa varwamAna kqwrima nyUroYnsa kI wulanA meM aXika jatila kAryoM kA prawiniXiwva kara sakawe hEM , isalie jEvika waMwrikA netavarka isa sAjiSa ciwraNa se BI badZA ho sakawA hE .		
439	439
S1 Biological neurons may represent more complicated functions than current articial neurons , so biological neural networks may be even larger than this plot portrays .	jEvika nyUrojZa mOjUxA kqwrima nyUrojana kI wulanA meM aXika jatila kAryoM kA prawiniXiwva kara sakawe hEM, isalie jEvika waMwrikA netavarka isa plAta portarI se
S2 Biological neurons may represent more complicated functions than current articial neurons	jEvika nyUrojZa mOjUxA kqwrima nyUronsa kI wulanA meM aXika jatila kAryoM kA prawiniXiwva kara sakawe hEM
NP3 Biological neurons	jEvika nyUrona
NNS5 neurons	nyUrona
VP6_LWG may represent	ho sakawA hE prawiniXiwva
NP10 more complicated functions	aXika jatila kArya
ADJP11 more complicated	aXika jatila
NNS14 functions	samAroha
PP15 than current articial neurons	mOjUxA kqwrima nyUrona kI wulanA meM
NP17 current articial neurons	varwamAna kqwrima nyUrona
NNS20 neurons	nyUrona
,21 ,	,
CC22 so	Ese meM
S23 biological neural networks may be even larger than this plot portrays	jEvika waMwrikA netavarka isa plAta portare se BI badZe ho sakawe hEM
NP24 biological neural networks	jEvika waMwrikA netavarka
NNS27 networks	netavarka
VP28_LWG may be	ho sakawA hE
ADJP32 even larger than this plot portrays	isa plAta portare se BI badZA
ADJP33 even larger	yahAM waka ki badZA
SBAR36 than this plot portrays	isa plAta portare kI wulanA meM
S38 this plot portrays	yaha plAta portare
NP39 this plot	yaha BUKaMda
VP42_LWG portrays	portare

----
0044	A simple machine learning algorithm called logistic regression can determine whether to recommend cesarean delivery (Mor-Yosef et al., 1990).	 eka sarala maSIna aXigama elgoriWma jise saMBAra prawigamana kahA jAwA hE , yaha nirXAriwa kara sakawA hE ki kyA sIjeriyana dilIvarI kI siPAriSa kI jAe ( Mor -Ysef eta ala , 1990 )		
43	43
S1 A simple machine learning algorithm called logistic regression can determine whether to recommend cesarean delivery ( Mor - Yosef et al. , 1990 ) .	loYjistika prawiPala nAmaka eka sAXAraNa maSIna sIKane vAlA elgorixama nirXAriwa kara sakawA hE ki kyA sIjZariyana dilIvarI (mora-yosePa eta ala, 1990) kI si
NP2 A simple machine learning algorithm	eka sAXAraNa maSIna sIKa rahA elgorixama
NML5 machine learning	maSIna sIKanA
VP9_LWG called	bulAyA gayA
SBAR11 logistic regression can determine whether to recommend cesarean delivery ( Mor - Yosef et al. , 1990 )	loYjistika prawiPala nirXAriwa kara sakawA hE ki kyA sIjZariyana dilIvarI (mora-yosePa eta ala, 1990) kI siPAriSa karanA hE
S12 logistic regression can determine whether to recommend cesarean delivery ( Mor - Yosef et al. , 1990 )	loYjistika prawiPala nirXAriwa kara sakawA hE ki kyA sIjZariyana dilIvarI (mora-yosePa eta ala, 1990) kI siPAriSa karanA hE
NP13 logistic regression	loYjistika prawiPala
VP16_LWG can determine	nirXAriwa kara sakawe hEM
SBAR20 whether to recommend cesarean delivery ( Mor - Yosef et al. , 1990 )	kyA sIjZariyana dilIvarI kI siPAriSa karanA hE (mora-yosePa eta ala, 1990)
S22 to recommend cesarean delivery ( Mor - Yosef et al. , 1990 )	sijeriyana dilIvarI kI siPAriSa karane ke lie (mora-yosePa eta ala, 1990)
VP23_LWG to recommend	siPAriSa karane ke lie
NP27 cesarean delivery ( Mor - Yosef et al. , 1990 )	sIjZariyana dilIvarI (mora- yosePa eta ala, 1990)
PRN30 ( Mor - Yosef et al. , 1990 )	(mora- yosePa eta ala, 1990)
NP32 Mor - Yosef et al.	mora  yosePa eta ala
NP33 Mor	mora
PP35 - Yosef	- yosePa
SYM36 -	-
NP37 Yosef	yosePa
ADVP39 et al.	eta ala
,42 ,	,
NP43 1990	1990

----
00441	In retrospect, it is not particularly surprising that neural networks with fewer neurons than a leech were unable to solve sophisticated articial intelligence prob- lems.	 pUrvavyApI meM , yaha viSeRa rUpa se AScarya kI bAwa nahIM hE ki joMka kI wulanA meM kama nyUroYnsa vAle waMwrikA netavarka pariRkqwa kqwrima buxXi proba - lemsa ko hala karane meM asamarWa We .		
440	440
S1 In retrospect , it is not particularly surprising that neural networks with fewer neurons than a leech were unable to solve sophisticated articial intelligence prob - lems .	retrospekta meM yaha viSeRa rUpa se AScarya kI bAwa nahIM hE ki lIca kI wulanA meM kama nyUronsa vAle waMwrikA netavarka pariRkqwa kqwrima kqwrima buxXimawwA jAMca
PP2 In retrospect	retrospekta meM
NP4 retrospect	retrospekta
,6 ,	,
NP7 it	yaha
VP9_LWG is not	nahIM hE
ADJP12 particularly surprising	KAsakara hErAnI kI bAwa
SBAR15 that neural networks with fewer neurons than a leech were unable to solve sophisticated articial intelligence prob - lems	lIca se kama nyUrona vAle waMwrikA netavarka pariRkqwa kqwrima kqwrima buxXimawwA jAMca ke lie asamarWa We- lemsa
S17 neural networks with fewer neurons than a leech were unable to solve sophisticated articial intelligence prob - lems	eka lIca se kama nyUrona vAle waMwrikA netavarka pariRkqwa kqwrima kqwrima buxXimawwA jAMca ke lie asamarWa We- lemsa
NP18 neural networks with fewer neurons than a leech	loca se kama nyUrona vAle waMwrikA netavarka
NP19 neural networks	waMwrikA netavarka
NNS21 networks	netavarka
PP22 with fewer neurons than a leech	lIca kI wulanA meM kama nyUrona ke sAWa
NP24 fewer neurons than a leech	lIca kI wulanA meM kama nyUrona
NP25 fewer neurons	kama nyUrona
NNS27 neurons	nyUrona
PP28 than a leech	lIca kI wulanA meM
NP30 a leech	eka lIca
VP33_LWG were	We
ADJP35 unable to solve sophisticated articial intelligence prob - lems	pariRkqwa kqwrima buxXimawwA parIkRA kA samAXAna karane meM asamarWa - elaemaesa
S37 to solve sophisticated articial intelligence prob - lems	pariRkqwa kqwrima buxXimawwA parIkRA ke samAXAna ke lie - lemsa
VP38_LWG to solve	hala karane ke lie
NP42 sophisticated articial intelligence prob - lems	pariRkqwa kqwrima buxXimawwA jAMca - lemsa
NP43 sophisticated articial intelligence	pariRkqwa kqwrima buxXimawwA
ADJP47 prob - lems	jAMca- lemsa

----
00442	Even todays networks, which we consider quite large from a computational systems point of view, are smaller than the nervous system of even relatively primitive vertebrate animals like frogs.	 Aja ke netavarka BI , jinheM hama kampyUteSanala sistama kI xqRti se kAPI badZe mAnawe hEM , meMDaka jEse apekRAkqwa Axima kaSerukI prANiyoM ke waMwrikA waMwra se Cote hEM 		
441	441
S1 Even today s networks , which we consider quite large from a computational systems point of view , are smaller than the nervous system of even relatively primitive vertebrate animals like frogs .	yahAM waka ki Aja ke netavarka, jinheM hama eka kampyUteSanala sistama ke xqRtikoNa se kAPI badZe mAnawe hEM, yahAM waka ki meMDakoM jEse apekRA
NP-TMP2 Even today	Aja BI
NP5 s networks , which we consider quite large from a computational systems point of view ,	'ke netavarka, jinheM hama kaMpyUteSanala sistama ke xqRtikoNa se kAPI badZe mAnawe hEM,
NP6 s networks	ke netavarka
NNS8 networks	netavarka
,9 ,	,
SBAR10 which we consider quite large from a computational systems point of view	jise hama kampyUtoriyala sistama ke xqRtikoNa se kAPI badZA mAnawe hEM
WHNP11 which	jo ki
S13 we consider quite large from a computational systems point of view	hama eka kampyUtoriyala sistama ke xqRtikoNa se kAPI badZe mAnawe hEM
NP14 we	hama
VP16_LWG consider	vicAra kareM
ADJP18 quite large from a computational systems point of view	kampyUteSanala sistama ke xqRtikoNa se kAPI badZA
PP21 from a computational systems point of view	eka kampyUteSanala sistama ke xqRtikoNa se
NP23 a computational systems point of view	eka kampyUteSanala sistama xqRtikoNa
NP24 a computational systems point	eka kampyUteSanala sistama poYiMta
NML26 computational systems	kampyUteSanala sistama
NNS28 systems	sistama
PP30 of view	najara
NP32 view	xqSya
,34 ,	,
VP35_LWG are	hEM
ADJP37 smaller than the nervous system of even relatively primitive vertebrate animals like frogs	yahAM waka ki meMDakoM jEse apekRAkqwa Axima kaSerukAoM ke paSuoM kI waMwrikA waMwra kI wulanA meM CotA
ADJP38 smaller	CotA
PP40 than the nervous system of even relatively primitive vertebrate animals like frogs	yahAM waka ki apekRAkqwa Axima kaSerukAoM jEse meMDakoM kI waMwrikA waMwra kI wulanA meM
NP42 the nervous system of even relatively primitive vertebrate animals like frogs	yahAM waka ki apekRAkqwa Axima kaSerukAoM jEse meMDakoM kI waMwrikA waMwra
NP43 the nervous system	waMwrikA waMwra
PP47 of even relatively primitive vertebrate animals like frogs	yahAM waka ki apekRAkqwa Axima kaSerukAoM jEse meMDakoM kI
NP49 even relatively primitive vertebrate animals like frogs	yahAM waka ki apekRAkqwa Axima kaSerukAoM jEse meMDakoM kI waraha
NP50 even relatively primitive vertebrate animals	yahAM waka ki apekRAkqwa Axima kaSerukAoM ke jAnavara BI
ADJP52 relatively primitive	apekRAkqwa Axima
NNS56 animals	jAnavaroM
PP57 like frogs	meMDakoM kI waraha
NP59 frogs	meMDaka
NNS60 frogs	meMDaka

----
00443	The increase in model size over time, due to the availability of faster CPUs, the advent of general purpose GPUs (described in section 12.1.2), faster network connectivity and better software infrastructure for distributed computing, is one of the most important trends in the history of deep learning.	 samaya ke sAWa moYdala ke AkAra meM vqxXi , wejI se sIpIyU kI upalabXawA ke kAraNa , sAmAnya uxxeSya ke Agamana ( XArA 12 . 1 . 2 meM nihiwa ) jIpIyU , wejI se netavarka kanektivitI Ora viwariwa kaMpyUtiMga ke lie behawara soYPtaveyara mUla saMracanA , gahana SikRaNa ke iwihAsa meM sabase mahawvapUrNa pravqwwiyoM meM se eka hE .		
442	442
S1 The increase in model size over time , due to the availability of faster CPUs , the advent of general purpose GPUs ( described in section 12.1.2 ) , faster network connectivity and better software infrastructure for distributed computing , is one of the most important trends in the history of deep learning .	samaya ke sAWa moYdala ke AkAra meM baDZowarI, wejI se sIpIyU kI upalabXawA ke kAraNa sAmAnya uxxeSya ke jIpIyU (XArA 11988) meM varNiwa Ora viwariwa ka
NP2 The increase in model size over time , due to the availability of faster CPUs , the advent of general purpose GPUs ( described in section 12.1.2 ) , faster network connectivity and better software infrastructure for distributed computing ,	samaya ke sAWa moYdala ke AkAra meM baDZowarI, wejI se sIpIyU kI upalabXawA ke kAraNa sAmAnya uxxeSya ke jIpIyU kA Agamana (XArA 11988) meM varNiwa,
NP3 The increase	baDZowarI
PP6 in model size over time	samaya ke sAWa moYdala sAija meM
NP8 model size over time	samaya ke sAWa moYdala kA AkAra
NP9 model size	moYdala sAija
PP12 over time	samaya ke sAWa
NP14 time	samaya
,16 ,	,
PP17 due to the availability of faster CPUs , the advent of general purpose GPUs ( described in section 12.1.2 ) , faster network connectivity and better software infrastructure for distributed computing	wejI se sIpIyU kI upalabXawA ke kAraNa sAmAnya uxxeSya ke jIpIyU kA Agamana (XArA 1(2007) meM varNiwa), viwariwa kaMpyUtiMga ke lie weja netavarka
NP20 the availability of faster CPUs , the advent of general purpose GPUs ( described in section 12.1.2 ) , faster network connectivity and better software infrastructure for distributed computing	wejI se sIpIyU kI upalabXawA, sAmAnya uxxeSya ke jIpIyU kA Agamana ( anuBAga 1(2007) meM varNiwa), weja netavarka kanektivitI Ora viwariwa kaMpa
NP21 the availability	upalabXawA
PP24 of faster CPUs , the advent of general purpose GPUs ( described in section 12.1.2 ) , faster network connectivity and better software infrastructure for distributed computing	wejI se sIpIyU kA Agamana, sAmAnya uxxeSya jIpIyU kA Agamana ( anuBAga 11988) meM varNiwa, weja netavarka kanektivitI Ora viwariwa kaMpyUtiMga ke
NP26 faster CPUs , the advent of general purpose GPUs ( described in section 12.1.2 ) , faster network connectivity and better software infrastructure for distributed computing	weja sIpIyU, sAmAnya uxxeSya ke jIpIyU kA Agamana ( anuBAga 1(2007) meM varNiwa), viwariwa kaMpyUtiMga ke lie weja netavarka kanektivitI Ora behawa
NP27 faster CPUs	weja sIpIyU
NNS29 CPUs	sIpIyU
,30 ,	,
NP31 the advent of general purpose GPUs ( described in section 12.1.2 )	sAmAnya uxxeSya ke jIpIyU kA Agamana (XArA 1(2007) meM varNiwa)
NP32 the advent	Agamana
PP35 of general purpose GPUs	sAmAnya uxxeSya ke jIpIyU ke
NP37 general purpose GPUs	sAmAnya uxxeSya jIpIyU
NNS40 GPUs	jIpIyU
PRN41 ( described in section 12.1.2 )	( anuBAga 1(2007) meM varNiwa)
S43 described in section 12.1.2	XArA 1(2007) meM varNiwa
VP44_LWG described	varNiwa
PP46 in section 12.1.2	XArA 1(2007) meM
NP48 section 12.1.2	XArA 1syA.
,52 ,	,
NP53 faster network connectivity	weja netavarka kanektivitI
CC57 and	Ora
NP58 better software infrastructure for distributed computing	viwariwa kaMpyUtiMga ke lie behawara soYPtaveyara iMPrAstrakcara
NP59 better software infrastructure	behawara soYPtaveyara iMPrAstrakcara
PP63 for distributed computing	viwariwa kaMpyUtiMga ke lie
NP65 distributed computing	viwariwa kaMpyUtiMga
,68 ,	,
VP69_LWG is	hE
NP71 one of the most important trends in the history of deep learning	gaharI sIKa ke iwihAsa meM sabase mahawvapUrNa ruJAnoM meM se eka
NP72 one	eka
PP74 of the most important trends in the history of deep learning	gaharI sIKa ke iwihAsa meM sabase mahawvapUrNa ruJAnoM kA
NP76 the most important trends in the history of deep learning	gaharI sIKa ke iwihAsa meM sabase mahawvapUrNa ruJAna
NP77 the most important trends	sabase mahawvapUrNa ruJAna
ADJP79 most important	sabase mahawvapUrNa
NNS82 trends	ruJAna
PP83 in the history of deep learning	gaharI sIKa ke iwihAsa meM
NP85 the history of deep learning	gaharI sIKa kA iwihAsa
NP86 the history	iwihAsa
PP89 of deep learning	gaharI sIKa kA
NP91 deep learning	gaharI sIKa

----
00444	This trend is generally expected to continue well into the future.	 isa pravqwwi AmawOra para BaviRya meM acCI waraha se jArI raKane kI ummIxa hE .		
443	443
S1 This trend is generally expected to continue well into the future .	yaha ruJAna Ama wOra para BaviRya meM acCI waraha se jArI rahane kI ummIxa hE
NP2 This trend	yaha ruJAna
VP5_LWG is generally expected	AmawOra para ummIxa hE
ADVP7 generally	Ama wOra para
S11 to continue well into the future	BaviRya meM acCI waraha se jArI raKane ke lie
VP12_LWG to continue well	acCI waraha se jArI raKane ke lie
ADVP16 well	acCI waraha se
PP18 into the future	BaviRya meM
NP20 the future	BaviRya

----
00445	21 CHAPTER 1.	 21 CHAPTER 1 .		
444	444
FRAG1 21 CHAPTER 1 .	21 cEptara 1.
NP2 21	21
NP4 CHAPTER 1	cEptara 1

----
00446	INTRODUCTION 1950 1985 2000 2015 Year 10 1 10 2 10 3 10 4 Connections per neuron 1 2 3 4 5 6 7 8 9 10 Fruit y Mouse Cat Human Figure 1.10	 sUcanA prOxyogikI 1950 1985 2000 2015 varRa 10 1 10 2 10 3 10 10 4 kanekSana prawi nyUroYna 1 2 3 4 5 6 7 8 9 10 Pala makKI mAusa billI mAnava ciwra 1 . 10		
445	445
FRAG1 INTRODUCTION 1950198520002015 Year 10 1 10 2 10 3 10 4 Connections per neuron 1 2 3 4 5 6 7 8 9 10 Fruit y Mouse Cat Human Figure 1.10	uxGAtana 1950 1985 2000 varRa 10 1 10 2 10 3 10 4 kanekSana prawi nABika 1 2 3 4 5 6 7 8 9 10 Pala PlAI mAusa kEta mAnava Akqwi 1.10
NP2 INTRODUCTION	paricaya
NP4 1950198520002015	1950 1985 2000 2015
NP6 Year	sAla-ba-ba-ba-ba-ba-ba-ba-ba.
NP8 10 1 10 2 10 3 10 4 Connections per neuron 1 2 3 4 5 6 7 8 9 10	10 1 10 2 10 3 10 4 kanekSana prawi nyUrona 1 2 3 4 5 6 7 8 9 10
NP9 10 1 10 2 10 3 10 4 Connections per neuron	10 1 10 2 10 3 10 4 kanekSana prawi nyUrona
NP10 10 1 10 2 10 3 10 4 Connections	10 1 10 2 10 3 10 4 kanekSana
PP20 per neuron	prawi nyUrona
NP22 neuron	nyUrona
NP24 1 2 3 4 5 6 7 8 9 10	1 2 3 4 5 6 7 8 9 10
NP35 Fruit y	PaloM kA makKana
NP38 Mouse Cat Human	mAusa billI mAnava
NP42 Figure 1.10	AMkadZA 1.10

----
00447	: Number of connections per neuron over time.	 : samaya ke sAWa prawi nyUroYna kanekSana kI saMKyA .		
446	446
NP1 : Number of connections per neuron over time .	- samaya ke sAWa prawi nyUroina prawi kanekSana kI saMKyA
NP3 Number of connections per neuron	prawi nyUronoYla ke kanekSana kI saMKyA
NP4 Number	naMbara
PP6 of connections per neuron	prawi nyUrona ke kanekSana
NP8 connections per neuron	prawi nyUrona ke kanekSana
NP9 connections	kanekSana
NNS10 connections	kanekSana
PP11 per neuron	prawi nyUrona
NP13 neuron	nyUrona
PP15 over time	samaya ke sAWa
NP17 time	samaya

----
00448	Initially, the number of connec- tions between neurons in articial neural networks was limited by hardware capabilities.	 prAraMBa meM , kqwrima waMwrikA netavarkoM meM nyUroYnsa ke bIca saMvahana - AyanoM kI saMKyA hArdaveyara kRamawAoM xvArA sImiwa WI .		
447	447
S1 Initially , the number of connec - tions between neurons in articial neural networks was limited by hardware capabilities .	prAraMBika wOra para kqwrima waMwrikA netavarka meM nyUrona ke bIca cayana kI saMKyA hArdaveyara kRamawAoM se sImiwa WI
ADVP2 Initially	prAraMBika wOra para
,4 ,	,
NP5 the number of connec - tions between neurons in articial neural networks	koYnseka kI saMKyA- kqwrima waMwrikA netavarka meM nyUrona ke bIca cayana
NP6 the number	naMbara
PP9 of connec - tions between neurons in articial neural networks	SaMku kI - kqwrima waMwrikA netavarka meM nyUrona ke bIca JUTa
NP11 connec - tions between neurons in articial neural networks	SaMku - kqwrima waMwrikA netavarka meM nyUrona ke bIca JUTa
NP12 connec - tions	SaMku- XArAez
NNS15 tions	warpaNa
PP16 between neurons in articial neural networks	kqwrima waMwrikA netavarka meM nyUrona ke bIca
NP18 neurons in articial neural networks	kqwrima waMwrikA netavarka meM nyUrona
NP19 neurons	nyUrona
NNS20 neurons	nyUrona
PP21 in articial neural networks	kqwrima waMwrikA netavarka meM
NP23 articial neural networks	kqwrima waMwrikA netavarka
NNS26 networks	netavarka
VP27_LWG was limited	sImiwa WA
PP31 by hardware capabilities	hArdaveyara kRamawAoM se
NP33 hardware capabilities	hArdaveyara kRamawAeM
NNS35 capabilities	kRamawAeM

----
00449	Today, the number of connections between neurons is mostly a design consideration.	 Aja , nyUroYnsa ke bIca kanekSana kI saMKyA jyAxAwara eka dijAina vicAra hE .		
448	448
S1 Today , the number of connections between neurons is mostly a design consideration .	Aja nyUrona ke bIca kanekSana kI saMKyA jyAxAwara eka dijAina vicAra hE
NP-TMP2 Today	Aja
,4 ,	,
NP5 the number of connections between neurons	nyUrona ke bIca kanekSana kI saMKyA
NP6 the number	naMbara
PP9 of connections between neurons	nyUrona ke bIca kanekSana
NP11 connections between neurons	nyUrona ke bIca kanekSana
NP12 connections	kanekSana
NNS13 connections	kanekSana
PP14 between neurons	nyUrona ke bIca
NP16 neurons	nyUrona
NNS17 neurons	nyUrona
VP18_LWG is mostly	jyAxAwara hE
ADVP20 mostly	jyAxAwara
NP22 a design consideration	eka dijAina vicAra

----
00450	Some articial neural networks have nearly as many connections per neuron as a cat, and it is quite common for other neural networks to have as many connections per neuron as smaller mammals like mice.	 kuCa kqwrima waMwrikA netavarka eka billI ke rUpa meM nyUroYna prawi kaI kanekSana ke rUpa meM lagaBaga hE , Ora yaha anya waMwrikA netavarka ke lie kAPI Ama hE ki cUhoM jEse Cote swanaXAriyoM ke rUpa meM nyUroYna prawi kaI kanekSana hE .		
449	449
S1 Some articial neural networks have nearly as many connections per neuron as a cat , and it is quite common for other neural networks to have as many connections per neuron as smaller mammals like mice .	kuCa kqwrima waMwrikA netavarkoM meM eka billI ke rUpa meM prawi nyUrona ke lie lagaBaga kanekSana howe hEM, Ora anya waMwrikA netavarkoM ke lie yaha cU
S2 Some articial neural networks have nearly as many connections per neuron as a cat	kuCa kqwrima waMwrikA netavarkoM meM billI ke rUpa meM prawi nyUrona ke lagaBaga kanekSana howe hEM
NP3 Some articial neural networks	kuCa kqwrima waMwrikA netavarka
NNS7 networks	netavarka
VP8_LWG have	pAsa hE
NP10 nearly as many connections per neuron as a cat	billI ke rUpa meM prawi nyUronoYla ke lagaBaga jiwane kanekSana
NP11 nearly as many connections	lagaBaga iwane kanekSana
QP12 nearly as many	lagaBaga uwane hI
NNS16 connections	kanekSana
PP17 per neuron as a cat	billI ke rUpa meM prawi nyUrona
NP19 neuron as a cat	billI ke rUpa meM nyUrona
NP20 neuron	nyUrona
PP22 as a cat	billI ke rUpa meM
NP24 a cat	eka billI
,27 ,	,
CC28 and	Ora
S29 it is quite common for other neural networks to have as many connections per neuron as smaller mammals like mice	anya waMwrikA netavarka ke lie cUhoM kI waraha Cote swanaXAriyoM ke rUpa meM prawi nyUrona ke lie uwanA hI kanekSana honA kAPI sAmAnya hE
NP30 it	yaha
VP32_LWG is	hE
ADJP34 quite common for other neural networks	anya waMwrikA netavarka ke lie kAPI Ama
PP37 for other neural networks	anya waMwrikA netavarka ke lie
NP39 other neural networks	anya waMwrikA netavarka
NNS42 networks	netavarka
S43 to have as many connections per neuron as smaller mammals like mice	cUhoM kI waraha Cote swanaXAriyoM ke rUpa meM prawi nyUronoYla ke pAsa uwane hI kanekSana hone ke lie
VP44_LWG to have	karane ke lie
PP48 as many connections per neuron	prawi nyUrona ke jiwane kanekSana
NP50 many connections per neuron	prawi nyUrona ke kaI kanekSana
NP51 many connections	kaI kanekSana
NNS53 connections	kanekSana
PP54 per neuron	prawi nyUrona
NP56 neuron	nyUrona
PP58 as smaller mammals like mice	cUhoM kI waraha Cote-Cote swanaXArI
NP60 smaller mammals like mice	cUhoM kI waraha Cote swanaXArI
NP61 smaller mammals	Cote swanaXArI
NNS63 mammals	swanaXArI
PP64 like mice	cUhoM kI waraha
NP66 mice	cUhe
NNS67 mice	cUhe

----
0045	A simple machine learning algorithm called naive Bayes can separate legitimate e-mail from spam e-mail.	 eka sarala maSIna larniMga algoriWama jise Bole beyoY kahA jAwA hE , spEma I - mela se vEXa i - mela ko alaga kara sakawA hE 		
44	44
S1 A simple machine learning algorithm called naive Bayes can separate legitimate e-mail from spam e-mail .	BolI KAxa nAmaka eka sAXAraNa maSIna sIKane vAlA elgorixama spEma I-mela se vEXa I-mela alaga kara sakawA hE
NP2 A simple machine learning algorithm	eka sAXAraNa maSIna sIKa rahA elgorixama
NML5 machine learning	maSIna sIKanA
VP9_LWG called	bulAyA gayA
SBAR11 naive Bayes can separate legitimate e-mail from spam e-mail	BolI KAne vAle svEmpa I-mela se alaga kara sakawe hEM vEXa I-mela
S12 naive Bayes can separate legitimate e-mail from spam e-mail	BolI KAne vAle svEmpa I-mela se alaga kara sakawe hEM vEXa I-mela
NP13 naive Bayes	BolI KAdZI
VP16_LWG can separate	alaga kara sakawe hEM
NP20 legitimate e-mail	vEXa I-mela
PP23 from spam e-mail	spEma I-mela se
NP25 spam e-mail	spEma I-mela

----
00451	Even the human brain does not have an exorbitant amount of connections per neuron.	 yahAM waka ki mAnava maswiRka meM prawi nyUroYna kanekSana kI awyaXika mAwrA nahIM howI hE .		
450	450
S1 Even the human brain does not have an exorbitant amount of connections per neuron .	yahAM waka ki mAnava maswiRka meM prawi nyUroNu prawi kanekSana kI awirikwa mAwrA nahIM howI
NP2 Even the human brain	yahAM waka ki mAnava maswiRka BI
VP7_LWG does not have	nahIM howA
NP12 an exorbitant amount of connections per neuron	prawi nyUrona prawi kanekSanoM kI eka awyaXika mAwrA
NP13 an exorbitant amount	eka awyaXika rASi
PP17 of connections per neuron	prawi nyUrona ke kanekSana
NP19 connections per neuron	prawi nyUrona ke kanekSana
NP20 connections	kanekSana
NNS21 connections	kanekSana
PP22 per neuron	prawi nyUrona
NP24 neuron	nyUrona

----
00452	Biological neural network sizes from Wikipedia (2015).	 jEvika waMwrikA netavarka kA AkAra vikipIdiyA se (2015 I . )		
451	451
NP1 Biological neural network sizes from Wikipedia ( 2015 ) .	vikipIdiyA ( 2015) se jEvika waMwrikA netavarka kA AkAra
NP2 Biological neural network sizes	jEvika waMwrikA netavarka ke AkAra
NML3 Biological neural network	jEvika waMwrikA netavarka
NNS7 sizes	AkAra
PP8 from Wikipedia ( 2015 )	vikipIdiyA se ( 2015)
NP10 Wikipedia ( 2015 )	vikipIdiyA ( 2015)
PRN12 ( 2015 )	( 2015)
NP14 2015	2015

----
00453	1.	 1 .		
452	452
LST1 1 .	1.

----
00454	Adaptive linear element (Widrow and Ho, 1960) 2.	 anukUlI rEKika wawva ( Widrow Ora Hoff , 1960 meM 2		
453	453
S1 Adaptive linear element ( Widrow and Ho , 1960 ) 2 .	anukUla parivarwana wawva (vidro Ora hoYPa, 1960) 2
VP2_LWG Adaptive	anukUla
NP4 linear element ( Widrow and Ho , 1960 ) 2	lAinara wawva (vidro Ora hoYPa, 1960) 2
NP5 linear element ( Widrow and Ho , 1960 )	lAinara wawva (vidro Ora hoYPa, 1960)
PRN8 ( Widrow and Ho , 1960 )	(vidro Ora hoYPa, 1960)
NP10 Widrow and Ho	vidro Ora hoYPa
CC12 and	Ora
NNP11 Widrow	vixro
NNP13 Ho	hoYPa
,14 ,	,
NP15 1960	1960
NP18 2	2

----
00455	Neocognitron (Fukushima, 1980) 3.	 nekognitroYna ( Fukushima , 1980 ) 3 .		
454	454
NP1 Neocognitron ( Fukushima , 1980 ) 3 .	nikoMkSanara (PukuSimA, 1980) 3
NP2 Neocognitron ( Fukushima , 1980 )	nikoMkSanara (PukuSimA, 1980)
PRN4 ( Fukushima , 1980 )	(PukuSimA, 1980)
NP6 Fukushima , 1980	PukuSimA, 1980
,8 ,	,
NP11 3	3

----
00456	GPU-accelerated convolutional network	 jIpIyU - sEksareteda koYnvolyUSanala netavarka		
455	455
NP1 GPU - accelerated convolutional network	jIpIyU- vqxXiSIla netavarka ko gawi xI
NP2 GPU	jIpIyU
NP5 accelerated convolutional network	vqxXiSIla netavarka ko gawi xI

----
00457	(Chellapilla et al., 2006) 4.	 ( chellapillaet al 2006 ) 4 .		
456	456
NP1 ( Chellapilla et al. , 2006 ) 4 .	(celApIlA eta ala., 2006 ) 4.
NP2 ( Chellapilla et al. , 2006 )	(cellapIlA ewa ala., 2006 )
NP4 Chellapilla et al.	cellapilA eta ala
NP5 Chellapilla	cellapilA
ADVP7 et al.	eta ala
,10 ,	,
NP-TMP11 2006	2006
NP14 4	4

----
00458	Deep Boltzmann machine (Salakhutdinov and Hinton, 2009a) 5.	 dIpa boYtasana maSIna (Salakhutdinov Ora Hinton , 2009 ) 5 .		
457	457
NP1 Deep Boltzmann machine ( Salakhutdinov and Hinton , 2009a ) 5 .	xIpa boltjamEna maSIna (sallaKuttenova Ora hiMtana, 2009 e) 5
NP2 Deep Boltzmann machine ( Salakhutdinov and Hinton , 2009a )	xIpa boltjamEna maSIna (sallaKuttenova Ora hiMtana, 2009 e)
NML3 Deep Boltzmann	xIpa boltjamEna
PRN7 ( Salakhutdinov and Hinton , 2009a )	(sallaKuttevina Ora hiMtana, 2009 e)
NP9 Salakhutdinov and Hinton	salaKuttevina Ora hiMtana
CC11 and	Ora
NNP10 Salakhutdinov	salAKZwaxivina
NNP12 Hinton	hiMtana
,13 ,	,
NP14 2009a	2009 e.
NP17 5	5

----
00459	Unsupervised convolutional network (	 anupayogI saMvalayana netavarka (		
458	458
NP1 Unsupervised convolutional network (	aprawyASiwa parivarwanaSIla netavarka (

----
00460	Jarrett et al., 2009) 6.	 jeretta eta ala , 2009 6 .		
459	459
NP1 Jarrett et al. , 2009 ) 6 .	jarAta eta ala, 2009 ) 6
NP2 Jarrett et al. , 2009 )	jarAta eta ala, 2009 )
NP3 Jarrett	jarAta
ADVP5 et al.	eta ala
PRN8 , 2009 )	, 2009 )
,9 ,	,
NP10 2009	2009
NP13 6	6

----
0046	The performance of these simple machine learning algorithms depends heavily on the representation of the data they are given.	 ina sarala maSIna sIKane elgorixama kA praxarSana unheM xie gae detA ke prawiniXiwva para kAPI nirBara karawA hE .		
45	45
S1 The performance of these simple machine learning algorithms depends heavily on the representation of the data they are given .	ina sAXAraNa maSIna sIKane vAle elgorixama kA praxarSana una detA ke prawiniXiwva para BArI nirBara karawA hE jo unheM xiyA jAwA hE
NP2 The performance of these simple machine learning algorithms	ina sAXAraNa maSIna sIKane ke elgorixama kA praxarSana
NP3 The performance	praxarSana
PP6 of these simple machine learning algorithms	ina sAXAraNa maSIna sIKane vAle elgorixama
NP8 these simple machine learning algorithms	ye sAXAraNa maSIna sIKa rahe elgorixama
NML11 machine learning	maSIna sIKanA
NNS14 algorithms	elgorixama
VP15_LWG depends heavily	BArI nirBara karawA hE
ADVP17 heavily	BArI-Barakama
PP19 on the representation of the data	AMkadZoM ke prawiniXiwva para
NP21 the representation of the data	AMkadZoM kA prawiniXiwva
NP22 the representation	prawiniXiwva
PP25 of the data	detA kA
NP27 the data	detA
NNS29 data	detA
SBAR30 they are given	unheM xiyA jAwA hE
S31 they are given	unheM xiyA jAwA hE
NP32 they	ve
VP34_LWG are given	xie jAwe hEM

----
00461	GPU-accelerated multilayer perceptron	 jIpIyU - esIra septoYna		
460	460
NP1 GPU - accelerated multilayer perceptron	jIpIyU- maltIletara ke avaSoRaNa ko weja kiyA
NP2 GPU	jIpIyU
NP5 accelerated multilayer perceptron	maltIletara avaSoRaNa ko weja kiyA

----
00462	(Ciresan et al.	 ( Ciresan eta ala )		
461	461
S1 ( Ciresan et al. .	(siresana eta ala.
VP3_LWG Ciresan et	siresana eta
ADVP5 et al.	eta ala

----
00463	, 2010) 7.	  2010 kA , 7 .		
462	462
FRAG1 , 2010 ) 7 .	, 2010 ) 7.
NP2 , 2010	, 2010
,3 ,	,
NP5 )	)
NP7 7	7

----
00464	Distributed autoencoder (	 viwariwa autoencoder (		
463	463
S1 Distributed autoencoder (	viwariwa oYtonakodara (
INTJ2 Distributed	viwariwa
VP4_LWG autoencoder	oYtonakodara

----
00465	Le et al.	 le eta ala		
464	464
NP1 Le et al. .	le eta ala.
NP2 Le	le
ADVP4 et al.	eta ala

----
00466	, 2012) 8.	 , 2012 kA 8 .		
465	465
FRAG1 , 2012 ) 8 .	, 2012 ) 8.
NP2 , 2012	, 2012
,3 ,	,
NP5 )	)
NP7 8	8

----
00467	Multi-GPU convolutional network (	 maltI - jIpIyU saMvaliwa netavarka (		
466	466
NP1 Multi-GPU convolutional network (	maltI-jIpIyU saMviliyana netavarka (

----
00468	Krizhevsky et al.	 Krizhevsky eta ala .		
467	467
FRAG1 Krizhevsky et al. .	kriJevaskI eta ala.
ADVP2 Krizhevsky	kriJevskI
ADVP4 et al.	eta ala

----
00469	, 2012) 9.	 , 2012 kA 9 .		
468	468
FRAG1 , 2012 ) 9 .	, 2012 ) 9.
NP2 , 2012	, 2012
,3 ,	,
NP5 )	)
NP7 9	9

----
00470	COTS HPC unsupervised convolutional network (	 COTS HPC apariBARiwa saMvaliwa netavarka (		
469	469
FRAG1 COTS HPC unsupervised convolutional network (	sIotIsI ecapIsI aprawyASiwa viviXa netavarka (
NP2 COTS HPC	sIotIsI ecapIsI
NP5 unsupervised convolutional network	aprawyASiwa parivarwanaSIla netavarka
NP9 (	(

----
0047	For example, when logistic regression is used to recommend cesarean delivery, the AI system does not examine the patient directly.	 uxAharaNa ke lie , jaba rasaxa prawigamana sijeriyana prasava kI siPAriSa karane ke lie iswemAla kiyA jAwA hE , eAI praNAlI rogI kI sIXe jAMca nahIM karawA hE .		
46	46
S1 For example , when logistic regression is used to recommend cesarean delivery , the AI system does not examine the patient directly .	uxAharaNa ke lie jaba sIjZariyana dilIvarI kI siPAriSa ke lie loYjistika rispAMsesa kA iswemAla kiyA jAwA hE wo eAI praNAlI sIXe marIja kI jAMca nahIM kara
PP2 For example	uxAharaNa ke lie
NP4 example	uxAharaNa
,6 ,	,
SBAR7 when logistic regression is used to recommend cesarean delivery	jaba sIjZariyana dilIvarI kI siPAriSa ke lie loYjistika prawiSoXa kA iswemAla kiyA jAwA hE
WHADVP8 when	kaba
S10 logistic regression is used to recommend cesarean delivery	sIjZariyana dilIvarI kI siPAriSa ke lie loYjistika prawiSoXa kA iswemAla kiyA jAwA hE
NP11 logistic regression	loYjistika prawiPala
VP14_LWG is used	iswemAla howA hE
S18 to recommend cesarean delivery	sijeriyana dilIvarI kI siPAriSa karane ke lie
VP19_LWG to recommend	siPAriSa karane ke lie
NP23 cesarean delivery	sIjZariyana dilIvarI
,26 ,	,
NP27 the AI system	eAI praNAlI
VP31_LWG does not examine directly	sIXe jAMca nahIM karawA
NP36 the patient	marIja
ADVP39 directly	sIXe wOra para

----
00471	Coates et al., 2013) 10.	 kotsa eta ala , 2013 10 .		
470	470
NP1 Coates et al. , 2013 ) 10 .	koYteja eta ala, 2013 ) 10.
NP2 Coates et al. , 2013 )	koYteja eta ala, 2013 )
NP3 Coates	kota
ADVP5 et al.	eta ala
PRN8 , 2013 )	, 2013 )
,9 ,	,
NP10 2013	2013
NP13 10	10

----
00472	GoogLeNet (Szegedy et al., 2014a)	 goglineta		
471	471
FRAG1 GoogLeNet ( Szegedy et al. , 2014a )	goYgaleneta (sajeMxe eta ala, 2014a)
NP2 GoogLeNet	goYgaleneta
FRAG5 Szegedy et al. , 2014a	sajordI eta ala, 2014a
NP6 Szegedy et al.	sajordI eta ala
ADVP8 et al.	eta ala
,11 ,	,
NP12 2014a	2014 e.

----
00473	1.2.4	 1 . 2 . 4		
472	472
FRAG1 1.2.4	1.2.4
NP2 1.2.4	1.2.4

----
00474	Increasing Accuracy, Complexity and Real-World Impact	 baDZawI pariSuxXawA , jatilawA Ora vAswavika - varlda praBAva		
473	473
S1 Increasing Accuracy , Complexity and Real - World Impact	baDZawI AbakArI, jatilawA Ora vAswavika - viSva praBAva
VP2_LWG Increasing	baDZawI jA rahI hE
NP4 Accuracy , Complexity and Real - World Impact	Aropa, jatilawA Ora vAswavika - viSva praBAva
NML5 Accuracy , Complexity and Real	Aropa, jatilawA Ora vAswavika
,7 ,	,
CC9 and	Ora
NNP8 Complexity	jatilawA
NNP10 Real	asalI
NML12 World Impact	viSva praBAva

----
00475	Since the 1980s, deep learning has consistently improved in its ability to provide accurate recognition and prediction.	 1980 ke xaSaka ke bAxa se , gaharI sIKane lagAwAra satIka mAnyawA Ora BaviRyavANI praxAna karane kI kRamawA meM suXAra kiyA hE .		
474	474
S1 Since the 1980s , deep learning has consistently improved in its ability to provide accurate recognition and prediction .	1980 ke xaSaka se gaharI sIKa meM lagAwAra satIka mAnyawA Ora BaviRyavANI karane kI kRamawA meM suXAra huA hE
PP2 Since the 1980s	1980 ke xaSaka se
NP4 the 1980s	1980 ke xaSaka
NNS6 1980s	1980 ke xaSaka meM
,7 ,	,
NP8 deep learning	gaharI sIKa
VP11_LWG has consistently improved	lagAwAra suXAra huA hE
ADVP13 consistently	lagAwAra
PP17 in its ability to provide accurate recognition and prediction	satIka mAnyawA Ora BaviRyavANI upalabXa karAne kI apanI kRamawA meM
NP19 its ability to provide accurate recognition and prediction	satIka mAnyawA Ora BaviRyavANI upalabXa karAne kI isakI kRamawA
S22 to provide accurate recognition and prediction	satIka mAnyawA Ora BaviRyavANiyAM upalabXa karAne ke lie
VP23_LWG to provide	upalabXa karAne ke lie
NP27 accurate recognition and prediction	satIka mAnyawA Ora BaviRyavANI
CC30 and	Ora
NN29 recognition	mAnyawA
NN31 prediction	BaviRyavANI

----
00476	Moreover, deep learning has consistently been applied with success to broader and broader sets of applications.	 isake alAvA , gaharI sIKane lagAwAra AvexanoM ke vyApaka Ora vyApaka seta ke lie saPalawA ke sAWa lAgU kiyA gayA hE .		
475	475
S1 Moreover , deep learning has consistently been applied with success to broader and broader sets of applications .	isake alAvA, anuprayogoM ke vyApaka Ora vyApaka setoM ke lie saPalawA ke sAWa gaharI sIKa ko lagAwAra lAgU kiyA gayA hE
ADVP2 Moreover	isake alAvA Ora BI
,4 ,	,
NP5 deep learning	gaharI sIKa
VP8_LWG has consistently been applied	lagAwAra lagAyA gayA hE Avexana
ADVP10 consistently	lagAwAra
PP16 with success	saPalawA ke sAWa
NP18 success	saPalawA
PP20 to broader and broader sets of applications	AvexanoM ke vyApaka Ora vyApaka setoM waka
NP22 broader and broader sets of applications	AvexanoM ke vyApaka Ora vyApaka seta
NP23 broader and broader sets	vyApaka Ora vyApaka seta
ADJP24 broader and broader	vyApaka Ora vyApaka
CC26 and	Ora
JJR25 broader	vyApaka
JJR27 broader	vyApaka
NNS28 sets	seta
PP29 of applications	AvexanoM kI
NP31 applications	Avexana
NNS32 applications	Avexana

----
00477	The earliest deep models were used to recognize individual objects in tightly cropped, extremely small images (	 sabase pahale gahare moYdaloM kA upayoga vyakwigawa vaswuoM ko kasakara Pasala meM pahacAnane ke lie kiyA jAwA WA , awyaMwa CotI CaviyoM (		
476	476
S1 The earliest deep models were used to recognize individual objects in tightly cropped , extremely small images (	kasakara KIMcane meM vyakwigawa vaswuoM ko pahacAnane ke lie sabase pahale gahare moYdaloM kA iswemAla kiyA gayA WA, behaxa CotI CaviyoM meM (
NP2 The earliest deep models	sabase pahale gahare moYdala
NNS6 models	moYdala
VP7_LWG were used	iswemAla kiyA jAwA WA
S11 to recognize individual objects in tightly cropped , extremely small images (	kasakara KIMcane meM vyakwigawa vaswuoM ko pahacAnane ke lie, behaxa CotI CaviyAM (
VP12_LWG to recognize	pahacAnane ke lie
NP16 individual objects	vyakwigawa vaswueM
NNS18 objects	vaswueM
PP19 in tightly cropped , extremely small images (	kasakara KIMcane meM, behaxa CotI CaviyAM (
NP21 tightly cropped , extremely small images (	kasakara katA huA, behaxa CotI CaviyAM (
NP22 tightly cropped , extremely small images	kasakara KIMceM, behaxa CotI CaviyAM
ADJP23 tightly cropped , extremely small	kasakara katA huA, behaxa CotA
ADJP24 tightly cropped	kasakara kroYpa kiyA
,27 ,	,
ADJP28 extremely small	behaxa CotA
NNS31 images	CaviyAM

----
00478	Rumelhart et al., 1986a).	 rUmalahArta eta ala , 1986		
477	477
NP1 Rumelhart et al. , 1986a ) .	rUmAlahArta eta ala, 1986a )
NP2 Rumelhart et al.	Rumelhart eta ala
NP3 Rumelhart	rUmAlahArta
ADVP5 et al.	eta ala
,8 ,	,
NP9 1986a	1986a
NNS10 1986a	1986a

----
00479	Since then there has been a gradual increase in the size of images neural networks could process.	 waba se CaviyoM ke AkAra meM XIre XIre vqxXi huI hE waMwrikA netavarka prakriyA kara sakawA hE .		
478	478
S1 Since then there has been a gradual increase in the size of images neural networks could process .	waba se CaviyoM waMwrikA netavarka ke AkAra meM kramabaxXa vqxXi huI hE
SBAR2 Since then there has been a gradual increase in the size of images	waba se CaviyoM ke AkAra meM XIre-XIre ijAPA huA hE
S4 then there has been a gradual increase in the size of images	Pira CaviyoM ke AkAra meM XIre-XIre ijAPA huA hE
ADVP5 then	Pira
NP7 there	vahAM
VP9_LWG has been	rahA hE
NP13 a gradual increase in the size of images	CaviyoM ke AkAra meM XIre-XIre baDZowarI
NP14 a gradual increase	kramika vqxXi
PP18 in the size of images	CaviyoM ke AkAra meM
NP20 the size of images	CaviyoM kA AkAra
NP21 the size	AkAra
PP24 of images	CaviyoM kI
NP26 images	CaviyAM
NNS27 images	CaviyAM
NP28 neural networks	waMwrikA netavarka
NNS30 networks	netavarka
VP31_LWG could process	prakriyA kara sakawe We

----
00480	Modern object recognition networks process rich high-resolution photographs and do not 22 CHAPTER 1.	 AXunika vaswu aBijFAna netavarka ucca - viBexana Poto kI prakriyA karawA hE Ora 22 CHAPTER 1 nahIM karawA hE 		
479	479
S1 Modern object recognition networks process rich high - resolution photographs and do not 22 CHAPTER 1 .	AXunika vaswu mAnyawA netavarka ucca-saMkalpiwa wasvIreM prakriyA karawe hEM Ora 22 cEptara 1 nahIM karawe hEM
NP2 Modern	AXunika
VP4_LWG object	vaswu
S6 recognition networks process rich high - resolution photographs and do not 22 CHAPTER 1	mAnyawA netavarka prakriyA ucca - saMkalpiwa wasvIreM Ora 22 aXyAya 1 nahIM
NP7 recognition networks	mAnyawA netavarka
NNS9 networks	netavarka
VP10_LWG process do	prakriyA karawe hEM
NP13 rich high - resolution photographs	amIra ucca-rijolyUSana wasvIreM
NML15 high - resolution	ucca - saMkalpa
NNS19 photographs	wasvIreM
CC20 and	Ora
VP11 process rich high - resolution photographs	prakriyA ucca-rijolyUSana wasvIreM
VP21 do not 22 CHAPTER 1	22 cEptara 1 na kareM
NP23 not 22 CHAPTER	22 cEptara nahIM
NP27 1	1

----
0048	Instead, the doctor tells the system several pieces of relevant information, such as the presence or absence of a uterine scar.	 isake bajAya , cikiwsaka isa praNAlI ko prAsaMgika jAnakArI ke kaI tukadZe bawAwA hE , jEse garBASaya ke niSAna kI upasWiwi yA anupasWiwi .		
47	47
S1 Instead , the doctor tells the system several pieces of relevant information , such as the presence or absence of a uterine scar .	isake bajAya doYktara saMbaMXiwa sUcanAoM ke kaI tukadZe bawAwe hEM, jEse ki upasWiwi yA anAdZI niSAna kI anupasWiwi
ADVP2 Instead	isake bajAya
,4 ,	,
NP5 the doctor	doYktara
VP8_LWG tells	bawAwA hE
NP10 the system	sistama
NP13 several pieces of relevant information , such as the presence or absence of a uterine scar	prAsaMgika sUcanAoM ke kaI tukadZe, jEse ki AnuvaMSika niSAna kI mOjUxagI yA anupasWiwi
NP14 several pieces	kaI tukadZe
NNS16 pieces	tukadZe
PP17 of relevant information	prAsaMgika jAnakArI kA
NP19 relevant information	prAsaMgika jAnakArI
,22 ,	,
PP23 such as the presence or absence of a uterine scar	jEse ki AnuvaMSika niSAna kI mOjUxagI yA anupasWiwi
NP26 the presence or absence of a uterine scar	AnuvaMSika niSAna kI upasWiwi yA anupasWiwi
NP27 the presence or absence	upasWiwi yA anupasWiwi
CC30 or	yA
NN29 presence	mOjUxagI
NN31 absence	anupasWiwi
PP32 of a uterine scar	eka garBASaya niSAna kI
NP34 a uterine scar	eka garBASaya niSAna

----
00481	INTRODUCTION 1950 1985 2000 2015 2056 Year 10 2 10 1	 INTRODUCOR 1950 1985 2000 2015 2056 varRa 10 2 10 1		
480	480
FRAG1 INTRODUCTION 1950198520002015 2056 Year 10  2 10  1	uxGAtana 1950 1985 2000 2015 2056 varRa 10 2 10 1
NP2 INTRODUCTION	paricaya
NP4 1950198520002015 2056	1950 1985 2000 2015 2048
NP7 Year	sAla-ba-ba-ba-ba-ba-ba-ba-ba.
NP9 10  2	10-2
SYM11 	+
NP13 10  1	10- 1
SYM15 	+

----
00482	10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 10 10	 10 0 10 1 10 2 10 10 3 10 4 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10		
481	481
S1 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 10 10	10 0 10 1 10 1 10 4 10 4 10 6 10 6 10 7 10 8 10 9 10 10
NP2 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 10 10	10 0 10 1 10 1 10 4 10 4 10 6 10 6 10 7 10 8 10 9 10 10

----
00483	10 11 Number of neurons (logarithmic scale)	 10 11 nyUroYnsa kI saMKyA (logarithmic pEmAne para		
482	482
FRAG1 10 11 Number of neurons ( logarithmic scale )	10 11 saMKyA meM nyUrona (lognemika skela)
NP2 10	10
PP4 11 Number of neurons	11 saMKyA meM nyUrona
NP5 11 Number	11 naMbara
NP9 neurons	nyUrona
NNS10 neurons	nyUrona
NP12 logarithmic scale	loYgaretamika skela

----
00484	1	 1		
483	483
LST1 1	1

----
00485	2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sponge Roundworm Leech Ant Bee Frog Octopus Human Figure 1.11: Increasing neural network size over time.	 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 spaMja golakqmi lIca cIMtI 20 20 20 spaMja oYktopasa mAnava ciwra 111 baDZawI waMwrikA netavarka AkAra samaya ke sAWa		
484	484
NP1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sponge Roundworm Leech Ant Bee Frog Octopus Human Figure 1.11 : Increasing neural network size over time .	2 3 4 5 6 7 8 9 10 11 12 13 14 14 17 17 18 19 20 spaMja rAuMdavarma leca eMtI bI Proga oYktopasa mAnavIya AMkadZA 1.11 - samaya ke sAWa waMwra
NP2 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sponge Roundworm Leech Ant Bee Frog Octopus Human Figure 1.11	2 3 4 5 6 7 8 9 10 11 12 13 14 14 17 17 18 19 20 spaMja rAuMdavarma leca eMtI bI Proga oYktopasa mAnava AMkadZA 1.16
NP3 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20	2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
NP23 Sponge Roundworm Leech Ant Bee Frog Octopus Human	spaMja rAuMdavarma leca eMtI bI Proga oYktopasa mAnava
NP32 Figure 1.11	AMkadZA  1.10
NP36 Increasing neural network size over time	samaya ke sAWa waMwrikA netavarka kA baDZawA AkAra
NP37 Increasing neural network size	baDZawI waMwrikA netavarka kA AkAra
NML39 neural network	waMwrikA netavarka
PP43 over time	samaya ke sAWa
NP45 time	samaya

----
00486	Since the introduction of hidden units, articial neural networks have doubled in size roughly every 2.4 years.	 CupI ikAiyoM ke SurU hone ke bAxa se , kqwrima waMwrikA netavarka kA AkAra mote wOra para hara 2 . 4 varRa meM xogunA ho gayA hE 		
485	485
S1 Since the introduction of hidden units , articial neural networks have doubled in size roughly every 2.4 years .	CipI huI ikAiyoM ke paricaya ke bAxa se kqwrima waMwrikA netavarka hara 2.4 sAla meM badZe pEmAne para xogunA ho gayA hE
PP2 Since the introduction of hidden units	cUMki CipI huI ikAiyoM kA paricaya
NP4 the introduction of hidden units	CipI huI ikAiyoM kA paricaya
NP5 the introduction	paricaya
PP8 of hidden units	CipI huI ikAiyoM kI
NP10 hidden units	CipI huI ikAiyAM
NNS12 units	ikAiyAM
,13 ,	,
NP14 articial neural networks	kqwrima waMwrikA netavarka
NNS17 networks	netavarka
VP18_LWG have doubled	xogunA kara cuke hEM
PP22 in size	AkAra meM
NP24 size	AkAra
NP-TMP26 roughly every 2.4 years	mote wOra para hara 2.4 sAla
QP27 roughly every 2.4	mote wOra para hara 2.4
NNS31 years	sAla-ba-ba-ba-ba-ba-ba-ba.

----
00487	Biological neural network sizes from Wikipedia (2015).	 jEvika waMwrikA netavarka kA AkAra vikipIdiyA se (2015 I . )		
486	486
NP1 Biological neural network sizes from Wikipedia ( 2015 ) .	vikipIdiyA ( 2015) se jEvika waMwrikA netavarka kA AkAra
NP2 Biological neural network sizes	jEvika waMwrikA netavarka ke AkAra
NML3 Biological neural network	jEvika waMwrikA netavarka
NNS7 sizes	AkAra
PP8 from Wikipedia ( 2015 )	vikipIdiyA se ( 2015)
NP10 Wikipedia ( 2015 )	vikipIdiyA ( 2015)
PRN12 ( 2015 )	( 2015)
NP14 2015	2015

----
00488	1.	 1 .		
487	487
LST1 1 .	1.

----
00489	Perceptron (Rosenblatt, 1958, 1962) 2.	 parseptoYna (Rosenblatt , 1958 , 1962 ) 2		
488	488
NP1 Perceptron ( Rosenblatt , 1958 , 1962 ) 2 .	paraseptaroYna (rosenableta, 1958, 1962) 2
NP2 Perceptron ( Rosenblatt , 1958 , 1962 )	paraseptaroYna (rosenableta, 1958, 1962)
PRN4 ( Rosenblatt , 1958 , 1962 )	(rosenableta, 1958, 1962)
NP6 Rosenblatt	rosenableta
,8 ,	,
NP9 1958 , 1962	1958, 1962
,11 ,	,
NP14 2	2

----
00490	Adaptive linear element (Widrow and Ho, 1960) 3.	 anukUlI rEKika wawva ( Widrow Ora Hoff , 1960 meM 3		
489	489
S1 Adaptive linear element ( Widrow and Ho , 1960 ) 3 .	anukUla parivarwana wawva (vidro Ora hoYPa, 1960) 3
VP2_LWG Adaptive	anukUla
NP4 linear element ( Widrow and Ho , 1960 ) 3	lAinara wawva (vidro Ora hoYPa, 1960) 3
NP5 linear element ( Widrow and Ho , 1960 )	lAinara wawva (vidro Ora hoYPa, 1960)
PRN8 ( Widrow and Ho , 1960 )	(vidro Ora hoYPa, 1960)
NP10 Widrow and Ho	vidro Ora hoYPa
CC12 and	Ora
NNP11 Widrow	vixro
NNP13 Ho	hoYPa
,14 ,	,
NP15 1960	1960
NP18 3	3

----
0049	Each piece of information included in the representation of the patient is known as a feature .	 rogI ke prawiniXiwva meM SAmila jAnakArI ke prawyeka tukadZA eka viSeRawA ke rUpa meM jAnA jAwA hE .		
48	48
S1 Each piece of information included in the representation of the patient is known as a feature .	rogI ke prawiniXiwva meM SAmila prawyeka sUcanA kA tukadZA eka viSeRawA ke rUpa meM jAnA jAwA hE
NP2 Each piece of information included in the representation of the patient	rogI ke prawiniXiwva meM SAmila prawyeka sUcanA kA tukadZA
NP3 Each piece	prawyeka tukadZA
PP6 of information included in the representation of the patient	marIja ke prawiniXiwva meM SAmila jAnakArI
NP8 information included in the representation of the patient	marIja ke prawiniXiwva meM SAmila jAnakArI
NP9 information	jAnakArI
VP11_LWG included	SAmila
PP13 in the representation of the patient	rogI ke prawiniXiwva meM
NP15 the representation of the patient	rogI kA prawiniXiwva
NP16 the representation	prawiniXiwva
PP19 of the patient	marIja kA
NP21 the patient	marIja
VP24_LWG is known	jAnA jAwA hE
PP28 as a feature	eka viSeRawA ke rUpa meM
NP30 a feature	eka viSeRawA

----
00491	Neocognitron (Fukushima, 1980) 4.	 nekognitroYna ( Fukushima , 1980 ) 4 .		
490	490
NP1 Neocognitron ( Fukushima , 1980 ) 4 .	nikoMkSanara (PukuSimA, 1980) 4
NP2 Neocognitron ( Fukushima , 1980 )	nikoMkSanara (PukuSimA, 1980)
PRN4 ( Fukushima , 1980 )	(PukuSimA, 1980)
NP6 Fukushima , 1980	PukuSimA, 1980
,8 ,	,
NP11 4	4

----
00492	Early back-propagation network (Rumelhart et al., 1986b) 5.	 AraMBika bEka - apa netavarka ( Rumelhart eta ala , 1986b 5 .		
491	491
FRAG1 Early back - propagation network ( Rumelhart et al. , 1986b ) 5 .	prAraMBika rUpa se- pracAra netavarka ( Rumelhart eta ala., 1986b) 5.
ADVP2 Early back	jalxI vApasa
,5 -	-
NP6 propagation network ( Rumelhart et al. , 1986b ) 5	pracAra netavarka (rumAlahArta eta ala., 1986b) 5
NP7 propagation network ( Rumelhart et al. , 1986b )	pracAra netavarka (rumAlahArta eta ala., 1986b)
PRN10 ( Rumelhart et al. , 1986b )	(rumAlahArta eta ala., 1986b)
NP12 Rumelhart et al. , 1986b	rUmAlahArta eta ala., 1986b
NP13 Rumelhart et al.	Rumelhart eta ala
NP14 Rumelhart	rUmAlahArta
ADVP16 et al.	eta ala
,19 ,	,
NP20 1986b	1986b
NNS21 1986b	1986b
NP23 5	5

----
00493	Recurrent neural network for speech recognition (Robinson and Fallside, 1991)	 BARaNa mAnyawA ke lie samavarwI waMwrikA netavarka ( robinasana Ora PAlasAida , 1991		
492	492
NP1 Recurrent neural network for speech recognition ( Robinson and Fallside , 1991 )	BARaNa mAnyawA ke lie varwamAna waMwrikA netavarka (roYbinsana Ora PoYlasAida, 1991)
NP2 Recurrent neural network	varwamAna waMwrikA netavarka
PP6 for speech recognition ( Robinson and Fallside , 1991 )	BARaNa mAnyawA ke lie (roYbinsana Ora PoYlasAida, 1991)
NP8 speech recognition ( Robinson and Fallside , 1991 )	BARaNa mAnyawA (roYbinsana Ora PoYlasAida, 1991)
PRN11 ( Robinson and Fallside , 1991 )	(roYbinsana Ora PoYlasAida, 1991)
NP13 Robinson and Fallside	roYbinsana Ora PoYlasAida
CC15 and	Ora
NNP14 Robinson	roYbinsana
NNP16 Fallside	PoYlasAida
,17 ,	,
NP18 1991	1991

----
00494	6.	 6 .		
493	493
NP1 6 .	6.

----
00495	Multilayer perceptron for speech recognition (Bengio et al.	 BARaNa mAnyawA ke lie maltIleyara parseptoYna ( Bengio eta ala )		
494	494
NP1 Multilayer perceptron for speech recognition ( Bengio et al. .	BARaNa mAnyawA ke lie bahuswarIya avaXAraNA (beMgiyo eta ala.
NP2 Multilayer perceptron for speech recognition	BARaNa mAnyawA ke lie bahuswarIya avaSoRaka
NP3 Multilayer perceptron	bahuAyAmI avaXAraNA
PP6 for speech recognition	BARaNa mAnyawA ke lie
NP8 speech recognition	BARaNa mAnyawA
NP12 Bengio et al.	beMjiyo eta ala
NP13 Bengio	beMgIo
ADVP15 et al.	eta ala

----
00496	, 1991) 7.	  1991 kA , 7 .		
495	495
NP1 , 1991 ) 7 .	, 1991 ) 7.
NP3 1991	1991
NP5 )	)
NP7 7	7

----
00497	Mean eld sigmoid belief network (Saul et al., 1996)	 arWa PIlda avagrahiwa viSvAsa netavarka (Saul eta ala , 1996 )		
496	496
S1 Mean eld sigmoid belief network ( Saul et al. , 1996 )	mIna PIlda sigamoxa mAnyawA netavarka (SEla eta ala, 1996)
NP2 Mean	mawalaba
VP4_LWG eld	Kewa
NP6 sigmoid belief network ( Saul et al. , 1996 )	Ahamoxa mAnyawA netavarka (SOela eta ala, 1996)
PRN10 ( Saul et al. , 1996 )	(SOela eta ala, 1996)
NP12 Saul et al. , 1996	SAUla eta ala., 1996
NP13 Saul et al.	SAUla eta ala
NP14 Saul	SAUla
ADVP16 et al.	eta ala
,19 ,	,
NP20 1996	1996

----
00498	8.	 8 .		
497	497
NP1 8 .	8.

----
00499	LeNet-5	 leneta - 5		
498	498
FRAG1 LeNet - 5	leneta- 5
NP2 LeNet	leneta
NP5 5	5

----
00500	(LeCun et al.	 ( LeCun et al . )		
499	499
S1 ( LeCun et al. .	(lekana eta ala.
VP3_LWG LeCun et	lekana eta
ADVP5 et al.	eta ala

----
0050	Logistic regression learns how each of these features of the patient correlates with various outcomes.	 loYjistika prawigamana sIKawA hE ki kEse rogI kI ina viSeRawAoM meM se prawyeka viBinna pariNAmoM ke sAWa sahasaMbaMXiwa karawA hE .		
49	49
S1 Logistic regression learns how each of these features of the patient correlates with various outcomes .	loYjistika ripreSana se pawA calawA hE ki rogI ke inameM se prawyeka viSeRawAeM viBinna pariNAmoM se kEse saMbaMXiwa howI hEM
NP2 Logistic regression	loYjistika prawiPala
VP5_LWG learns	sIKawA hE
SBAR7 how each of these features of the patient correlates with various outcomes	kEse viBinna pariNAmoM se saMbaMXiwa howA hE rogI kI inameM se prawyeka viSeRawAeM
WHADVP8 how	kEse
S10 each of these features of the patient correlates with various outcomes	rogI ke inameM se prawyeka viSeRawAoM kA viBinna pariNAmoM se saMbaMXiwa
NP11 each of these features of the patient	marIja ke inameM se prawyeka viSeRawAeM
NP12 each	prawyeka
PP14 of these features of the patient	marIja ke ina PIcarsa kI
NP16 these features of the patient	marIja ke ye PIcarsa
NP17 these features	ye PIcarsa
NNS19 features	PIcarsa
PP20 of the patient	marIja kA
NP22 the patient	marIja
VP25_LWG correlates	saMbaMXiwa
PP27 with various outcomes	viBinna pariNAmoM se
NP29 various outcomes	viBinna pariNAma
NNS31 outcomes	pariNAma

----
00501	, 1998b) 9.	  1998 , 9 .		
500	500
FRAG1 , 1998b ) 9 .	, 1998b ) 9.
NP2 , 1998b	, 1998b
,3 ,	,
NP5 )	)
NP7 9	9

----
00502	Echo state network (Jaeger and Haas, 2004)	 iko steta netavarka (Jeger and Haas , 2004 )		
501	501
FRAG1 Echo state network ( Jaeger and Haas , 2004 )	iko rAjya netavarka (jEgara Ora hAsa, 2004)
NP2 Echo state network	iko rAjya netavarka
FRAG7 Jaeger and Haas , 2004	jEgara Ora hAsa, 2004
NP8 Jaeger and Haas	jEgara Ora hAsa
CC10 and	Ora
NNP9 Jaeger	jayagara
NNP11 Haas	hAsa
,12 ,	,
NP13 2004	2004

----
00503	10.	 10 .		
502	502
NP1 10 .	10.

----
00504	Deep belief network (Hinton et al., 2006) 11.	 gaharA viSvAsa netavarka		
503	503
FRAG1 Deep belief network ( Hinton et al. , 2006 ) 11 .	gaharI mAnyawA netavarka (hiMtana eta ala, 2006) 11.
FRAG2 Deep belief network ( Hinton et al. , 2006 )	gaharI mAnyawA netavarka (hiMtana eta ala, 2006)
NP3 Deep belief network	gaharA viSvAsa netavarka
NP8 Hinton et al. , 2006	hiMtana eta ala., 2006
NP9 Hinton et al.	hiMtana eta ala
NP10 Hinton	hiMtana
ADVP12 et al.	eta ala
,15 ,	,
NP-TMP16 2006	2006
NP19 11	11

----
00505	GPU-accelerated convolutional network	 jIpIyU - sEksareteda koYnvolyUSanala netavarka		
504	504
NP1 GPU - accelerated convolutional network	jIpIyU- vqxXiSIla netavarka ko gawi xI
NP2 GPU	jIpIyU
NP5 accelerated convolutional network	vqxXiSIla netavarka ko gawi xI

----
00506	(Chellapilla et al., 2006) 12.	 (Chellapilla eta ala , 2006 12 .		
505	505
NP1 ( Chellapilla et al. , 2006 ) 12 .	(celApIlA eta ala., 2006 ) 12.
NP2 ( Chellapilla et al. , 2006 )	(cellapIlA ewa ala., 2006 )
NP4 Chellapilla et al.	cellapilA eta ala
NP5 Chellapilla	cellapilA
ADVP7 et al.	eta ala
,10 ,	,
NP-TMP11 2006	2006
NP14 12	12

----
00507	Deep Boltzmann machine (Salakhutdinov and Hinton, 2009a) 13.	 dIpa bloYtjamEna maSIna ( Salakhutdinov Ora hEnatana , 2009 ) 13 .		
506	506
NP1 Deep Boltzmann machine ( Salakhutdinov and Hinton , 2009a ) 13 .	xIpa boltjamEna maSIna (sallaKuttenova Ora hiMtana, 2009 e) 13
NP2 Deep Boltzmann machine ( Salakhutdinov and Hinton , 2009a )	xIpa boltjamEna maSIna (sallaKuttenova Ora hiMtana, 2009 e)
NML3 Deep Boltzmann	xIpa boltjamEna
PRN7 ( Salakhutdinov and Hinton , 2009a )	(sallaKuttevina Ora hiMtana, 2009 e)
NP9 Salakhutdinov and Hinton	salaKuttevina Ora hiMtana
CC11 and	Ora
NNP10 Salakhutdinov	salAKZwaxivina
NNP12 Hinton	hiMtana
,13 ,	,
NP14 2009a	2009 e.
NP17 13	13

----
00508	GPU-accelerated deep belief network (	 GPU - PEsalAwI gaharA viSvAsa netavarka (		
507	507
NP1 GPU - accelerated deep belief network (	jIpIyU- gaharI mAnyawA netavarka ko gawi xI (
NML2 GPU - accelerated	jIpIyU- weja
NML6 deep belief	gaharA viSvAsa

----
00509	Raina et al.	 rEnA eta ala .		
508	508
NP1 Raina et al. .	rEnA eta ala.
NP2 Raina	rEnA
ADVP4 et al.	eta ala

----
00510	, 2009) 14.	 , 2009 kA 14		
509	509
FRAG1 , 2009 ) 14 .	, 2009 ) 14.
NP2 , 2009	, 2009
,3 ,	,
NP5 )	)
NP7 14	14

----
0051	However, it cannot inuence how features are dened in any way.	 hAlAMki , yaha praBAva nahIM kara sakawA ki lakRaNoM ko kisI BI waraha pariBARiwa kiyA jAwA hE .		
50	50
S1 However , it can not inuence how features are dened in any way .	hAlAMki, yaha praBAva nahIM dAla sakawA ki viSeRawAeM kisI BI waraha se pariBARiwa kI jAwI hEM
ADVP2 However	hAlAMki
,4 ,	,
NP5 it	yaha
VP7_LWG can not inuence	praBAva nahIM dAla sakawe
SBAR12 how features are dened in any way	kiwane PIcarsa ko kisI BI waraha se pariBARiwa kiyA jAwA hE
WHADVP13 how	kEse
S15 features are dened in any way	PIcarsa ko kisI BI waraha se pariBARiwa kiyA jAwA hE
NP16 features	PIcarsa
NNS17 features	PIcarsa
VP18_LWG are dened	pariBARiwa hEM
PP22 in any way	kisI BI waraha se
NP24 any way	kisI BI waraha se

----
00511	Unsupervised convolutional network (	 anupayogI saMvalayana netavarka (		
510	510
NP1 Unsupervised convolutional network (	aprawyASiwa parivarwanaSIla netavarka (

----
00512	Jarrett et al., 2009) 15.	 jreta eta ala , 2009 15 .		
511	511
NP1 Jarrett et al. , 2009 ) 15 .	jarAta eta ala, 2009 ) 15
NP2 Jarrett et al. , 2009 )	jarAta eta ala, 2009 )
NP3 Jarrett	jarAta
ADVP5 et al.	eta ala
PRN8 , 2009 )	, 2009 )
,9 ,	,
NP10 2009	2009
NP13 15	15

----
00513	GPU-accelerated multilayer perceptron	 jIpIyU - esIra septoYna		
512	512
NP1 GPU - accelerated multilayer perceptron	jIpIyU- maltIletara ke avaSoRaNa ko weja kiyA
NP2 GPU	jIpIyU
NP5 accelerated multilayer perceptron	maltIletara avaSoRaNa ko weja kiyA

----
00514	(Ciresan et al., 2010)	 ( kristana eta ala , 2010 )		
513	513
FRAG1 ( Ciresan et al. , 2010 )	(siresana eta ala, 2010)
NP3 Ciresan et al.	siresana eta ala
ADVP5 et al.	eta ala
,8 ,	,
NP9 2010	2010

----
00515	16.	 16 .		
514	514
NP1 16 .	16.

----
00516	OMP-1 network (Coates and Ng, 2011) 17.	 oemapI - 1 netavarka ( koYtsa eMda enajI ) , 2011 17 .		
515	515
NP1 OMP - 1 network ( Coates and Ng , 2011 ) 17 .	oemapI- 1 netavarka (kota Ora enajI, 2011) 17
NP2 OMP - 1 network ( Coates and Ng , 2011 )	oemapI- 1 netavarka (kota Ora enajI, 2011)
NML3 OMP - 1	oemapI- 1
PRN8 ( Coates and Ng , 2011 )	(kota Ora enajI, 2011)
NP10 Coates and Ng	kota Ora enajI
CC12 and	Ora
NNP11 Coates	kota
NNP13 Ng	enajI
,14 ,	,
NP15 2011	2011
NP18 17	17

----
00517	Distributed autoencoder (	 viwariwa autoencoder (		
516	516
S1 Distributed autoencoder (	viwariwa oYtonakodara (
INTJ2 Distributed	viwariwa
VP4_LWG autoencoder	oYtonakodara

----
00518	Le et al., 2012) 18.	 leaI eta ala , 2012 18 .		
517	517
NP1 Le et al. , 2012 ) 18 .	le eta ala, 2012 ) 18
NP2 Le et al. , 2012 )	le eta ala, 2012 )
NP3 Le	le
ADVP5 et al.	eta ala
PRN8 , 2012 )	, 2012 )
,9 ,	,
NP10 2012	2012
NP13 18	18

----
00519	Multi-GPU convolutional network (	 maltI - jIpIyU saMvaliwa netavarka (		
518	518
NP1 Multi-GPU convolutional network (	maltI-jIpIyU saMviliyana netavarka (

----
00520	Krizhevsky et al.	 Krizhevsky eta ala .		
519	519
FRAG1 Krizhevsky et al. .	kriJevaskI eta ala.
ADVP2 Krizhevsky	kriJevskI
ADVP4 et al.	eta ala

----
0052	If logistic regression were given an MRI scan of the patient, rather than the doctors formalized report, it would not be able to make useful predictions.	 yaxi rogI ko loYjistika rIgreSana emaAraAI skEna xiyA jAwA , bajAya cikiwsaka kI OpacArika riporta ke , wo vaha upayogI BaviRyavANiyAM nahIM kara sakegA 		
51	51
S1 If logistic regression were given an MRI scan of the patient , rather than the doctor s formalized report , it would not be able to make useful predictions .	yaxi loYjistika ripreSana ko marIja kA emaAraAI skEna xiyA gayA, wo doYktara kI PoYrmUlI riporta ke bajAya yaha upayogI BaviRyavANiyAM nahIM kara pAegA
SBAR2 If logistic regression were given an MRI scan of the patient , rather than the doctor s formalized report	yaxi loYjistika ripreSana ko marIja kA emaAraAI skEna xiyA gayA, wo doYktara kI PoYrmUlI riporta ke bajAya
S4 logistic regression were given an MRI scan of the patient , rather than the doctor s formalized report	loYjistika ripreSana ko marIja kA emaAraAI skEna xiyA gayA WA, balki doYktara kI PoYrmUlI riporta
NP5 logistic regression	loYjistika prawiPala
VP8_LWG were given	xie gae We
NP12 an MRI	eka emaAraAI
NP15 scan of the patient , rather than the doctor s formalized report	doYktara kI PoYrmUlI riporta ke bajAya rogI kA skEna,
NP16 scan of the patient	marIja kA skEna
NP17 scan	skEna kareM
PP19 of the patient	marIja kA
NP21 the patient	marIja
,24 ,	,
CONJP25 rather than	balki isase jyAxA
NP28 the doctor s formalized report	doYktara kI PoYrmUlI riporta
NML30 doctor s	doYktara kI
,35 ,	,
NP36 it	yaha
VP38_LWG would not be	nahIM howA
ADJP43 able to make useful predictions	upayogI BaviRyavANiyAM karane meM sakRama
S45 to make useful predictions	upayogI BaviRyavANiyAM karane ke lie
VP46_LWG to make	banAne ke lie
NP50 useful predictions	upayogI BaviRyavANiyAM
NNS52 predictions	BaviRyavANiyAM

----
00521	, 2012) 19.	 , 2012 kA 19		
520	520
FRAG1 , 2012 ) 19 .	, 2012 ) 19.
NP2 , 2012	, 2012
,3 ,	,
NP5 )	)
NP7 19	19

----
00522	COTS HPC unsupervised convolutional network (	 COTS HPC apariBARiwa saMvaliwa netavarka (		
521	521
FRAG1 COTS HPC unsupervised convolutional network (	sIotIsI ecapIsI aprawyASiwa viviXa netavarka (
NP2 COTS HPC	sIotIsI ecapIsI
NP5 unsupervised convolutional network	aprawyASiwa parivarwanaSIla netavarka
NP9 (	(

----
00523	Coates et al., 2013) 20.	 kAitsa eta ala , 2013 20		
522	522
NP1 Coates et al. , 2013 ) 20 .	koYteja eta ala, 2013 ) 20
NP2 Coates et al. , 2013 )	koYteja eta ala, 2013 )
NP3 Coates	kota
ADVP5 et al.	eta ala
PRN8 , 2013 )	, 2013 )
,9 ,	,
NP10 2013	2013
NP13 20	20

----
00524	GoogLeNet (Szegedy et al., 2014a) have a requirement that the photo be cropped near the object to be recognized (Krizhevsky et al., 2012).	 gUgala - neta		
523	523
S1 GoogLeNet ( Szegedy et al. , 2014a ) have a requirement that the photo be cropped near the object to be recognized ( Krizhevsky et al. , 2012 ) .	goYgaleneta (sajordI eta ala., 2014a) kI eka AvaSyakawA hE ki mAnyawA prApwa hone vAlI vaswu ke pAsa Poto KiMcavAyA jAe (kriJveskI eta
NP2 GoogLeNet ( Szegedy et al. , 2014a )	goYgaleneta (sajeMxe eta ala, 2014a)
NP3 GoogLeNet ( Szegedy	goYgaleneta (sajeMxesI)
ADVP7 et al.	eta ala
PRN10 , 2014a )	, 2014 e )
,11 ,	,
NP12 2014a	2014 e.
VP15_LWG have	pAsa hE
NP17 a requirement	eka AvaSyakawA
SBAR20 that the photo be cropped near the object to be recognized ( Krizhevsky et al. , 2012 )	ki mAnyawA prApwa hone vAlI vaswu ke pAsa Poto ko kroYpa kiyA jAe (kriJevaskI eta ala., 2012 )
S22 the photo be cropped near the object to be recognized ( Krizhevsky et al. , 2012 )	mAnyawA prApwa hone vAlI vaswu ke pAsa Poto KiMcavAI jAe (kriJevaskI eta ala., 2012 )
NP23 the photo	Poto
VP26_LWG be cropped	kroYpa honA cAhie
PP30 near the object	vaswu ke pAsa
NP32 the object	vaswu
S35 to be recognized	mAnyawA prApwa honI hE
VP36_LWG to be recognized	mAnyawA prApwa honI hE
PRN42 ( Krizhevsky et al. , 2012 )	(kriJevaskI eta ala., 2012 )
ADVP44 Krizhevsky et al.	kriJevaskI eta ala
ADVP45 Krizhevsky	kriJevskI
,49 ,	,
NP50 2012	2012

----
00525	Similarly, the earliest networks could recognize only two kinds of objects (or in some cases, the absence or presence of a single kind of object), while these modern networks typically recognize at least 1,000 dierent categories of objects.	 isI prakAra , Axi netavarka kevala xo prakAra kI vaswuoM ko pahacAna sakawe We ( jEse ki kuCa mAmaloM meM , eka ekala prakAra kI vaswu kI upasWiwi yA anupasWiwi , jabaki ye AXunika netavarka AmawOra para vaswuoM kI kama se kama 1 , 000 viBinna SreNiyoM ko pahacAnawe hEM .		
524	524
S1 Similarly , the earliest networks could recognize only two kinds of objects ( or in some cases , the absence or presence of a single kind of object ) , while these modern networks typically recognize at least 1,000 dierent categories of objects .	isI waraha sabase pahale netavarka kevala xo prakAra kI vaswuoM ko pahacAna sakawe We (yA kuCa mAmaloM meM, eka hI waraha kI vaswuoM kI anupasWiwi yA upasWiwi)
ADVP2 Similarly	isI waraha
,4 ,	,
NP5 the earliest networks	sabase pahale netavarka
NNS8 networks	netavarka
VP9_LWG could recognize	pahacAna sakawe We
NP13 only two kinds of objects ( or in some cases , the absence or presence of a single kind of object )	kevala xo prakAra kI vaswueM (yA kuCa mAmaloM meM, eka hI waraha kI vaswu kI anupasWiwi yA upasWiwi)
NP14 only two kinds	kevala xo waraha ke
QP15 only two	kevala xo hI
NNS18 kinds	waraha-waraha kI waraha
PP19 of objects ( or in some cases , the absence or presence of a single kind of object )	vaswuoM kI (yA kuCa mAmaloM meM, eka hI waraha kI vaswu kI anupasWiwi yA upasWiwi)
NP21 objects ( or in some cases , the absence or presence of a single kind of object )	vaswueM (yA kuCa mAmaloM meM, eka hI waraha kI vaswu kI anupasWiwi yA upasWiwi)
NP22 objects	vaswueM
NNS23 objects	vaswueM
CC25 LRB	LRB
-LRB-24 (	(
NP26 in some cases , the absence or presence of a single kind of object	kuCa mAmaloM meM eka hI waraha kI vaswu kI anupasWiwi yA upasWiwi
PP27 in some cases	kuCa mAmaloM meM
NP29 some cases	kuCa mAmale
NNS31 cases	mAmale
,32 ,	,
NP33 the absence or presence	anupasWiwi yA upasWiwi
CC36 or	yA
NN35 absence	anupasWiwi
NN37 presence	mOjUxagI
PP38 of a single kind of object	eka hI waraha kI vaswu kA
NP40 a single kind of object	eka hI waraha kI vaswu
NP41 a single kind	eka hI waraha kA
PP45 of object	vaswu kA
NP47 object	vaswu
,50 ,	,
SBAR51 while these modern networks typically recognize at least 1,000 dierent categories of objects	jabaki ye AXunika netavarka AmawOra para vaswuoM kI kama se kama 1,000 alaga-alaga SreNiyoM ko pahacAnawe hEM
S53 these modern networks typically recognize at least 1,000 dierent categories of objects	ye AXunika netavarka AmawOra para vaswuoM kI kama se kama 1,000 alaga-alaga SreNiyoM ko pahacAnawe hEM
NP54 these modern networks	ye AXunika netavarka
NNS57 networks	netavarka
ADVP58 typically	AmawOra para
VP60_LWG recognize	pahacAna
NP62 at least 1,000 dierent categories of objects	vaswuoM kI kama se kama 1,000 alaga-alaga SreNiyAM
NP63 at least 1,000 dierent categories	kama se kama 1,000 alaga-alaga SreNiyoM
QP64 at least 1,000	kama se kama 1,000
ADVP65 at least	kama se kama
NNS70 categories	SreNiyoM
PP71 of objects	vaswuoM kA
NP73 objects	vaswueM
NNS74 objects	vaswueM

----
00526	The largest contest in object recognition is the ImageNet 23 CHAPTER 1.	 vaswu mAnyawA meM sabase badZA mukAbalA imejaneta 23 CHAPTER 1 hE 		
525	525
S1 The largest contest in object recognition is the ImageNet 23 CHAPTER 1 .	vaswu mAnyawA meM sabase badZA mukAbalA hE imejaneta 23 ceptara 1
NP2 The largest contest in object recognition	vaswu mAnyawA meM sabase badZA mukAbalA
NP3 The largest contest	sabase badZA mukAbalA
PP7 in object recognition	vaswu mAnyawA meM
NP9 object recognition	vaswu mAnyawA
VP12_LWG is	hE
NP14 the ImageNet 23 CHAPTER 1	imejaneta 23 ceptara 1
NML16 ImageNet 23	imejaneta 23

----
00527	INTRODUCTION	 INTRODUCCR		
526	526
NP1 INTRODUCTION	paricaya

----
00528	Large Scale Visual Recognition Challenge (ILSVRC) held each year.	 badZe pEmAne para xqSya aBijFAna cEleMja (AIelaesavIArasIsI ) prawyeka varRa Ayojiwa kiyA jAwA hE 		
527	527
S1 Large Scale Visual Recognition Challenge ( ILSVRC ) held each year .	badZe pEmAne para xqSya pahacAna cEleMja (AIelaesavIArasI) prawyeka varRa Ayojiwa kI gaI
NP2 Large Scale Visual Recognition Challenge ( ILSVRC )	badZe pEmAne para xqSya pahacAna cEleMja (AIelaesavIArasI)
NML3 Large Scale	badZe pEmAne para
NP10 ILSVRC	AIelaesavIArasI
VP13_LWG held	Ayojiwa
NP-TMP15 each year	prawyeka varRa

----
00529	A dramatic moment in the meteoric rise of deep learning came when a convolutional network won this challenge for the rst time and by a wide margin, bringing down the state-of-the-art top-5 error rate from 26.1 percent to 15.3 percent (Krizhevsky et al., 2012), meaning that the convolutional network produces a ranked list of possible categories for each image, and the correct category appeared in the rst ve entries of this list for all but 15.3 percent of the test examples.	 gahare sIKane ke mOsamI uwWAna meM eka nAtakIya kRaNa AyA jaba eka saMvalayana netavarka pahalI bAra isa cunOwI ko jIwA Ora eka vyApaka mArjina ke xvArA , nIce lAne ke lie rAjya ke SIrRa - 5 wruti xara 261 prawiSawa se 153 prawiSawa ( kerijowa sUcI )		
528	528
S1 A dramatic moment in the meteoric rise of deep learning came when a convolutional network won this challenge for the rst time and by a wide margin , bringing down the state - of - the - art top - 5 error rate from 26.1 percent to 15.3 percent ( Krizhevsky et al. , 2012 ) , meaning that the convolutional network produces a ranked list of possible categories for each image , and the correct category appeared in the rst ve entries of this list for all but 15.3 percent of the test examples .	gaharI sIKa ke ulkAlika uxaya meM eka nAtakIya kRaNa usa samaya AyA jaba eka saMviliyana netavarka ne pahalI bAra isa cunOwI ko jIwa liyA Ora vyApaka mArjina se rAjya
S2 A dramatic moment in the meteoric rise of deep learning came when a convolutional network won this challenge for the rst time and by a wide margin , bringing down the state - of - the - art top - 5 error rate from 26.1 percent to 15.3 percent ( Krizhevsky et al. , 2012 ) , meaning that the convolutional network produces a ranked list of possible categories for each image	gaharI sIKa ke ulkAlika uxaya meM eka nAtakIya pala usa samaya AyA jaba eka saMviliyana netavarka ne pahalI bAra isa cunOwI ko jIwa liyA Ora vyApaka mArjina se rAjya
NP3 A dramatic moment in the meteoric rise of deep learning	gaharI sIKa ke ulkApAwa meM eka nAtakIya pala
NP4 A dramatic moment	eka nAtakIya pala
PP8 in the meteoric rise of deep learning	gaharI sIKa meM ulkApiMda kI uCAla
NP10 the meteoric rise of deep learning	gaharI sIKa kA ulkApiMda uxaya
NP11 the meteoric rise	ulkApAwa kA uxaya
PP15 of deep learning	gaharI sIKa kA
NP17 deep learning	gaharI sIKa
VP20_LWG came	AyA WA
SBAR22 when a convolutional network won this challenge for the rst time and by a wide margin , bringing down the state - of - the - art top - 5 error rate from 26.1 percent to 15.3 percent ( Krizhevsky et al. , 2012 ) , meaning that the convolutional network produces a ranked list of possible categories for each image	jaba eka saMviliyana netavarka ne pahalI bAra isa cunOwI ko jIwa liyA Ora vyApaka aMwara se rAjya ko nIce lAwe hue -kA-kA-Arta toYpa- 5 wruti
WHADVP23 when	kaba
S25 a convolutional network won this challenge for the rst time and by a wide margin , bringing down the state - of - the - art top - 5 error rate from 26.1 percent to 15.3 percent ( Krizhevsky et al. , 2012 ) , meaning that the convolutional network produces a ranked list of possible categories for each image	eka saMviliyana netavarka ne pahalI bAra isa cunOwI ko jIwa liyA Ora vyApaka mArjina xvArA rAjya ko nIce lAwe hue  - kalA SIrRa - 5 wruti xara 26.1 prawiSawa
NP26 a convolutional network	eka saMviliyana netavarka
VP30_LWG won	jIwa gae
NP32 this challenge	yaha cunOwI
PP35 for the rst time and by a wide margin	pahalI bAra Ora cOdZA aMwara se
PP36 for the rst time	pahalI bAra
NP38 the rst time	pahalI bAra
CC42 and	Ora
PP43 by a wide margin	vyApaka aMwara se
NP45 a wide margin	vyApaka aMwara
,49 ,	,
S50 bringing down the state - of - the - art top - 5 error rate from 26.1 percent to 15.3 percent ( Krizhevsky et al. , 2012 ) , meaning that the convolutional network produces a ranked list of possible categories for each image	rAjya ko nIce lAwe hue - of- kalA SIrRa - 5 wruti xara 26.1 prawiSawa se 15.3 prawiSawa (kriJveskI eta ala., 2012), arWAwa prawyeka Cavi ke
VP51_LWG bringing down meaning	arWa lAnA
PRT54 down	nIce
NP56 the state - of - the - art top - 5 error rate	rAjya - kA - - kalA SIrRa - 5 wruti xara
ADJP58 state - of - the - art	rAjya - kA - - kalA
NML66 top - 5	SIrRa - 5
PP72 from 26.1 percent	26.1 prawiSawa se
NP74 26.1 percent	26.1 prawiSawa
PP77 to 15.3 percent ( Krizhevsky et al. , 2012 )	15.3 prawiSawa waka (kriJevaskI eta ala., 2012 )
NP79 15.3 percent ( Krizhevsky et al. , 2012 )	15.3 prawiSawa (kriJevaskI eta ala., 2012 )
PRN82 ( Krizhevsky et al. , 2012 )	(kriJevaskI eta ala., 2012 )
NP84 Krizhevsky et al. , 2012	kriJevaskI eta ala., 2012
NP85 Krizhevsky et al.	kriJevaskI eta ala
NP86 Krizhevsky	kriJevskI
ADVP88 et al.	eta ala
,91 ,	,
NP92 2012	2012
,95 ,	,
SBAR98 that the convolutional network produces a ranked list of possible categories for each image	yaha saMviliyana netavarka prawyeka Cavi ke lie saMBAviwa SreNiyoM kI eka rEMkiwa sUcI kA uwpAxana karawA hE
S100 the convolutional network produces a ranked list of possible categories for each image	viBAjanakArI netavarka prawyeka Cavi ke lie saMBAviwa SreNiyoM kI eka rEMkiwa sUcI kA uwpAxana karawA hE
NP101 the convolutional network	viBAjanakArI netavarka
VP105_LWG produces	uwpAxana karawA hE
NP107 a ranked list of possible categories for each image	prawyeka Cavi ke lie saMBAviwa SreNiyoM kI eka rEMkiwa sUcI
NP108 a ranked list	eka rEMkda lista
PP112 of possible categories for each image	prawyeka Cavi ke lie saMBAviwa SreNiyoM kI
NP114 possible categories for each image	prawyeka Cavi ke lie saMBAviwa SreNiyAM
NP115 possible categories	saMBAviwa SreNiyoM
NNS117 categories	SreNiyoM
PP118 for each image	prawyeka Cavi ke lie
NP120 each image	prawyeka Cavi
,123 ,	,
CC124 and	Ora
S125 the correct category appeared in the rst ve entries of this list for all but 15.3 percent of the test examples	saBI ke lie isa sUcI kI pahalI pAMca meM se sahI SreNI xiKAI xI lekina parIkRA uxAharaNa ke 15.3 prawiSawa
NP126 the correct category	sahI SreNI
VP130_LWG appeared	xiKAI xiyA
PP132 in the rst ve entries of this list	isa sUcI kI pahalI pAMca meM SAmila
NP134 the rst ve entries of this list	isa sUcI kI pahalI pAMca praviRti
NP135 the rst ve entries	pahalI pAMca praviRti
NNS139 entries	praviRti
PP140 of this list	isa sUcI kI
NP142 this list	yaha lista
PP145 for all but 15.3 percent of the test examples	saBI ke lie lekina 15.3 prawiSawa parIkRaNa uxAharaNa
NP147 all but 15.3 percent of the test examples	saBI lekina parIkRA uxAharaNa kA 15.3 prawiSawa
NP148 all but 15.3 percent	saBI lekina 15.3 prawiSawa
QP149 all but 15.3	saBI lekina 15.3
CC151 but	lekina
DT150 all	saBI
CD152 15.3	15.3
PP154 of the test examples	parIkRA uxAharaNoM kI
NP156 the test examples	parIkRA uxAharaNa
NNS159 examples	uxAharaNa

----
00530	Since then, these competitions are consistently won by deep convolutional nets, and as of this writing, advances in deep learning have brought the latest top-5 error rate in this contest down to 3.6 percent, as shown in gure 1.12.	 waba se , ina prawiyogiwAoM ko lagAwAra gahare saMvalana jAloM xvArA jIwA jAwA hE , Ora isa leKana ke rUpa meM , gaharI sIKane meM agrima isa prawiyogiwA meM navInawama SIrRa - 5 wruti xara ko 3 . 6 prawiSawa waka le AyA hE , jEsA ki aMka 112 meM xiKAyA gayA hE .		
529	529
S1 Since then , these competitions are consistently won by deep convolutional nets , and as of this writing , advances in deep learning have brought the latest top - 5 error rate in this contest down to 3.6 percent , as shown in gure 1.12 .	waba se lagAwAra ina prawiyogiwAoM ko gahare saMviliyana neta xvArA jIwa milawI hE, Ora isa leKa ke rUpa meM gaharI sIKa meM pragawi isa prawiyogiwA meM letesta toYpa
SBAR2 Since then , these competitions are consistently won by deep convolutional nets , and as of this writing	waba se hI ina prawiyogiwAoM ko lagAwAra gahare saMviliyana netsa se jIwa milawI hE, Ora isa leKa ke rUpa meM
S4 then , these competitions are consistently won by deep convolutional nets , and as of this writing	Pira, ina prawiyogiwAoM ko lagAwAra gahare saMviliyana netsa se jIwa milawI hE, Ora isa leKa ke rUpa meM
ADVP5 then	Pira
,7 ,	,
NP8 these competitions	ina prawiyogiwAoM
NNS10 competitions	prawiyogiwAoM
VP11_LWG are consistently won	lagAwAra howI hEM jIwa
ADVP13 consistently	lagAwAra
PP17 by deep convolutional nets , and as of this writing	gahare saMviliyana neta xvArA, Ora isa leKana ke rUpa meM
PP18 by deep convolutional nets	gahare saMviliyana jAla se
NP20 deep convolutional nets	gaharA saMviliyana jAla
NNS23 nets	jAla
,24 ,	,
CC25 and	Ora
RB26 as	jEsA ki
PP27 of this writing	isa leKana kA
NP29 this writing	yaha leKana
,32 ,	,
NP33 advances in deep learning	gaharI sIKa meM unnawi
NP34 advances	edavAMsa
NNS35 advances	edavAMsa
PP36 in deep learning	gaharI sIKa meM
NP38 deep learning	gaharI sIKa
VP41_LWG have brought down	nIce lAyA hE
NP45 the latest top - 5 error rate	navInawama SIrRa - 5 wruti xara
NML48 top - 5	SIrRa - 5
PP54 in this contest	isa prawiyogiwA meM
NP56 this contest	yaha prawiyogiwA
ADVP59 down to 3.6 percent	nIce 3.6 prawiSawa waka
PP61 to 3.6 percent	3.6 prawiSawa waka
NP63 3.6 percent	3.6 PIsaxI
,66 ,	,
SBAR67 as shown in gure 1.12	jEsA ki AMkadZe 1.12 meM xiKAyA gayA
S69 shown in gure 1.12	AMkadZe meM xiKAyA gayA 1.12
VP70_LWG shown	xiKAyA gayA
PP72 in gure 1.12	AMkadZe 1.12
NP74 gure 1.12	AMkadZA 1.12

----
0053	Individual pixels in an MRI scan have negligible correlation with any complications that might occur during delivery.	 emaAraAI skEna meM vyakwigawa piksala kA prasava ke xOrAna hone vAlI kisI BI jatilawAoM ke sAWa nagaNya sahasaMbaMXa howA hE 		
52	52
S1 Individual pixels in an MRI scan have negligible correlation with any complications that might occur during delivery .	emaAraAI skEna meM vyakwigawa piksela kisI BI jatilawAoM se saMbaMXiwa howA hE jo dilIvarI ke xOrAna ho sakawI hE
NP2 Individual pixels in an MRI scan	emaAraAI skEna meM vyakwigawa piksala
NP3 Individual pixels	vyakwigawa piksela
NNS5 pixels	piksela
PP6 in an MRI scan	emaAraAI skEna meM
NP8 an MRI scan	eka emaAraAI skEna
VP12_LWG have	pAsa hE
NP14 negligible correlation with any complications that might occur during delivery	dilIvarI ke xOrAna ho sakawI hE kisI jatilawAoM se lAparavAhI kA saMbaMXa
NP15 negligible correlation	yogya saMbaMXa
PP18 with any complications that might occur during delivery	kisI BI jatilawAoM ke sAWa jo dilIvarI ke xOrAna ho sakawI hE
NP20 any complications that might occur during delivery	dilIvarI ke xOrAna ho sakawI hE koI jatilawAeM
NP21 any complications	koI jatilawAeM
NNS23 complications	jatilawAeM
SBAR24 that might occur during delivery	dilIvarI ke xOrAna ho sakawA hE EsA
WHNP25 that	vaha
S27 might occur during delivery	dilIvarI ke xOrAna ho sakawA hE
VP28_LWG might occur	ho sakawA hE
PP32 during delivery	dilIvarI ke xOrAna
NP34 delivery	dilIvarI

----
00531	Deep learning has also had a dramatic impact on speech recognition.	 gaharI SikRA kA BI BARaNa mAnyawA para nAtakIya praBAva padZA hE 		
530	530
S1 Deep learning has also had a dramatic impact on speech recognition .	BARaNa mAnyawA para gaharI sIKa kA BI nAtakIya asara padZA hE
NP2 Deep learning	gaharI sIKa
VP5_LWG has also had	usake pAsa BI hE
ADVP7 also	sAWa hI
NP11 a dramatic impact on speech recognition	BARaNa mAnyawA para nAtakIya praBAva
NP12 a dramatic impact	nAtakIya praBAva
PP16 on speech recognition	BARaNa mAnyawA para
NP18 speech recognition	BARaNa mAnyawA

----
00532	After improving throughout the 1990s, the error rates for speech recognition stagnated starting in about 2000.	 1990 ke xaSaka Bara meM suXAra ke bAxa , BARaNa mAnyawA ke lie wruti xaroM lagaBaga 2000 meM SurU ho gayA .		
531	531
S1 After improving throughout the 1990s , the error rates for speech recognition stagnated starting in about 2000 .	1990 ke xaSaka meM suXAra ke bAxa lagaBaga 2000 meM BARaNa mAnyawA ke lie wruti xareM SurU ho gaIM
PP2 After improving throughout the 1990s	pUre 1990 ke xaSaka meM suXAra ke bAxa
S4 improving throughout the 1990s	pUre 1990 ke xaSaka meM suXAra
VP5_LWG improving	suXAra
PP7 throughout the 1990s	pUre 1990 ke xaSaka meM
NP9 the 1990s	1990 ke xaSaka
NNS11 1990s	1990 ke xaSaka
,12 ,	,
NP13 the error rates for speech recognition	spIca mAnyawA ke lie wruti kI xareM
NP14 the error rates	wruti kI xareM
NNS17 rates	xareM
PP18 for speech recognition	BARaNa mAnyawA ke lie
NP20 speech recognition	BARaNa mAnyawA
VP23_LWG stagnated	Tapa
S25 starting in about 2000	lagaBaga 2000 meM SurU
VP26_LWG starting	SuruAwa
PP28 in about 2000	lagaBaga 2000 meM
NP30 about 2000	lagaBaga 2000
QP31 about 2000	lagaBaga 2000

----
00533	The introduction of deep learning (Dahl et al., 2010; Deng et al., 2010b; Seide et al., 2011; Hinton et al., 2012a) to speech recognition resulted in a sudden drop in error rates, with some error rates cut in half.	 dIdahla eta ala 2010 dIdahla dIMga eta ala dIMga ewata , 2010 dIdaba Eta ala sIda , 2011 ecanatana ke sAWa kuCa bolane kI anumawi xene ke kAraNa acAnaka galawI xaroM meM katOwI kI gaI 		
532	532
S1 The introduction of deep learning ( Dahl et al. , 2010 ; Deng et al. , 2010b ; Seide et al. , 2011 ; Hinton et al. , 2012a ) to speech recognition resulted in a sudden drop in error rates , with some error rates cut in half .	gaharI sIKa kA paricaya (dAha eta ala., 2010 ; xeMga eta ala., 2010bI ; sida eta ala., 2011 ; BARaNa mAnyawA ke lie acAnaka gira
NP2 The introduction of deep learning ( Dahl et al. , 2010 ; Deng et al. , 2010b ; Seide et al. , 2011 ; Hinton et al. , 2012a ) to speech recognition	gaharI sIKa kA paricaya (dAha eta ala., 2010 ; xeMga eta ala., 2010bI ; sijeda eta ala., 2011 ; BARaNa mAnyawA ke lie hiMtana
NP3 The introduction	paricaya
PP6 of deep learning ( Dahl et al. , 2010 ; Deng et al. , 2010b ; Seide et al. , 2011 ; Hinton et al. , 2012a ) to speech recognition	gaharI sIKa (dAha eta ala., 2010 ; deMga eta ala., 2010 bI ; sida eta ala., 2011 ; BARaNa mAnyawA ke lie hiMtana eta ala.
NP8 deep learning ( Dahl et al. , 2010 ; Deng et al. , 2010b ; Seide et al. , 2011 ; Hinton et al. , 2012a ) to speech recognition	gaharI sIKa (dAha eta ala., 2010 ; xeMga eta ala., 2010 bI ; sida eta ala., 2011 ; BARaNa mAnyawA ke lie hiMtana eta ala.
NP9 deep learning ( Dahl et al. , 2010 ; Deng et al. , 2010b ; Seide et al. , 2011 ; Hinton et al. , 2012a )	gaharI sIKa (dAha eta ala., 2010 ; deMga eta ala., 2010bI ; sida eta ala., 2011 ; hiMtana eta ala., 2012a )
PRN12 ( Dahl et al. , 2010 ; Deng et al. , 2010b ; Seide et al. , 2011 ; Hinton et al. , 2012a )	(dAha eta ala., 2010 ; deMga eta ala., 2010bI ; sida eta ala., 2011 ; hiMtana eta ala., 2012a )
FRAG14 Dahl et al. , 2010	dAhala eta ala., 2010
NP15 Dahl et al.	dAhala eta ala
ADVP17 et al.	eta ala
,20 ,	,
NP21 2010	2010
FRAG24 Deng et al. , 2010b	xeMga eta ala, 2010b
NP25 Deng et al.	xeMga eta ala
ADVP27 et al.	eta ala
,30 ,	,
NP31 2010b	2010b
FRAG34 Seide et al. , 2011	sieda eta ala, 2011
NP35 Seide et al.	sieda eta ala
ADVP37 et al.	eta ala
,40 ,	,
NP41 2011	2011
FRAG44 Hinton et al. , 2012a	hiMtana eta ala., 2012a
NP45 Hinton et al.	hiMtana eta ala
ADVP47 et al.	eta ala
,50 ,	,
NP51 2012a	2012 e.
PP54 to speech recognition	BARaNa mAnyawA ke lie
NP56 speech recognition	BARaNa mAnyawA
VP59_LWG resulted	pariNAma
PP61 in a sudden drop	acAnaka droYpa meM
NP63 a sudden drop	acAnaka AI giraPwArI
PP67 in error rates	wruti xaroM meM
NP69 error rates	wruti kI xareM
NNS71 rates	xareM
,72 ,	,
PP73 with some error rates cut in half	AXe meM katOwI ke sAWa kuCa wruti xareM
NP75 some error rates cut in half	kuCa wruti xaroM meM AXA katOwI
NP76 some error rates	kuCa wruti xareM
NNS79 rates	xareM
VP80_LWG cut	katOwI
PP82 in half	AXe meM
NP84 half	AXA

----
00534	We explore this history in more detail in section 12.3.	 hama isa iwihAsa ko aXika viswAra se xeKeM anuBAga 12 . 3 meM 		
533	533
S1 We explore this history in more detail in section 12.3 .	hama XArA 12.3 meM isa iwihAsa kA aXika viswAra se pawA lagAwe hEM
NP2 We	hama
VP4_LWG explore	walASeM
NP6 this history	yaha iwihAsa
PP9 in more detail in section 12.3	XArA 12.3 meM aXika viswAra se
NP11 more detail in section 12.3	XArA 12.3 meM aXika vivaraNa
NP12 more detail	aXika viswAra
PP15 in section 12.3	XArA 12.3 meM
NP17 section 12.3	XArA 12.3

----
00535	Deep networks have also had spectacular successes for pedestrian detection and image segmentation	 dIpa netavarkoM ko pExala yAwrI pahacAna Ora Cavi viBAjana ke lie BI SAnaxAra saPalawA milI hE 		
534	534
S1 Deep networks have also had spectacular successes for pedestrian detection and image segmentation	pExala yAwrI ke pawA lagAne Ora Cavi saMvarXana ke lie BI gahare netavarka kI SAnaxAra saPalawAeM milI hEM
NP2 Deep networks	gahare netavarka
NNS4 networks	netavarka
VP5_LWG have also had	usake pAsa BI hE
ADVP7 also	sAWa hI
NP11 spectacular successes for pedestrian detection and image segmentation	pExala yAwrI ke pawA lagAne Ora Cavi saMvarxXana ke lie SAnaxAra saPalawAeM
NP12 spectacular successes	SAnaxAra saPalawAeM
NNS14 successes	saPalawAeM
PP15 for pedestrian detection and image segmentation	pExala yAwrI ke pawA lagAne Ora Cavi saMvarXana ke lie
NP17 pedestrian detection and image segmentation	pExala yAwrI kA pawA lagAnA Ora Cavi saMvarXana
NML19 detection and image	pawA lagAnA Ora Cavi
CC21 and	Ora
NN20 detection	pawA lagAnA
NN22 image	Cavi

----
00536	(Sermanet et al., 2013; Farabet et al., 2013; Couprie et al., 2013) and yielded superhuman performance in trac sign classication (	 ala - namAjZe kZurESa 2013 ) ke mahIne meM humAyuM ke gutaKe meM gutaKe meM gutaKe meM gutaKe meM gutaKe kA vyApAra		
535	535
NP1 ( Sermanet et al. , 2013 ; Farabet et al. , 2013 ; Couprie et al. , 2013 ) and yielded superhuman performance in trac sign classication (	(sarbeta eta ala., 2013 ; PErAbewa eta ala., 2013 ; kUparI eta ala., 2013 ) Ora trEPika sAina vargIkaraNa meM suparahamana pra
NP3 Sermanet et al. , 2013	sarmaneta eta ala., 2013
NP4 Sermanet et al.	sarmaneta eta ala
NP5 Sermanet	Sermanet
ADVP7 et al.	eta ala
,10 ,	,
NP11 2013	2013
NP14 Farabet et al.	ParAbewa eta ala
NP15 Farabet	PEraveta
ADVP17 et al.	eta ala
,20 ,	,
NP21 2013 ; Couprie et al. , 2013 ) and yielded superhuman performance in trac sign classication	2013 ; trEPika sAina vargIkaraNa meM kUparI eta ala., 2013 ) Ora upalabXa karAyA gayA suparahamana praxarSana
NP22 2013	2013
NP25 Couprie et al. , 2013 ) and yielded superhuman performance in trac sign classication	kUparI eta ala., 2013 ) Ora trEPika sAina klAseSana meM hue suparahyUmana praxarSana
NP26 Couprie et al. , 2013 )	kUparI eta ala, 2013 )
NP27 Couprie	kUparI
ADVP29 et al.	eta ala
PRN32 , 2013 )	, 2013 )
,33 ,	,
NP34 2013	2013
CC37 and	Ora
NP38 yielded superhuman performance in trac sign classication	yAwAyAwa cinha vargIkaraNa meM upalabXa kiyA gayA suparahyUmana praxarSana
NP39 yielded superhuman performance	upalabXa karAyA suparahyUmana praxarSana
PP43 in trac sign classication	trEPika sAina klAseSana meM
NP45 trac sign classication	yAwAyAwa cinha vargIkaraNa

----
00537	Ciresan et al., 2012).	 kAyarsana eta ala , 2012		
536	536
NP1 Ciresan et al. , 2012 ) .	siresana eta ala, 2012 )
NP2 Ciresan et al.	siresana eta ala
NP3 Ciresan	sIrena
ADVP5 et al.	eta ala
,8 ,	,
NP9 2012	2012

----
00538	At the same time that the scale and accuracy of deep networks have increased, 2010 2011 2012 2013 2014 2015 Year 0.00 0.05 0.10 0.15	 sAWa hI gahare netavarkoM kA pEmAnA Ora satIkawA baDZI hE , 2010 2011 2012 2014 2015 varRa 0 . 0 .05 0 .10 0 .15		
537	537
FRAG1 At the same time that the scale and accuracy of deep networks have increased , 2010201120122013 2014 2015 Year 0.00 0.05 0.10 0.15	vahIM gahare netavarka kA pEmAnA Ora satIkawA baDZa gaI hE, 2010 2011 2012 2013 2014 2015 varRa 0Sca000 0
S2 At the same time that the scale and accuracy of deep networks have increased , 2010201120122013 2014 2015	vahIM gahare netavarka kA pEmAnA Ora satIkawA baDZI hE, 2010 2011 2012 2013 2014 2015 2015
PP3 At the same time that the scale and accuracy of deep networks have increased , 2010201120122013 2014 2015	vahIM gahare netavarka kA pEmAnA Ora satIkawA baDZI hE, 2010 2011 2012 2013 2014 2015 2015
NP5 the same time that the scale and accuracy of deep networks have increased , 2010201120122013 2014 2015	vahIM gahare netavarka kA pEmAnA Ora satIkawA baDZI hE, 2010 2011 2012 2013 2014 2015 2015
NP6 the same time that the scale and accuracy of deep networks have increased	vahIM gahare netavarka kA pEmAnA Ora satIkawA baDZI
NP7 the same time	vahI samaya
SBAR11 that the scale and accuracy of deep networks have increased	ki gahare netavarka kA pEmAnA Ora satIkawA baDZI hE
S13 the scale and accuracy of deep networks have increased	gahare netavarka kA pEmAnA Ora satIkawA baDZI hE
NP14 the scale and accuracy of deep networks	gahare netavarka kA pEmAnA Ora satIkawA
NP15 the scale and accuracy	pEmAnA Ora satIkawA
CC18 and	Ora
NN17 scale	pEmAnA
NN19 accuracy	satIkawA
PP20 of deep networks	gahare netavarkoM kI
NP22 deep networks	gahare netavarka
NNS24 networks	netavarka
VP25_LWG have increased	baDZa gaI hE
,29 ,	,
NP30 2010201120122013 2014 2015	2010 2011 2012 2013 2014 2015
NP34 Year	sAla-ba-ba-ba-ba-ba-ba-ba-ba.
NP36 0.00 0.05 0.10 0.15	0.0.0.0.0.

----
00539	0.20 0.25 0.30 ILSVRC classication error rate Figure 1.12: Decreasing error rate over time.	 0 .20 025 0 .30 ILSVRC vargIkaraNa wruti xara ciwra 1 . 12 : samaya ke sAWa wruti xara meM kamI		
538	538
NP1 0.20 0.25 0.30 ILSVRC classication error rate Figure 1.12 : Decreasing error rate over time .	0.20 0.2 0.30 AIelaesavIArasI klAseSana wruti xara 1.12 H samaya ke sAWa kama ho rahI wruti xara
NP2 0.20 0.25 0.30 ILSVRC classication error rate Figure 1.12	0.20 0.2 0.30 AIelaesavIArasI klAseSana wruti xara 1.20
NML3 0.20 0.25 0.30	0.20 0 0.30
NP14 Decreasing error rate over time	samaya ke sAWa kama ho rahI wruti xara
NP15 Decreasing error rate	GatawI wruti xara
PP19 over time	samaya ke sAWa
NP21 time	samaya

----
00540	Since deep networks reached the scale necessary to compete in the ImageNet Large Scale Visual Recognition Challenge, they have consistently won the competition every year, yielding lower and lower error rates each time.	 cUMki gahare netavarka imejaneta badZe pEmAne para xqSya mAnyawA cEleMja meM prawisparXA karane ke lie AvaSyaka pEmAne para pahuMce , ve lagAwAra hara sAla prawiyogiwA jIwa liyA hE , hara bAra kama Ora kama wruti xaroM kA uwpAxana .		
539	539
S1 Since deep networks reached the scale necessary to compete in the ImageNet Large Scale Visual Recognition Challenge , they have consistently won the competition every year , yielding lower and lower error rates each time .	cUMki gahare netavarka imejaneta badZe pEmAne para vijuala rikoYrdiMga cEleMja meM prawisparXA karane ke lie AvaSyaka pEmAne para pahuMca gae hEM, isa
SBAR2 Since deep networks reached the scale necessary to compete in the ImageNet Large Scale Visual Recognition Challenge	cUMki gahare netavarka imejaneta badZe pEmAne para vijuala rikArdiMga cEleMja meM prawisparXA karane ke lie AvaSyaka pEmAne para pahuMca gae hEM
S4 deep networks reached the scale necessary to compete in the ImageNet Large Scale Visual Recognition Challenge	imejaneta badZe pEmAne para vijuala rikArdiMga cEleMja meM prawisparXA ke lie jarUrI pEmAne para pahuMce gahare netavarka
NP5 deep networks	gahare netavarka
NNS7 networks	netavarka
VP8_LWG reached	pahuMca gae
NP10 the scale	pEmAnA
S13 necessary to compete in the ImageNet Large Scale Visual Recognition Challenge	imejaneta badZe pEmAne para xqSya pahacAna cEleMja meM prawisparXA ke lie AvaSyaka
ADJP14 necessary to compete in the ImageNet Large Scale Visual Recognition Challenge	imejaneta badZe pEmAne para xqSya pahacAna cEleMja meM prawisparXA ke lie AvaSyaka
S16 to compete in the ImageNet Large Scale Visual Recognition Challenge	imejaneta badZe pEmAne para xqSya pahacAna cEleMja meM prawisparXA karane ke lie
VP17_LWG to compete	mukAbalA karane ke lie
PP21 in the ImageNet Large Scale Visual Recognition Challenge	imejaneta badZe pEmAne para xqSya pahacAna cEleMja meM
NP23 the ImageNet Large Scale Visual Recognition Challenge	imejaneta badZe pEmAne para xqSya pahacAna cEleMja
NML26 Large Scale	badZe pEmAne para
,32 ,	,
NP33 they	ve
VP35_LWG have consistently won	lagAwAra jIwa cuke hEM
ADVP37 consistently	lagAwAra
NP41 the competition	prawiyogiwA
NP-TMP44 every year	hara sAla
,47 ,	,
S48 yielding lower and lower error rates each time	hara bAra kama Ora kama wruti kI xareM kama
VP49_LWG yielding	nOkarI karanA
NP51 lower and lower error rates	kama Ora kama wruti xareM
ADJP52 lower and lower	kama Ora nimna
CC54 and	Ora
JJR53 lower	nicale
JJR55 lower	nicale
NNS57 rates	xareM
NP-TMP58 each time	hara bAra

----
0054	This dependence on representations is a general phenomenon that appears throughout computer science and even daily life.	 prawiniXiwvoM para yaha nirBarawA eka sAmAnya GatanA hE jo pUre kaMpyUtara vijFAna Ora yahAM waka ki xEnika jIvana meM prakata howI hE .		
53	53
S1 This dependence on representations is a general phenomenon that appears throughout computer science and even daily life .	prawiniXiwvoM para yaha nirBarawA eka sAmAnya GatanA hE jo pUre kaMpyUtara vijFAna Ora yahAM waka ki xEnika jIvana meM xiKAI xewA hE
NP2 This dependence on representations	prawiniXiwva para yaha nirBarawA
NP3 This dependence	yaha nirBarawA
PP6 on representations	prawiniXiwva para
NP8 representations	prawiniXiwva
NNS9 representations	prawiniXiwva
VP10_LWG is	hE
NP12 a general phenomenon that appears throughout computer science and even daily life	eka sAmAnya GatanA jo pUre kaMpyUtara vijFAna Ora yahAM waka ki xEnika jIvana meM xiKAI xewA hE
NP13 a general phenomenon	eka sAmAnya GatanA
SBAR17 that appears throughout computer science and even daily life	jo pUre kaMpyUtara sAiMsa Ora yahAM waka ki xEnika jIvana meM xiKAI xewA hE
WHNP18 that	vaha
S20 appears throughout computer science and even daily life	pUre kaMpyUtara sAiMsa Ora yahAM waka ki xEnika jIvana meM xiKAI xewA hE
VP21_LWG appears	xiKAI xewA hE
PP23 throughout computer science and even daily life	pUre kaMpyUtara sAiMsa Ora yahAM waka ki xEnika jIvana
NP25 computer science and even daily life	kaMpyUtara sAiMsa Ora yahAM waka ki xEnika jIvana
NP26 computer science	kampyUtara sAiMsa
CC29 and	Ora
NP30 even daily life	yahAM waka ki xEnika jIvana

----
00541	Data from Russakovsky et al.	 rasaskovaskI eta ala se detA		
540	540
NP1 Data from Russakovsky et al. .	rUsAkovaskI eta ala se detA
NP2 Data	detA
PP4 from Russakovsky	rUsAkovaskI se
NP6 Russakovsky	rUsAkovaskI
ADVP8 et al.	eta ala

----
00542	(2014b)	 ( 2014 )		
541	541
NP1 ( 2014b )	( 2014b)

----
00543	and He et al.	 Ora vaha eta ala .		
542	542
FRAG1 and He et al. .	Ora vaha eta ala 

----
00544	(2015).	 (2015 )		
543	543
FRAG1 ( 2015 ) .	( 2015).
NP3 2015	2015

----
00545	24 CHAPTER 1.	 24 CHAPTER 1 .		
544	544
FRAG1 24 CHAPTER 1 .	24 cEptara 1.
NP2 24	24
NP4 CHAPTER 1	cEptara 1

----
00546	INTRODUCTION so has the complexity of the tasks that they can solve.	 INTRODUCASA meM una kAryoM kI jatilawA hE jinheM ve hala kara sakawe hEM .		
545	545
S1 INTRODUCTION so has the complexity of the tasks that they can solve .	paricaya Ese meM una kAryoM kI jatilawA howI hE jinheM ve hala kara sakawe hEM
NP2 INTRODUCTION	paricaya
ADVP4 so	Ese meM
VP6_LWG has	usake pAsa hE
NP8 the complexity of the tasks that they can solve	jina kAryoM kA samAXAna kara sakawe hEM usakI jatilawA
NP9 the complexity	jatilawA
PP12 of the tasks that they can solve	jina kAryoM kA samAXAna kara sakawe hEM vo
NP14 the tasks that they can solve	jina kAryoM ko hala kara sakawe hEM vo
NP15 the tasks	kAryoM
NNS17 tasks	kAryoM
SBAR18 that they can solve	ki ve hala kara sakawe hEM
S20 they can solve	ve hala kara sakawe hEM
NP21 they	ve
VP23_LWG can solve	hala kara sakawe hEM

----
00547	Goodfellow et al.	 gudaPelo eta ala .		
546	546
NP1 Goodfellow et al. .	gudaPoYla eta ala.
NP2 Goodfellow	gudaPoYla
ADVP4 et al.	eta ala

----
00548	(2014d) showed that neural networks could learn to output an entire sequence of characters transcribed from an image, rather than just identifying a single object.	 (2014 ) se pawA calA ki waMwrikA netavarka eka Cavi se aMkiwa varNoM ke eka pUre anukrama ko Autaputa xenA sIKa sakawe hEM , bajAya sirPa eka vaswu kI pahacAna karane ke .		
547	547
S1 ( 2014d ) showed that neural networks could learn to output an entire sequence of characters transcribed from an image , rather than just identifying a single object .	( 2014d) meM xiKAyA gayA ki waMwrikA netavarka eka Cavi se liKe gae pAwroM kA eka pUrA anukrama nikAlanA sIKa sakawA hE, balki sirPa eka vaswu kI pahacAna karanA
NP2 ( 2014d )	( 2014)
VP6_LWG showed	xiKAyA gayA
SBAR8 that neural networks could learn to output an entire sequence of characters transcribed from an image , rather than just identifying a single object	vaha waMwrikA netavarka sirPa eka vaswu kI pahacAna karane ke bajAya kisI Cavi se preriwa kie gae pAwroM kA eka pUrA anukrama nikAlanA sIKa sakawA hE
S10 neural networks could learn to output an entire sequence of characters transcribed from an image , rather than just identifying a single object	waMwrikA netavarka sirPa eka vaswu kI pahacAna karane ke bajAya kisI Cavi se preriwa kie gae pAwroM kA eka pUrA anukrama nikAlanA sIKa sakawe hEM
NP11 neural networks	waMwrikA netavarka
NNS13 networks	netavarka
VP14_LWG could learn	sIKa sakawe We
PP18 to output	Autaputa karane ke lie
NP20 output	Autaputa
NP22 an entire sequence of characters transcribed from an image , rather than just identifying a single object	eka Cavi se preRiwa kiraxAroM kA eka pUrA krama, sirPa eka hI vaswu kI pahacAna karane ke bajAya
NP23 an entire sequence	eka pUrA krama
PP27 of characters transcribed from an image , rather than just identifying a single object	eka Cavi se preriwa kiraxAroM, balki sirPa eka vaswu kI pahacAna karane ke bajAya
NP29 characters transcribed from an image , rather than just identifying a single object	eka Cavi se aMkiwa pAwra, sirPa eka hI vaswu kI pahacAna karane ke bajAya
NP30 characters	pAwra
NNS31 characters	pAwra
VP32_LWG transcribed	varNiwa
PP34 from an image , rather than just identifying a single object	eka Cavi se, balki sirPa eka vaswu kI pahacAna karane ke bajAya
PP35 from an image	eka Cavi se
NP37 an image	eka Cavi
,40 ,	,
PP42 than just identifying a single object	sirPa eka hI vaswu kI pahacAna karane kI wulanA meM
S44 just identifying a single object	sirPa eka hI vaswu kI pahacAna
ADVP45 just	basa
VP47_LWG identifying	pahacAna
NP49 a single object	eka hI vaswu

----
00549	Previously, it was widely believed that this kind of learning required labeling of the individual elements of the sequence (Glehre and Bengio, 2013).	 pahale , yaha vyApaka rUpa se mAnA jAwA WA ki isa waraha ke sIKane ke lie anukrama ke vyakwigawa wawvoM kI lebaliMga kI AvaSyakawA howI hE ( Glhre Ora beMjiyo , 2013 )		
548	548
S1 Previously , it was widely believed that this kind of learning required labeling of the individual elements of the sequence ( Glehre and Bengio , 2013 ) .	pahale yaha mAnA jA rahA WA ki isa waraha kI sIKa ko krama ke vyakwigawa wawvoM (gulacehare Ora beMgiyo, 2013) ke lebalAijiMga kI AvaSyakawA howI hE
ADVP2 Previously	pahale kI bAwa
,4 ,	,
NP5 it	yaha
VP7_LWG was widely believed	badZe pEmAne para mAnA jAwA WA
ADVP9 widely	vyApaka rUpa se
SBAR13 that this kind of learning required labeling of the individual elements of the sequence	ki isa waraha kI sIKa ko anukrama ke vyakwigawa wawvoM kA lebalAija karane kI AvaSyakawA
S15 this kind of learning required labeling of the individual elements of the sequence	isa waraha kI sIKa ko anukrama ke vyakwigawa wawvoM kA lebalAija karane kI AvaSyakawA
NP16 this kind of learning	isa waraha kI sIKa
NP17 this kind	isa waraha kA
PP20 of learning	sIKane kI
NP22 learning	sIKanA
VP24_LWG required	AvaSyaka
NP26 labeling of the individual elements of the sequence	anukrama ke vyakwigawa wawvoM kA lebalAijiMga
NP27 labeling	lebaliMga
PP29 of the individual elements of the sequence	anukrama ke vyakwigawa wawvoM kA
NP31 the individual elements of the sequence	krama ke vyakwigawa wawva
NP32 the individual elements	vyakwigawa wawva
NNS35 elements	wawva
PP36 of the sequence	krama kA
NP38 the sequence	kramAMka
PRN41 ( Glehre and Bengio , 2013 )	(gulacehare Ora beMgiyo, 2013)
NP43 Glehre and Bengio	gulacehare Ora beMgiyo
CC45 and	Ora
NNP44 Glehre	gulacehare
NNP46 Bengio	beMgIo
,47 ,	,
NP48 2013	2013

----
00550	Recurrent neural networks, such as the LSTM sequence model mentioned above, are now used to model relationships between sequences and other sequences rather than just xed inputs.	 punarAvarwI waMwrikA netavarka , jEse ki Upara varNiwa LSTM anukrama moYdala , aba kevala niSciwa inaputa ke bajAya anukramoM Ora anya anukramoM ke bIca saMbaMXoM ko moYdala karane ke lie prayoga kiyA jAwA hE .		
549	549
S1 Recurrent neural networks , such as the LSTM sequence model mentioned above , are now used to model relationships between sequences and other sequences rather than just xed inputs .	varwamAna waMwrikA netavarka, jEse ki Upara bawAe gae elaesatIema kramAMka moYdala ke rUpa meM aba sirPa waya inaputa ke bajAya kramoM Ora anya
NP2 Recurrent neural networks , such as the LSTM sequence model mentioned above ,	varwamAna waMwrikA netavarka, jEse ki elaesatIema kramAMka moYdala kA upara ulleKa kiyA gayA,
NP3 Recurrent neural networks	varwamAna waMwrikA netavarka
NNS6 networks	netavarka
,7 ,	,
PP8 such as the LSTM sequence model mentioned above	jEse elaesatIema kramAMka moYdala kA ukwa ulleKa
NP11 the LSTM sequence model mentioned above	elaesatIema kramAMka moYdala kA ukwa ulleKa
NP12 the LSTM sequence model	elaesatIema kramAMka moYdala
VP17_LWG mentioned above	Upara bawAyA gayA ulleKa
ADVP19 above	Upara
,21 ,	,
VP22_LWG are now used	aba iswemAla kiyA jAwA hE
ADVP24 now	aba
PP28 to model relationships between sequences and other sequences rather than just xed inputs	sirPa waya inaputa ke bajAya krama Ora anya kramoM ke bIca saMbaMXoM ko moYdala karane ke lie
NP30 model relationships between sequences and other sequences rather than just xed inputs	sirPa waya inaputa ke bajAya krama Ora anya kramoM ke bIca saMbaMXa
NP31 model relationships between sequences and other sequences	xqSyoM Ora anya xqSyoM ke bIca moYdala saMbaMXa
NP32 model relationships	moYdala riSwoM
NNS34 relationships	riSwoM
PP35 between sequences and other sequences	xqSyoM Ora anya xqSyoM ke bIca
NP37 sequences and other sequences	xqSya Ora anya xqSya
NP38 sequences	xqSya
NNS39 sequences	xqSya
CC40 and	Ora
NP41 other sequences	anya krama
NNS43 sequences	xqSya
CONJP44 rather than	balki isase jyAxA
NP47 just xed inputs	basa Piksda inaputa
NNS50 inputs	inaputa

----
0055	In computer science, operations such as searching a collection of data can proceed exponentially faster if the collec- tion is structured and indexed intelligently.	 kaMpyUtara vijFAna meM , detA ke saMgraha ko Kojane jEsI saMkriyAeM GAwIya rUpa se wejI se Age baDZa sakawI hEM yaxi koleka - tIna ko saMraciwa Ora buxXimAna rUpa se anukramiwa kiyA jAe .		
54	54
S1 In computer science , operations such as searching a collection of data can proceed exponentially faster if the collec - tion is structured and indexed intelligently .	kaMpyUtara sAiMsa meM yaxi sahayogI - saMracanAwmaka rUpa se saMracanAwmaka rUpa se sUcIbaxXa Ora sUcIbaxXa ho wo detA saMgraha kI Koja jEse paricAlana
PP2 In computer science	kampyUtara sAiMsa meM
NP4 computer science	kampyUtara sAiMsa
,7 ,	,
NP8 operations such as searching a collection of data	AMkadZoM ke saMgraha kI Koja jEse oYpareSana
NP9 operations	oYpareSana
NNS10 operations	oYpareSana
PP11 such as searching a collection of data	jEse detA kA kalekSana KojanA
S14 searching a collection of data	dAtA kA kalekSana KojanA
VP15_LWG searching	Koja rahA hE
NP17 a collection of data	dAtA kA saMgraha
NP18 a collection	eka saMgraha
PP21 of data	detA kA
NP23 data	detA
NNS24 data	detA
VP25_LWG can proceed exponentially faster	wejI se Age baDZa sakawe hEM Age
ADVP29 exponentially	lagAwAra
ADVP31 faster if the collec - tion is structured and indexed intelligently	sahayogI - saMracanAwmaka Ora sUkRmawA se sUcIbaxXa ho wo jyAxA weja
ADVP32 faster	weja
SBAR34 if the collec - tion is structured and indexed intelligently	yaxi sahayogI - liMga saMracanAwmaka Ora sUkRmawA se sUcIbaxXa hE
S36 the collec - tion is structured and indexed intelligently	sahayogI- liMga saMraciwa Ora sUkRmawA se sUcIbaxXa hE
NP37 the collec - tion	sahayogI- xAna
VP42_LWG is structured indexed intelligently	buxXijIvI sUcakAMka hE
CC46 and	Ora
VBN45 structured	saMracanAwmaka
VBN47 indexed	sUcakAMka
ADVP48 intelligently	buxXimAna

----
00551	This sequence-to-sequence learning seems to be on the cusp of revolutionizing another application: machine translation (Sutskever et al.	 yaha anukrama - se - anukrama SikRaNa eka anya anuprayoga ke krAMwikaraNa ke muhAne para prawIwa howA hEH maSIna anuvAxa (Sutskever eta ala ) .		
550	550
S1 This sequence - to - sequence learning seems to be on the cusp of revolutionizing another application : machine translation ( Sutskever et al. .	yaha krama  se- anukrama sIKanA eka Ora Avexana ko krAMwikArI banAne ke aSuBa krama para lagawA hEH maSIna anuvAxa (sawsakevara eta ala..
NP2 This sequence - to - sequence learning	yaha krama- se- anukrama sIKanA
NML4 sequence - to - sequence	krama- se- krama
VP11_LWG seems	lagawA hE lagawA hE
S13 to be on the cusp of revolutionizing another application : machine translation ( Sutskever et al.	eka Ora Avexana ko krAMwikArI banAne ke aSuBa rUpa meM honA cAhieH maSIna anuvAxa (sawsakevara eta ala
VP14_LWG to be	honA cAhie
PP18 on the cusp of revolutionizing another application : machine translation ( Sutskever et al.	eka Ora Avexana ko krAMwikArI banAne ke aSuBa krama para maSIna anuvAxa (sawsakevara eta ala
NP20 the cusp of revolutionizing another application : machine translation ( Sutskever et al.	eka Ora Avexana ko krAMwikArI banAne kA SuBaH maSIna anuvAxa (sawsakevara eta ala
NP21 the cusp	aSuBa
PP24 of revolutionizing another application : machine translation ( Sutskever et al.	eka Ora Avexana ko krAMwikArI karane ke lie maSIna anuvAxa (sawsakevara eta ala
S26 revolutionizing another application : machine translation ( Sutskever et al.	eka Ora Avexana ko krAMwikArI - maSIna anuvAxa (sawsakevara eta ala
VP27_LWG revolutionizing	krAMwikArI
NP29 another application : machine translation ( Sutskever et al.	eka Ora Avexana H maSIna anuvAxa (sawsakevara eta ala
NP30 another application : machine translation	eka Ora AvexanaH maSIna anuvAxa
NML32 application : machine	AvexanaH maSIna
NP38 Sutskever	sawaskevAra
ADVP40 et al.	eta ala

----
00552	, 2014; Bahdanau et al., 2015).	  bI . bI . dI . ala . , 2015		
551	551
FRAG1 , 2014 ; Bahdanau et al. , 2015 ) .	, 2014 ; bahaxAnO eta ala., 2015 ).
NP2 , 2014	, 2014
,3 ,	,
FRAG6 Bahdanau et al. , 2015	bahaxAnu eta ala., 2015
NP7 Bahdanau et al.	bahaxAnu eta ala
ADVP9 et al.	eta ala
,12 ,	,
NP13 2015	2015

----
00553	This trend of increasing complexity has been pushed to its logical conclusion with the introduction of neural Turing machines (Graves et al., 2014) that learn to read from memory cells and write arbitrary content to memory cells.	 baDZawI jatilawA kI isa pravqwwi ko isake wArkika niRkarRa para pahuMcA xiyA gayA hE jisameM waMwrikA waMwra maSInoM ( grevsa eta ala , 2014 ) kI SuruAwa kI gaI hE jo smqwi koSikAoM se paDZanA sIKawe hEM Ora smqwi koSikAoM ko manamAnA sAmagrI liKawe hEM 		
552	552
S1 This trend of increasing complexity has been pushed to its logical conclusion with the introduction of neural Turing machines ( Graves et al. , 2014 ) that learn to read from memory cells and write arbitrary content to memory cells .	waMwrikA wuriMga maSInoM (groYsa eta ala, 2014) ke paricaya ke sAWa baDZawe jatilawA kA yaha calana isake wArkika niRkarRa para Xakela xiyA gayA hE
NP2 This trend of increasing complexity	jatilawA baDZane kA yaha calana
NP3 This trend	yaha ruJAna
PP6 of increasing complexity	baDZawI jatilawA kI
NP8 increasing complexity	baDZawI jatilawA
VP11_LWG has been pushed	XakkA lagAyA gayA hE
PP17 to its logical conclusion	isake wArkika niRkarRa para
NP19 its logical conclusion	isakA wArkika niRkarRa
PP23 with the introduction of neural Turing machines ( Graves et al. , 2014 ) that learn to read from memory cells and write arbitrary content to memory cells	waMwrikA tariMga maSIna (groYsa eta ala, 2014) ke paricaya ke sAWa jo memorI koSikAoM se paDZanA sIKeM Ora memorI selsa ko manamAnI sAmagrI liKeM
NP25 the introduction of neural Turing machines ( Graves et al. , 2014 ) that learn to read from memory cells and write arbitrary content to memory cells	waMwrikA tarniMga maSInoM (groYsa eta ala, 2014) kA paricaya jo yAxaxASwa koSikAoM se paDZanA sIKeM Ora memorI selsa ko manamAnI sAmagrI liKeM
NP26 the introduction of neural Turing machines ( Graves et al. , 2014 )	waMwrikA tariMga maSIna (groYsa eta ala, 2014) kA paricaya
NP27 the introduction	paricaya
PP30 of neural Turing machines ( Graves et al. , 2014 )	waMwrikA tariMga maSIna (groYsa eta ala, 2014)
NP32 neural Turing machines ( Graves et al. , 2014 )	waMwrikA tariMga maSIna (groYsa eta ala, 2014 )
NML33 neural Turing	waMwrikA tariMga
NNS36 machines	maSIneM
PRN37 ( Graves et al. , 2014 )	(kabroM eta ala, 2014)
NP39 Graves et al. , 2014	kabroM eta ala, 2014
NP40 Graves et al.	kabroM eta ala
NP41 Graves	kabroM
ADVP43 et al.	eta ala
,46 ,	,
NP47 2014	2014
SBAR50 that learn to read from memory cells and write arbitrary content to memory cells	yaha yAxaxASwa koSikAoM se paDZanA sIKeM Ora memorI koSikAoM ko manamAnI sAmagrI liKeM
WHNP51 that	vaha
S53 learn to read from memory cells and write arbitrary content to memory cells	memorI selsa se paDZanA sIKeM Ora memorI selsa ko manamAnI sAmagrI liKeM
VP54_LWG learn	jAneM
S56 to read from memory cells and write arbitrary content to memory cells	memorI selsa se paDZane ke lie Ora memorI selsa ko manamAnI sAmagrI liKane ke lie
VP57_LWG to read write	paDZane ke lie liKane ke lie
PP62 from memory cells	yAxaxASwa koSikAoM se
NP64 memory cells	yAxaxASwa koSikAeM
NNS66 cells	koSikAeM
CC67 and	Ora
VP60 read from memory cells	memorI selsa se paDZe
VP68 write arbitrary content to memory cells	memorI selsa ko liKeM manamAnI sAmagrI
NP70 arbitrary content	manamAnI sAmagrI
PP73 to memory cells	yAxaxASwa koSikAoM ko
NP75 memory cells	yAxaxASwa koSikAeM
NNS77 cells	koSikAeM

----
00554	Such neural networks can learn simple programs from examples of desired behavior.	 Ese waMwrika netavarka sarala progrAmoM ko vAMCiwa vyavahAra ke uxAharaNoM se sIKa sakawe hEM 		
553	553
S1 Such neural networks can learn simple programs from examples of desired behavior .	Ese waMwrikA netavarka vAMCiwa vyavahAra ke uxAharaNoM se sAXAraNa kAryakrama sIKa sakawe hEM
NP2 Such neural networks	Ese waMwrikA netavarka
NNS5 networks	netavarka
VP6_LWG can learn	sIKa sakawe hEM
NP10 simple programs	sAXAraNa kAryakrama
NNS12 programs	kAryakrama
PP13 from examples of desired behavior	manacAhA vyavahAra ke uxAharaNoM se
NP15 examples of desired behavior	manacAhA vyavahAra ke uxAharaNa
NP16 examples	uxAharaNa
NNS17 examples	uxAharaNa
PP18 of desired behavior	manacAhA vyavahAra
NP20 desired behavior	manacAhA vyavahAra

----
00555	For example, they can learn to sort lists of numbers given examples of scrambled and sorted sequences.	 uxAharaNa ke lie , ve skambaliwa Ora anukramiwa anukramoM ke uxAharaNa xie gae saMKyAoM kI sUciyoM ko CAMtanA sIKa sakawe hEM .		
554	554
S1 For example , they can learn to sort lists of numbers given examples of scrambled and sorted sequences .	uxAharaNa ke lie, ve krEmpda Ora cayaniwa kramoM ke uxAharaNoM ke xie gae saMKyAoM kI sUciyoM ko sUcIbaxXa karanA sIKa sakawe hEM
PP2 For example	uxAharaNa ke lie
NP4 example	uxAharaNa
,6 ,	,
NP7 they	ve
VP9_LWG can learn	sIKa sakawe hEM
S13 to sort lists of numbers given examples of scrambled and sorted sequences	skrEmbda Ora cayaniwa krama ke uxAharaNoM ke xie gae naMbaroM kI sUciyoM ko CAztane ke lie
VP14_LWG to sort	waraha-waraha karane ke lie
NP18 lists of numbers given examples of scrambled and sorted sequences	naMbaroM kI sUciyAM wale hue Ora CaMte hue kramoM ke uxAharaNa xie
NP19 lists	sUciyAM
NNS20 lists	sUciyAM
PP21 of numbers given examples of scrambled and sorted sequences	naMbaroM ke xie gae skrEmbalda Ora cayaniwa kramoM ke uxAharaNa
NP23 numbers given examples of scrambled and sorted sequences	skrEmbda Ora cayaniwa kramoM ke uxAharaNa xie naMbara
NP24 numbers	naMbara
NNS25 numbers	naMbara
VP26_LWG given	xiyA gayA
NP28 examples of scrambled and sorted sequences	KarAba Ora CaMte hue xqSyoM ke uxAharaNa
NP29 examples	uxAharaNa
NNS30 examples	uxAharaNa
PP31 of scrambled and sorted sequences	wale hue Ora sulaJe hue kramoM kA
NP33 scrambled and sorted sequences	KarAba Ora sulaJe krama
ADJP34 scrambled and sorted	wale hue Ora sulaJe
CC36 and	Ora
VBN35 scrambled	wale hue Dera
JJ37 sorted	hala kiyA gayA
NNS38 sequences	xqSya

----
00556	This self-programming technology is in its infancy, but in the future it could in principle be applied to nearly any task.	 yaha sva - progrAmiMga wakanIka apane SESava kAla meM hE , lekina BaviRya meM ise sExXAMwika rUpa se lagaBaga kisI BI kArya ke lie lAgU kiyA jA sakawA hE .		
555	555
S1 This self - programming technology is in its infancy , but in the future it could in principle be applied to nearly any task .	yaha sva-pravarwana prOxyogikI apanI PEMsI meM hE, lekina BaviRya meM ise lagaBaga kisI kArya ke lie lAgU kiyA jA sakawA hE
S2 This self - programming technology is in its infancy	yaha sva- progrAmiMga wakanIka apanI PEMsI meM hE
NP3 This self - programming technology	yaha sva- progrAmiMga wakanIka
ADJP5 self - programming	sva- progrAmiMga
VP10_LWG is	hE
PP12 in its infancy	isakI PEMsI meM
NP14 its infancy	isakI PEMsI
,17 ,	,
CC18 but	lekina
S19 in the future it could in principle be applied to nearly any task	BaviRya meM ise lagaBaga kisI kArya meM lAgU kiyA jA sakawA hE
PP20 in the future	BaviRya meM
NP22 the future	BaviRya
NP25 it	yaha
VP27_LWG could be applied	lagAyA jA sakawA WA Avexana
PP29 in principle	sixXAMwa meM
NP31 principle	sixXAMwa
PP37 to nearly any task	lagaBaga kisI kArya ke lie
NP39 nearly any task	lagaBaga koI kArya
QP40 nearly any	lagaBaga koI

----
00557	Another crowning achievement of deep learning is its extension to the domain of reinforcement learning .	 gaharI SikRA kI eka Ora pramuKa upalabXi hE usakA pravarwana sIKane ke kRewra meM viswAra 		
556	556
S1 Another crowning achievement of deep learning is its extension to the domain of reinforcement learning .	gaharI sIKa kI eka Ora wAjapoSI upalabXi isakA viswAra pravarwana sIKane ke domena ke lie hE
NP2 Another crowning achievement of deep learning	gaharI sIKa kI eka Ora wAjapoSI upalabXi
NP3 Another crowning achievement	eka Ora wAjapoSI upalabXi
PP7 of deep learning	gaharI sIKa kA
NP9 deep learning	gaharI sIKa
VP12_LWG is	hE
NP14 its extension	isakA viswAra
PP17 to the domain of reinforcement learning	suxqDIkaraNa sIKane ke domena waka
NP19 the domain of reinforcement learning	pravarwana sIKane kA domena
NP20 the domain	domena
PP23 of reinforcement learning	suxqDIkaraNa sIKane kI
NP25 reinforcement learning	suxqDIkaraNa sIKanA

----
00558	In the context of reinforcement learning, an autonomous agent must learn to perform a task by trial and error, without any guidance from the human operator.	 pravarwana sIKane ke saMxarBa meM , eka svAyawwa ejeMta ko parIkRaNa Ora wruti ke xvArA eka kArya karanA sIKanA cAhie , mAnava oYparetara se kisI mArgaxarSana ke binA .		
557	557
S1 In the context of reinforcement learning , an autonomous agent must learn to perform a task by trial and error , without any guidance from the human operator .	pravarwana sIKa ke saMxarBa meM, eka svAyawwa ejeMta ko mAnava saMcAlaka se binA kisI mArgaxarSana ke, trAyala Ora wruti xvArA kArya karanA sIKanA cAhie
PP2 In the context of reinforcement learning	suxqDZIkaraNa sIKane ke saMxarBa meM
NP4 the context of reinforcement learning	suxqDIkaraNa sIKane kA saMxarBa
NP5 the context	saMxarBa
PP8 of reinforcement learning	suxqDIkaraNa sIKane kI
NP10 reinforcement learning	suxqDIkaraNa sIKanA
,13 ,	,
NP14 an autonomous agent	eka svAyawwa ejeMta
VP18_LWG must learn	sIKanA cAhie
S22 to perform a task by trial and error , without any guidance from the human operator	trAyala Ora wruti xvArA kArya karanA, mAnava saMcAlaka se binA kisI mArgaxarSana ke
VP23_LWG to perform	praxarSana karane ke lie
NP27 a task	eka tAska
PP30 by trial and error	trAyala Ora wruti se
NP32 trial and error	trAyala Ora wruti
CC34 and	Ora
NN33 trial	trAyala
NN35 error	wruti
,36 ,	,
PP37 without any guidance from the human operator	mAnava saMcAlaka se binA kisI mArgaxarSana ke
NP39 any guidance from the human operator	mAnava saMcAlaka se koI mArgaxarSana
NP40 any guidance	koI mArgaxarSana
PP43 from the human operator	mAnava saMcAlaka se
NP45 the human operator	mAnava saMcAlaka

----
00559	DeepMind demonstrated that a reinforcement learning system based on deep learning is capable of learning to play Atari video games, reaching human-level performance on many tasks	 xIpamaNda ne yaha praxarSiwa kiyA ki gahana SikRaNa para AXAriwa eka majabUwa SikRaNa praNAlI , awulya vIdiyo gema Kelane ke lie sIKane meM sakRama hE , jo kaI kAryoM para mAnava swara ke praxarSana waka pahuMcawA hE 		
558	558
S1 DeepMind demonstrated that a reinforcement learning system based on deep learning is capable of learning to play Atari video games , reaching human - level performance on many tasks	dIpamAiMda ne praxarSana kiyA ki gaharI sIKa para AXAriwa eka suxqDZIkaraNa sIKa praNAlI atArI vIdiyo gema Kelane ke lie sIKane meM sakRama hE, mAnava waka pahu
NP2 DeepMind	xIpa ximAga
VP4_LWG demonstrated	praxarSana kiyA
SBAR6 that a reinforcement learning system based on deep learning is capable of learning to play Atari video games , reaching human - level performance on many tasks	ki gaharI sIKa para AXAriwa eka suxqDZIkaraNa sIKa praNAlI atArI vIdiyo gema Kelane ke lie sIKane meM sakRama hE, mAnava waka pahuMca rahA hE swarIya praxarSana
S8 a reinforcement learning system based on deep learning is capable of learning to play Atari video games , reaching human - level performance on many tasks	gaharI sIKa para AXAriwa eka suxqDZIkaraNa sIKa praNAlI, atArI vIdiyo gema Kelane ke lie sIKane meM sakRama hE, mAnava waka pahuMca rahA hE swarIya praxarSana
NP9 a reinforcement learning system based on deep learning	gaharI sIKa para AXAriwa eka suxqDZIkaraNa sIKa praNAlI
NP10 a reinforcement learning system	eka suxqDZIkaraNa sIKa praNAlI
NML12 reinforcement learning	suxqDIkaraNa sIKanA
PP16 based on deep learning	gaharI sIKa para AXAriwa
PP18 on deep learning	gaharI sIKa para
NP20 deep learning	gaharI sIKa
VP23_LWG is	hE
ADJP25 capable of learning to play Atari video games , reaching human - level performance on many tasks	atArI vIdiyo gema Kelane ke lie sIKane meM sakRama, mAnava waka pahuMca rahA - kaI kAryoM para swarIya praxarSana
PP27 of learning to play Atari video games , reaching human - level performance on many tasks	atArI vIdiyo gema KelanA sIKanA, mAnava waka pahuMcanA- kaI kAryoM para swarIya praxarSana
S29 learning to play Atari video games , reaching human - level performance on many tasks	atArI vIdiyo gema KelanA sIKanA, mAnava waka pahuMcanA - kaI kAryoM para swarIya praxarSana
VP30_LWG learning reaching	sIKa rahA hE pahuMcanA
S33 to play Atari video games	atArI vIdiyo gema Kelane ke lie
VP34_LWG to play	Kelane ke lie
NP38 Atari video games	atArI vIdiyo gema
NNS41 games	Kela
,42 ,	,
NP45 human - level performance on many tasks	mAnava- kaI kAryoM para swarIya praxarSana
NP46 human - level performance	mAnava-swarIya praxarSana
NML47 human - level	mAnava-swara
PP52 on many tasks	kaI kAryoM para
NP54 many tasks	kaI kArya
NNS56 tasks	kAryoM

----
00560	(Mnih et al., 2015).	 ( Mnih eta ala , 2015 )		
559	559
FRAG1 ( Mnih et al. , 2015 ) .	(minha eta ala, 2015 )
NP3 Mnih et al.	muniha eta ala
ADVP5 et al.	eta ala
,8 ,	,
NP9 2015	2015

----
0056	People can easily perform arithmetic on Arabic numerals but nd arithmetic on Roman numerals much more time consuming.	 loga arabI aMkoM para aMkagaNiwa AsAnI se kara sakawe hEM lekina romana aMkoM para aMkagaNiwa ko bahuwa aXika samaya lene vAle pAwe hEM .		
55	55
S1 People can easily perform arithmetic on Arabic numerals but nd arithmetic on Roman numerals much more time consuming .	loga AsAnI se arabI aMkoM para aMkagaNiwa kara sakawe hEM lekina romana aMkoM para aMkagaNiwa aXika samaya lagawA hE
NP2 People	logoM
NNS3 People	logoM
VP4_LWG can easily perform nd	AsAnI se pA sakawe hEM praxarSana
ADVP6 easily	AsAnI se
NP11 arithmetic	aMkagaNiwa
PP13 on Arabic numerals	arabI saMKyAoM para
NP15 Arabic numerals	arabI aMkagaNiwa
NNS17 numerals	aMkagaNiwa
CC18 but	lekina
VP9 perform arithmetic on Arabic numerals	arabI aMkoM para aMkagaNiwa kareM praxarSana
VP19 nd arithmetic on Roman numerals much more time consuming	romana aMkoM para aMkagaNiwa KojeM bahuwa aXika samaya kA sevana
NP21 arithmetic	aMkagaNiwa
PP23 on Roman numerals much more time consuming	romana saMKyAoM para bahuwa aXika samaya kA sevana
NP25 Roman numerals much more time consuming	romana naMbara bahuwa aXika samaya le rahe hEM
NP26 Roman numerals much more	romana naMbaroM meM bahuwa aXika
NP27 Roman numerals	romana aMkoM
NNS29 numerals	aMkagaNiwa
ADJP30 much more	bahuwa jyAxA
ADJP33 time consuming	samaya kA sevana

----
00561	Deep learning has also signicantly improved the performance of reinforcement learning for robotics (Finn et al., 2015).	 gahana aXyayana ne robotiksa (Pina eta ala , 2015 ) ke lie pravarwana sIKane ke kArya meM BI mahawvapUrNa suXAra kiyA hE 		
560	560
S1 Deep learning has also signicantly improved the performance of reinforcement learning for robotics ( Finn et al. , 2015 ) .	gaharI sIKa se robotiksa (Pina eta ala, 2015) ke lie suxqDZIkaraNa sIKane ke praxarSana meM BI KAsA suXAra huA hE
NP2 Deep learning	gaharI sIKa
VP5_LWG has also signicantly improved	isase BI kAPI suXAra huA hE
ADVP7 also	sAWa hI
ADVP9 signicantly	KAsa wOra para
NP13 the performance of reinforcement learning	suxqDIkaraNa sIKane kA praxarSana
NP14 the performance	praxarSana
PP17 of reinforcement learning	suxqDIkaraNa sIKane kI
NP19 reinforcement learning	suxqDIkaraNa sIKanA
PP22 for robotics	robotiksa ke lie
NP24 robotics	robotiksa
NNS25 robotics	robotiksa
PRN26 ( Finn et al. , 2015 )	(Pina eta ala, 2015)
NP28 Finn et al. , 2015	Pina eta ala, 2015
NP29 Finn et al.	Pina eta ala
NP30 Finn	Pina
ADVP32 et al.	eta ala
,35 ,	,
NP36 2015	2015

----
00562	Many of these applications of deep learning are highly protable.	 gahana SikRaNa ke ina anuprayogoM meM se kaI awyaXika lABaxAyaka hEM .		
561	561
S1 Many of these applications of deep learning are highly protable .	gaharI sIKa ke inameM se kaI Avexana behaxa lABaxAyaka hEM
NP2 Many of these applications of deep learning	gaharI sIKa ke inameM se kaI Avexana
NP3 Many	kaI
PP5 of these applications of deep learning	gaharI sIKa ke ina AvexanoM kA
NP7 these applications of deep learning	gaharI sIKa ke ye Avexana
NP8 these applications	ye Avexana
NNS10 applications	Avexana
PP11 of deep learning	gaharI sIKa kA
NP13 deep learning	gaharI sIKa
VP16_LWG are	hEM
ADJP18 highly protable	awyaXika lABaxAyaka

----
00563	Deep learning is now used by many top technology companies, including Google, Microsoft, Facebook, IBM, Baidu, Apple, Adobe, Netix, NVIDIA, and NEC.	 gahana aXigama kA upayoga aba kaI SIrRa prOxyogikI kaMpaniyoM xvArA kiyA jAwA hE , jinameM gUgala , mAikrosoYPta , Pesabuka , AIbIema , bExu , eppala , edoba , netaPliksa , enavIAIe , Ora enaIsI SAmila hEM .		
562	562
S1 Deep learning is now used by many top technology companies , including Google , Microsoft , Facebook , IBM , Baidu , Apple , Adobe , Netix , NVIDIA , and NEC .	gaharI sIKa kA iswemAla aba kaI toYpa teknoloYjI kaMpaniyoM xvArA kiyA jAwA hE, jisameM gUgala, mAikrosoYPta, AIbIema, bEdU, E
NP2 Deep learning	gaharI sIKa
VP5_LWG is now used	aba iswemAla howA hE
ADVP7 now	aba
PP11 by many top technology companies , including Google , Microsoft , Facebook , IBM , Baidu , Apple , Adobe , Netix , NVIDIA , and NEC	gUgala, mAikrosoYPta, Pesabuka, AIbIema, bEdU, eppala, edavo, netaPliksa, enavIenaAI samewa kaI SIrRa prOxyogikI kaMpaniyoM
NP13 many top technology companies , including Google , Microsoft , Facebook , IBM , Baidu , Apple , Adobe , Netix , NVIDIA , and NEC	gUgala, mAikrosoYPta, Pesabuka, AIbIema, bEdU, eppala, edavo, netaPliksa, enavIenaAI sahiwa kaI SIrRa prOxyogikI kaMpaniyAM
NP14 many top technology companies	kaI toYpa teknoloYjI kaMpaniyAM
NML16 top technology	SIrRa wakanIka
NNS19 companies	kaMpaniyoM
,20 ,	,
PP21 including Google , Microsoft , Facebook , IBM , Baidu , Apple , Adobe , Netix , NVIDIA , and NEC	jisameM gUgala, mAikrosoYPta, Pesabuka, AIbIema, bEdU, Epala, edavoketa, netaPliksa, enavIenaAI, Ora enasII
NP23 Google , Microsoft , Facebook , IBM , Baidu , Apple , Adobe , Netix , NVIDIA , and NEC	gUgala, mAikrosoYPta, Pesabuka, AIbIema, bEdU, eppala, edavoketa, netaPliksa, enavIenaAI, Ora enasII
,25 ,	,
,27 ,	,
,29 ,	,
,31 ,	,
,33 ,	,
,35 ,	,
,37 ,	,
,39 ,	,
,41 ,	,
CC42 and	Ora
NNP43 NEC	enasII

----
00564	Advances in deep learning have also depended heavily on advances in software infrastructure.	 gahana SikRaNa meM pragawi BI soYPtaveyara mUla saMracanA meM pragawi para kAPI nirBara rahI hE 		
563	563
S1 Advances in deep learning have also depended heavily on advances in software infrastructure .	gaharI sIKa meM hone vAlI unnawi BI soYPtaveyara iMPrAstrakcara meM unnawiyoM para BArI nirBara karawI hE
NP2 Advances in deep learning	gaharI sIKa meM unnawi
NP3 Advances	edavAMsa
NNS4 Advances	edavAMsa
PP5 in deep learning	gaharI sIKa meM
NP7 deep learning	gaharI sIKa
VP10_LWG have also depended heavily	BArI ASriwa BI hue hEM
ADVP12 also	sAWa hI
ADVP16 heavily	BArI-Barakama
PP18 on advances in software infrastructure	soYPtaveyara iMPrAstrakcara meM vqxXi para
NP20 advances in software infrastructure	soYPtaveyara iMPrAstrakcara meM unnawi
NP21 advances	edavAMsa
NNS22 advances	edavAMsa
PP23 in software infrastructure	soYPtaveyara iMPrAstrakcara meM
NP25 software infrastructure	soYPtaveyara iMPrAstrakcara

----
00565	Software libraries such as Theano (Bergstra et al., 2010; Bastien et al., 2012), PyLearn2 (Goodfellow et al.	 soYPtaveyara puswakAlaya jEse ki Theergtra eta ala , 2010 rutien eta ala , PyLearn2		
564	564
NP1 Software libraries such as Theano ( Bergstra et al. , 2010 ; Bastien et al. , 2012 ) , PyLearn2 ( Goodfellow et al. .	Weno (bargastrA eta ala., 2010 ; baswIna eta ala., 2012 ), pAilarna 2 (gudaPoYla eta ala..
NP2 Software libraries such as Theano ( Bergstra et al. , 2010 ; Bastien et al. , 2012 )	Weno (bargastrA eta ala, 2010 ; baswIna ewa ala., 2012 )
NP3 Software libraries	soYPtaveyara lAibrerI
NNS5 libraries	puswakAlayoM
PP6 such as Theano ( Bergstra et al. , 2010 ; Bastien et al. , 2012 )	jEse Weno (bargastrA eta ala., 2010 ; baswIna ewa ala., 2012 )
NP9 Theano ( Bergstra et al. , 2010 ; Bastien et al. , 2012 )	Weno (bargastrA eta ala., 2010 ; baswIna eta ala., 2012 )
PRN11 ( Bergstra et al. , 2010 ; Bastien et al. , 2012 )	(bargastrA eta ala., 2010 ; baswIna eta ala., 2012 )
FRAG13 Bergstra et al. , 2010	bargarastrA eta ala., 2010
NP14 Bergstra et al.	bargarastrA eta ala
ADVP16 et al.	eta ala
,19 ,	,
NP20 2010	2010
FRAG23 Bastien et al. , 2012	baswIna eta ala., 2012
NP24 Bastien et al.	baswIna eta ala
ADVP26 et al.	eta ala
,29 ,	,
NP30 2012	2012
,33 ,	,
NP34 PyLearn2 ( Goodfellow	pAilarna 2 ( gudaPoYlara
ADVP38 et al.	eta ala

----
00566	, 2013c), Torch (Collobert et al.	  2013eka , torca ( kolobarta eta ala )		
565	565
NP1 , 2013c ) , Torch ( Collobert et al. .	, 2013 sI), toYrca (kolobarta eta ala..
NP2 , 2013c ) , Torch	, 2013 sI), toYrca
PRN3 , 2013c ) ,	, 2013c ),
,4 ,	,
NP5 2013c )	2013c)
,8 ,	,
NP11 Collobert et al.	kolobarta eta ala
NP12 Collobert	kolobarta
ADVP14 et al.	eta ala

----
00567	, 2011b), DistBelief (Dean et al., 2012), Cae (Jia, 2013), MXNet (Chen et al., 2015), and	 Written By & # 44 ; DistBelief		
566	566
S1 , 2011b ) , DistBelief ( Dean et al. , 2012 ) , Cae ( Jia , 2013 ) , MXNet ( Chen et al. , 2015 ) , and	, 2011b), distribilita ( dIna eta ala., 2012 ), kEPe (jiyA, 2013 ), emaecaeta (cena eta ala, 2015 ),
NP2 , 2011b ) , DistBelief ( Dean et al. , 2012 ) , Cae ( Jia , 2013 ) , MXNet ( Chen et al. , 2015 ) ,	, 2011 bI), distribilita ( dIna eta ala., 2012 ), kEPe (jiyA, 2013 ), emaecaeta (cena eta ala, 2015 ),
NP3 , 2011b )	, 2011b )
,4 ,	,
,7 ,	,
NP8 DistBelief ( Dean et al. , 2012 ) , Cae ( Jia , 2013 ) , MXNet ( Chen et al. , 2015 )	distribyUSana ( dIna eta ala., 2012), kEPe (jiyA, 2013 ), emaecaeta (cena eta ala., 2015 )
PRN10 ( Dean et al. , 2012 ) , Cae ( Jia , 2013 ) , MXNet ( Chen et al. , 2015 )	( dIna eta ala., 2012 ), kEPe ( jiyA, 2013 ), emaecaeta (cena eta ala., 2015 )
NP12 Dean et al. , 2012 ) , Cae ( Jia , 2013 ) , MXNet ( Chen et al.	dIna eta ala., 2012 ), kEPe ( jiyA, 2013 ), emaecaeta (cena eta ala.
NP13 Dean et al. , 2012 )	dIna eta ala, 2012 )
NP14 Dean	dIna
ADVP16 et al.	eta ala
PRN19 , 2012 )	, 2012 )
,20 ,	,
NP21 2012	2012
,24 ,	,
NP25 Cae ( Jia , 2013 )	kEPe (jiyA, 2013)
PRN27 ( Jia , 2013 )	(jiyA, 2013)
NP29 Jia	jiyA
,31 ,	,
NP32 2013	2013
,35 ,	,
NP36 MXNet ( Chen et al.	emyUjeta (cena eta ala
NP37 MXNet ( Chen	emyUjeta (cena
ADVP41 et al.	eta ala
,44 ,	,
NP45 2015	2015
,48 ,	,
VP49_LWG and	Ora

----
00568	TensorFlow (Abadi et al., 2015) have all supported important research projects or commercial products.	 TensorFlow ( AbAxI eta ala , 2015 ) sabane mahawvapUrNa anusaMXAna pariyojanAoM yA vANijyika uwpAxoM kA samarWana kiyA hE 		
567	567
S1 TensorFlow ( Abadi et al. , 2015 ) have all supported important research projects or commercial products .	seMsaraPlU (abAxI eta ala., 2015) ne saBI ko samarWana xiyA hE ahama anusaMXAna pariyojanAoM yA vANijyika uwpAxoM kA
NP2 TensorFlow ( Abadi et al. , 2015 )	seMsaraPlU (abAxI eta ala, 2015)
NP3 TensorFlow ( Abadi	seMsaraPlU (abAxI
ADVP7 et al.	eta ala
PRN10 , 2015 )	, 2015 )
,11 ,	,
NP12 2015	2015
VP15_LWG have all supported	saBI ne kiyA samarWana
NP20 important research projects or commercial products	mahawvapUrNa anusaMXAna pariyojanAeM yA vANijyika uwpAxa
NP21 important research projects	mahawvapUrNa anusaMXAna pariyojanAeM
NNS24 projects	pariyojanAeM
CC25 or	yA
NP26 commercial products	vANijyika uwpAxa
NNS28 products	uwpAxoM

----
00569	Deep learning has also made contributions to other sciences.	 gahana aXyayana ne anya vijFAnoM meM BI yogaxAna xiyA hE 		
568	568
S1 Deep learning has also made contributions to other sciences .	gaharI sIKa ne anya vijFAnoM meM BI yogaxAna xiyA hE
NP2 Deep learning	gaharI sIKa
VP5_LWG has also made	sAWa hI banAyA hE
ADVP7 also	sAWa hI
NP11 contributions	yogaxAna
NNS12 contributions	yogaxAna
PP13 to other sciences	anya vijFAnoM ko
NP15 other sciences	anya vijFAna
NNS17 sciences	vijFAna

----
00570	Modern convolu- tional networks for object recognition provide a model of visual processing that neuroscientists can study (DiCarlo, 2013).	 vaswu mAnyawA ke lie AXunika saMvaliwa netavarka xqSya prakramaNa kA eka moYdala praxAna karawe hEM jisakA waMwrikA vijFAnI aXyayana kara sakawe hEM (DiCarlo , 2013 )		
569	569
S1 Modern convolu - tional networks for object recognition provide a model of visual processing that neuroscientists can study ( DiCarlo , 2013 ) .	AXunika kaMvelyU - vaswu mAnyawA ke lie viBAjanakArI netavarka xqSya prasaMskaraNa kA eka moYdala upalabXa karAwe hEM jo nyUro vEjFAnika aXyayana kara sakawe hEM (di
NP2 Modern convolu - tional networks for object recognition	AXunika kaMvelyU - vaswu mAnyawA ke lie viBAjanakArI netavarka
NP3 Modern convolu - tional networks	AXunika kaMtrola- teliyana netavarka
NML5 convolu - tional	kaMvolu- SanivAra
NNS9 networks	netavarka
PP10 for object recognition	vaswu mAnyawA ke lie
NP12 object recognition	vaswu mAnyawA
VP15_LWG provide	praxAna kareM
NP17 a model of visual processing	xqSya prasaMskaraNa kA moYdala
NP18 a model	eka moYdala
PP21 of visual processing	xqSya prasaMskaraNa kA
NP23 visual processing	xqSya prasaMskaraNa
SBAR26 that neuroscientists can study	vo nyUro vEjFAnika paDZa sakawe hEM
S28 neuroscientists can study	nyUro vEjFAnika kara sakawe hEM aXyayana
NP29 neuroscientists	nyUro vEjFAnika
NNS30 neuroscientists	nyUro vEjFAnika
VP31_LWG can study	paDZAI kara sakawe hEM
PRN35 ( DiCarlo , 2013 )	(di kArlo, 2013)
NP37 DiCarlo	di kArlo
,39 ,	,
NP40 2013	2013

----
0057	It is not surprising that the choice of representation has an enormous eect on the performance of machine learning algorithms.	 yaha AScarya kI bAwa nahIM hE ki prawiniXiwva ke cayana maSIna sIKane elgorixama ke praxarSana para eka viSAla praBAva hE .		
56	56
S1 It is not surprising that the choice of representation has an enormous eect on the performance of machine learning algorithms .	yaha AScarya kI bAwa nahIM hE ki prawiniXiwva kI pasaMxa kA maSIna sIKane ke elgorixama ke praxarSana para BArI praBAva padZawA hE
NP2 It	yaha
VP4_LWG is not	nahIM hE
ADJP7 surprising	hErAnI kI bAwa
SBAR9 that the choice of representation has an enormous eect on the performance of machine learning algorithms	prawiniXiwva kI pasaMxa kA maSIna sIKane ke elgorixama ke praxarSana para BArI asara padZawA hE
S11 the choice of representation has an enormous eect on the performance of machine learning algorithms	prawiniXiwva kI pasaMxa kA maSIna sIKane ke elgorixama ke praxarSana para BArI asara padZawA hE
NP12 the choice of representation	prawiniXiwva kI pasaMxa
NP13 the choice	pasaMxa
PP16 of representation	prawiniXiwva kA
NP18 representation	prawiniXiwva
VP20_LWG has	usake pAsa hE
NP22 an enormous eect on the performance of machine learning algorithms	maSIna sIKane ke elgorixama ke praxarSana para BArI asara
NP23 an enormous eect	eka bahuwa badZA asara
PP27 on the performance of machine learning algorithms	maSIna sIKane ke elgorixama ke praxarSana para
NP29 the performance of machine learning algorithms	maSIna sIKane ke elgorixama kA praxarSana
NP30 the performance	praxarSana
PP33 of machine learning algorithms	maSIna sIKane ke elgorixama
NP35 machine learning algorithms	maSIna sIKa rahe elgorixama
NML36 machine learning	maSIna sIKanA
NNS39 algorithms	elgorixama

----
00571	Deep learning also provides useful tools for processing massive amounts of data and making useful predictions in scientic 25 CHAPTER 1.	 gahana aXyayana BI detA kI BArI mAwrA ke prasaMskaraNa Ora vEjFAnika 25 CHAPTER 1 meM upayogI BaviRyavANiyAM karane ke lie upayogI upakaraNa praxAna karawA hE .		
570	570
S1 Deep learning also provides useful tools for processing massive amounts of data and making useful predictions in scientic 25 CHAPTER 1 .	gaharI sIKa se AMkadZe kI BArI mAwrA meM prasaMskaraNa Ora vEjFAnika 25 cEptara 1 meM upayogI BaviRyavANiyAM banAne ke lie upayogI upakaraNa BI upalabXa howe
NP2 Deep learning	gaharI sIKa
ADVP5 also	sAWa hI
VP7_LWG provides	praxAna karawA hE
NP9 useful tools	upayogI upakaraNa
NNS11 tools	upakaraNa
PP12 for processing massive amounts of data and making useful predictions in scientic 25 CHAPTER 1	detA kI BArI mAwrA meM prasaMskaraNa karane Ora vEjFAnika 25 cEptara 1 meM upayogI BaviRyavANiyAM banAne ke lie
S14 processing massive amounts of data and making useful predictions in scientic 25 CHAPTER 1	BArI mAwrA meM detA kA prasaMskaraNa karanA Ora vEjFAnika 25 cEptara 1 meM upayogI BaviRyavANiyAM banAnA
VP15_LWG processing making	prasaMskaraNa karanA
NP18 massive amounts of data	badZI mAwrA meM detA
NP19 massive amounts	badZe pEmAne para howI hE mAwrA
NNS21 amounts	rASi
PP22 of data	detA kA
NP24 data	detA
NNS25 data	detA
CC26 and	Ora
VP16 processing massive amounts of data	BArI mAwrA meM dAtA prosesiMga
VP27 making useful predictions in scientic 25 CHAPTER 1	vEjFAnika 25 cEptara 1 meM upayogI BaviRyavANiyAM kara rahe hEM
NP29 useful predictions in scientic 25 CHAPTER 1	vEjFAnika 25 cEptara 1 meM upayogI BaviRyavANiyAM
NP30 useful predictions	upayogI BaviRyavANiyAM
NNS32 predictions	BaviRyavANiyAM
PP33 in scientic 25 CHAPTER 1	vEjFAnika 25 cEptara 1 meM
NP35 scientic 25 CHAPTER 1	vEjFAnika 25 cEptara 1
NML36 scientic 25	vEjFAnika 25

----
00572	INTRODUCTION elds.	 INTRODUCTION PZIldsa .		
571	571
S1 INTRODUCTION elds .	paricaya kRewra
NP2 INTRODUCTION	paricaya
VP4_LWG elds	KewoM

----
00573	It has been successfully used to predict how molecules will interact in order to help pharmaceutical companies design new drugs (Dahl et al., 2014), to search for subatomic particles (Baldi et al., 2014), and to automatically parse microscope images used to construct a 3-D map of the human brain (Knowles-Barley et al., 2014).	 isakA saPalawApUrvaka upayoga yaha BaviRyavANI karane ke lie kiyA gayA hE ki kEse aNu xavA kaMpaniyoM ko naI xavAoM ke dijAina meM maxaxa karane ke lie bAwacIwa kareMge ( Dahl eta ala , 2014 ) , upa - BOwika kaNoM kI Koja karane ke lie ( Baldiet al , 2014 ) Ora svacAliwa rUpa se eka maswiRka mAnaciwra ke nirmANa ke lie iswemAla kiyA		
572	572
S1 It has been successfully used to predict how molecules will interact in order to help pharmaceutical companies design new drugs ( Dahl et al. , 2014 ) , to search for subatomic particles ( Baldi et al. , 2014 ) , and to automatically parse microscope images used to construct a 3 - D map of the human brain ( Knowles - Barley et al. , 2014 ) .	yaha BaviRyavANI karane ke lie saPalawApUrvaka iswemAla kiyA gayA hE ki xavA kaMpaniyoM ke nae kaNoM (dAha eta ala, 2014 ) ke nirmANa ke lie aNa
NP2 It	yaha
VP4_LWG has been successfully used	saPalawApUrvaka iswemAla kiyA gayA hE
ADVP8 successfully	saPalawApUrvaka
S12 to predict how molecules will interact in order to help pharmaceutical companies design new drugs ( Dahl et al. , 2014 ) , to search for subatomic particles ( Baldi et al. , 2014 ) , and to automatically parse microscope images used to construct a 3 - D map of the human brain ( Knowles - Barley et al. , 2014 )	yaha BaviRyavANI karane ke lie ki xavA kaMpaniyoM ke nae naSe ke dijAina (dAha eta ala, 2014 ) ke lie upa-paramANu kaNoM kI Koja ke
VP13_LWG to predict to search to automatically parse	aparivarwiwa rUpa se Kojane kI BaviRyavANI karane ke lie
SBAR18 how molecules will interact in order to help pharmaceutical companies design new drugs ( Dahl et al. , 2014 )	xavA kaMpaniyoM ke nae dragsa (dAha eta ala, 2014 ) ke dijAina meM maxaxa ke lie aNu kisa prakAra se bAwacIwa kareMge
WHADVP19 how	kEse
S21 molecules will interact in order to help pharmaceutical companies design new drugs ( Dahl et al. , 2014 )	xavA kaMpaniyoM ke nae xavAoM ke dijAina (dAha eta ala, 2014 ) meM aNuoM kI maxaxa ke lie bAwacIwa hogI
NP22 molecules	aNuoM
NNS23 molecules	aNuoM
VP24_LWG will interact	bAwacIwa kareMge
PP28 in order to help pharmaceutical companies design new drugs	xavA kaMpaniyoM ke nae xavAoM ke dijAina meM maxaxa karane ke lie
NP30 order to help pharmaceutical companies design new drugs	xavA kaMpaniyoM ke nae xavAoM ke dijAina meM maxaxa karane ke AxeSa
S32 to help pharmaceutical companies design new drugs	xavA kaMpaniyoM ke nae xavAoM ke dijAina meM maxaxa karane ke lie
VP33_LWG to help	maxaxa ke lie
S37 pharmaceutical companies design new drugs	xavA kaMpaniyAM naI xavAoM kA dijAina
NP38 pharmaceutical companies	xavA kaMpaniyoM
NNS40 companies	kaMpaniyoM
VP41_LWG design	dijAina
NP43 new drugs	naI xavAeM
NNS45 drugs	dragsa
PRN46 ( Dahl et al. , 2014 )	(dAha eta ala., 2014 )
NP48 Dahl et al. , 2014	dAhala eta ala., 2014
NP49 Dahl et al.	dAhala eta ala
NP50 Dahl	dAha
ADVP52 et al.	eta ala
,55 ,	,
NP56 2014	2014
,59 ,	,
PP64 for subatomic particles ( Baldi et al. , 2014 )	upa-paramANu kaNoM ke lie (bAldI eta ala., 2014 )
NP66 subatomic particles ( Baldi et al. , 2014 )	upa-paramANu kaNa (bAldI eta ala., 2014 )
NNS68 particles	kaNa
PRN69 ( Baldi et al. , 2014 )	(bAldI eta ala., 2014 )
NP71 Baldi et al. , 2014	bAlxI eta ala., 2014
NP72 Baldi et al.	bAldI eta ala
NP73 Baldi	bAldI
ADVP75 et al.	eta ala
,78 ,	,
NP79 2014	2014
,82 ,	,
CC83 and	Ora
VP84 to automatically parse microscope images used to construct a 3 - D map of the human brain ( Knowles - Barley et al. , 2014 )	svacAliwa rUpa se prasAriwa mAikroskopa CaviyoM ko eka 3 - mAnava maswiRka kA dI mEpa banAne ke lie iswemAla kiyA jAwA hE (nolsa - barle eta ala,
ADVP86 automatically	svacAliwa rUpa se
NP90 microscope images used to construct a 3 - D map of the human brain ( Knowles - Barley et al. , 2014 )	mAikroskopa CaviyAM eka 3 - mAnava maswiRka kA dI mEpa banAne ke lie iswemAla kI jAwI WIM (nolsa - barle eta ala., 2014 )
NP91 microscope images	mAikroskopa CaviyAM
NNS93 images	CaviyAM
VP94_LWG used	iswemAla
S96 to construct a 3 - D map of the human brain	3 kA nirmANa karane ke lie - mAnava maswiRka kA dI nakSA
VP97_LWG to construct	banAne ke lie
NP101 a 3 - D map of the human brain	eka 3- mAnava maswiRka kA dI nakSA
NP102 a 3 - D map	eka 3 - dI nakSA
NML104 3 - D	3 - dI
PP109 of the human brain	mAnava maswiRka kA
NP111 the human brain	mAnava maswiRka
PRN115 ( Knowles - Barley et al. , 2014 )	(jFAwa howA hE- barale eta ala., 2014 )
NP117 Knowles - Barley et al.	jAnalevA - barle eta ala
NP118 Knowles	jAnawe hEM
PP120 - Barley	- barale
SYM121 -	-
NP122 Barley	barale
ADVP124 et al.	eta ala
,127 ,	,
NP128 2014	2014

----
00574	We expect deep learning to appear in more and more scientic elds in the future.	 hameM ummIxa hE ki gahare jFAna BaviRya meM aXika se aXika vEjFAnika kRewroM meM prakata hoMge 		
573	573
S1 We expect deep learning to appear in more and more scientic elds in the future .	hameM ummIxa hE ki BaviRya meM aXika se aXika vEjFAnika kRewroM meM gaharI sIKa sAmane AegI
NP2 We	hama
VP4_LWG expect	ummIxa
NP6 deep learning	gaharI sIKa
S9 to appear in more and more scientic elds in the future	BaviRya meM aXika se aXika vEjFAnika kRewroM meM xiKane ke lie
VP10_LWG to appear	xiKane ke lie
PP14 in more and more scientic elds in the future	BaviRya meM aXika se aXika vEjFAnika kRewroM meM
NP16 more and more scientic elds in the future	BaviRya meM aXika se aXika vEjFAnika kRewra
NP17 more and more scientic elds	aXika se aXika vEjFAnika kRewra
ADJP18 more and more scientic	aXika se aXika vEjFAnika
ADVP19 more and more	jyAxA se jyAxA
CC21 and	Ora
RBR20 more	jyAxA
RBR22 more	jyAxA
NNS24 elds	KewoM
PP25 in the future	BaviRya meM
NP27 the future	BaviRya

----
00575	In summary, deep learning is an approach to machine learning that has drawn heavily on our knowledge of the human brain, statistics and applied math as it developed over the past several decades.	 sArAMSa meM , gaharI sIKane maSIna sIKane ke lie eka xqRtikoNa hE ki mAnava maswiRka , sAMKyikI Ora anuprayukwa gaNiwa ke hamAre jFAna para BArI AkarRiwa kiyA hE ke rUpa meM yaha piCale kaI xaSakoM meM vikasiwa kiyA hE .		
574	574
S1 In summary , deep learning is an approach to machine learning that has drawn heavily on our knowledge of the human brain , statistics and applied math as it developed over the past several decades .	varNana meM, gaharI sIKa maSIna sIKane kA xqRtikoNa hE jisane mAnava maswiRka ke hamAre jFAna, AMkadZe Ora Avexana gaNiwa para BArI KIMca liyA hE
PP2 In summary	viswAra meM
NP4 summary	varNana
,6 ,	,
NP7 deep learning	gaharI sIKa
VP10_LWG is	hE
NP12 an approach to machine learning that has drawn heavily on our knowledge of the human brain , statistics and applied math as it developed over the past several decades	maSIna sIKane kA eka xqRtikoNa jisane mAnava maswiRka ke hamAre jFAna, AMkadZe Ora gaNiwa ko lAgU kiyA hE kyoMki yaha piCale kaI xaSaka se vikasiwa
NP13 an approach	eka xqRtikoNa
PP16 to machine learning that has drawn heavily on our knowledge of the human brain , statistics and applied math as it developed over the past several decades	maSIna sIKane ke lie jisane mAnava maswiRka ke hamAre jFAna, AMkadZe Ora gaNiwa lagAyA hE jEsA ki piCale kaI xaSakoM se vikasiwa huA
NP18 machine learning that has drawn heavily on our knowledge of the human brain , statistics and applied math as it developed over the past several decades	maSIna sIKane jisane mAnava maswiRka ke hamAre jFAna, AMkadZe Ora gaNiwa lagAe jEsA ki piCale kaI xaSaka se vikasiwa huA
NP19 machine learning	maSIna sIKanA
SBAR22 that has drawn heavily on our knowledge of the human brain , statistics and applied math as it developed over the past several decades	jisane mAnava maswiRka ke hamAre jFAna, AMkadZe Ora gaNiwa lagAe jEsA ki piCale kaI xaSaka se vikasiwa huA
WHNP23 that	vaha
S25 has drawn heavily on our knowledge of the human brain , statistics and applied math as it developed over the past several decades	mAnava maswiRka ke hamAre jFAna, AMkadZe Ora gaNiwa lagAyA hE kyoMki yaha piCale kaI xaSaka se vikasiwa huA
VP26_LWG has drawn heavily	BArI KIMca cuke hEM
ADVP30 heavily	BArI-Barakama
PP32 on our knowledge of the human brain , statistics and applied math	mAnava maswiRka ke hamAre jFAna para AMkadZe, AMkadZe Ora gaNiwa lagAe
NP34 our knowledge of the human brain , statistics and applied math	mAnava maswiRka ke hamAre jFAna, AMkadZe Ora gaNiwa lagAe
NP35 our knowledge	hamArA jFAna
PP38 of the human brain , statistics and applied math	mAnava maswiRka ke, AMkadZe Ora gaNiwa lagAe
NP40 the human brain , statistics and applied math	mAnava maswiRka, AMkadZe Ora gaNiwa lagAyA
NP41 the human brain	mAnava maswiRka
,45 ,	,
NP46 statistics	AMkadZe
NNS47 statistics	AMkadZe
CC48 and	Ora
NP49 applied math	gaNiwa lagAyA
SBAR52 as it developed over the past several decades	jEsA ki piCale kaI xaSaka se vikasiwa huA
S54 it developed over the past several decades	yaha piCale kaI xaSaka se vikasiwa huA
NP55 it	yaha
VP57_LWG developed	vikasiwa
PP59 over the past several decades	piCale kaI xaSaka se
NP61 the past several decades	piCale kaI xaSaka
NNS65 decades	xaSakoM

----
00576	In recent years, deep learning has seen tremendous growth in its popularity and usefulness, largely as the result of more powerful computers, larger datasets and techniques to train deeper networks.	 hAla ke varRoM meM , gaharI sIKane ne apanI lokapriyawA Ora upayogiwA meM awyaXika vqxXi xeKI hE , muKya rUpa se aXika SakwiSAlI kaMpyUtaroM , gahare netavarkoM ko praSikRiwa karane ke lie badZe detAseta Ora wakanIkoM ke pariNAmasvarUpa .		
575	575
S1 In recent years , deep learning has seen tremendous growth in its popularity and usefulness , largely as the result of more powerful computers , larger datasets and techniques to train deeper networks .	hAla ke varRoM meM gaharI sIKa meM isakI lokapriyawA Ora upayogiwA meM jabaraxaswa vqxXi xeKane ko milI hE, badZe pEmAne para kampyUtara, badZe detAbesa Ora
PP2 In recent years	hAla ke varRoM meM
NP4 recent years	hAla ke varRoM meM
NNS6 years	sAla-ba-ba-ba-ba-ba-ba-ba.
,7 ,	,
NP8 deep learning	gaharI sIKa
VP11_LWG has seen	xeKA hE najara
NP15 tremendous growth	jabaraxaswa vqxXi
PP18 in its popularity and usefulness , largely as the result of more powerful computers , larger datasets and techniques	apanI lokapriyawA Ora upayogiwA meM, Bale hI aXika SakwiSAlI kaMpyUtara, badZe detAbetsa Ora wakanIkoM kA pariNAma
PP19 in its popularity and usefulness	apanI lokapriyawA Ora upayogiwA meM
NP21 its popularity and usefulness	isakI lokapriyawA Ora upayogiwA
CC24 and	Ora
NN23 popularity	lokapriyawA
NN25 usefulness	upayogiwA
,26 ,	,
PP28 as the result of more powerful computers , larger datasets and techniques	aXika SakwiSAlI kaMpyUtara, badZe detAbetsa Ora wakanIkoM kA pariNAma
NP30 the result of more powerful computers , larger datasets and techniques	aXika SakwiSAlI kaMpyUtara, badZe detAbetsa Ora wakanIkoM kA pariNAma
NP31 the result	pariNAma
PP34 of more powerful computers , larger datasets and techniques	aXika SakwiSAlI kaMpyUtara, badZe detAbetsa Ora wakanIkoM kI
NP36 more powerful computers , larger datasets and techniques	aXika SakwiSAlI kaMpyUtara, badZe detAbesa Ora wakanIkI
NP37 more powerful computers	aXika SakwiSAlI kaMpyUtara
ADJP38 more powerful	aXika SakwiSAlI
NNS41 computers	kaMpyUtara
,42 ,	,
NP43 larger datasets and techniques	badZe detAbesa Ora wakanIkI
NNS45 datasets	detAbesa
CC46 and	Ora
NNS47 techniques	wakanIkoM
S48 to train deeper networks	gahare netavarkoM ko praSikRiwa karane ke lie
VP49_LWG to train	treniMga ke lie
NP53 deeper networks	gahare netavarka
NNS55 networks	netavarka

----
00577	The years ahead are full of challenges and opportunities to improve deep learning even further and to bring it to new frontiers.	 Ane vAle varRa cunOwiyoM Ora avasaroM se Bare hEM wAki gahana aXyayana ko Ora Age baDZAyA jA sake Ora ise naI sImAoM meM lAyA jA sake 		
576	576
S1 The years ahead are full of challenges and opportunities to improve deep learning even further and to bring it to new frontiers .	Age BI gaharI sIKa suXArane Ora ise naI sImAoM meM lAne ke avasaroM se Bare hue hEM
NP2 The years	varRoM kI
NNS4 years	sAla-ba-ba-ba-ba-ba-ba-ba.
ADVP5 ahead	Age
VP7_LWG are	hEM
ADJP9 full of challenges and opportunities to improve deep learning even further and to bring it to new frontiers	cunOwiyoM se BarI Ora gaharI sIKa ko Age BI behawara banAne Ora ise naI sImAoM meM lAne ke avasaroM
PP11 of challenges and opportunities	cunOwiyoM Ora avasaroM kI
NP13 challenges and opportunities	cunOwiyAM Ora avasara
NNS14 challenges	cunOwiyAM
CC15 and	Ora
NNS16 opportunities	mOke
S17 to improve deep learning even further and to bring it to new frontiers	Age BI gaharI sIKa ko behawara banAne Ora ise naI sImAoM meM lAne ke lie
VP18_LWG to improve even further to bring	lAne ke lie Ora BI suXAra karane ke lie
NP23 deep learning	gaharI sIKa
ADVP26 even	yahAM waka ki
ADVP28 further	Age
CC30 and	Ora
VP19 to improve deep learning even further	Age BI gaharI sIKa meM suXAra lAne ke lie
VP31 to bring it to new frontiers	ise naI sImAoM meM lAne ke lie
NP35 it	yaha
PP37 to new frontiers	naI sImAoM ko
NP39 new frontiers	naI sImAeM
NNS41 frontiers	sImAeM

----
00578	26	 26		
577	577
FRAG1 26	26
NP2 26	26

----
0058	For a simple visual example, see gure 1.1.	 eka sarala xqSya uxAharaNa ke lie , xeKeM Akqwi 1 . 1 .		
57	57
S1 For a simple visual example , see gure 1.1 .	eka sAXAraNa xqSya uxAharaNa ke lie, AMkadZA 1.1 xeKeM
PP2 For a simple visual example	eka sAXAraNa xqSya uxAharaNa ke lie
NP4 a simple visual example	eka sAXAraNa xqSya uxAharaNa
,9 ,	,
VP10_LWG see	xeKeM
NP12 gure 1.1	AMkadZA 1.1

----
0059	Many articial intelligence tasks can be solved by designing the right set of features to extract for that task, then providing these features to a simple machine learning algorithm.	 kaI kqwrima buxXi kAryoM ko usa kArya ke lie nikAlane ke lie suviXAoM ke sahI seta ko dijAina karake hala kiyA jA sakawA hE , wo ina suviXAoM ko eka sarala maSIna sIKane elgoriWma ke lie praxAna karawA hE .		
58	58
S1 Many articial intelligence tasks can be solved by designing the right set of features to extract for that task , then providing these features to a simple machine learning algorithm .	usa kArya ke lie nikAlane ke lie viSeRawAoM ke sahI seta ko dijAina karake kaI kqwrima buxXimawwA kAryoM kA samAXAna kiyA jA sakawA hE, Pira ina viSeRawAoM ko
NP2 Many articial intelligence tasks	kaI kqwrima buxXimawwApUrNa buxXi kArya
NNS6 tasks	kAryoM
VP7_LWG can be solved	hala kiyA jA sakawA hE
PP13 by designing the right set of features	PIcarsa ke rAita seta ko dijAina kara
S15 designing the right set of features	PIcarsa ke rAita seta kA dijAina
VP16_LWG designing	dijAiniMga
NP18 the right set of features	PIcarsa kA sahI seta
NP19 the right set	sahI seta
PP23 of features	PIcarsa kI
NP25 features	PIcarsa
NNS26 features	PIcarsa
S27 to extract for that task	usa kArya ke lie nikAlane ke lie
VP28_LWG to extract	nikAlane ke lie
PP32 for that task	usa kArya ke lie
NP34 that task	vaha kArya
,37 ,	,
S38 then providing these features to a simple machine learning algorithm	Pira ina PIcarsa ko eka sAXAraNa maSIna sIKane ke elgorixama ko upalabXa karavAnA
ADVP39 then	Pira
VP41_LWG providing	praxAna karanA
NP43 these features	ye PIcarsa
NNS45 features	PIcarsa
PP46 to a simple machine learning algorithm	eka sAXAraNa maSIna sIKane ke elgorixama ke lie
NP48 a simple machine learning algorithm	eka sAXAraNa maSIna sIKa rahA elgorixama
NML51 machine learning	maSIna sIKanA

----
0060	For example, a useful feature for speaker identication from sound is an estimate of the size of the speakers vocal tract.	 uxAharaNa ke lie , Xvani se vakwA kI pahacAna ke lie eka upayogI viSeRawA vakwA ke svara waMwra ke AkAra kA anumAna hE 		
59	59
S1 For example , a useful feature for speaker identication from sound is an estimate of the size of the speaker s vocal tract .	uxAharaNa ke lie, spIkara kI pahacAna ke lie upayogI viSeRawA spIkara ke muKara mArga ke AkAra kA anumAna hE
PP2 For example	uxAharaNa ke lie
NP4 example	uxAharaNa
,6 ,	,
NP7 a useful feature for speaker identication from sound	Xvani se spIkara kI pahacAna ke lie eka upayogI viSeRawA
NP8 a useful feature	eka upayogI viSeRawA
PP12 for speaker identication from sound	Xvani se spIkara kI pahacAna ke lie
NP14 speaker identication from sound	spIkara kI pahacAna Xvani se
NP15 speaker identication	spIkara kI pahacAna
PP18 from sound	Xvani se
NP20 sound	AvAja
VP22_LWG is	hE
NP24 an estimate of the size of the speaker s vocal tract	spIkara ke muKara mArga ke AkAra kA anumAna
NP25 an estimate	eka anumAna
PP28 of the size of the speaker s vocal tract	spIkara ke muKara mArga ke AkAra kA
NP30 the size of the speaker s vocal tract	spIkara kI muKara XArA kA AkAra
NP31 the size	AkAra
PP34 of the speaker s vocal tract	spIkara kI muKara XArA
NP36 the speaker s vocal tract	spIkara kI muKara XArA
NML38 speaker s	spIkara kI

----
0061	This feature gives a strong clue as to whether the speaker is a man, woman, or child.	 yaha viSeRawA isa bAwa kA majabUwa saMkewa xewI hE ki vakwA puruRa hE yA swrI , yA baccA 		
60	60
S1 This feature gives a strong clue as to whether the speaker is a man , woman , or child .	yaha suviXA eka majabUwa surAga xewI hE jEse ki spIkara eka puruRa, mahilA, yA baccA ho
NP2 This feature	yaha viSeRawA
VP5_LWG gives as	jEsA xewA hE vEsA
NP7 a strong clue	eka majabUwa surAga
ADVP11 as to whether the speaker is a man , woman , or child	jEse ki spIkara AxamI, swrI ho, yA bacce
PP13 to whether the speaker is a man , woman , or child	cAhe bolane vAlA AxamI, swrI ho, yA bacce
SBAR15 whether the speaker is a man , woman , or child	cAhe spIkara manuRya, swrI ho, yA bacce
S17 the speaker is a man , woman , or child	spIkara hE puruRa, swrI, yA bacce
NP18 the speaker	spIkara
VP21_LWG is	hE
NP23 a man , woman , or child	eka AxamI, swrI, yA baccA
,26 ,	,
,28 ,	,
CC29 or	yA
NN30 child	bacce

----
0062	For many tasks, however, it is dicult to know what features should be extracted.	 kaI kAryoM ke lie , waWApi , yaha jAnanA kaTina hE ki kina viSeRawAoM ko nikAlA jAnA cAhie .		
61	61
S1 For many tasks , however , it is dicult to know what features should be extracted .	kaI kAryoM ke lie hAlAMki yaha jAnanA muSkila hE ki kina viSeRawAoM ko nikAlA jAnA cAhie
PP2 For many tasks	kaI kAryoM ke lie
NP4 many tasks	kaI kArya
NNS6 tasks	kAryoM
,7 ,	,
ADVP8 however	hAlAMki
,10 ,	,
NP11 it	yaha
VP13_LWG is	hE
ADJP15 dicult to know what features should be extracted	jAnanA muSkila hE ki kina viSeRawAoM ko nikAlA jAnA cAhie
S17 to know what features should be extracted	jAnane ke lie kyA viSeRawAeM nikAlI jAnI cAhie
VP18_LWG to know	jAnane ke lie
SBAR22 what features should be extracted	kyA viSeRawAeM nikAlI jAnI cAhie
WHNP23 what	kyA
S25 features should be extracted	PIcarsa nikAle jAeM
NP26 features	PIcarsa
NNS27 features	PIcarsa
VP28_LWG should be extracted	nikAle jAne cAhie

----
0063	For example, suppose that we would like to write a program to detect cars in photographs.	 uxAharaNa ke lie , mAna lIjie ki hama wasvIroM meM kAroM kA pawA lagAne ke lie eka progrAma liKanA cAheMge .		
62	62
S1 For example , suppose that we would like to write a program to detect cars in photographs .	uxAharaNa ke lie, mAna lIjie ki hama PotogrAPa meM kAroM kA pawA lagAne ke lie eka kAryakrama liKanA cAheMge
PP2 For example	uxAharaNa ke lie
NP4 example	uxAharaNa
,6 ,	,
VP7_LWG suppose	mAna lIjie
SBAR9 that we would like to write a program to detect cars in photographs	ki hama PotogrAPa meM kAroM kA pawA lagAne ke lie eka kAryakrama liKanA cAheMge
S11 we would like to write a program to detect cars in photographs	hama PotogrAPa meM kAroM kA pawA lagAne ke lie eka kAryakrama liKanA cAheMge
NP12 we	hama
VP14_LWG would like	pasaMxa kareMge
S18 to write a program to detect cars in photographs	PotogrAPa meM kAroM kA pawA lagAne ke lie eka kAryakrama liKane ke lie
VP19_LWG to write	liKane ke lie
NP23 a program	eka kAryakrama
S26 to detect cars in photographs	wasvIroM meM kAroM kA pawA lagAne ke lie
VP27_LWG to detect	kA pawA lagAne ke lie
NP31 cars	kAreM
NNS32 cars	kAreM
PP33 in photographs	wasvIroM meM
NP35 photographs	wasvIreM
NNS36 photographs	wasvIreM

----
0064	We know that cars have wheels, so we might like to use the presence of a wheel as a feature.	 hama jAnawe hEM ki kAroM ke pahie howe hEM , isalie hama eka viSeRawA ke rUpa meM eka pahiyA kI upasWiwi kA upayoga karanA pasaMxa ho sakawA hE .		
63	63
S1 We know that cars have wheels , so we might like to use the presence of a wheel as a feature .	hama jAnawe hEM ki kAroM ke pahie howe hEM, isalie hama eka vhIla kI upasWiwi kA upayoga eka suviXA ke rUpa meM karanA cAhawe hEM
S2 We know that cars have wheels	hama jAnawe hEM ki kAroM ke pahie hEM
NP3 We	hama
VP5_LWG know	jAnie
SBAR7 that cars have wheels	usa kAroM ke pahie hEM
S9 cars have wheels	kAroM ke pahie hEM
NP10 cars	kAreM
NNS11 cars	kAreM
VP12_LWG have	pAsa hE
NP14 wheels	pahie
NNS15 wheels	pahie
,16 ,	,
CC17 so	Ese meM
S18 we might like to use the presence of a wheel as a feature	hama eka PIcara ke rUpa meM vhIla kI mOjUxagI kA upayoga karanA cAhawe hEM
NP19 we	hama
VP21_LWG might like	pasaMxa A sakawA hE
S25 to use the presence of a wheel as a feature	PIcara ke wOra para vhIla kI mOjUxagI kA iswemAla karane ke lie
VP26_LWG to use	iswemAla ke lie
NP30 the presence of a wheel as a feature	eka PIcara ke rUpa meM vhIla kI mOjUxagI
NP31 the presence	mOjUxagI
PP34 of a wheel as a feature	eka PIcara ke rUpa meM eka pahiyA kA
NP36 a wheel as a feature	eka PIcara ke rUpa meM eka pahiyA
NP37 a wheel	eka pahiyA
PP40 as a feature	eka viSeRawA ke rUpa meM
NP42 a feature	eka viSeRawA

----
0065	Unfortunately, it is dicult to describe exactly what a wheel looks like in terms of pixel values.	 xurBAgya se , yaha TIka se varNana karane ke lie muSkila hE ki piksela mUlyoM ke saMxarBa meM eka pahiyA kEsA xiKawA hE .		
64	64
S1 Unfortunately , it is dicult to describe exactly what a wheel looks like in terms of pixel values .	xurBAgya se yaha varNana karanA muSkila hE ki piksala mUlyoM ke mAmale meM eka pahiyA kEsA xiKawA hE
ADVP2 Unfortunately	xurBAgya se
,4 ,	,
NP5 it	yaha
VP7_LWG is	hE
ADJP9 dicult to describe exactly what a wheel looks like in terms of pixel values	piksala mUlyoM ke mAmale meM eka pahiyA kEsA xiKawA hE TIka isakA varNana karanA muSkila
S11 to describe exactly what a wheel looks like in terms of pixel values	bilkula varNana karane ke lie ki piksala mUlyoM ke mAmale meM eka pahiyA kEsA xiKawA hE
VP12_LWG to describe	varNana karane ke lie
SBAR16 exactly what a wheel looks like in terms of pixel values	vAswava meM piksala mUlyoM ke mAmale meM eka pahiyA kEsA xiKawA hE
WHNP17 exactly what	vAswava meM kyA
S20 a wheel looks like in terms of pixel values	piksala mUlyoM ke mAmale meM eka pahiyA xiKawA hE
NP21 a wheel	eka pahiyA
VP24_LWG looks	laga rahA hE
PP26 like in terms of pixel values	jEse piksela mUlyoM ke mAmale meM
PP28 in terms of pixel values	piksela mUlyoM ke mAmale meM
NP30 terms of pixel values	piksela mUlyoM kI SarweM
NP31 terms	SarwoM
NNS32 terms	SarwoM
PP33 of pixel values	piksala mUlyoM kI
NP35 pixel values	piksala ke mUlya
NNS37 values	mUlyoM

----
0066	A wheel has a simple geometric shape, but its image may be complicated by shadows falling on the wheel, the sun glaring o the metal parts of the wheel, the fender of the car or an object in the 3 CHAPTER 1.	 eka pahiye kA AkAra sarala jyAmiwIya howA hE , lekina isakI Cavi pahiye para girane vAlI CAyAoM se jatila ho sakawI hE , sUrya pahiye ke XAwu BAgoM se bAhara nikalawA hE , kAra kA PeMdara yA 3 CHAPTER 1 meM koI vaswu		
65	65
S1 A wheel has a simple geometric shape , but its image may be complicated by shadows falling on the wheel , the sun glaring o the metal parts of the wheel , the fender of the car or an object in the 3 CHAPTER 1 .	eka pahiyA meM eka sAXAraNa AnuvaMSika AkAra howA hE, lekina pahie para girane se isakI Cavi jatila ho sakawI hE, vhIla ke XAwu ke hissoM, kAra ke PeMdara yA 3 ca
S2 A wheel has a simple geometric shape	eka pahiyA meM eka sAXAraNa AnuvaMSika AkAra howA hE
NP3 A wheel	eka pahiyA
VP6_LWG has	usake pAsa hE
NP8 a simple geometric shape	eka sAXAraNa AnuvaMSika AkAra
,13 ,	,
CC14 but	lekina
S15 its image may be complicated by shadows falling on the wheel , the sun glaring o the metal parts of the wheel , the fender of the car or an object in the 3 CHAPTER 1	isakI Cavi vhIla para girane se jatila ho sakawI hE, pahie ke XAwu ke hissoM, kAra ke PeMdara yA 3 cEptara 1 meM eka vaswu xvArA ujAgara hone se isakI Ca
NP16 its image	isakI Cavi
VP19_LWG may be	ho sakawA hE
ADJP23 complicated by shadows falling on the wheel , the sun glaring o the metal parts of the wheel , the fender of the car or an object in the 3 CHAPTER 1	vhIla para girane vAlI CAyA se jatila, pahiyA ke XAwu ke hissoM, kAra ke PeMdara yA 3 cEptara 1 meM eka vaswu xvArA GUrawA sUraja
PP25 by shadows falling on the wheel , the sun glaring o the metal parts of the wheel , the fender of the car or an object in the 3 CHAPTER 1	pahiyA para girawI CAyA se, pahiyA ke XAwu ke hissoM, kAra ke PeMdara yA 3 cEptara 1 meM eka vaswu kA camakawA huA sUraja
NP27 shadows falling on the wheel , the sun glaring o the metal parts of the wheel , the fender of the car or an object in the 3 CHAPTER 1	pahie para girane vAlI CAyA, pahiyA ke XAwu ke hissoM, kAra ke PeMdara yA 3 cEptara 1 meM eka vaswu ko baMxa karane vAlA sUraja
NP28 shadows	CAyA
NNS29 shadows	CAyA
VP30_LWG falling	gira rahA hE
PP32 on the wheel , the sun glaring o the metal parts of the wheel , the fender of the car or an object in the 3 CHAPTER 1	pahie para, pahiyA ke XAwu ke hissoM, kAra ke PeMdara yA 3 cEptara 1 meM eka vaswu ko baMxa karane vAlA sUraja
NP34 the wheel , the sun glaring o the metal parts of the wheel , the fender of the car or an object in the 3 CHAPTER 1	pahiyA, pahiyA ke XAwu ke hissoM, kAra ke PeMdara yA 3 cEptara 1 meM eka vaswu ko baMxa karane vAlA sUraja
NP35 the wheel	pahiyA
,38 ,	,
NP39 the sun glaring o	sUraja BadZaka rahA hE
NP44 the metal parts of the wheel	pahiyA ke XAwu ke hissoM
NP45 the metal parts	XAwu ke hissoM
NNS48 parts	BAgoM
PP49 of the wheel	pahiyA kA
NP51 the wheel	pahiyA
,54 ,	,
NP55 the fender of the car	kAra kA PeMdara
NP56 the fender	PeMdara
PP59 of the car	kAra kI
NP61 the car	kAra
CC64 or	yA
NP65 an object in the 3 CHAPTER 1	3 cEptara 1 meM eka vaswu
NP66 an object	eka vaswu
PP69 in the 3 CHAPTER 1	3 cEptara 1 meM
NP71 the 3 CHAPTER 1	3 cEptara 1
NP72 the 3 CHAPTER	3 cEptara
NP-TMP76 1	1

----
0067	INTRODUCTION	 INTRODUCCR		
66	66
NP1 INTRODUCTION	paricaya

----
0068	 	 		
67	67

----
0069		 . . . . . . . . . . . . . . . . . . . . . . . . . . .		
68	68

----
0070	 	 		
69	69

----
0071		 . . . . . . . . . . . . . . . . . . . . . . . . . . .		
70	70

----
0072	Figure 1.1:	 ciwra 1 . 1 :		
71	71
NP1 Figure 1.1 :	AMkadZA 1.1H

----
0073	Example of dierent representations: suppose we want to separate two categories of data by drawing a line between them in a scatterplot.	 viBinna aByAvexanoM kA uxAharaNaH mAna lIjie ki hama detA kI xo SreNiyoM ko alaga karanA cAhawe hEM eka reKA KIMcakara eka skEtarapoYta meM unake bIca eka reKA KIMcakara .		
72	72
S1 Example of dierent representations : suppose we want to separate two categories of data by drawing a line between them in a scatterplot .	alaga-alaga prawiniXiwvoM kA uxAharaNaH mAna lIjie ki hama eka biKarAva meM unake bIca eka paMkwi nikAlakara detA kI xo SreNiyoM ko alaga karanA cAhawe hEM
NP2 Example of dierent representations	alaga-alaga prawiniXiwvoM kA uxAharaNa
NP3 Example	uxAharaNa
PP5 of dierent representations	alaga-alaga prawiniXiwva kI
NP7 dierent representations	alaga-alaga prawiniXiwva
NNS9 representations	prawiniXiwva
VP11_LWG suppose	mAna lIjie
SBAR13 we want to separate two categories of data by drawing a line between them in a scatterplot	hama eka biKarAva meM unake bIca eka paMkwi nikAlakara dAtA kI xo SreNiyoM ko alaga karanA cAhawe hEM
S14 we want to separate two categories of data by drawing a line between them in a scatterplot	hama eka biKarAva meM unake bIca eka paMkwi nikAlakara dAtA kI xo SreNiyoM ko alaga karanA cAhawe hEM
NP15 we	hama
VP17_LWG want	cAhawe hEM
S19 to separate two categories of data by drawing a line between them in a scatterplot	biKarAva meM unake bIca eka paMkwi nikAlakara dAtA kI xo SreNiyoM ko alaga karane ke lie
VP20_LWG to separate	alaga karane ke lie
NP24 two categories of data	detA kI xo SreNiyAM
NP25 two categories	xo SreNiyAM
NNS27 categories	SreNiyoM
PP28 of data	detA kA
NP30 data	detA
NNS31 data	detA
PP32 by drawing a line between them in a scatterplot	biKarAva meM unake bIca eka paMkwi nikAlakara
S34 drawing a line between them in a scatterplot	biKarAva meM unake bIca eka paMkwi KIMcanA
VP35_LWG drawing	drAiMga
NP37 a line between them	unake bIca eka paMkwi
NP38 a line	eka paMkwi
PP41 between them	unake bIca
NP43 them	unheM
PP45 in a scatterplot	eka biKarAva meM
NP47 a scatterplot	eka biKarAva

----
0074	In the plot on the left, we represent some data using Cartesian coordinates, and the task is impossible.	 bAIM ora ke BUKaMda meM , hama kArtesiyana nirxeSAMka kA upayoga kara kuCa detA kA prawiniXiwva karawe hEM , Ora kArya asaMBava hE .		
73	73
S1 In the plot on the left , we represent some data using Cartesian coordinates , and the task is impossible .	bAIM ora BUKaMda meM, hama kArtesiyAna nirxeSAMka kA upayoga karake kuCa detA kA prawiniXiwva karawe hEM, Ora kArya asaMBava hE
S2 In the plot on the left , we represent some data using Cartesian coordinates	bAIM ora BUKaMda meM, hama kArtesiyAna nirxeSAMka kA upayoga karake kuCa detA kA prawiniXiwva karawe hEM
PP3 In the plot on the left	bAIM ora plAta meM
NP5 the plot on the left	bAIM ora sAjiSa
NP6 the plot	plAta
PP9 on the left	bAIM ora
NP11 the left	bAIM ora
,14 ,	,
NP15 we	hama
VP17_LWG represent	prawiniXiwva
NP19 some data	kuCa dAtA
NNS21 data	detA
S22 using Cartesian coordinates	kArtesiyAna nirxeSAMka kA upayoga karanA
VP23_LWG using	iswemAla karanA
NP25 Cartesian coordinates	kArtesiyAna nirxeSAMka
NNS27 coordinates	nirxeSAMka
,28 ,	,
CC29 and	Ora
S30 the task is impossible	kAma asaMBava hE
NP31 the task	tAska
VP34_LWG is	hE
ADJP36 impossible	asaMBava

----
0075	In the plot on the right, we represent the data with polar coordinates and the task becomes simple to solve with a vertical line.	 xAeM BUKaMda meM , hama XruvIya nirxeSAMka ke sAWa detA kA prawiniXiwva karawe hEM Ora kArya eka UrXvAXara lAina ke sAWa hala karane ke lie sarala ho jAwA hE .		
74	74
S1 In the plot on the right , we represent the data with polar coordinates and the task becomes simple to solve with a vertical line .	xAIM ora BUKaMda meM, hama XruvIya nirxeSAMka ke sAWa detA kA prawiniXiwva karawe hEM Ora eka varlda lAina ke sAWa samAXAna karanA sAXAraNa ho jAwA hE
PP2 In the plot on the right	xAIM ora BUKaMda meM
NP4 the plot on the right	xAIM ora sAjiSa
NP5 the plot	plAta
PP8 on the right	xAIM ora
NP10 the right	xAIM ora
,13 ,	,
NP14 we	hama
VP16_LWG represent	prawiniXiwva
SBAR18 the data with polar coordinates and the task becomes simple to solve with a vertical line	XruvIya nirxeSAMka vAle AMkadZe Ora vartikala lAina ke sAWa hala karanA sAXAraNa ho jAwA hE kArya
S19 the data with polar coordinates and the task becomes simple to solve with a vertical line	XruvIya nirxeSAMka vAle AMkadZe Ora vartikala lAina ke sAWa hala karanA sAXAraNa ho jAwA hE kArya
NP20 the data with polar coordinates and the task	XruvIya nirxeSAMka vAle AMkadZe Ora kArya
NP21 the data	detA
NNS23 data	detA
PP24 with polar coordinates and the task	XruvIya nirxeSAMka Ora kArya ke sAWa
NP26 polar coordinates and the task	XruvIya nirxeSAMka Ora kArya
NP27 polar coordinates	XruvIya nirxeSAMka
NNS29 coordinates	nirxeSAMka
CC30 and	Ora
NP31 the task	tAska
VP34_LWG becomes	bana jAwA hE
ADJP36 simple to solve with a vertical line	vartikala lAina ke sAWa hala karane ke lie sAXAraNa
S38 to solve with a vertical line	vartikala lAina ke sAWa hala karane ke lie
VP39_LWG to solve	hala karane ke lie
PP43 with a vertical line	vartikala lAina ke sAWa
NP45 a vertical line	eka vartikala lAina

----
0076	(Figure produced in collaboration with David Warde- Farley.) foreground obscuring part of the wheel, and so on.	 devida vArda PArle ke sahayoga se wEyAra kiyA gayA yaha kapadZA pahiye ke agravarwI BAga meM sWiwa hE 		
75	75
S1 ( Figure produced in collaboration with David Warde - Farley . )	(devida vArde ke sahayoga se wEyAra kiyA gayA AMkadZA- PZArlI.. )
NP3 Figure	AMkadZA
VP5_LWG produced	uwpAxiwa
PP7 in collaboration	sahayoga se
NP9 collaboration	sahayoga
PP11 with David Warde - Farley	devida vArde ke sAWa- PZArlI
NP13 David Warde - Farley	devida vArde- PZArlI

----
0077	One solution to this problem is to use machine learning to discover not only the mapping from representation to output but also the representation itself.	 isa samasyA kA eka samAXAna yaha hE ki na kevala prawiniXiwva se uwpAxana ke lie mAnaciwraNa kA pawA lagAne ke lie maSIna aXigama kA upayoga kiyA jAe , balki svayaM prawiniXiwva BI kiyA jAe 		
76	76
S1 One solution to this problem is to use machine learning to discover not only the mapping from representation to output but also the representation itself .	isa samasyA kA eka samAXAna maSIna sIKa kA upayoga na kevala prawiniXiwva se Autaputa balki prawiniXiwva ko BI Kojane ke lie hE
NP2 One solution to this problem	isa samasyA kA eka samAXAna
NP3 One solution	eka samAXAna
PP6 to this problem	isa samasyA ko
NP8 this problem	yaha samasyA
VP11_LWG is	hE
S13 to use machine learning to discover not only the mapping from representation to output but also the representation itself	prawiniXiwva se lekara Autaputa waka na kevala mEpiMga kA upayoga karane ke lie maSIna sIKane kA upayoga karanA
VP14_LWG to use	iswemAla ke lie
NP18 machine learning	maSIna sIKanA
S21 to discover not only the mapping from representation to output but also the representation itself	prawiniXiwva se lekara Autaputa waka na kevala mEpiMga kI Koja karanA balki prawiniXiwva BI
VP22_LWG to discover	Kojane ke lie
NP26 not only the mapping from representation to output but also the representation itself	na kevala prawiniXiwva se lekara Autaputa ke lie mEpiMga balki prawiniXiwva BI svayaM
CONJP27 not only	na kevala
NP30 the mapping from representation to output	prawiniXiwva se lekara Autaputa waka mEpiMga
NP31 the mapping	mEpiMga
PP34 from representation to output	prawiniXiwva se lekara Autaputa waka
NP36 representation to output	Autaputa kA prawiniXiwva
NP37 representation	prawiniXiwva
PP39 to output	Autaputa karane ke lie
NP41 output	Autaputa
CC43 but	lekina
ADVP44 also	sAWa hI
NP46 the representation itself	prawiniXiwva svayaM
NP47 the representation	prawiniXiwva
NP50 itself	hI Kuxa

----
0078	This approach is known as representation learning .	 isa xqRtikoNa prawiniXiwva sIKane ke rUpa meM jAnA jAwA hE .		
77	77
S1 This approach is known as representation learning .	yaha xqRtikoNa prawiniXiwva sIKane ke rUpa meM jAnA jAwA hE
NP2 This approach	yaha xqRtikoNa
VP5_LWG is known	jAnA jAwA hE
PP9 as representation learning	prawiniXiwva sIKane ke rUpa meM
NP11 representation learning	prawiniXiwva sIKanA

----
0079	Learned representations often result in much better performance than can be obtained with hand-designed representations.	 vixvawa aByAvexanoM kA pariNAma aksara haswaleKiwa aByAvexanoM se prApwa kie jA sakane kI wulanA meM kahIM behawara praxarSana howA hE .		
78	78
S1 Learned representations often result in much better performance than can be obtained with hand - designed representations .	sIKe hue prawiniXiwva aksara hAWa se prApwa kie jA sakawe hEM jyAxA behawara praxarSana kA pariNAma banawe hEM- dijAina kie gae prawiniXiwva
NP2 Learned representations	sIKA prawiniXiwva
NNS4 representations	prawiniXiwva
ADVP5 often	aksara
VP7_LWG result	pariNAma
PP9 in much better performance	bahuwa behawara praxarSana meM
NP11 much better performance	bahuwa behawara praxarSana
ADJP12 much better	bahuwa behawara
SBAR16 than can be obtained with hand - designed representations	hAWa se prApwa kI jA sakawI hE - dijAina prawiniXiwva
S18 can be obtained with hand - designed representations	hAWa se prApwa kiyA jA sakawA hE- dijAina kie gae prawiniXiwva
VP19_LWG can be obtained	prApwa ho sakawA hE
PP25 with hand - designed representations	hAWa se - dijAina prawiniXiwva
NP27 hand - designed representations	hAWa- dijAina prawiniXiwva
ADJP28 hand - designed	hAWa- dijAina
NNS32 representations	prawiniXiwva

----
0080	They also enable AI systems to rapidly adapt to new tasks, with minimal human intervention.	 ve BI eAI praNAliyoM ko nae kAryoM ke lie wejI se anukUliwa karane meM sakRama banAwA hE , kama se kama mAnavIya haswakRepa ke sAWa .		
79	79
S1 They also enable AI systems to rapidly adapt to new tasks , with minimal human intervention .	ve eAI praNAliyoM ko BI kama se kama mAnavIya haswakRepa ke sAWa nae kAryoM ke lie wejI se anukUla banAne meM sakRama banAwe hEM
NP2 They	unhoMne
ADVP4 also	sAWa hI
VP6_LWG enable	sakRama
NP8 AI systems	eAI sistama
NNS10 systems	sistama
S11 to rapidly adapt to new tasks , with minimal human intervention	nae kAryoM ke lie wejI se anukUla banAne ke lie, nyUnawama mAnavIya haswakRepa ke sAWa
VP12_LWG to rapidly adapt	wejI se anukUlana karane ke lie
ADVP14 rapidly	wejI se
PP18 to new tasks	nae kAryoM ko
NP20 new tasks	nae kArya
NNS22 tasks	kAryoM
,23 ,	,
PP24 with minimal human intervention	kama se kama mAnavIya haswakRepa ke sAWa
NP26 minimal human intervention	nyUnawama mAnavIya haswakRepa

----
0081	A representation learning algorithm can discover a good set of features for a simple task in minutes, or for a complex task in hours to months.	 eka prawiniXiwva sIKane elgoriWma minata meM eka sarala kArya ke lie suviXAoM kA eka acCA seta kA pawA lagA sakawe hEM , yA GaMte se mahInoM meM eka jatila kArya ke lie .		
80	80
S1 A representation learning algorithm can discover a good set of features for a simple task in minutes , or for a complex task in hours to months .	eka prawiniXiwva sIKane vAlA elgorixama minatoM meM eka sAXAraNa kArya ke lie viSeRawAoM ke eka acCe seta kI Koja kara sakawA hE, yA GaMtoM se mahIne meM eka ja
NP2 A representation learning algorithm	eka prawiniXiwva sIKane vAlA elgorixama
VP7_LWG can discover	Koja sakawe hEM
NP11 a good set of features for a simple task in minutes , or for a complex task in hours	minatoM meM eka sAXAraNa kArya ke lie suviXAoM kA eka acCA seta, yA GaMtoM meM jatila kArya ke lie
NP12 a good set	eka acCA seta
PP16 of features for a simple task in minutes , or for a complex task in hours	minatoM meM eka sAXAraNa kArya ke lie suviXAoM kI, yA GaMtoM meM jatila kArya ke lie
NP18 features for a simple task in minutes , or for a complex task in hours	minatoM meM eka sAXAraNa kArya ke lie suviXAeM, yA GaMtoM meM jatila kArya ke lie
NP19 features	PIcarsa
NNS20 features	PIcarsa
PP21 for a simple task in minutes , or for a complex task in hours	minatoM meM eka sAXAraNa kArya ke lie, yA GaMtoM meM jatila kArya ke lie
PP22 for a simple task in minutes	minatoM meM sAXAraNa kArya ke lie
NP24 a simple task in minutes	minatoM meM eka sAXAraNa kArya
NP25 a simple task	eka sAXAraNa kArya
PP29 in minutes	minatoM meM
NP31 minutes	minata
NNS32 minutes	minata
,33 ,	,
CC34 or	yA
PP35 for a complex task in hours	GaMtoM meM jatila kArya ke lie
NP37 a complex task in hours	GaMtoM meM eka jatila kArya
NP38 a complex task	eka jatila kArya
PP42 in hours	GaMtoM meM
NP44 hours	GaMte
NNS45 hours	GaMte
PP46 to months	mahInoM waka
NP48 months	mahInoM
NNS49 months	mahInoM

----
0082	Manually designing features for a complex task requires a great deal of human time and eort; it can take decades for an entire community of researchers.	 eka jatila kArya ke lie mEnyuala rUpa se dijAina viSeRawAoM ko mAnava samaya kA eka badZA sOxA kI AvaSyakawA hE Ora prayAsa yaha SoXakarwAoM ke pUre samuxAya meM xaSakoM laga sakawA hE .		
81	81
S1 Manually designing features for a complex task requires a great deal of human time and eort ; it can take decades for an entire community of researchers .	eka jatila kArya ke lie mEnuala dijAiniMga viSeRawAoM ke lie mAnava samaya Ora prayAsa kI bahuwa AvaSyakawA howI hE; anusaMXAnakarwAoM ke eka pUre samuxAya ke lie
S2 Manually designing features for a complex task requires a great deal of human time and eort	eka jatila kArya ke lie mEnuala dijAiniMga viSeRawAoM ko mAnava samaya Ora prayAsa ke lie bahuwa aXika AvaSyakawA howI hE
ADVP3 Manually	mEnualalI
S5 designing features for a complex task	eka jatila kArya ke lie dijAiniMga PIcarsa
VP6_LWG designing	dijAiniMga
NP8 features	PIcarsa
NNS9 features	PIcarsa
PP10 for a complex task	eka jatila kArya ke lie
NP12 a complex task	eka jatila kArya
VP16_LWG requires	jarUrawa hE
NP18 a great deal of human time and eort	mAnava samaya Ora prayAsa kA eka badZA sOxA
NP19 a great deal	eka badZA sOxA
PP23 of human time and eort	mAnava samaya Ora prayAsa kA
NP25 human time and eort	mAnava samaya Ora prayAsa
CC28 and	Ora
NN27 time	samaya
NN29 eort	prayAsa
S31 it can take decades for an entire community of researchers	anusaMXAnakarwAoM ke eka pUre samuxAya ke lie isameM xaSakoM kA samaya laga sakawA hE
NP32 it	yaha
VP34_LWG can take	le sakawe hEM
NP38 decades	xaSakoM
NNS39 decades	xaSakoM
PP40 for an entire community of researchers	anusaMXAnakarwAoM ke eka pUre samuxAya ke lie
NP42 an entire community of researchers	anusaMXAnakarwAoM kA eka pUrA samuxAya
NP43 an entire community	eka pUrA samuxAya
PP47 of researchers	anusaMXAnakarwAoM kA
NP49 researchers	SoXakarwAoM
NNS50 researchers	SoXakarwAoM

----
0083	The quintessential example of a representation learning algorithm is the au- toencoder .	 eka aByAvexana aXigama elgoriWma kA bahusAMskqwika uxAharaNa O - woMkodara hE 		
82	82
S1 The quintessential example of a representation learning algorithm is the au - toencoder .	eka prawiniXiwva sIKane vAle elgorixama kA SuxXa uxAharaNa eyU - tonakodara hE
NP2 The quintessential example of a representation learning algorithm	eka prawiniXiwva sIKane vAle elgorixama kA SuxXa uxAharaNa
NP3 The quintessential example	kviMtalajarUrI uxAharaNa
PP7 of a representation learning algorithm	eka prawiniXiwva sIKane vAlA elgorixama
NP9 a representation learning algorithm	eka prawiniXiwva sIKane vAlA elgorixama
VP14_LWG is	hE
NP16 the au - toencoder	oYu - tonakodara

----
0084	An autoencoder is the combination of an encoder function, which converts the input data into a dierent representation, and a decoder function, which converts the new representation back into the original format.	 eka svacaliwakodaka , eka enakodara Palana kA saMyojana hE , jo inaputa detA ko Binna prawiniXiwva meM baxalawA hE , Ora eka dIkodara Palana hE , jo nae aByAvexana ko vApasa mUla prArUpa meM baxalawA hE .		
83	83
S1 An autoencoder is the combination of an encoder function , which converts the input data into a dierent representation , and a decoder function , which converts the new representation back into the original format .	eka oYtonakodara eka enkoletara PaMkSana kA saMyojana howA hE, jo inaputa detA ko alaga ripreSana meM baxala xewA hE, Ora eka dikodara PaMkSana meM baxala xewA hE, jo
NP2 An autoencoder	eka oYtonakodara
VP5_LWG is	hE
NP7 the combination of an encoder function , which converts the input data into a dierent representation , and a decoder function , which converts the new representation back into the original format	eka enkoletara PaMkSana kA saMyojana, jo inaputa detA ko eka alaga prawiniXiwva meM parivarwiwa karawA hE, Ora eka dikodara PaMkSana meM parivarwiwa karawA hE, jo nae
NP8 the combination of an encoder function , which converts the input data into a dierent representation	eka enkoletara PaMkSana kA saMyojana, jo inaputa dAtA ko alaga prawiniXiwva meM parivarwiwa karawA hE
NP9 the combination	saMyojana
PP12 of an encoder function , which converts the input data into a dierent representation	eka enkodeMta PaMkSana kA, jo inaputa dAtA ko alaga prawiniXiwva meM parivarwiwa karawA hE
NP14 an encoder function , which converts the input data into a dierent representation	eka enkodeMta PaMkSana, jo inaputa dAtA ko alaga prawiniXiwva meM parivarwiwa karawA hE
NP15 an encoder function	eka enkodara PaMkSana
,19 ,	,
SBAR20 which converts the input data into a dierent representation	jo inaputa dAtA ko alaga prawiniXiwva meM parivarwiwa karawA hE
WHNP21 which	jo ki
S23 converts the input data into a dierent representation	inaputa dAtA ko alaga prawiniXiwva meM parivarwiwa karawA hE
VP24_LWG converts	parivarwana karawA hE
NP26 the input data	inaputa dAtA
NNS29 data	detA
PP30 into a dierent representation	eka alaga prawiniXiwva meM
NP32 a dierent representation	eka alaga prawiniXiwva
,36 ,	,
CC37 and	Ora
NP38 a decoder function , which converts the new representation back into the original format	eka dikodara PaMkSana, jo nae prawiniXiwva ko vApasa mUla prArUpa meM parivarwiwa karawA hE
NP39 a decoder function	eka dikodara PaMkSana
,43 ,	,
SBAR44 which converts the new representation back into the original format	jo nae prawiniXiwva ko mUla prArUpa meM vApasa parivarwiwa karawA hE
WHNP45 which	jo ki
S47 converts the new representation back into the original format	nae prawiniXiwva ko vApasa mUla prArUpa meM parivarwiwa karawA hE
VP48_LWG converts back	vApasa parivarwana karawA hE
NP50 the new representation	nayA prawiniXiwva
ADVP54 back into the original format	mUla prArUpa meM vApasa
PP56 into the original format	mUla prArUpa meM
NP58 the original format	mUla prArUpa

----
0085	Autoencoders are trained to preserve as much information as possible when an input is run through the encoder and then the decoder, but they are also trained to make the new representation have various nice properties.	 oYtonakodarsa ko aXika se aXika jAnakArI surakRiwa raKane ke lie praSikRiwa kiyA jAwA hE jaba eka inaputa ko enakodara Ora Pira dikodara ke mAXyama se calAyA jAwA hE , lekina unheM nae prawiniXiwva ko viBinna acCe guNa banAne ke lie BI praSikRiwa kiyA jAwA hE .		
84	84
S1 Autoencoders are trained to preserve as much information as possible when an input is run through the encoder and then the decoder , but they are also trained to make the new representation have various nice properties .	oYtoinakodara ko praSikRiwa kiyA jAwA hE ki jaba enakodara ke mAXyama se koI inaputa calAyA jAwA hE Ora Pira dikodara ke mAXyama se inaputa calAyA jAwA hE
S2 Autoencoders are trained to preserve as much information as possible when an input is run through the encoder and then the decoder	oYtonakodaroM ko praSikRiwa kiyA jAwA hE ki jaba enakodara ke mAXyama se koI inaputa calAyA jAwA hE Ora Pira dikodara
NP3 Autoencoders	oYtonakodara
NNS4 Autoencoders	oYtonakodara
VP5_LWG are trained	praSikRiwa hEM
S9 to preserve as much information as possible when an input is run through the encoder and then the decoder	jaba enakodara ke mAXyama se koI inaputa calAyA jAwA hE Ora usake bAxa dikodara
VP10_LWG to preserve	saMrakRaNa ke lie
PP14 as much information as possible	jiwanA saMBava ho sake jAnakArI
NP16 much information as possible	jiwanA ho sake jyAxA jAnakArI
NP17 much information	bahuwa jyAxA jAnakArI
ADJP20 as possible	jiwanA saMBava ho
SBAR23 when an input is run through the encoder and then the decoder	jaba inakodara ke jarie inaputa calAyA jAwA hE Ora Pira dikodara
WHADVP24 when	kaba
S26 an input is run through the encoder and then the decoder	iMkodara ke jarie eka inaputa calAyA jAwA hE Ora Pira dikodara
NP27 an input	eka inaputa
VP30_LWG is run	calAyA jAwA hE
PP34 through the encoder and then the decoder	iMkodara ke mAXyama se Ora Pira dikodara
NP36 the encoder and then the decoder	iMkodara Ora Pira dikodara
NP37 the encoder	iMkodara
CC40 and	Ora
NP41 then the decoder	Pira dikodara
,45 ,	,
CC46 but	lekina
S47 they are also trained to make the new representation have various nice properties	nae prawiniXiwva ko banAne ke lie BI praSikRiwa hEM viBinna acCe guNa hEM
NP48 they	ve
VP50_LWG are also trained	praSikRiwa BI hEM
ADVP52 also	sAWa hI
S56 to make the new representation have various nice properties	nae prawiniXiwva ko banAne ke lie viBinna acCe guNa howe hEM
VP57_LWG to make	banAne ke lie
S61 the new representation have various nice properties	nae prawiniXiwva meM viBinna acCe guNa howe hEM
NP62 the new representation	nayA prawiniXiwva
VP66_LWG have	pAsa hE
NP68 various nice properties	viBinna acCI guNavawwA
NNS71 properties	guNoM

----
0086	Dierent kinds of autoencoders aim to achieve dierent kinds of properties.	 viBinna prakAra ke oYtonakodarsa kA lakRya viBinna prakAra kI saMpawwiyoM ko prApwa karanA howA hE 		
85	85
S1 Dierent kinds of autoencoders aim to achieve dierent kinds of properties .	viBinna prakAra ke oYtonektaroM kA lakRya viBinna prakAra kI saMpawwiyoM ko hAsila karanA hE
NP2 Dierent kinds of autoencoders	alaga-alaga waraha ke oYtonakodara
NP3 Dierent kinds	alaga-alaga waraha ke
NNS5 kinds	waraha-waraha kI waraha
PP6 of autoencoders	oYtonakodaroM kI
NP8 autoencoders	oYtonakodara
NNS9 autoencoders	oYtonakodara
VP10_LWG aim	uxxeSya
S12 to achieve dierent kinds of properties	alaga-alaga waraha kI saMpawwiyoM ko hAsila karane ke lie
VP13_LWG to achieve	hAsila karane ke lie
NP17 dierent kinds of properties	viBinna prakAra ke guNa
NP18 dierent kinds	alaga-alaga waraha ke
NNS20 kinds	waraha-waraha kI waraha
PP21 of properties	saMpawwiyoM kA
NP23 properties	guNoM
NNS24 properties	guNoM

----
0087	When designing features or algorithms for learning features, our goal is usually to separate the factors of variation that explain the observed data.	 jaba viSeRawAoM yA elgorixama ko sIKane ke lie dijAina kiyA jAwA hE , hamArA lakRya AmawOra para viBinnawA ke kArakoM ko alaga karane ke lie hE ki manAyA detA kI vyAKyA karawe hEM .		
86	86
S1 When designing features or algorithms for learning features , our goal is usually to separate the factors of variation that explain the observed data .	jaba sIKane ke PIcarsa ke lie PIcarsa yA elgorixama dijAina karanA howA hE wo hamArA lakRya AmawOra para usa vikqwi ke kArakoM ko alaga karanA howA
SBAR2 When designing features or algorithms for learning features	jaba sIKane ke PIcarsa ke lie PIcarsa yA elgorixama dijAina
WHADVP3 When	kaba
S5 designing features or algorithms for learning features	sIKane ke PIcarsa ke lie dijAiniMga PIcarsa yA elgorixama
VP6_LWG designing	dijAiniMga
NP8 features or algorithms	PIcarsa yA elgorixama
NNS9 features	PIcarsa
CC10 or	yA
NNS11 algorithms	elgorixama
PP12 for learning features	sIKane ke PIcarsa ke lie
NP14 learning features	sIKane ke PIcarsa
NNS16 features	PIcarsa
,17 ,	,
NP18 our goal	hamArA lakRya
VP21_LWG is usually	AmawOra para howA hE
ADVP23 usually	AmawOra para
S25 to separate the factors of variation that explain the observed data	vinimaya ke kArakoM ko alaga karane ke lie jo nirIkRiwa AMkadZoM kI vyAKyA karawA hE
VP26_LWG to separate	alaga karane ke lie
NP30 the factors of variation that explain the observed data	vinimaya ke kAraka jo nirIkRiwa AMkadZoM kI vyAKyA karawe hEM
NP31 the factors	kAraKAne
NNS33 factors	kAraKAne
PP34 of variation that explain the observed data	viBAjana jo nirIkRiwa AMkadZoM kI vyAKyA karawA hE
NP36 variation that explain the observed data	vinimaya jo nirIkRiwa AMkadZoM kI vyAKyA karawA hE
NP37 variation	viviXawA
SBAR39 that explain the observed data	yaha nirIkRiwa AMkadZoM kI vyAKyA
WHNP40 that	vaha
S42 explain the observed data	avalokiwa AMkadZoM kI vyAKyA kareM
VP43_LWG explain	samaJAwe hEM
NP45 the observed data	avalokiwa AMkadZe
NNS48 data	detA

----
0088	In this 4 CHAPTER 1.	 isameM 4 CHAPTER 1 .		
87	87
FRAG1 In this 4 CHAPTER 1 .	isa 4 cEptara 1 meM
PP2 In this 4 CHAPTER	isa 4 cEptara meM
NP4 this 4 CHAPTER	yaha 4 cEptara
NP8 1	1

----
0089	INTRODUCTION context, we use the word factors simply to refer to separate sources of inuence; the factors are usually not combined by multiplication.	 INTRODUCTION saMxarBa meM , hama PEktara Sabxa kA prayoga karawe hEM , jisase ki viBinna GatakoM xvArA saMyukwa rUpa se praBAviwa hone vAle viBinna srowoM kA ulleKa kiyA jA sake , AmawOra para guNana nahIM howe .		
88	88
S1 INTRODUCTION context , we use the word  factors  simply to refer to separate sources of inuence ; the factors are usually not combined by multiplication .	paricaya saMxarBa, hama kAraka Sabxa kA upayoga kevala praBAva ke alaga-alaga srowoM kA saMxarBiwa karane ke lie karawe hEM; kAraka AmawOra para viBAjana se saMyukwa
S2 INTRODUCTION context , we use the word  factors  simply to refer to separate sources of inuence	paricaya saMxarBa, hama kAraka Sabxa kA upayoga sirPa praBAva ke alaga-alaga srowoM kA saMxarBiwa karane ke lie karawe hEM
NP3 INTRODUCTION context	paricaya saMxarBa
,6 ,	,
NP7 we	hama
VP9_LWG use simply	basa iswemAla
NP11 the word  factors 	Sabxa kAraka
NP12 the word	Sabxa
NP16 factors	kAraKAne
NNS17 factors	kAraKAne
ADVP19 simply	basa iwanA hI
S21 to refer to separate sources of inuence	praBAva ke alaga sUwroM kA jikra karane ke lie
VP22_LWG to refer	saMxarBiwa karane ke lie
PP26 to separate sources of inuence	praBAva ke alaga sUwroM ke lie
NP28 separate sources of inuence	praBAva ke alaga srowa
NP29 separate sources	alaga sUwroM
NNS31 sources	sUwroM
PP32 of inuence	praBAva kA
NP34 inuence	praBAva
S37 the factors are usually not combined by multiplication	kAraka AmawOra para viBAjana se saMyukwa nahIM howe
NP38 the factors	kAraKAne
NNS40 factors	kAraKAne
VP41_LWG are usually not combined	AmawOra para saMyukwa nahIM howe
ADVP43 usually	AmawOra para
PP48 by multiplication	viBAjana se
NP50 multiplication	viBAjana

----
0090	Such factors are often not quantities that are directly observed.	 isa waraha ke kAraka aksara EsI mAwrAeM nahIM hEM jo sIXe xeKI jAwI hEM .		
89	89
S1 Such factors are often not quantities that are directly observed .	Ese kAraKAne aksara mAwrA meM Ese nahIM howe, jinheM sIXe xeKA jAwA hE
NP2 Such factors	Ese kArakoM
NNS4 factors	kAraKAne
VP5_LWG are often	aksara howe hEM
ADVP7 often not	aksara nahIM
NP10 quantities that are directly observed	jina mAwrAoM kA sIXA nirIkRaNa kiyA jAwA hE
NP11 quantities	mAwrA
NNS12 quantities	mAwrA
SBAR13 that are directly observed	jinheM sIXe xeKA jAwA hE
WHNP14 that	vaha
S16 are directly observed	sIXe xeKA jAwA hE
VP17_LWG are directly observed	sIXe xeKA jAwA hE
ADVP19 directly	sIXe wOra para

----
0091	Instead, they may exist as either unobserved objects or unobserved forces in the physical world that aect observable quantities.	 isake bajAya , ve yA wo anArakRiwa vaswuoM ke rUpa meM mOjUxa ho sakawe hEM yA BOwika jagawa meM anArakRiwa SakwiyoM ke rUpa meM , jo xeKane yogya mAwrAoM ko praBAviwa karawe hEM .		
90	90
S1 Instead , they may exist as either unobserved objects or unobserved forces in the physical world that aect observable quantities .	isake bajAya, ve BOwika saMsAra meM yA wo aprawyASiwa vaswuoM yA aprawyASiwa baloM ke rUpa meM mOjUxa ho sakawe hEM jo paryavekRaka mAwrA ko praBAviwa karawA hE
ADVP2 Instead	isake bajAya
,4 ,	,
NP5 they	ve
VP7_LWG may exist	mOjUxa ho sakawA hE
PP11 as either unobserved objects or unobserved forces	jEsA ki yA wo aprawyASiwa vaswueM yA aprawyASiwa baloM
NP13 either unobserved objects or unobserved forces	yA wo aprawyASiwa vaswueM yA aprawyASiwa bala

----
0092	They may also exist as constructs in the human mind that provide useful simplifying explanations or inferred causes of the observed data.	 ve mAnava maswiRka meM nirmANa ke rUpa meM BI mOjUxa ho sakawe hEM jo prekRiwa detA kI upayogI sarala vyAKyA yA anumAniwa kAraNoM ko praxAna karawe hEM .		
91	91
S1 They may also exist as constructs in the human mind that provide useful simplifying explanations or inferred causes of the observed data .	ve mAnava mana meM nirmANa ke rUpa meM BI mOjUxa ho sakawe hEM jo nirIkRiwa AMkadZoM ke upayogI saralIkqwa spaRtIkaraNa yA anuciwa kAraNa praxAna karawA hE
NP2 They	unhoMne
VP4_LWG may also exist inferred	anubaMXiwa BI mOjUxa ho sakawe hEM
ADVP7 also	sAWa hI
PP11 as constructs in the human mind that provide useful simplifying explanations	mAnava mana meM nirmANa ke rUpa meM jo upayogI saralIkqwa spaRtIkaraNa praxAna karawA hE
NP13 constructs in the human mind that provide useful simplifying explanations	mAnava mana meM nirmANa karawA hE jo upayogI saralIkqwa spaRtIkaraNa praxAna karawA hE
NP14 constructs	nirmANa karawA hE
NNS15 constructs	nirmANa karawA hE
PP16 in the human mind that provide useful simplifying explanations	mAnava mana meM jo upayogI saralIkqwa spaRtIkaraNa praxAna karawA hE
NP18 the human mind that provide useful simplifying explanations	mAnava mana jo upayogI saralIkqwa spaRtIkaraNa praxAna karawA hE
NP19 the human mind	mAnava mana
SBAR23 that provide useful simplifying explanations	yaha upayogI saralIkqwa spaRtIkaraNa praxAna karawA hE
WHNP24 that	vaha
S26 provide useful simplifying explanations	upayogI saralIkqwa spaRtIkaraNa upalabXa karAeM
VP27_LWG provide	praxAna kareM
ADJP29 useful simplifying explanations	upayogI saralIkqwa spaRtIkaraNa
S31 simplifying explanations	saralIkqwa spaRtIkaraNa
VP32_LWG simplifying	saralIkqwa
NP34 explanations	spaRtIkaraNa
NNS35 explanations	spaRtIkaraNa
CC36 or	yA
VP5 may also exist as constructs in the human mind that provide useful simplifying explanations	mAnava mana meM nirmANa ke rUpa meM BI mOjUxa ho sakawA hE jo upayogI saralIkqwa spaRtIkaraNa praxAna karawA hE
VP37 inferred causes of the observed data	nirIkRiwa AMkadZoM ke anuciwa kAraNa
NP39 causes of the observed data	avalokiwa AMkadZoM ke kAraNa
NP40 causes	kAraNa
NNS41 causes	kAraNa
PP42 of the observed data	avalokiwa AMkadZoM kA
NP44 the observed data	avalokiwa AMkadZe
NNS47 data	detA

----
0093	They can be thought of as concepts or abstractions that help us make sense of the rich variability in the data.	 ve avaXAraNAoM yA amUrwanoM ke rUpa meM socA jA sakawA hE jo hameM detA meM samqxXa viviXawA kA ehasAsa karAne meM maxaxa karawe hEM .		
92	92
S1 They can be thought of as concepts or abstractions that help us make sense of the rich variability in the data .	unheM avaXAraNA yA avawAra ke rUpa meM socA jA sakawA hE jo hameM detA meM samqxXa viviXawA kI samaJa banAne meM maxaxa karawA hE
NP2 They	unhoMne
VP4_LWG can be thought	socA jA sakawA hE
PP10 of as concepts or abstractions that help us make sense of the rich variability in the data	avaXAraNA yA avawAra ke rUpa meM jo hameM detA meM samqxXa viviXawA kI samaJa banAne meM maxaxa karawA hE
NP13 concepts or abstractions that help us make sense of the rich variability in the data	avaXArAeM yA avawAra jo hameM detA meM samqxXa viviXawA kI samaJa banAne meM maxaxa karawe hEM
NP14 concepts or abstractions	avaXAraNA yA avawAra
NNS15 concepts	avaXAraNAez
CC16 or	yA
NNS17 abstractions	amUrwawA
SBAR18 that help us make sense of the rich variability in the data	yaha hameM detA meM samqxXa viviXawA kI samaJa banAne meM maxaxa karawA hE
WHNP19 that	vaha
S21 help us make sense of the rich variability in the data	detA meM amIra viviXawA kA BAva banAne meM hamArI maxaxa kareM
VP22_LWG help	maxaxa karo
S24 us make sense of the rich variability in the data	hama detA meM samqxXa viviXawA kA arWa banAwe hEM
NP25 us	hameM
VP27_LWG make	banAo
NP29 sense of the rich variability in the data	detA meM amIra viviXawA kI BAvanA
NP30 sense	samaJaxArI
PP32 of the rich variability in the data	detA meM amIra viviXawA kI
NP34 the rich variability in the data	detA meM amIra viviXawA
NP35 the rich variability	amIra viviXawA
PP39 in the data	AMkadZoM meM
NP41 the data	detA
NNS43 data	detA

----
0094	When analyzing a speech recording, the factors of variation include the speakers age, their sex, their accent and the words they are speaking.	 jaba kisI BARaNa rikoYrdiMga kA viSleRaNa kiyA jAwA hE wo viBinnawA ke kArakoM meM vakwA kI Ayu , unakA liMga , unake lahaje Ora ve jo Sabxa bola rahe hEM SAmila hEM .		
93	93
S1 When analyzing a speech recording , the factors of variation include the speaker s age , their sex , their accent and the words they are speaking .	jaba eka BARaNa rikoYrdiMga kA viSleRaNa karawe hEM wo viviXawA ke kArakoM meM spIkara kI umra, unakA liMga, unakA uccAraNa Ora una SabxoM ko SAmila kiyA jAwA hE jo
SBAR2 When analyzing a speech recording , the factors of variation include the speaker s age , their sex , their accent and the words	jaba eka BARaNa rikoYrdiMga kA viSleRaNa karawe hEM wo viviXawA ke kArakoM meM spIkara kI umra, unake liMga, unake uccAraNa Ora Sabxa SAmila hEM
WHADVP3 When	kaba
S5 analyzing a speech recording , the factors of variation include the speaker s age , their sex , their accent and the words	eka BARaNa rikoYrdiMga kA viSleRaNa karawe hue, vErieMteSana ke kArakoM meM spIkara kI umra, unake liMga, unake uccAraNa Ora Sabxa SAmila
S6 analyzing a speech recording	eka BARaNa rikoYrdiMga kA viSleRaNa
VP7_LWG analyzing	viSleRaNa
NP9 a speech recording	eka BARaNa rikoYrdiMga
,13 ,	,
NP14 the factors of variation	viviXawA ke kAraka
NP15 the factors	kAraKAne
NNS17 factors	kAraKAne
PP18 of variation	viviXawA kA
NP20 variation	viviXawA
VP22_LWG include	SAmila
NP24 the speaker s age , their sex , their accent and the words	spIkara kI umra, unakA seksa, unakA uccAraNa Ora Sabxa
NP25 the speaker s age	spIkara kI umra
,30 ,	,
NP31 their sex	unakA seksa
,34 ,	,
NP35 their accent	unakA lahajA
CC38 and	Ora
NP39 the words	Sabxa
NNS41 words	Sabxa
NP42 they	ve
VP44_LWG are speaking	bola rahe hEM

----
0095	When analyzing an image of a car, the factors of variation include the position of the car, its color, and the angle and brightness of the sun.	 jaba kisI kAra kI Cavi kA viSleRaNa kiyA jAwA hE , wo viBinnawA ke kArakoM meM kAra kI sWiwi , usake raMga , Ora sUrya ke koNa Ora camaka SAmila hEM .		
94	94
S1 When analyzing an image of a car , the factors of variation include the position of the car , its color , and the angle and brightness of the sun .	jaba kisI kAra kI Cavi kA viSleRaNa karawe hEM wo verieSana ke kArakoM meM kAra kI sWiwi, usakA raMga, Ora sUraja kI koNa Ora camaka SAmila hEM
SBAR2 When analyzing an image of a car	jaba kAra kI Cavi kA viSleRaNa
WHADVP3 When	kaba
S5 analyzing an image of a car	eka kAra kI Cavi kA viSleRaNa
VP6_LWG analyzing	viSleRaNa
NP8 an image of a car	kAra kI Cavi
NP9 an image	eka Cavi
PP12 of a car	eka kAra kI
NP14 a car	eka kAra
,17 ,	,
NP18 the factors of variation	viviXawA ke kAraka
NP19 the factors	kAraKAne
NNS21 factors	kAraKAne
PP22 of variation	viviXawA kA
NP24 variation	viviXawA
VP26_LWG include	SAmila
NP28 the position of the car , its color , and the angle and brightness of the sun	kAra kI sWiwi, usakA raMga, Ora sUraja kI koNa Ora camaka
NP29 the position	sWiwi
PP32 of the car , its color , and the angle and brightness of the sun	kAra kA, usakA raMga, Ora sUraja kA koNa Ora camaka
NP34 the car , its color , and the angle and brightness of the sun	kAra, usakA raMga, Ora sUraja kA koNa Ora camaka
NP35 the car	kAra
,38 ,	,
NP39 its color	isakA raMga
,42 ,	,
CC43 and	Ora
NP44 the angle and brightness of the sun	sUraja kA koNa Ora camaka
NP45 the angle and brightness	koNa Ora ujjvalawA
CC48 and	Ora
NN47 angle	koNa
NN49 brightness	ujjvalawA
PP50 of the sun	sUraja kI
NP52 the sun	sUraja

----
0096	A major source of diculty in many real-world articial intelligence applications is that many of the factors of variation inuence every single piece of data we are able to observe.	 kaI vAswavika xuniyA kqwrima buxXi anuprayogoM meM kaTinAI kA eka pramuKa srowa yaha hE ki viBinnawA ke kaI kAraka hara eka detA ke tukadZe ko praBAviwa karawe hEM jise hama xeKane meM sakRama howe hEM .		
95	95
S1 A major source of diculty in many real - world articial intelligence applications is that many of the factors of variation inuence every single piece of data we are able to observe .	kaI vAswavika - viSva kqwrima buxXimawwA anuprayogoM meM kaTinAI kA eka badZA srowa yaha hE ki viviXawA ke kaI kAraka dAtA ke hara eka tukadZe ko praBAviwa karawe hEM ji
NP2 A major source of diculty in many real - world articial intelligence applications	kaI vAswavika - viSva kqwrima KuPiyA AvexanoM meM kaTinAI kA eka badZA srowa
NP3 A major source	eka pramuKa srowa
PP7 of diculty in many real - world articial intelligence applications	kaI vAswavika - viSva kqwrima buxXimawwA anuprayogoM meM kaTinAI kI
NP9 diculty in many real - world articial intelligence applications	kaI vAswavika - viSva kqwrima buxXimawwA anuprayogoM meM kaTinAI
NP10 diculty	muSkila
PP12 in many real - world articial intelligence applications	kaI vAswavika - viSva kqwrima buxXimawwA ke Avexana
NP14 many real - world articial intelligence applications	kaI vAswavika- viSva kqwrima buxXimawwA anuprayoga
NML16 real - world	asalI-xuniyA
NNS22 applications	Avexana
VP23_LWG is	hE
SBAR25 that many of the factors of variation inuence every single piece of data we are able to observe	ki viviXawA ke kaI kAraka dAtA ke hara eka tukadZe ko praBAviwa karawe hEM jinakA hama nirIkRaNa kara pAwe hEM
S27 many of the factors of variation inuence every single piece of data we are able to observe	viviXawA ke kaI kAraka dAtA ke hara eka tukadZe ko praBAviwa karawe hEM jinheM hama xeKa pAwe hEM
NP28 many of the factors of variation	viviXawA ke kaI kAraka
NP29 many	kaI
PP31 of the factors of variation	viviXawA ke kArakoM kI
NP33 the factors of variation	viviXawA ke kAraka
NP34 the factors	kAraKAne
NNS36 factors	kAraKAne
PP37 of variation	viviXawA kA
NP39 variation	viviXawA
VP41_LWG inuence	praBAva
NP43 every single piece of data we are able to observe	detA kA hara eka tukadZA hama xeKa sakawe hEM
NP44 every single piece	hara eka tukadZA
PP48 of data we are able to observe	detA kA hama nirIkRaNa kara pA rahe hEM
NP50 data we are able to observe	detA hama xeKa pA rahe hEM
NP51 data	detA
NNS52 data	detA
SBAR53 we are able to observe	hama nirIkRaNa kara pA rahe hEM
S54 we are able to observe	hama nirIkRaNa kara pA rahe hEM
NP55 we	hama
VP57_LWG are	hEM
ADJP59 able to observe	nirIkRaNa karane meM sakRama
S61 to observe	nirIkRaNa ke lie
VP62_LWG to observe	nirIkRaNa ke lie

----
0097	The individual pixels in an image of a red car might be very close to black at night.	 eka lAla kAra kI Cavi meM vyakwigawa piksala rAwa meM kAle ke bahuwa karIba ho sakawA hE .		
96	96
S1 The individual pixels in an image of a red car might be very close to black at night .	lAla kAra kI Cavi meM vyakwigawa piksala rAwa meM kAle raMga ke bahuwa karIba ho sakawe hEM
NP2 The individual pixels in an image of a red car	lAla kAra kI Cavi meM vyakwigawa piksela
NP3 The individual pixels	vyakwigawa piksela
NNS6 pixels	piksela
PP7 in an image of a red car	lAla kAra kI Cavi meM
NP9 an image of a red car	lAla kAra kI Cavi
NP10 an image	eka Cavi
PP13 of a red car	lAla kAra kI
NP15 a red car	eka lAla kAra
VP19_LWG might be	ho sakawA hE
ADJP23 very close to black	kAle raMga ke bahuwa karIba
PP26 to black	kAle ko
ADJP28 black	kAle raMga ke
PP30 at night	rAwa ko
NP32 night	rAwa

----
0098	The shape of the cars silhouette depends on the viewing angle.	 kAra ke siloPeta kA AkAra xeKane ke koNa para nirBara karawA hE 		
97	97
S1 The shape of the car s silhouette depends on the viewing angle .	kAra kA silhUta xeKane ke koNa para nirBara karawA hE
NP2 The shape of the car s silhouette	kAra ke silhUta ke AkAra
NP3 The shape	AkAra
PP6 of the car s silhouette	kAra ke silhUta ke
NP8 the car s silhouette	kAra kA silhUta
NML10 car s	kAra kI
VP14_LWG depends	nirBara karawA hE
PP16 on the viewing angle	xeKane ke koNa para
NP18 the viewing angle	xeKane kA koNa

----
0099	Most applications require us to disentangle the factors of variation and discard the ones that we do not care about.	 aXikAMSa anuprayogoM ke lie hameM viBinnawA ke kArakoM ko alaga karane Ora una bAwoM ko wyAgane kI AvaSyakawA howI hE jinakI hama paravAha nahIM karawe 		
98	98
S1 Most applications require us to disentangle the factors of variation and discard the ones that we do not care about .	aXikAMSa AvexanoM meM hameM viviXawA ke kArakoM ko viGatiwa karane Ora una logoM ko wyAgane kI AvaSyakawA howI hE, jinakI hameM paravAha nahIM hE
NP2 Most applications	aXikAMSa Avexana
NNS4 applications	Avexana
VP5_LWG require	jarUrawa
S7 us to disentangle the factors of variation and discard the ones that we do not care about	hama viviXawA ke kArakoM ko BaMga karane Ora una logoM ko wyAgane ke lie jinheM hama paravAha nahIM karawe
NP8 us	hameM
VP10_LWG to disentangle discard	wyAgane ke lie wyAgapawra
NP15 the factors of variation	viviXawA ke kAraka
NP16 the factors	kAraKAne
NNS18 factors	kAraKAne
PP19 of variation	viviXawA kA
NP21 variation	viviXawA
CC23 and	Ora
VP13 disentangle the factors of variation	viviXawA ke kArakoM ko nApasaMxa kareM
VP24 discard the ones that we do not care about	una logoM ko wyAga xeM, jinakI hameM paravAha nahIM hE
NP26 the ones	vo loga
NNS28 ones	vAle
SBAR29 that we do not care about	ki hameM paravAha nahIM hE
S31 we do not care about	hameM paravAha nahIM hE
NP32 we	hama
VP34_LWG do not care about	XyAna na xeM
PRT39 about	ke bAre meM

----
00100	Of course, it can be very dicult to extract such high-level, abstract features from raw data.	 beSaka , kacce detA se isa waraha ke ucca swara , amUrwa viSeRawAoM ko nikAlanA bahuwa muSkila ho sakawA hE .		
99	99
S1 Of course , it can be very dicult to extract such high - level , abstract features from raw data .	beSaka, kacce detA se iwane ucca-swara, amUrwa viSeRawAoM ko nikAlanA bahuwa muSkila ho sakawA hE
ADVP2 Of course	beSaka
,5 ,	,
NP6 it	yaha
VP8_LWG can be	ho sakawA hE
ADJP12 very dicult to extract such high - level , abstract features from raw data	Ese ucca - swara, kacce detA se avaSoRiwa suviXAeM nikAlanA bahuwa muSkila
S15 to extract such high - level , abstract features from raw data	Ese ucca - swara nikAlane ke lie kacce detA se avaSoRiwa suviXAeM
VP16_LWG to extract	nikAlane ke lie
NP20 such high - level , abstract features from raw data	Ese ucca - swara, kacce detA se avaSoRiwa suviXAeM
NP21 such high - level , abstract features	Ese ucca - swara, amUrwa viSeRawAeM
NML23 high - level	ucca - swara
,27 ,	,
NNS29 features	PIcarsa
PP30 from raw data	kacce AMkadZoM se
NP32 raw data	kaccA detA
NNS34 data	detA

