अध्याय 1 परिचय निवेशकों लंबे समय से मशीनों है कि लगता है बनाने का सपना देखा है .
यह इच्छा कम से कम प्राचीन यूनान के समय की है ।

जब प्रोग्राम योग्य कंप्यूटरों की पहली कल्पना की गई तो लोगों ने सोचा कि क्या इस तरह की मशीनें बुद्धिमान हो सकती हैं , एक के निर्माण से सौ साल पहले ( Lovlece , 1842 )
आज , कृत्रिम बुद्धि ( एआईआई ) कई व्यावहारिक अनुप्रयोगों और सक्रिय अनुसंधान विषयों के साथ फलता - फूलता क्षेत्र है .
हम बुद्धिमान सॉफ्टवेयर के लिए दिनचर्या श्रम स्वचालित करने के लिए , भाषण या छवियों को समझने के लिए , चिकित्सा में निदान और बुनियादी वैज्ञानिक अनुसंधान का समर्थन करते हैं .
कृत्रिम बुद्धि के आरंभिक दिनों में इस क्षेत्र ने तेजी से उन समस्याओं का समाधान और समाधान किया जो मनुष्यों के लिए बौद्धिक रूप से कठिन हैं लेकिन कंप्यूटर के लिए अपेक्षाकृत सीधे - सीधे आगे हैं -
समस्याओं है कि औपचारिक , गणितीय नियमों की एक सूची द्वारा वर्णित किया जा सकता है .
कृत्रिम बुद्धि के लिए सच्ची चुनौती उन कार्यों को हल करने में सिद्ध हुई जो लोगों के लिए करना आसान हैं लेकिन लोगों के लिए औपचारिक रूप से वर्णन करना कठिन है - ऐसे प्रतीक जिन्हें हम सहज भाव से हल करते हैं , जो बोलचाल के शब्दों या चेहरों को पहचानने की तरह स्वतः महसूस करते हैं ।
यह पुस्तक इन और अधिक सहज ज्ञान युक्त समस्याओं के समाधान के बारे में है ।
यह समाधान कंप्यूटर को अनुभव से सीखने और अवधारणाओं के एक पदानुक्रम के संदर्भ में दुनिया को समझने की अनुमति देने के लिए है , प्रत्येक अवधारणा सरल अवधारणाओं के साथ अपने संबंध के माध्यम से परिभाषित .
अनुभव से ज्ञान इकट्ठा करके , यह दृष्टिकोण कंप्यूटर की आवश्यकता के सभी ज्ञान को औपचारिक रूप से निर्दिष्ट करने के लिए मानव ऑपरेटरों की आवश्यकता से बचता है .
अवधारणाओं का पदानुक्रम कंप्यूटर को सरल अवधारणाओं से बाहर बनाकर जटिल अवधारणाओं को सीखने में सक्षम बनाता है .
यदि हम एक ग्राफ खींचते हैं कि कैसे इन अवधारणाओं 1 CHAPTER 1
INTRODUConstellation name ( optional )
इस कारण से , हम इस दृष्टिकोण एआई गहरी सीखने के लिए कहते हैं .
एआई की प्रारंभिक सफलताओं में से कई अपेक्षाकृत बांझ और औपचारिक वातावरण में हुई और उन्हें कंप्यूटर की दुनिया के बारे में अधिक जानकारी की आवश्यकता नहीं थी .
उदाहरण के लिए , आईबीएम की डीप ब्लू शतरंज खेल प्रणाली ने 1997 में विश्व चैंपियन गारी कास्परोव को हराया ( हासु , 2002 )
शतरंज निस्संदेह एक बहुत ही सरल संसार है , जिसमें केवल साठ - चौंसठ स्थान और चौंतीस टुकड़े हैं जो केवल कठोर रूप से परिबद्ध तरीके से चल सकते हैं ।
एक सफल शतरंज रणनीति को छोड़ना एक जबरदस्त उपलब्धि है , लेकिन चुनौती शतरंज के टुकड़ों के सेट का वर्णन करने और कंप्यूटर के लिए अनुमति योग्य चालों की कठिनाई के कारण नहीं है .
शतरंज पूरी तरह से पूरी तरह से औपचारिक नियमों की एक बहुत संक्षिप्त सूची द्वारा वर्णित किया जा सकता है , आसानी से प्रोग्रामर द्वारा समय से आगे प्रदान की .
विडम्बना यह है कि अमूर्त और औपचारिक कार्य जो मनुष्य के लिए सबसे कठिन मानसिक उपक्रमों में से हैं , कंप्यूटर के लिए सबसे आसान हैं ।
कंप्यूटर लंबे समय से सर्वश्रेष्ठ मानव शतरंज खिलाड़ी को भी हराने में सफल रहे हैं , लेकिन हाल ही में वस्तुओं या भाषण को पहचानने के लिए औसत मानव की कुछ क्षमताओं का मिलान शुरू कर दिया है .
व्यक्ति के दैनिक जीवन के लिए विश्व के बारे में अपार ज्ञान की आवश्यकता होती है ।
इस ज्ञान का अधिकांश भाग व्यक्तिपरक और अन्तर्ज्ञानात्मक है , और इसलिए औपचारिक रूप से इसका प्रतिपादन करना कठिन है ।
कंप्यूटर एक बुद्धिमान तरीके से व्यवहार करने के लिए इस एक ही ज्ञान पर कब्जा करने की जरूरत है .
कृत्रिम बुद्धि में एक प्रमुख चुनौती यह है कि इस अनौपचारिक ज्ञान को कंप्यूटर में कैसे प्राप्त किया जाए ।
कई कृत्रिम बुद्धिमत्ता परियोजनाओं ने दुनिया के बारे में औपचारिक भाषाओं में जानकारी को कड़ाई से जानने की कोशिश की है ।
एक कंप्यूटर तार्किक अनुमान नियमों का उपयोग कर इन औपचारिक भाषाओं में बयानों के बारे में स्वचालित रूप से तर्क कर सकते हैं .
इसे कृत्रिम बुद्धि का ज्ञान आधार दृष्टिकोण कहा जाता है ।
इनमें से किसी भी परियोजना को बड़ी सफलता नहीं मिली है ।
ऐसी सबसे प्रसिद्ध परियोजनाओं में से एक है साइक ( लीनाट और गुह , 1989 )
साइक एक अनुमान इंजन है और साइकल नामक भाषा में कथनों का डेटाबेस है ।
इन बयानों को मानव पर्यवेक्षकों के एक स्टाफ द्वारा दर्ज किया जाता है .
यह एक अपरिच्छिन्न प्रक्रिया है ।
लोग दुनिया का सही वर्णन करने के लिए पर्याप्त जटिलता के साथ औपचारिक नियम बनाने के लिए संघर्ष करते हैं .
उदाहरण के लिए , Cyc सुबह फ्रेड शेव नामक व्यक्ति के बारे में एक कहानी समझने में विफल रहा ( Linde , 1992 )
इसके अनुमान इंजन ने कहानी में एक असंगति का पता लगायाः यह पता था कि लोगों के पास विद्युत पुर्जे नहीं हैं , लेकिन चूंकि फ्रेड एक विद्युत रज़र था , इसलिए यह विश्वास करता था कि विद्युत पुर्जे होते हैं .
इसलिए उसने पूछा कि क्या फ्रेड अभी भी शेव करते समय एक व्यक्ति था ?
कठोर ज्ञान पर भरोसा करने वाली प्रणालियों के सामने आने वाली कठिनाइयों से यह संकेत मिलता है कि एआई प्रणालियों को अपना ज्ञान प्राप्त करने की क्षमता की आवश्यकता है , जिसमें 2 सीएचएपीटर 1 निकाला जाता है ।
कच्चे डेटा से INTRODUC® पैटर्न
इस क्षमता को मशीन लर्निंग के नाम से जाना जाता है ।
मशीन लर्निंग की शुरुआत से कंप्यूटर वास्तविक दुनिया के ज्ञान से संबंधित समस्याओं से निपटने और व्यक्तिपरक दिखने वाले निर्णय लेने में सक्षम हो गए .
संभार प्रतिगमन नामक एक सरल मशीन अधिगम एल्गोरिथ्म यह निर्धारित कर सकता है कि क्या सीजेरियन डिलीवरी की सिफारिश की जाए ( मोर - योसेफ एट अल , 1990 )
एक सरल मशीन लर्निंग एल्गोरिदम जिसे नैव बेज़ कहा जाता है , स्पैम ई - मेल से वैध ई - मेल को अलग कर सकता है ।
इन सरल मशीन अधिगम एल्गोरिदम का प्रदर्शन उनके द्वारा दिए गए डेटा के निरूपण पर अत्यधिक निर्भर करता है .
उदाहरण के लिए , जब संभार प्रतिगमन का प्रयोग सिजेरियन डिलीवरी की सिफारिश के लिए किया जाता है , एआई प्रणाली रोगी की सीधे जांच नहीं करती है .
इसके बजाय , चिकित्सक प्रणाली को प्रासंगिक जानकारी के कई टुकड़े बताता है , जैसे कि गर्भाशय के निशान की उपस्थिति या अनुपस्थिति .
रोगी के प्रतिनिधित्व में शामिल जानकारी के प्रत्येक टुकड़े एक विशेषता के रूप में जाना जाता है .
लॉजिस्टिक प्रतिगमन यह सीखता है कि रोगी की इन विशेषताओं में से प्रत्येक विभिन्न परिणामों से सहसंबंधित कैसे होता है ।
हालांकि , यह प्रभावित नहीं कर सकते कि कैसे सुविधाओं को किसी भी तरह से परिभाषित कर रहे हैं .
यदि संभारिक प्रतिगमन को रोगी का एमआरआई स्कैन दिया जाता , बजाय चिकित्सक की औपचारिक रिपोर्ट के , तो यह उपयोगी भविष्यवाणियां नहीं कर सकेगा .
एमआरआई स्कैन में व्यक्तिगत पिक्सल का प्रसव के दौरान होने वाली किसी भी जटिलताओं से नगण्य सहसंबंध होता है ।
अभ्यावेदनों पर यह निर्भरता एक सामान्य घटना है जो पूरे कंप्यूटर विज्ञान और यहां तक कि दैनिक जीवन में प्रकट होती है .
कंप्यूटर विज्ञान में , डेटा के संग्रह को खोजने जैसी संक्रियाएं घातीय रूप से तेजी से आगे बढ़ सकती हैं यदि कोलेक - टीन को संरचित और बुद्धिमान रूप से अनुक्रमित किया जाए .
लोग अरबी अंकों पर अंकगणित आसानी से कर सकते हैं लेकिन रोमन अंकों पर अंकगणित को अधिक समय लेने पर पाते हैं ।
यह आश्चर्य की बात नहीं है कि प्रतिनिधित्व के चयन का मशीन लर्निंग एल्गोरिदम के प्रदर्शन पर भारी प्रभाव पड़ता है .
एक सरल दृश्य उदाहरण के लिए , देखें आंकड़ा 1 . 1 .
कई कृत्रिम बुद्धि कार्यों को उस कार्य के लिए निकालने के लिए सुविधाओं के सही सेट को डिजाइन करके हल किया जा सकता है , फिर एक सरल मशीन सीखने एल्गोरिथ्म के लिए इन सुविधाओं को प्रदान करता है .
उदाहरण के लिए , ध्वनि से वक्ता की पहचान के लिए एक उपयोगी विशेषता वक्ता के मुखर पथ के आकार का अनुमान है .
इस विशेषता से एक मजबूत सुराग मिलता है कि क्या वक्ता एक पुरुष , महिला , या बच्चा है .
कई कार्यों के लिए , तथापि , यह जानना कठिन है कि किन विशेषताओं को निकाला जाना चाहिए .
उदाहरण के लिए , मान लीजिए कि हम तस्वीरों में कारों का पता लगाने के लिए एक प्रोग्राम लिखना चाहेंगे .
हम जानते हैं कि कारों में पहिए होते हैं , इसलिए हम एक विशेषता के रूप में एक पहिया की उपस्थिति का उपयोग करना पसंद कर सकते हैं .
दुर्भाग्य से , यह बिल्कुल क्या एक पहिया पिक्सेल मूल्यों के संदर्भ में की तरह लग रहा है का वर्णन करने के लिए मुश्किल है .
एक पहिये का आकार सरल ज्यामितीय होता है , लेकिन इसकी छवि चक्र पर पड़ने वाली छायाओं से जटिल हो सकती है , सूर्य पहिये के धातु भागों से चमकता हुआ , कार का लिंग या वस्तु 3CHAPTER 1 में
INTRODUDUDC
.
.
.
.
चित्र 1 . 1
विभिन्न अभ्यावेदनों का उदाहरणः मान लीजिए कि हम एक प्रकीर्णन में उनके बीच एक रेखा खींच कर डेटा की दो श्रेणियों को अलग करना चाहते हैं .
बाईं ओर के भूखंड में , हम कार्टेसियन निर्देशांक का उपयोग कर कुछ डेटा का प्रतिनिधित्व करते हैं , और कार्य असंभव है .
दाहिनी ओर के भूखंड में , हम ध्रुवीय निर्देशांकों के साथ डेटा का प्रतिनिधित्व करते हैं और एक ऊर्ध्वाधर रेखा के साथ हल करने के लिए कार्य सरल हो जाता है .

इस समस्या का एक समाधान मशीन अधिगम का उपयोग न केवल प्रतिनिधित्व से आउटपुट तक मानचित्रण का पता लगाने के लिए करना है , बल्कि स्वयं प्रतिनिधित्व का भी पता लगाना है .
इस दृष्टिकोण प्रतिनिधित्व सीखने के रूप में जाना जाता है .
विद्वत अभ्यावेदनों के परिणामस्वरूप अक्सर हस्तहस्ताक्षरित अभ्यावेदनों से प्राप्त की जा सकने वाली तुलना में कहीं बेहतर प्रदर्शन होता है .
वे एआई सिस्टम को नए कार्यों के लिए तेजी से अनुकूलित करने में भी सक्षम बनाते हैं , जिसमें न्यूनतम मानवीय हस्तक्षेप होता है .
एक प्रतिनिधित्व सीखने एल्गोरिथ्म मिनट में एक सरल कार्य के लिए , या घंटे से महीनों में एक जटिल कार्य के लिए सुविधाओं का एक अच्छा सेट का पता लगा सकते हैं .
मैनुअल रूप से डिजाइन जटिल कार्य के लिए मानव समय की विशेषताओं की आवश्यकता होती है और यह प्रयास शोधकर्ताओं के दशकों के पूरे समुदाय के लिए किया जा सकता है ।
अभ्यावेदन अधिगम एल्गोरिथ्म का सारभूत उदाहरण औ - तोंकडर है ।
एक ऑटोनकोडर , एक एनकोडर फलन का संयोजन होता है , जो इनपुट डेटा को भिन्न निरूपण , और एक विकोडक फलन में परिवर्तित करता है , जो नए निरूपण को वापस मूल प्रारूप में परिवर्तित करता है .
ऑटोनकोडर को अधिक से अधिक जानकारी सुरक्षित रखने के लिए प्रशिक्षित किया जाता है जब एक इनपुट एनकोडर के माध्यम से चलाया जाता है और फिर डिकोडर , लेकिन उन्हें यह भी प्रशिक्षित किया जाता है कि नए प्रतिनिधित्व में विभिन्न अच्छे गुण हों .
विभिन्न प्रकार के ऑटोनकोडर का उद्देश्य विभिन्न प्रकार के गुण प्राप्त करना होता है ।
जब सुविधाओं या एल्गोरिदम सीखने सुविधाओं के लिए डिजाइनिंग , हमारे लक्ष्य आमतौर पर विभिन्नता के कारकों को अलग करने के लिए है कि मनाया डेटा की व्याख्या .
इसमें 4 CHAPTER 1
INTROD & # 44 ; हम प्रयोग करते हैं & # 44 ; अलग - अलग स्रोतों के लिए & # 44 ; " सामान्य रूप से गुणन कारक नहीं होते हैं ।
इस तरह के कारक अक्सर मात्राओं है कि सीधे मनाया जाता है नहीं कर रहे हैं .
इसके बजाय , वे या तो अपरिग्रहित वस्तुओं के रूप में मौजूद हो सकते हैं या भौतिक जगत में अपरिग्रहित बलों के रूप में , जो वेध योग्य मात्राओं को प्रभावित करते हैं .
वे मानव मस्तिष्क में निर्माण के रूप में भी मौजूद हो सकते हैं जो प्रेक्षित डेटा की उपयोगी सरलीकरण व्याख्या या अनुमानित कारणों प्रदान करते हैं .
उन्हें ऐसी अवधारणाओं या अमूर्तनों के रूप में सोचा जा सकता है जो हमें आंकड़ों में समृद्ध भिन्नता का बोध कराने में मदद करती हैं ।
जब किसी भाषण रिकॉर्डिंग का विश्लेषण किया जाता है तो विभिन्नता के कारकों में वक्ता की आयु , उनका लिंग , उनके लहजे और वे शब्द शामिल हैं जो वे बोल रहे हैं .
जब किसी कार की छवि का विश्लेषण किया जाता है तो विभिन्नता के कारकों में कार की स्थिति , उसके रंग , और सूर्य के कोण और चमक शामिल हैं .
कई वास्तविक दुनिया कृत्रिम बुद्धि अनुप्रयोगों में कठिनाई का एक प्रमुख स्रोत यह है कि विभिन्नता के कई कारक हर एक डेटा के टुकड़े को प्रभावित करते हैं जिसका हम निरीक्षण करने में सक्षम हैं .
लाल कार की छवि में व्यक्तिगत पिक्सल रात में काले के बहुत करीब हो सकता है .
कार के सिलोयूट का आकार देखने के कोण पर निर्भर करता है ।
अधिकांश अनुप्रयोगों के लिए हमें विभिन्नता के कारकों को अलग करने और उन कारकों को त्यागने की आवश्यकता होती है जिनकी हम परवाह नहीं करते हैं .
बेशक , कच्चे डेटा से इस तरह के उच्च स्तर , अमूर्त सुविधाओं को निकालना बहुत मुश्किल हो सकता है .
विभिन्नता के इन कारकों में से कई , जैसे कि वक्ता के लहजे , की पहचान केवल परिष्कृत , डेटा की लगभग मानवीय समझ का उपयोग करके की जा सकती है .
जब मूल समस्या के समाधान के लिए प्रतिनिधित्व प्राप्त करना लगभग उतना ही कठिन होता है , तो अभ्यावेदन अधिगम पहली नजर में हमारी सहायता नहीं करता ।
गहन अधिगम इस केंद्रीय समस्या का समाधान अंतर्मुखी निरूपण द्वारा करता है जो अन्य , सरल निरूपणों के संदर्भ में व्यक्त किए जाते हैं ।
डीप लर्निंग कंप्यूटर को सरल अवधारणाओं में से जटिल अवधारणाओं का निर्माण करने में सक्षम बनाता है ।
चित्र 1 . 2 से पता चलता है कि कैसे एक गहरी शिक्षण प्रणाली सरल अवधारणाओं , जैसे कोनों और contours , जो बारी में किनारों के संदर्भ में परिभाषित कर रहे हैं के संयोजन के द्वारा एक व्यक्ति की छवि की अवधारणा का प्रतिनिधित्व कर सकते हैं .
एक गहन अधिगम मॉडल का सारभूत उदाहरण है फीड सरल गहरा नेटवर्क , या मल्टीलेयर पर्सेप्टॉन (
Name
समारोह कई सरल कार्यों की रचना के द्वारा निर्मित है .
हम इनपुट का एक नया प्रतिनिधित्व प्रदान करने के रूप में एक अलग गणितीय समारोह के प्रत्येक अनुप्रयोग के बारे में सोच सकते हैं .
डेटा के लिए सही प्रतिनिधित्व सीखने का विचार गहरे सीखने पर एक प्रति विशिष्ट प्रदान करता है .
गहन अधिगम पर एक अन्य परिप्रेक्ष्य यह है कि गहराई कंप्यूटर को बहुसंकेतन कंप्यूटर प्रोग्राम सीखने में सक्षम बनाती है ।
अभ्यावेदन की प्रत्येक परत को 5 CHAPTER 1 के बाद कंप्यूटर की स्मृति की स्थिति के रूप में सोचा जा सकता है .
दृश्य परत
प्रथम प्रच्छन्न परत
दूसरी छुपी हुई परत
CAR PERSON LANIMAL आउटपुट ( विषयगत पहचान )
चित्र 1 . 2 : एक गहन शिक्षण मॉडल का प्रदर्शन
कंप्यूटर के लिए कच्चे संवेदी इनपुट डेटा के अर्थ को समझना कठिन होता है , जैसे कि यह छवि पिक्सेल मूल्यों के संग्रह के रूप में प्रदर्शित होती है .
पिक्सल के सेट से वस्तु पहचान के लिए फंक्शन मैपिंग बहुत जटिल है .
यदि सीधे हल कर लिया जाए तो इस मानचित्रण को सीखना या उसका मूल्यांकन करना बीमायोग्य लगता है ।
डीप लर्निंग इस कठिनाई का समाधान नीड़ित सरल मानचित्रण की एक श्रृंखला में वांछित जटिल मानचित्रण को तोड़कर करती है , प्रत्येक मॉडल की एक अलग परत द्वारा वर्णित है .
इनपुट दृश्य परत पर प्रस्तुत किया जाता है , इसलिए नाम दिया गया है क्योंकि यह चर है कि हम निरीक्षण करने में सक्षम हैं शामिल हैं .
फिर छुपी परतों की एक श्रृंखला छवि से लगातार अमूर्त विशेषताओं को निकालती है .
इन परतों को " परतों को " परतों को " परतों को " परतों को " परतों को " परतों को " परतों को " परतों को " परतों को " परतों को " परतों को " परतों को " परतों को " परतों को " कहा जाता है ।
यहाँ छवियों प्रत्येक छिपा इकाई द्वारा प्रतिनिधित्व विशेषता के प्रकार की कल्पना कर रहे हैं .
पिक्सल को देखते हुए , पहली परत आसानी से किनारों की पहचान कर सकते हैं , पड़ोसी पिक्सल की चमक की तुलना करके .
किनारों की पहली छुपी परत के वर्णन को देखते हुए , दूसरी छुपी परत आसानी से कोनों और विस्तारित कोनों की खोज कर सकती है , जो किनारों के संग्रह के रूप में पहचानी जा सकती है .
कोनों और कोनों के संदर्भ में छवि के दूसरे छुपे परत के वर्णन को देखते हुए , तीसरी छुपी परत विशिष्ट वस्तुओं के पूरे भागों का पता लगा सकती है , जिसमें संचरों और कोनों के विशिष्ट संग्रहों का पता लगाया जा सकता है .
अंत में , इसमें निहित वस्तु भागों के संदर्भ में छवि के इस वर्णन का उपयोग छवि में मौजूद वस्तुओं को पहचानने के लिए किया जा सकता है .
ज़ाइलर और फर्गस (2014 ) से अनुमति के साथ पुनः प्रस्तुत छवियों
6 CHAPTER 1 1
समानांतर अनुदेशों के एक अन्य समुच्चय को निष्पादित करने के लिए INTRODUC देखें ।
अधिक गहराई वाले नेटवर्क अनुक्रम में अधिक अनुदेशों को निष्पादित कर सकते हैं ।
अनुक्रमिक अनुदेश महान शक्ति प्रदान करते हैं क्योंकि बाद के अनुदेश पूर्व अनुदेशों के परिणामों को वापस संदर्भित कर सकते हैं .
Ac
गहन अधिगम के इस दृष्टिकोण के अनुरूप , परत के सक्रियण में सभी सूचनाएं आवश्यक रूप से इनपुट की व्याख्या करने वाले विभिन्नता के कारकों को कूटबद्ध करती हैं .
अभ्यावेदन राज्य सूचना को भी भंडारित करता है जो एक ऐसे प्रोग्राम को निष्पादित करने में मदद करता है जो इनपुट का बोध करा सकता है .
यह राज्य सूचना एक पारंपरिक कंप्यूटर प्रोग्राम में एक काउंटर या सूचक के अनुरूप हो सकती है .
इसका इनपुट की सामग्री से विशेष रूप से कोई संबंध नहीं है , लेकिन यह मॉडल को अपने प्रसंस्करण को व्यवस्थित करने में मदद करता है .
एक मॉडल की गहराई को मापने के दो मुख्य तरीके हैं .
पहला दृश्य अनुक्रमिक अनुदेशों की संख्या पर आधारित है , जिन्हें वास्तुकला का मूल्यांकन करने के लिए निष्पादित किया जाना चाहिए .
हम इसे एक प्रवाह चार्ट के माध्यम से सबसे लंबे पथ की लंबाई के रूप में सोच सकते हैं जो मॉडल के प्रत्येक आउटपुट को अपने इनपुट दिए गए की गणना करने का वर्णन करता है .
जिस प्रकार दो समतुल्य कंप्यूटर प्रोग्रामों की लंबाई भिन्न - भिन्न होगी , यह निर्भर करता है कि प्रोग्राम किस भाषा में लिखा जाता है , उसी प्रकार एक फ्लो चार्ट के रूप में एक ही फलन तैयार किया जा सकता है जिसमें विभिन्न गहराई होती है , यह निर्भर करता है कि हम फ्लो चार्ट में किस प्रकार के व्यक्तिगत चरणों के रूप में प्रयोग करने की अनुमति देते हैं ।
चित्र 1 . 3 इस बात को स्पष्ट करता है कि भाषा के इस चयन से एक ही वास्तुशिल्प के लिए दो अलग - अलग माप कैसे हो सकते हैं ।
x 1 x 1 x 1 w 1 www ।
×
x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2
x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2
w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w
2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
w 2 × + ×
तत्व सेट + ×

तत्व सेट लॉजिस्टिक प्रतिगमन लॉजिस्टिक प्रतिगमन चित्र 1 . 3 :
कम्प्यूटेशनल रेखांकन का निष्क्रमण एक आउटपुट में इनपुट प्रतिचित्रण करता है , जहां प्रत्येक नोड एक संक्रिया निष्पादित करता है ।
गहराई इनपुट से आउटपुट तक सबसे लंबे पथ की लंबाई है , लेकिन क्या एक संभावित कम्प्यूटेशनल कदम का गठन करता है की परिभाषा पर निर्भर करता है .
अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन अभिकलन है
यदि हम जोड़ , गुणन और रसद सिग्मिड का उपयोग हमारे कंप्यूटर भाषा के तत्वों के रूप में करते हैं , तो इस मॉडल गहराई तीन
यदि हम रसद प्रतिगमन को एक तत्व के रूप में ही देखते हैं , तो इस मॉडल में गहराई एक है .
7 CHAPTER 1
एक अन्य दृष्टिकोण , जिसका प्रयोग गहरे प्रोबाबिलिस्टिक मॉडलों द्वारा किया जाता है , मॉडल की गहराई को कम्प्यूटेशनल ग्राफ की गहराई नहीं बल्कि ग्राफ की गहराई को दर्शाता है कि अवधारणाएं एक दूसरे से कैसे संबंधित हैं .
इस मामले में , प्रत्येक अवधारणा के निरूपण की गणना के लिए आवश्यक अभिकलन के प्रवाह संचित्र की गहराई स्वयं अवधारणाओं के ग्राफ से कहीं अधिक गहरी हो सकती है .
इसका कारण यह है कि सरल अवधारणाओं की प्रणाली की समझ को अधिक जटिल अवधारणाओं के बारे में जानकारी दी जा सकती है ।
उदाहरण के लिए , छाया में एक आँख वाले चेहरे की छवि का अवलोकन करने वाली एआई प्रणाली प्रारंभ में केवल एक आँख देख सकती है .
यह पता लगाने के बाद कि एक चेहरा मौजूद है , सिस्टम तब अनुमान लगा सकता है कि एक दूसरी आंख शायद भी मौजूद है .
इस मामले में , अवधारणाओं के ग्राफ में केवल दो परतें शामिल हैं - आंखों के लिए परत और चेहरों के लिए एक परत - लेकिन अभिकलन के ग्राफ में 2 एन परतें शामिल हैं यदि हम अन्य एन बार दिए गए प्रत्येक अवधारणा के अपने अनुमान को परिष्कृत करते हैं .
क्योंकि यह हमेशा स्पष्ट नहीं है कि इन दो विचारों में से कौन सा कम्प्यूटेशनल ग्राफ की गहराई , या प्रोबाबिलिस्टिक मॉडलिंग ग्राफ की गहराई - सबसे अधिक प्रासंगिक है , और क्योंकि विभिन्न लोग सबसे छोटे तत्वों के विभिन्न सेट चुनते हैं , जिससे उनके रेखांकन का निर्माण करने के लिए एक भी सही मूल्य नहीं है ।
न ही इस बारे में आम सहमति है कि किसी मॉडल को “दीप” के रूप में अर्हता प्राप्त करने के लिए कितनी गहराई की आवश्यकता होती है .
हालांकि , गहरी सीखने सुरक्षित रूप से मॉडलों के अध्ययन के रूप में माना जा सकता है जो या तो सीखा कार्यों या पारंपरिक मशीन सीखने की तुलना में विद्वत अवधारणाओं की अधिक राशि शामिल है .
संक्षेपित करने के लिए , गहरी जानकारी , इस पुस्तक का विषय है , एआई के लिए एक दृष्टिकोण है .
विशेष रूप से , यह मशीन लर्निंग का एक प्रकार है , एक तकनीक है जो कंप्यूटर सिस्टम को अनुभव और डेटा के साथ सुधार करने में सक्षम बनाती है .
हम प्रतिवाद करते हैं कि मशीन लर्निंग एआई सिस्टम के निर्माण के लिए एकमात्र व्यवहार्य दृष्टिकोण है जो जटिल वास्तविक दुनिया वातावरण में संचालित हो सकता है .
गहन अधिगम , एक विशेष प्रकार का मशीन अधिगम है , जो विश्व को अवधारणाओं के नीड़ित पदानुक्रम के रूप में निरूपित करके महान शक्ति और लचीलापन प्राप्त करता है , जिसमें प्रत्येक अवधारणा सरल अवधारणाओं के संबंध में परिभाषित होती है , और कम अमूर्त निरूपणों के संदर्भ में परिकलित होती है ।
चित्र 1 . 4 इन विभिन्न एआई विधाओं के बीच संबंध को स्पष्ट करता है ।
चित्र 1 . 5 प्रत्येक कृति के बारे में एक उच्च स्तरीय योजना देता है ।
1 . 1
कौन इस किताब को पढ़ना चाहिए ?
यह पुस्तक कई तरह के पाठकों के लिए उपयोगी हो सकती है , लेकिन हमने इसे दो लक्षित दर्शकों को ध्यान में रखकर लिखा ।
इन लक्षित दर्शकों में से एक है विश्वविद्यालय के छात्र ( स्नातक या स्नातक की पढ़ाई मशीन सीखने के बारे में , उन लोगों सहित जो गहरी सीखने और कृत्रिम बुद्धि अनुसंधान में एक कैरियर की शुरुआत कर रहे हैं )
अन्य 8 CHAPTER 1 .
Comment
लॉजिस्टिक प्रतिगमन
उदाहरणः
छिछले ऑटोकोडर्स
उदाहरणः
MLPs चित्र 1 . 4 :
एक वेन आरेख यह दर्शाता है कि कितना गहरा अधिगम एक प्रकार का निरूपण अधिगम है , जो बदले में एक प्रकार का मशीन अधिगम है , जो कई के लिए प्रयुक्त होता है लेकिन सभी दृष्टिकोण एआई के लिए नहीं .
वेन आरेख के प्रत्येक अनुभाग में एक एआई प्रौद्योगिकी का एक उदाहरण शामिल है .
लक्षित दर्शक सॉफ्टवेयर इंजीनियर हैं जिनके पास मशीन लर्निंग या स्थैतिक पृष्ठभूमि नहीं है , लेकिन वे तेजी से एक हासिल करना चाहते हैं और अपने उत्पाद या मंच में गहरी सीखने का उपयोग शुरू करना चाहते हैं .
डीप लर्निंग पहले से ही कई सॉफ्ट - वेयर विधाओं में उपयोगी साबित हुई है , जिसमें कंप्यूटर विजन , स्पीच और ऑडियो प्रोसेसिंग , प्राकृतिक भाषा संसाधन , रोबोटिक्स , जैव सूचना विज्ञान और रसायन विज्ञान , वीडियो गेम , खोज इंजन , ऑनलाइन विज्ञापन और वित्त शामिल हैं .
इस पुस्तक को तीन भागों में संगठित किया गया है ताकि विभिन्न प्रकार के पाठकों को बैठाया जा सके ।
भाग मैं बुनियादी गणितीय उपकरण और मशीन सीखने अवधारणाओं का परिचय देता है .
भाग 2 सबसे स्थापित गहरी सीखने एल्गोरिदम , जो अनिवार्य रूप से हल प्रौद्योगिकियों रहे हैं का वर्णन करता है .
भाग III अधिक सट्टा विचारों का वर्णन करता है जो व्यापक रूप से गहरे सीखने में भविष्य के अनुसंधान के लिए महत्वपूर्ण माना जाता है .
9 CHAPTER 1
Name
हाथ से डिजाइन किया गया प्रोग्राम
आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट
फीचर आउटपुट इनपुट फीचर मैपिंग से फीचर आउटपुट इनपुट फीचर मैपिंग से हैंड - डिज़ाइन सुविधाएँ आउटपुट इनपुट
सुविधाओं आउटपुट अतिरिक्त परतों से सरल सुविधाओं का मानचित्रण नियम - पस्त सिस्टम क्लासिक मशीन अधिगम प्रतिनिधित्व अधिगम गहन अधिगम चित्र 1 . 5 : फ्लोचर यह दर्शाता है कि कैसे एक एआई प्रणाली के विभिन्न भाग विभिन्न एआई विधाओं के भीतर एक दूसरे से संबंधित हैं .
शेड बॉक्स उन घटकों को इंगित करते हैं जो डेटा से सीखने में सक्षम होते हैं .
पाठकों को उन भागों को छोड़ने के लिए स्वतंत्र महसूस करना चाहिए जो उनके हितों या पृष्ठभूमि को देखते हुए प्रासंगिक नहीं हैं .
रैखिक बीजगणित , संभावना , और मौलिक मशीन सीखने अवधारणाओं से परिचित पाठकों भाग मैं छोड़ सकते हैं , उदाहरण के लिए , जबकि जो लोग सिर्फ एक कार्य प्रणाली को लागू करना चाहते हैं भाग 2 से आगे पढ़ने की जरूरत नहीं है .
यह चुनने में मदद करने के लिए कि कौन सा 10 CHAPTER 1 .
Comment
परिचय भाग 1 : अनुप्रयुक्त मठ और मशीन लर्निंग बेसिक 2
रैखिक बीजगणित 3 .
संभावना और सूचना सिद्धांत 4 .
संख्यात्मक कंप्यूटिंग 5 .
मशीन लर्निंग बेसिक्स भाग - 2 :
डीप नेटवर्कः आधुनिक अभ्यास 6 .
डीप फीड सरल नेटवर्क 7 .
नियमितीकरण 8 . नियमितीकरण 8 .
अनुकूलन 9 .
सीएनएन 10
RNs 11
व्यावहारिक पद्धति 12 . व्यावहारिक पद्धति 12 .
अनुप्रयोग भाग III :
डीप लर्निंग रिसर्च 13 .
रैखिक फैक्टर मॉडल 14 .
ऑटोनकोडर 15 .
प्रतिनिधित्व अधिगम 16 .
संरचित सम्भावित मॉडल 17 .
मोंटे कार्लो विधि 18 .
विभाजन कार्य 19 . विभाजन कार्य 19 .
अनुमान 20
डीप जेनरेशन मॉडल चित्र 1 . 6 : पुस्तक का उच्च स्तरीय संगठन ।
एक अध्याय से दूसरे अध्याय तक तीर इंगित करता है कि पूर्व अध्याय उत्तरार्द्ध को समझने के लिए पूर्व अपेक्षित सामग्री है .
11 CHAPTER 1 1
INTRODUConstellation name ( optional )
हम यह जरूर मानते हैं कि सभी पाठक कंप्यूटर विज्ञान की पृष्ठभूमि से आते हैं ।
हम प्रोग्रामिंग , कम्प्यूटेशनल निष्पादन मुद्दों की एक बुनियादी समझ , जटिलता सिद्धांत , परिचयात्मक स्तर पथरी और ग्राफ सिद्धांत की कुछ शब्दावली के साथ परिचित मान लेते हैं .
डीप लर्निंग में 1 . 2 ऐतिहासिक ट्रेंड्स
किसी ऐतिहासिक संदर्भ के साथ गहन शिक्षण को समझना सबसे आसान है ।
गहरी सीखने का एक विस्तृत इतिहास प्रदान करने के बजाय , हम कुछ प्रमुख प्रवृत्तियों की पहचानः
• गहन अध्ययन का एक लंबा और समृद्ध इतिहास रहा है , लेकिन कई नामों से जाना जाता है , विभिन्न दार्शनिक दृष्टिकोणों को प्रतिबिंबित , और लोकप्रियता में वृद्धि हुई है .
• दीप शिक्षण अधिक उपयोगी हो गया है के रूप में उपलब्ध प्रशिक्षण डेटा की मात्रा में वृद्धि हुई है .
• डीप लर्निंग मॉडल समय के साथ आकार में वृद्धि हुई है के रूप में कंप्यूटर बुनियादी ढांचे ( गहरी सीखने के लिए हार्डवेयर और सॉफ्टवेयर ) में सुधार हुआ है .
• डीप लर्निंग समय के साथ बढ़ती सटीकता के साथ बढ़ती जटिल अनुप्रयोगों को हल किया है .
1 . 2 . 1
नेरल नेट के कई नाम और चंगिंग फोरून काम करते हैं
हम उम्मीद करते हैं कि इस पुस्तक के कई पाठकों को एक रोमांचक नई प्रौद्योगिकी के रूप में गहरी सीखने के बारे में सुना है , और एक उभरते क्षेत्र के बारे में एक पुस्तक में “हिस्टरी” का उल्लेख देखने पर आश्चर्य होता है .
वास्तव में , गहरी सीखने 1940 के दशक के लिए वापस तिथियाँ .
डीप लर्निंग केवल नई प्रतीत होती है , क्योंकि यह अपनी वर्तमान लोकप्रियता से पहले कई वर्षों के लिए अपेक्षाकृत अलोकप्रिय था , और क्योंकि यह कई अलग अलग नामों से चला गया है , केवल हाल ही में “डेप लर्निंग” कहा जा रहा है .
क्षेत्र कई बार rebrand किया गया है , विभिन्न शोधकर्ताओं और विभिन्न परिप्रेक्ष्यों के प्रभाव को प्रतिबिंबित .
गहन अध्ययन का एक व्यापक इतिहास इस पाठ्यपुस्तक के दायरे से बाहर है .
कुछ बुनियादी संदर्भ , तथापि , गहरी सीखने को समझने के लिए उपयोगी है .
मोटे तौर पर , विकास की तीन लहरें आई हैंः
1940 -1960 के दशक में साइबरनेटिक्स के नाम से जानी जाने वाली गहरी शिक्षा , जिसे 12 CHAPTER 1 में कनेक्शन के रूप में जाना जाता है .
इन्ट्राओडीयूसी 1940 1960 1970 1990 वर्ष
0 . 000000 0 . 00050 0 . 000100 0 . 00015

चित्र 1 . 7 :
कृत्रिम तंत्रिका जाल अनुसंधान की तीन ऐतिहासिक तरंगों में से दो , जैसा कि वाक्यांशों “साइबरनेटिक्स” और “संयोजन” या “नेरल नेटवर्क” की आवृत्ति द्वारा मापा जाता है , गूगल बुक्स के अनुसार ( तीसरी लहर अभी हाल ही में प्रकट होने वाली है )
1940 के दशक में पहली लहर , 1940 के दशक में , जैविक , जैविक , जैविक , जैविक , जैविक , जैविक , जैविक , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , सक्रिय , (
दूसरी लहर 1980 -1995 की अवधि के संबंधवादी दृष्टिकोण के साथ शुरू हुई , जिसमें बैक - प्रचार ( Rumelhart एट अल , 1986 ) एक या दो छुपी हुई परतों वाले तंत्रिका नेटवर्क को प्रशिक्षित करने के लिए
वर्तमान तरंग गहन अधिगम २००६ ( २००६ )

और अभी 2016 के रूप में पुस्तक रूप में प्रदर्शित हो रहा है
इसी प्रकार अन्य दो तरंगें भी इसी प्रकार की वैज्ञानिक गतिविधि से बहुत बाद में पुस्तक रूप में प्रकट हुई ।
1980 -1990 के दशक , और 2006 में गहरे शिक्षण के नाम से वर्तमान पुनरुत्थान
इसे मात्रात्मक रूप से 1 . 7 अंक में दर्शाया गया है ।
कुछ प्रारंभिक शिक्षण एल्गोरिदम जो आज हम पहचानते हैं , का उद्देश्य जैविक शिक्षण के कम्प्यूटेशनल मॉडल होना था , अर्थात सीखने के तरीके के मॉडल या मस्तिष्क में हो सकता है .
एक परिणाम के रूप में , एक नाम है कि गहरी सीखने के द्वारा चला गया है कृत्रिम तंत्रिका नेटवर्क ( एएनएनएस )
गहन अधिगम मॉडलों पर संगत परिप्रेक्ष्य यह है कि वे जैविक मस्तिष्क से प्रेरित प्रणालियां ( चाहे मानव मस्तिष्क हो या किसी अन्य पशु के मस्तिष्क ।
जबकि मशीनी शिक्षण के लिए प्रयुक्त तंत्रिका नेटवर्कों के प्रकारों का उपयोग कभी - कभी मस्तिष्क समारोह ( हैनटन और शैलिस , 1991 ) को समझने के लिए किया गया है , वे आम तौर पर जैविक कार्य के यथार्थवादी मॉडल बनने के लिए डिज़ाइन नहीं किए गए हैं .
गहन अध्ययन पर तंत्रिका परिप्रेक्ष्य दो मुख्य विचारों से प्रेरित है .
एक विचार यह है कि मस्तिष्क उदाहरण के द्वारा एक प्रमाण प्रदान करता है कि बुद्धिमान व्यवहार संभव है , और बुद्धि के निर्माण के लिए एक वैचारिक रूप से सीधा रास्ता मस्तिष्क के पीछे कम्प्यूटेशनल सिद्धांतों को रिवर्स इंजीनियर करने और उसकी कार्यक्षमता की नकल करने के लिए है .
एक और 13 CHAPTER 1 .
INTRODUCology परिप्रेक्ष्य यह है कि मस्तिष्क और उन सिद्धांतों को समझना अत्यंत रोचक होगा जो मानव बुद्धि को कम करते हैं , इसलिए मशीन लर्निंग मॉडल जो इन बुनियादी वैज्ञानिक प्रश्नों पर प्रकाश डालते हैं , इंजीनियरिंग अनुप्रयोगों को हल करने की क्षमता के अलावा उपयोगी हैं ।
आधुनिक शब्द “दीप अधिगम” मशीन अधिगम मॉडलों की वर्तमान नस्ल पर तंत्रिका विज्ञानी परिप्रेक्ष्य से परे है ।
यह रचना के एकाधिक स्तरों को सीखने के एक अधिक सामान्य सिद्धांत के लिए अपील करता है , जो मशीन सीखने के ढांचे में लागू किया जा सकता है जो आवश्यक रूप से तंत्रिका प्रेरित नहीं हैं .
आधुनिक गहन अध्ययन के सबसे पहले पूर्ववर्ती सरल रैखिक मॉडल थे जो तंत्रिका विज्ञानी परिप्रेक्ष्य से प्रेरित थे .
इन मॉडलों एन इनपुट मूल्यों एक्स 1 , का एक सेट लेने के लिए डिज़ाइन किया गया था .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
, x n और उन्हें एक आउटपुट y के साथ संबद्ध ।
इन मॉडलों वजन का एक सेट w 1 , सीख जाएगा .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
, w n और उनके आउटपुट f ( x , w = गणना
1 w 1 w 1 w 1 w 1 w 1 1 1 w 1 1 w 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

w n . n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
तंत्रिका नेटवर्क अनुसंधान की इस पहली लहर को साइबरनेटिक्स के नाम से जाना जाता था , जैसा कि अंक 1 . 7 में दर्शाया गया है ।

यह रैखिक मॉडल इनपुट के दो अलग अलग श्रेणियों को पहचान सकता है परीक्षण के द्वारा कि क्या एफ ( एक्स , w ) सकारात्मक या नकारात्मक है .
बेशक , मॉडल के लिए श्रेणियों की इच्छित परिभाषा के अनुरूप , सही ढंग से सेट करने के लिए आवश्यक वजन .
इन भारों को मानव संचालक द्वारा निर्धारित किया जा सकता था ।
1950 के दशक में , पर्सेप्टॉन (Rosenblatt , 1958 , 1962 ) पहला मॉडल बन गया जो प्रत्येक श्रेणी से इनपुट के उदाहरण दिए गए श्रेणियों को परिभाषित करने वाले भार को सीख सकता था .
अनुकूली रैखिक तत्व
( एक्स ) स्वयं एक वास्तविक संख्या की भविष्यवाणी करने के लिए ( Widrow और Hoff , 1960 ) और भी डेटा से इन संख्याओं की भविष्यवाणी करने के लिए सीख सकता है .
इन सरल अधिगम एल्गोरिदम ने मा - चीनी अधिगम के आधुनिक परिदृश्य को बहुत प्रभावित किया ।
प्रशिक्षण एल्गोरिथ्म का प्रयोग एडीएएलआईएनई के भार को अनुकूलित करने के लिए किया जाता है ।
एक एल्गोरिथ्म का विशेष मामला था जिसे stochastic प्रवणता कहते हैं ।
Stochastic प्रवणता एल्गोरिथ्म के हल्के संशोधित संस्करण आज गहरे सीखने मॉडल के लिए प्रमुख प्रशिक्षण एल्गोरिदम बने हुए हैं .
पर्सेप्टॉन और एडीएएलआईएन द्वारा प्रयुक्त फ ( x , w ) पर आधारित मॉडल रैखिक मॉडल कहलाते हैं .
ये मॉडल कुछ सर्वाधिक व्यापक रूप से प्रयुक्त मशीन लर्निंग मॉडल बने हुए हैं , हालांकि कई मामलों में इन्हें मूल मॉडलों से भिन्न तरीकों से प्रशिक्षित किया जाता है .
रैखिक मॉडलों की कई सीमाएं हैं .
सबसे प्रसिद्ध सबसे प्रसिद्ध & # 44 ; सबसे प्रसिद्ध & # 44 ; सबसे प्रसिद्ध & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & #
= 1 और f (
[ 1 ] , 0 . 0 ]
= 1 लेकिन f ( ( = 1 )
[ 1 ] 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 2
= . . . . . . . . . . . . . . . . . . . . .
जिन आलोचकों ने रैखिक मॉडलों में इन खामियों का अवलोकन किया , उन्होंने सामान्य रूप से जैविक रूप से प्रेरित सीखने के खिलाफ एक पीठ थपथपाई ( Minsky और पेपरेट , 1969 )
यह तंत्रिका नेटवर्क की लोकप्रियता में पहली बड़ी डुबकी थी .
14 CHAPTER 1
INTRODUConstellation name ( optional )
गहन शिक्षण अनुसंधान में तंत्रिका विज्ञान की घटती भूमिका का मुख्य कारण आज यह है कि हम केवल मस्तिष्क के बारे में पर्याप्त जानकारी नहीं है इसे एक गाइड के रूप में उपयोग करने के लिए
मस्तिष्क द्वारा प्रयुक्त वास्तविक एल्गोरिदम की गहरी समझ प्राप्त करने के लिए , हमें एक साथ ( कम से कम हजारों अंतर्संबंधित न्यूरॉन्स की गतिविधि की निगरानी करने में सक्षम होने की आवश्यकता होगी .
क्योंकि हम ऐसा नहीं कर पा रहे हैं , हम मस्तिष्क के कुछ सबसे सरल और अच्छी तरह से ज्ञात भागों ( ओलशासेन और फील्ड , 2005 ) को भी समझने से दूर हैं .
तंत्रिका विज्ञान ने हमें यह आशा करने का एक कारण दिया है कि एक एकल गहन अधिगम एल्गोरिथ्म कई भिन्न कार्यों को हल कर सकता है ।
तंत्रिका विज्ञानियों ने पाया है कि फर्नेस अपने मस्तिष्क के श्रवण प्रसंस्करण क्षेत्र के साथ “सीई” करना सीख सकते हैं यदि उनके मस्तिष्क को उस क्षेत्र में दृश्य संकेत भेजने के लिए पुनः भेजा जाता है
यह सुझाव है कि स्तनधारी मस्तिष्क के बहुत से विभिन्न कार्यों है कि मस्तिष्क हल करने के लिए हल करने के लिए एक एकल एल्गोरिथ्म का उपयोग कर सकते हैं .
इस परिकल्पना से पहले , मशीन अधिगम अनुसंधान अधिक खंडित था , जिसमें प्राकृतिक भाषा संसाधन , दृष्टि , गति नियोजन और भाषण मान्यता का अध्ययन करने वाले शोधकर्ताओं के विभिन्न समुदाय थे .
आज , ये अनुप्रयोग समुदाय अभी भी अलग हैं , लेकिन गहन शिक्षण अनुसंधान समूहों के लिए एक साथ कई या यहां तक कि इन सभी अनुप्रयोग क्षेत्रों का अध्ययन करना आम बात है .
हम तंत्रिका विज्ञान से कुछ रूखे दिशानिर्देश निकालने में सक्षम हैं ।
कई कम्प्यूटेशनल इकाइयों है कि केवल एक दूसरे के साथ अपनी बातचीत के माध्यम से बुद्धिमान बन जाने का मूल विचार मस्तिष्क से प्रेरित है .

अधिकांश तंत्रिका नेटवर्क आज एक मॉडल न्यूरॉन पर आधारित हैं , जिसे सुधारित रैखिक इकाई कहा जाता है .
मूल cognitron ( Fukushima , 1975 ) ने एक अधिक जटिल संस्करण की शुरुआत की जो हमारे मस्तिष्क समारोह के ज्ञान से अत्यधिक प्रेरित था .
सरलीकृत आधुनिक संस्करण कई दृष्टिकोणों से विचारों को समाहित करते हुए विकसित किया गया था , जिसमें नायर और हाइनटन (2010 ) और ग्लोरोट एट अल .

(2009 ) अधिक इंजीनियरिंग उन्मुख प्रभावों का हवाला देते हुए
जबकि तंत्रिका विज्ञान प्रेरणा का एक महत्वपूर्ण स्रोत है , यह एक कठोर गाइड के रूप में लेने की जरूरत नहीं है .
हम जानते हैं कि वास्तविक न्यूरॉन्स आधुनिक संशोधित रैखिक इकाइयों की तुलना में बहुत अलग कार्यों की गणना करते हैं , लेकिन अधिक तंत्रिका यथार्थवाद अभी तक मशीन सीखने के प्रदर्शन में सुधार के लिए नेतृत्व नहीं किया है .
इसके अलावा , जबकि तंत्रिका विज्ञान सफलतापूर्वक कई तंत्रिका नेटवर्क वास्तुकला को प्रेरित किया है , हम अभी तक तंत्रिका विज्ञान के लिए जैविक सीखने के बारे में पर्याप्त नहीं पता है सीखने एल्गोरिदम के लिए बहुत मार्गदर्शन की पेशकश करने के लिए हम इन वास्तुकला को प्रशिक्षित करने के लिए उपयोग
15 CHAPTER 1
INTRODUConstellation name ( optional )
जबकि यह सच है कि गहन अध्ययन शोधकर्ताओं को मस्तिष्क को अन्य मशीन सीखने के क्षेत्रों , जैसे कर्नेल मशीनों या बेसियन सांख्यिकी में काम कर रहे शोधकर्ताओं की तुलना में एक प्रभाव के रूप में उद्धृत करने की संभावना अधिक है , मस्तिष्क का अनुकरण करने के प्रयास के रूप में गहरी सीखने नहीं देखना चाहिए .
आधुनिक गहन अधिगम कई क्षेत्रों , विशेष रूप से लागू गणित मौलिक जैसे रैखिक बीजगणित , संभावना , सूचना सिद्धांत , और संख्यात्मक अनुकूलन से प्रेरणा लेता है .
जबकि कुछ गहरी सीखने के शोधकर्ता तंत्रिका विज्ञान को प्रेरणा के एक महत्वपूर्ण स्रोत के रूप में उद्धृत करते हैं , दूसरों को बिल्कुल भी तंत्रिका विज्ञान से संबंधित नहीं हैं .
यह ध्यान देने योग्य है कि कैसे एक एल्गोरिथम स्तर पर मस्तिष्क काम करता है समझने की कोशिश जीवित और अच्छी तरह से है .
इस प्रयास को मुख्य रूप से “संक्रामक तंत्रिका विज्ञान” के रूप में जाना जाता है और यह गहन अध्ययन से अध्ययन का एक अलग क्षेत्र है ।
शोधकर्ताओं के लिए दोनों क्षेत्रों के बीच आगे - पीछे चलना आम बात है ।
गहन अधिगम का क्षेत्र मुख्य रूप से इस बात से संबंधित है कि कैसे ऐसे कंप्यूटर सिस्टम का निर्माण किया जाए जो बुद्धि की आवश्यकता वाले कार्यों को सफलतापूर्वक हल करने में सक्षम हों , जबकि कम्प्यूटेशनल तंत्रिका विज्ञान का क्षेत्र मुख्य रूप से इस बात के अधिक सटीक मॉडल बनाने से संबंधित है कि मस्तिष्क वास्तव में कैसे कार्य करता है ।
1980 के दशक में , तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर कनेक्शनवाद , या समानांतर वितरित प्रक्रिया - नामक आंदोलन के माध्यम से बड़े भाग में उभरी .
अल

संज्ञानात्मक विज्ञान के संदर्भ में कनेक्शन का उदय हुआ .
संज्ञानात्मक विज्ञान मन को समझने के लिए एक अंतर्विषयक दृष्टिकोण है , विश्लेषण के कई अलग अलग स्तरों को संयोजित .
1980 के दशक के प्रारंभ में , अधिकांश संज्ञानात्मक वैज्ञानिकों ने प्रतीकात्मक तर्क के मॉडलों का अध्ययन किया .
उनकी लोकप्रियता के बावजूद , प्रतीकात्मक मॉडलों को इस संदर्भ में समझाना मुश्किल था कि कैसे मस्तिष्क वास्तव में उन्हें न्यूरॉन्स का उपयोग करके लागू कर सकता है .
कनेक्शनिस्टों ने संज्ञान के उन मॉडलों का अध्ययन करना शुरू किया जो वास्तव में तंत्रिका कार्यान्वयनों में आधारित हो सकते थे ( Touretzky और मिंटन , 1985 ) , जिसमें 1940 के दशक में मनोविज्ञानी डोनाल्ड हेब के कार्य से संबंधित कई विचारों को पुनः प्राप्त किया गया था .
कनेक्शनवाद में केंद्रीय विचार यह है कि सरल कम्प्यूटेशनल इकाइयों की एक बड़ी संख्या बुद्धिमान व्यवहार प्राप्त कर सकते हैं जब एक साथ नेटवर्क .
यह अंतर्दृष्टि जैविक तंत्रिका प्रणालियों में न्यूरॉन्स के लिए समान रूप से लागू होता है , जैसा कि यह कम्प्यूटेशनल मॉडलों में छुपी इकाइयों के लिए करता है .
1980 के दशक के संबंध आंदोलन के दौरान कई प्रमुख अवधारणाओं का उदय हुआ जो आज के गहन अध्ययन का केंद्र बनी हुई हैं ।
इन अवधारणाओं में से एक है वितरित प्रतिनिधित्व ( हैनटन एट अल , 1986 ) .
यह विचार है कि एक सिस्टम के लिए प्रत्येक इनपुट कई सुविधाओं द्वारा प्रतिनिधित्व किया जाना चाहिए , और प्रत्येक विशेषता कई संभावित इनपुट के प्रतिनिधित्व में शामिल होना चाहिए .
उदाहरण के लिए , मान लीजिए कि हमारे पास एक दृष्टि प्रणाली है जो 16 CHAPTER 1 को पहचान सकती है ।
इन्ट्राऑडयूक्यूट कार , ट्रक , और पक्षी , और ये वस्तुएं प्रत्येक लाल , हरा , या नीला हो सकती हैं ।
इन इनपुट का प्रतिनिधित्व करने का एक तरीका होगा एक अलग न्यूरॉन या छिपा इकाई है कि नौ संभावित संयोजनों में से प्रत्येक के लिए सक्रिय करता हैः लाल ट्रक , लाल कार , हरे ट्रक , और इतने पर
इसके लिए नौ विभिन्न न्यूरॉन्स की आवश्यकता होती है , और प्रत्येक न्यूरॉन को स्वतंत्र रूप से रंग और वस्तु पहचान की अवधारणा सीखनी चाहिए .
इस स्थिति पर सुधार करने के लिए एक तरीका है एक वितरित प्रतिनिधित्व का उपयोग करने के लिए , तीन न्यूरॉन्स के साथ वस्तु पहचान का वर्णन रंग और तीन न्यूरॉन्स के साथ .
इसके लिए नौ के स्थान पर केवल छह न्यूरॉन्स की आवश्यकता होती है , और लालिमा का वर्णन करने वाला न्यूरॉन कारों , ट्रकों और पक्षियों की छवियों से लालिमा के बारे में जानने में सक्षम है , न कि केवल वस्तुओं की एक विशिष्ट श्रेणी की छवियों से .
वितरित प्रतिनिधित्व की अवधारणा इस पुस्तक के लिए केंद्रीय है और अध्याय 15 में अधिक विस्तार से वर्णित है .
कनेक्शनिस्ट आंदोलन की एक अन्य प्रमुख उपलब्धि थी , गहरे न्यूट्रल नेटवर्कों को प्रशिक्षित करने के लिए बैक - प्रोगेशन का सुचिंतित उपयोग , बैक - प्रोगेशन एल्गोरिदम के लोकप्रियकरण ( 1987 )
यह एल्गोरिथ्म लोकप्रियता में वृद्धि और गिरावट आई है , लेकिन इस लेखन के रूप में , गहरे मॉडलों के प्रशिक्षण के लिए प्रमुख दृष्टिकोण है .
1990 के दशक के दौरान , शोधकर्ताओं ने तंत्रिका नेटवर्क के साथ मॉडलिंग अनुक्रम में महत्वपूर्ण प्रगति की .
होकर
(1994 )

आज , एलएसटीएम व्यापक रूप से कई अनुक्रम मॉडलिंग कार्यों के लिए प्रयोग किया जाता है , गूगल पर कई प्राकृतिक भाषा प्रसंस्करण कार्यों सहित .
तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर1990 के दशक के मध्य तक चली .
तंत्रिका नेटवर्कों और अन्य एआई प्रौद्योगिकियों पर आधारित वीन - टर्स ने निवेश की मांग करते समय अवास्तविक महत्वाकांक्षी दावे करने शुरू कर दिए ।
जब एआई अनुसंधान इन अनुचित उम्मीदों को पूरा नहीं किया , निवेशक निराश थे .
इसके साथ ही मशीनी शिक्षण के अन्य क्षेत्रों में भी प्रगति हुई ।

इन दो कारकों से तंत्रिका नेटवर्क की लोकप्रियता में गिरावट आई जो 2007 तक चली .
इस दौरान , तंत्रिका नेटवर्क कुछ कार्यों पर प्रभावशाली प्रदर्शन प्राप्त करते रहे (
लेकुन एट अल
अल .

टोरंटो विश्वविद्यालय में जिओफ्री हाइनटन के नेतृत्व में इस कार्यक्रम संयुक्त मशीन शिक्षण अनुसंधान समूहों , मांट्रियल विश्वविद्यालय में योशु बेंजियो , और न्यूयॉर्क विश्वविद्यालय में यान लीकुन
बहुसांस्कृतिक सीआईएफएपी अनुसंधान पहल 17 सीएचएपीटर 1 ।
INTRODUC® में तंत्रिका विज्ञानी और मानव और कंप्यूटर दृष्टि के विशेषज्ञ भी शामिल थे .
इस बिंदु पर , गहरे नेटवर्क आम तौर पर माना जाता था कि प्रशिक्षण के लिए बहुत मुश्किल है .
अब हम जानते हैं कि एल्गोरिदम है कि 1980 के दशक के बाद से अस्तित्व में है काफी अच्छी तरह से काम करते हैं , लेकिन यह स्पष्ट सिर्का 2006 नहीं था .
मुद्दा शायद बस है कि इन एल्गोरिदम बहुत कम्प्यूटेशनल रूप से महंगा था उस समय उपलब्ध हार्डवेयर के साथ बहुत प्रयोग की अनुमति देने के लिए .
तंत्रिका नेटवर्क अनुसंधान की तीसरी लहर 2006 में एक ब्रेक थ्रू के साथ शुरू हुई .
जिओफ्री हाइनटन ने दिखाया कि एक प्रकार का तंत्रिका नेटवर्क जिसे गहरा विश्वास नेटवर्क कहा जाता है , लालची परतवार पूर्वप्रवर्तन ( हैनटन एट अल 2006 ) नामक रणनीति का उपयोग करके कुशलतापूर्वक प्रशिक्षित किया जा सकता है , जिसका वर्णन हम खंड 151 में अधिक विस्तार से करते हैं .
अन्य संबद्ध अनुसंधान समूहों ने शीघ्रता से वही कार्यनीति अपनाई , वही कार्यनीति अपनाई जा सकती थी ( वही कार्यनीति अपनाई जा सकती थी )
अनुसंधान की इस लहर ने “प्राकृतिक सीखने” को लोकप्रिय बनाया ताकि शोधकर्ताओं ने इस बात पर जोर दिया कि अब गहरे तंत्रिका तंत्र को प्रशिक्षित करने में सक्षम हो गए हैं ।
इस समय , गहरे तंत्रिका नेटवर्कों ने अन्य मशीन लर्निंग प्रौद्योगिकियों के साथ - साथ हाथ से लिखी कार्यक्षमता पर आधारित एआई सिस्टम को आगे बढ़ाया .
तंत्रिका नेटवर्क की लोकप्रियता की यह तीसरी लहर इस लेखन के समय तक जारी है , हालांकि गहन शिक्षण अनुसंधान का ध्यान इस लहर के समय के भीतर नाटकीय रूप से बदल गया है .
तीसरी लहर की शुरुआत नई अनुपयुक्त अधिगम तकनीकों और छोटे डेटासेटों से अच्छी तरह से सामान्य करने के लिए गहरे मॉडलों की क्षमता पर ध्यान केंद्रित करने से हुई , लेकिन आज बहुत पुराने पर्यवेक्षित अधिगम एल्गोरिदम और बड़े लेबल वाले डेटासेटों को कम करने के लिए गहरे मॉडलों की क्षमता में अधिक रुचि है .
1 . 2
बढ़ता डेटासेट आकार
कोई भी सोच सकता है कि गहन शिक्षण अभी हाल ही में एक महत्वपूर्ण प्रौद्योगिकी के रूप में क्यों मान्यता प्राप्त हो गया है , भले ही कृत्रिम तंत्रिका नेटवर्क के साथ पहला प्रयोग 1950 के दशक में किया गया था .
गहन शिक्षण 1990 के दशक के बाद से वाणिज्यिक अनुप्रयोगों में सफलतापूर्वक इस्तेमाल किया गया है , लेकिन अक्सर एक प्रौद्योगिकी और कुछ है कि केवल एक विशेषज्ञ का उपयोग कर सकते हैं की तुलना में एक कला के अधिक माना जाता था , हाल ही में
यह सच है कि एक गहरी सीखने एल्गोरिथ्म से अच्छा प्रदर्शन प्राप्त करने के लिए कुछ कौशल की आवश्यकता है .
सौभाग्य से , प्रशिक्षण डेटा की मात्रा बढ़ने के रूप में आवश्यक कौशल की मात्रा कम हो जाती है .
आज जटिल कार्यों पर मानव निष्पादन तक पहुंचने वाली अधिगम एल्गोरिदम , 1980 के दशक में खिलौना समस्याओं को हल करने के लिए संघर्ष करने वाली अधिगम एल्गोरिदम के लगभग समान है , हालांकि इन एल्गोरिदम के साथ प्रशिक्षण देने वाले मॉडलों में 18 सीएचएपीटर 1 होता है ।
इन्ट्राऑडयूक्यूट में ऐसे परिवर्तन किए गए जो बहुत गहरे वास्तुशिल्पों के प्रशिक्षण को सरल बनाते हैं ।
सबसे महत्वपूर्ण नया विकास है कि आज हम इन एल्गोरिदम को सफल होने के लिए आवश्यक संसाधनों के साथ प्रदान कर सकते हैं .
चित्र 1 . 8 से पता चलता है कि कैसे बेंचमार्क डेटासेट के आकार का समय के साथ उल्लेखनीय विस्तार हुआ है ।
यह प्रवृत्ति समाज के बढ़ते अंकीकरण से प्रेरित है ।
हमारी अधिक से अधिक गतिविधियां कंप्यूटरों पर होती हैं , अधिक से अधिक हम जो करते हैं , उसका अभिलेखन किया जाता है ।
जैसे - जैसे हमारे कंप्यूटरों का एक साथ नेटवर्क बढ़ता जा रहा है , इन अभिलेखों को केंद्रीकृत करना और उन्हें मशीनी शिक्षण अनुप्रयोगों के लिए उपयुक्त डेटासेट में ठीक करना आसान होता जा रहा है ।
“बिग डाटा” की आयु 1900 1950 वर्ष 2000 2015 वर्ष 10 0 10 10 10 10 10 10 10 10 10 10 10 10 10 9 डेटासेट आकार ( संख्यात्मक उदाहरण

सार्वजनिक एसवीएचएन छविनेट
सीआईएफएआर - 10
इमेजनेट10k ILSVRC 2014 खेल - 1

समय के साथ डेटासेट आकार में वृद्धि .
आरंभिक संकलित मापों में , आरंभिक संकलित मापों में , आरंभिक संकलित मापों में , संकलित मापों में , संकलित मापों में , संकलित मापों में , संकलित मापों में , संकलित मापों में , संकलित मापों में , सन् 1900 के सांख्यिकीविदों द्वारा अध्ययन किया गया था ।
१९५० के दशक में , जैविक रूप से प्रेरित मशीनी शिक्षण के अग्रदूत , अक्सर छोटे सिंथेटिक डेटासेट , जैसे कि निम्न - क्रियोलियन बिटमैप ऑफ अक्षर , जो निम्न अभिकलन लागत और १९८६ के विशिष्ट प्रकार के कार्यों को सीखने में सक्षम थे .
1980 और 1990 के दशक में , मशीनी अधिगम अधिक सांख्यिकीय हो गया और इसने दसियों हजार उदाहरणों वाले बड़े डेटासेटों का लाभ उठाना शुरू किया , जैसे कि MNIST डेटासेट ( हस्तलिखित संख्याओं के 1 अंक में )
2000 के दशक के पहले दशक में , इस आकार के अधिक परिष्कृत डेटासेट , जैसे CIFAR - 10 डेटासेट ( Krizhevsky और Hinton , 2009 ) का उत्पादन जारी रहा .
उस दशक के अंत में और 2010 के पूर्वार्ध में , महत्वपूर्ण रूप से बड़े डेटासेट , जिसमें हजारों से दसियों लाख उदाहरण थे , पूरी तरह से बदल गया जो गहरी सीखने के साथ संभव था .

एम डेटासेट

ग्राफ के शीर्ष पर , हम देखते हैं कि अनुवादित वाक्यों के डेटासेट , जैसे कि कनाडा के हंसार्ड ( Brown एट अल , 1990 ) और WMT 2014 अंग्रेजी से फ्रेंच डेटासेट ( Schwenk 2014 ) के लिए विशेष रूप से आगे हैं .

Comment
MNIST डेटासेट से उदाहरण इनपुट
“नसूची” राष्ट्रीय मानक और प्रौद्योगिकी संस्थान , एजेंसी के लिए है जो मूल रूप से इस आंकड़े को एकत्र किया .
“M” “मॉड्ड ,” के लिए खड़ा है क्योंकि डेटा मशीन सीखने एल्गोरिदम के साथ आसान उपयोग के लिए पूर्वप्रक्रमित किया गया है .
MNIST डेटासेट में हस्तलिखित अंकों और संबद्ध लेबलों के स्कैन होते हैं , जो यह वर्णित करते हैं कि प्रत्येक छवि में 0 - 9 कौन सा अंक निहित है
यह सरल वर्गीकरण समस्या गहन शिक्षण अनुसंधान में सबसे सरल और सबसे व्यापक रूप से इस्तेमाल किया परीक्षणों में से एक है .
आधुनिक तकनीकों के समाधान के लिए काफी आसान होने के बावजूद यह लोकप्रिय बना हुआ है .
जिओफ्री हाइनटन ने इसे “ मशीन लर्निंग का डेरोसोफिला” कहा है , जिसका अर्थ है कि यह मशीन लर्निंग शोधकर्ताओं को नियंत्रित प्रयोगशाला स्थितियों में उनके एल्गोरिदम का अध्ययन करने में सक्षम बनाता है , बहुत कुछ जीवविज्ञानी अक्सर फल मक्खियों का अध्ययन करते हैं .
मशीनी शिक्षण को काफी आसान बना दिया है क्योंकि सांख्यिकीय आकलन का मुख्य भार , जो केवल थोड़ी मात्रा में डेटा का अवलोकन करने के बाद नए आंकड़ों को अच्छी तरह से विकसित करता है , काफी हल्का कर दिया गया है ।
2016 तक , अंगूठे का एक मोटा नियम यह है कि एक पर्यवेक्षित गहन अधिगम एल्गोरिथ्म आम तौर पर प्रति श्रेणी लगभग 5 , 000 लेबल वाले उदाहरणों के साथ स्वीकार्य प्रदर्शन प्राप्त करेगा और मिलान या 20 सीएचएपीटर 1 .
INTRODUC मानव प्रदर्शन से अधिक है जब एक डेटासेट के साथ प्रशिक्षित किया जाता है जिसमें कम से कम 10 मिलियन लेबल वाले उदाहरण होते हैं .
इससे छोटे डेटासेटों के साथ सफलतापूर्वक कार्य करना एक महत्वपूर्ण अनुसंधान क्षेत्र है , विशेष रूप से इस बात पर ध्यान केंद्रित करना कि हम बड़ी मात्रा में अविश्वसनीय उदाहरणों का लाभ कैसे ले सकते हैं , जिसमें अनुपयुक्त या अर्ध - प्रसुप्त अधिगम है ।
1 . 2 . 3
बढ़ते मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज तंत्रिका नेटवर्क बेतहाशा सफल हैं कि हम कम्प्यूटेशनल संसाधनों आज बहुत बड़े मॉडल चलाने के लिए है .
कनेक्शन - इस्म की एक मुख्य अंतर्दृष्टि यह है कि जानवर बुद्धिमान हो जाते हैं जब उनके कई न्यूरॉन्स एक साथ काम करते हैं .
एक व्यक्तिगत न्यूरॉन या न्यूरॉन्स के छोटे संग्रह विशेष रूप से उपयोगी नहीं है .
जैविक न्यूरॉन्स विशेष रूप से सघन रूप से जुड़े नहीं हैं .
जैसा कि आंकड़ा 1 .10 में देखा गया है , हमारे मशीन सीखने के मॉडलों भी दशकों के लिए स्तनधारी मस्तिष्क के परिमाण के एक क्रम के भीतर प्रति न्यूरॉन कई कनेक्शन था .
न्यूरॉन्स की कुल संख्या के संदर्भ में , तंत्रिका नेटवर्क आश्चर्यजनक रूप से काफी हाल तक छोटे रहे हैं , जैसा कि अंक 1 .11 में दिखाया गया है .
गुप्त इकाइयों के शुरू होने के बाद से , कृत्रिम तंत्रिका नेटवर्क मोटे तौर पर हर 2 . 4 साल में आकार में दोगुना हो गया है .
यह वृद्धि बड़ी स्मृति वाले तेज कंप्यूटरों और बड़े डेटासेटों की उपलब्धता से प्रेरित होती है ।
बड़े नेटवर्क अधिक जटिल कार्यों पर अधिक सटीकता प्राप्त करने में सक्षम होते हैं .
यह प्रवृत्ति दशकों तक जारी रहने के लिए तय लग रही है ।
जब तक नई प्रौद्योगिकियां तेजी से स्केलिंग को सक्षम नहीं करती , कृत्रिम तंत्रिका नेटवर्कों में कम से कम 2050 के दशक तक मानव मस्तिष्क के समान न्यूरॉन्स नहीं होंगे .
जैविक न्यूरॉन्स वर्तमान कृत्रिम न्यूरॉन्स की तुलना में अधिक जटिल कार्यों का प्रतिनिधित्व कर सकते हैं , इसलिए जैविक तंत्रिका नेटवर्क इस भूखंड चित्रण से भी बड़ा हो सकता है .
पुनर्विलोकन में , यह विशेष रूप से आश्चर्य की बात नहीं है कि जोंक की तुलना में कम न्यूरॉन्स वाले तंत्रिका नेटवर्क परिष्कृत कृत्रिम बुद्धि प्रोब - लेम हल करने में असमर्थ थे .
आज के नेटवर्क , जिसे हम कम्प्यूटेशनल सिस्टम की दृष्टि से काफी बड़े मानते हैं , मेंढक जैसे अपेक्षाकृत आदिम कशेरुकी प्राणियों के तंत्रिका तंत्र से भी छोटे हैं ।
समय के साथ मॉडल आकार में वृद्धि , तीव्र सीपीयू की उपलब्धता के कारण , सामान्य उद्देश्य जीपीयू का आगमन ( सेक्शन 12 . 1 . 2 में उल्लिखित ) , तीव्र नेटवर्क कनेक्टिविटी और वितरित कंप्यूटिंग के लिए बेहतर सॉफ्टवेयर मूल संरचना , गहन शिक्षण के इतिहास की सबसे महत्वपूर्ण प्रवृत्तियों में से एक है .
इस प्रवृत्ति को आम तौर पर भविष्य में अच्छी तरह से जारी रखने की उम्मीद है .
21 CHAPTER 1
इन्ट्राओडीयूसीशन 1950 वर्ष 2000 2015 वर्ष 10 10 2 10 3 10 4 कनेक्शन प्रति न्यूरॉन 1 2 3 4 5 8 9 फल मक्खी माउस बिल्ली मानव चित्र 110
: समय के साथ प्रति न्यूरॉन कनेक्शन की संख्या .
प्रारंभ में , कृत्रिम तंत्रिका नेटवर्क में न्यूरॉन्स के बीच कॉन्नेक आयनों की संख्या हार्डवेयर क्षमताओं द्वारा सीमित थी .
आज , न्यूरॉन्स के बीच कनेक्शन की संख्या ज्यादातर एक डिजाइन विचार है .
कुछ कृत्रिम तंत्रिका नेटवर्कों में बिल्ली के रूप में प्रति न्यूरॉन के लगभग उतने ही कनेक्शन होते हैं , और अन्य तंत्रिका नेटवर्कों के लिए चूहों जैसे छोटे स्तनधारियों के रूप में प्रति न्यूरॉन के अधिक से अधिक कनेक्शन होना आम बात है .
यहां तक कि मानव मस्तिष्क प्रति न्यूरॉन कनेक्शन की एक अनाप - शनाप राशि नहीं है .
विकिपीडिया से जैविक तंत्रिका नेटवर्क आकार (2015 )
1 . 1
अनुकूली रैखिक तत्व
नियोकोग्निट्रॉन
जीपीयू

डीप बोल्ट्जमैन मशीन
अपरूपित संवलयन नेटवर्क (
जेरेट एट अल
जीपीयू

↑ 2010 का 7वां
वितरित ऑटोनकोडर (
ले एट अल
↑ 2012 का 8 .
बहु - जीपीयू संवलयन नेटवर्क (
Krizhevsky एट अल .
↑ 2012 का 9वां
COTS HPC अनुपयुक्त संवलयन नेटवर्क (
कोट्स एट अल , 2013 10
गोगली
1 . 2 . 4
बढ़ती सटीकता , जटिलता और वास्तविक - वर्टल्ड प्रभाव
1980 के दशक के बाद से , गहरी सीखने सटीक मान्यता और भविष्यवाणी प्रदान करने की क्षमता में लगातार सुधार हुआ है .
इसके अलावा , गहरी सीखने लगातार सफलता के साथ अनुप्रयोगों के व्यापक और व्यापक सेट के लिए लागू किया गया है .
सबसे पहले गहरे मॉडलों का प्रयोग कसे हुए , अत्यंत छोटी छवियों में व्यक्तिगत वस्तुओं को पहचानने के लिए किया गया था (
रूमलहार्ट एट अल , 1986
तब से छवियों के आकार में क्रमिक वृद्धि हुई है तंत्रिका नेटवर्क प्रक्रिया कर सकते हैं .
आधुनिक वस्तु अभिज्ञान नेटवर्क उच्च विभेदन फोटोग्राफ की प्रक्रिया करता है और 22 सीएचएपीटर 1 नहीं करता है ।
INTRODUC 1950 1985 2000 2015 2056 साल 10 −2 10 −1
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10 11 नंबर के न्यूरॉन्स (logarithmic पैमाने पर
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

गुप्त इकाइयों के शुरू होने के बाद से , कृत्रिम तंत्रिका नेटवर्क मोटे तौर पर हर 2 . 4 साल में आकार में दोगुना हो गया है .
विकिपीडिया से जैविक तंत्रिका नेटवर्क आकार (2015 )
1 . 1
परिग्राही
अनुकूली रैखिक तत्व
नियोकोग्निट्रॉन ( Fukushima , 1980 )
प्रारंभिक पश्च - प्रचार नेटवर्क
भाषण मान्यता के लिए पुनरावर्ती तंत्रिका नेटवर्क
६
भाषण मान्यता के लिए मल्टीलेयर पर्सेप्टॉन
, 1991 का
अर्थ फील्ड अवग्रह विश्वास नेटवर्क
8 . 8
लेनेट

, 1998 का
इको स्टेट नेटवर्क
१०
गहरा विश्वास नेटवर्क
जीपीयू

डीप बोल्ट्जमैन मशीन
जीपीयू - शांत गहरा विश्वास नेटवर्क (
रैना एट अल .
↑ 2009 का 14वां
अपरूपित संवलयन नेटवर्क (
जेरेट एट अल
जीपीयू

16
ओएमपी - 1 नेटवर्क
वितरित ऑटोनकोडर (
ले एट अल
बहु - जीपीयू संवलयन नेटवर्क (
Krizhevsky एट अल .
↑ 2012 का इडियट 19 .
COTS HPC अनुपयुक्त संवलयन नेटवर्क (
कोट्स एट अल , 2013 20

इसी प्रकार , प्रारंभिक नेटवर्क केवल दो प्रकार की वस्तुओं को पहचान सकता था ( कुछ मामलों में , एक ही प्रकार की वस्तु की अनुपस्थिति या उपस्थिति , जबकि ये आधुनिक नेटवर्क आम तौर पर कम से कम 1 , 000 विभिन्न श्रेणियों की वस्तुओं को पहचानते हैं )
वस्तु मान्यता में सबसे बड़ी प्रतियोगिता इमेजनेट 23 CHAPTER 1 है ।
INTRODUDUDC
बड़े पैमाने पर दृश्य अभिज्ञान चैलेंज (आईएलएसवीआरसीसी ) प्रत्येक वर्ष आयोजित किया जाता है .
डीप लर्निंग के मौसमी उत्थान में एक नाटकीय क्षण आया जब एक संवलय नेटवर्क ने पहली बार इस चुनौती को जीता और एक व्यापक मार्जिन से , 261 प्रतिशत से 153 प्रतिशत तक की त्रुटि दर से
तब से , इन प्रतियोगिताओं को लगातार गहरे संवलयन जाल द्वारा जीता जाता है , और इस लेखन के रूप में , गहरी सीखने में अग्रिम इस प्रतियोगिता में नवीनतम शीर्ष - 5 त्रुटि दर 3 . 6 प्रतिशत तक ले आया है , जैसा कि अंक 112 में दिखाया गया है .
गहरी शिक्षा का भी भाषण मान्यता पर नाटकीय प्रभाव पड़ा है ।
1990 के दशक भर में सुधार के बाद , भाषण मान्यता के लिए त्रुटि दरों लगभग 2000 में शुरू हुआ .
परिचय
हम इस इतिहास की अधिक विस्तार से धारा 12 . 3 में खोज करते हैं ।
डीप नेटवर्कों में पैदल मार्ग का पता लगाने और छवि खंडन के लिए भी शानदार सफलताएं मिली हैं ।

सारा
इसी समय , गहरे नेटवर्कों के पैमाने और सटीकता में वृद्धि हुई है , 2010 2011 2013 2014 वर्ष 0 . 0 .05 0 .10 0 .15
0 . 20 0 .25 0 . 30 ILSVRC वर्गीकरण त्रुटि दर चित्र 1 . 12 : समय के साथ त्रुटि दर घटाना
चूंकि गहरे नेटवर्क इमेजनेट बड़े पैमाने पर दृश्य अभिज्ञान चैलेंज में प्रतिस्पर्धा करने के लिए आवश्यक पैमाने पर पहुंचे , वे लगातार प्रतिवर्ष प्रतियोगिता जीत लिया है , और हर बार कम और त्रुटि दरों का उत्पादन .
रसाकोवस्की एट अल से डेटा
(2014 )
और वह एट अल .
(2015 )

INTRODUConstellation name ( optional )
गुडफेलो एट अल

पहले , यह व्यापक रूप से माना जाता था कि इस तरह के सीखने के लिए अनुक्रम के व्यक्तिगत तत्वों की लेबलिंग आवश्यक है ( Gülçhre और बेंजियो , 2013
पुनरावर्ती तंत्रिका नेटवर्क , जैसे कि ऊपर उल्लिखित LSTM अनुक्रम मॉडल , अब केवल निश्चित इनपुट के बजाय अनुक्रमों और अन्य अनुक्रमों के बीच संबंधों को मॉडल करने के लिए प्रयोग किए जाते हैं .
ऐसा प्रतीत होता है कि यह अनुक्रम - से - अनुक्रम अधिगम किसी अन्य अनुप्रयोग में क्रांति लाने के कगार पर है ।

बढ़ती जटिलता की इस प्रवृत्ति को तंत्रिका ट्यूरिंग मशीनों ( ग्रेव्स एट अल , 2014 ) के शुरू होने के साथ अपने तार्किक निष्कर्ष पर धकेल दिया गया है जो स्मृति कोशिकाओं से पढ़ना सीखते हैं और स्मृति कोशिकाओं को मनमाना सामग्री लिखते हैं .
इस तरह के तंत्रिका नेटवर्क वांछित व्यवहार के उदाहरणों से सरल प्रोग्राम सीख सकते हैं .
उदाहरण के लिए , वे स्केम्बल्ड और अनुक्रमित अनुक्रमों के उदाहरण दिए गए संख्याओं की सूचियों को छांटना सीख सकते हैं .
यह आत्म - प्रोग्रामन प्रौद्योगिकी अपनी शैशवावस्था में है , लेकिन भविष्य में इसे सैद्धांतिक रूप से लगभग किसी भी कार्य पर लागू किया जा सकता है ।
गहरी शिक्षा की एक और प्रमुख उपलब्धि है उसका प्रवर्तन सीखने के क्षेत्र में विस्तार ।
प्रवर्तन अधिगम के संदर्भ में , एक स्वायत्त अभिकर्ता को परीक्षण और त्रुटि द्वारा एक कार्य करना सीखना चाहिए , मानव प्रचालक से किसी मार्गदर्शन के बिना .
डीपमिंड ने यह प्रदर्शित किया कि गहन अधिगम पर आधारित एक प्रवर्तन अधिगम प्रणाली अतरी वीडियो गेम खेलना सीखने में सक्षम है , जो कई कार्यों पर मानव स्तर के प्रदर्शन तक पहुंचता है ।

डीप लर्निंग ने रोबोटिक्स ( एफिन एट अल , 2015 ) के लिए पुनः प्रवर्तन सीखने के प्रदर्शन में भी उल्लेखनीय सुधार किया है ।
गहन शिक्षण के इन अनुप्रयोगों में से कई अत्यधिक लाभप्रद हैं .
डीप लर्निंग का उपयोग अब कई शीर्ष प्रौद्योगिकी कंपनियों द्वारा किया जाता है , जिनमें गूगल , माइक्रोसॉफ्ट , फेसबुक , आईबीएम , बैदु , एप्पल , एडोब , नेटफ्लिक्स , एनवीआईए , और एनईसी शामिल हैं
डीप लर्निंग में प्रगति भी सॉफ्टवेयर इन्फ्रास्ट्रक्चर में अग्रिमों पर काफी निर्भर किया है .
सॉफ्टवेयर पुस्तकालय जैसे


टेन्सरफ्लो
गहन अध्ययन ने अन्य विज्ञानों में भी योगदान दिया है ।
वस्तु मान्यता के लिए आधुनिक संवलित नेटवर्क दृश्य प्रसंस्करण का एक मॉडल प्रदान करते हैं जिसका तंत्रिका विज्ञानी अध्ययन कर सकते हैं (DiCarlo , 2013 )
गहन अध्ययन भी डेटा की भारी मात्रा के प्रसंस्करण और वैज्ञानिक 25 CHAPTER 1 में उपयोगी भविष्यवाणियां करने के लिए उपयोगी उपकरण प्रदान करता है .
Comment
इसका सफल प्रयोग यह भविष्यवाणी करने के लिए किया गया है कि फार्मास्यूटिकल कंपनियों को नई दवाएं डिजाइन करने में मदद करने के लिए कैसे अणु आपस में बातचीत करेंगे ( Dahl एट अल , 2014 ) , उप - परमाणु कणों की खोज करने के लिए ( Baldi माइक्रोस्कोप अल )
हम उम्मीद करते हैं कि गहन ज्ञान भविष्य में अधिक से अधिक वैज्ञानिक क्षेत्रों में प्रकट होगा .
सारांश में , गहन अधिगम मशीन अधिगम का एक दृष्टिकोण है जो पिछले कई दशकों में विकसित मानव मस्तिष्क , सांख्यिकी और अनुप्रयुक्त गणित के बारे में हमारे ज्ञान पर भारी पड़ा है ।
हाल के वर्षों में , गहरी शिक्षा ने अपनी लोकप्रियता और उपयोगिता में अत्यधिक वृद्धि देखी है , व्यापक रूप से अधिक शक्तिशाली कंप्यूटरों , गहरे नेटवर्कों को प्रशिक्षित करने के लिए बड़े डेटासेट और तकनीकों के परिणामस्वरूप .
आगे के वर्षों में गहरी शिक्षा को और भी बेहतर बनाने और उसे नई सीमाओं तक पहुंचाने की चुनौतियों और अवसरों से भरा हुआ है ।
26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26
