अध्याय 1 परिचय निवेशकों को लंबे समय से मशीनों है कि लगता है बनाने का सपना देखा है .</s>
यह इच्छा कम से कम प्राचीन यूनान के समय की है ।</s>
पौराणिक पात्रों पिग्मलियन , डेडलस , और हीफास्टस को सभी दंतकथाकार आविष्कारक , गैलेटा , तालोस , और पांडोरा को कृत्रिम जीवन माना जा सकता है ( ओविड मार्टिन 1997 और 1996 पशुचिकित्सा , 1996 )</s>
जब प्रोग्राम योग्य कंप्यूटरों की कल्पना पहली बार की गई , तो लोगों ने सोचा कि क्या ऐसी मशीनें बुद्धिमान बन सकती हैं , एक के निर्माण से सौ वर्ष पहले ( लॉवेलेंस , 1842 )</s>
आज , कृत्रिम बुद्धि कई व्यावहारिक अनुप्रयोगों और सक्रिय अनुसंधान विषयों के साथ एक फलता - फूलता क्षेत्र है .</s>
हम सामान्य श्रम को स्वचालित करने के लिए बुद्धिमान सॉफ्टवेयर की ओर देखते हैं , भाषण या छवियों को समझते हैं , दवा में निदान और बुनियादी वैज्ञानिक अनुसंधान का समर्थन करते हैं .</s>
कृत्रिम बुद्धि के प्रारंभिक दिनों में , फील्ड तेजी से निपटा और समस्याओं का समाधान है कि मानव के लिए बौद्धिक रूप से मुश्किल है लेकिन कंप्यूटर के लिए अपेक्षाकृत सीधे - आगे -</s>
समस्याओं है कि औपचारिक , गणित - विषयक नियमों की एक सूची द्वारा वर्णित किया जा सकता है .</s>
कृत्रिम बुद्धिमत्ता की सच्ची चुनौती उन कार्यों को हल करने में सिद्ध हुई जो लोगों के लिए आसान हैं लेकिन लोगों के लिए औपचारिक रूप से वर्णन करने के लिए कठिन हैं - उन संकेतों को जिन्हें हम अन्तर्निहित रूप से हल करते हैं , जो स्वतः अनुभव करते हैं , जैसे बोलचाल के शब्दों को पहचानते हैं या तस्वीरों में चेहरे ।</s>
यह पुस्तक इन अधिक सहज समस्याओं के समाधान के बारे में है ।</s>
इस समाधान है कि कंप्यूटर अनुभव से सीखने और अवधारणाओं के एक पदानुक्रम के संदर्भ में दुनिया को समझने की अनुमति देने के लिए , सरल अवधारणाओं के अपने संबंध के माध्यम से परिभाषित प्रत्येक अवधारणा के साथ .</s>
अनुभव से ज्ञान इकट्ठा करके , इस दृष्टिकोण मानव ऑपरेटरों के लिए सभी ज्ञान है कि कंप्यूटर की जरूरत है औपचारिक रूप से निर्दिष्ट करने की आवश्यकता से बचता है .</s>
संकल्पनाओं का पदानुक्रम कंप्यूटर को सरल संकल्पनाओं से निर्मित करके जटिल संकल्पनाओं को सीखने में सक्षम बनाता है ।</s>
यदि हम कैसे इन अवधारणाओं को दिखाने एक ग्राफ आकर्षित 1 अध्याय 1 ।</s>
INTRODUCTION एक दूसरे के ऊपर बने हैं , ग्राफ गहरा है , कई परतों के साथ ।</s>
इस कारण से , हम इस दृष्टिकोण एआई गहरी सीखने के लिए कहते हैं ।</s>
एआई की प्रारंभिक सफलताएं अपेक्षाकृत बंध्य और औपचारिक वातावरण में हुई और इसके लिए कंप्यूटरों को विश्व के बारे में अधिक जानकारी रखने की आवश्यकता नहीं पड़ी ।</s>
उदाहरण के लिए , आईबीएम की डीप ब्लू शतरंज खेलने की प्रणाली ने 1997 में विश्व चैंपियन गैरी कास्परोव को हराया ( होसु , 2002 )</s>
निस्संदेह , शतरंज एक बहुत ही सरल संसार है , जिसमें केवल साठ - चार स्थान और चौंतीस खंड होते हैं , जो केवल निरपवाद रुप से चल सकते हैं ।</s>
एक सफल शतरंज रणनीति का वर्णन करना एक बहुत बड़ी उपलब्धि है , लेकिन चुनौती शतरंज के टुकड़ों के सेट का वर्णन करने और कंप्यूटर के लिए अनुमति देने की कठिनाई के कारण नहीं है .</s>
शतरंज पूरी तरह से औपचारिक नियमों की एक बहुत ही संक्षिप्त सूची द्वारा वर्णित किया जा सकता है , आसानी से प्रोग्रामर द्वारा समय से पहले प्रदान की .</s>
विडंबना यह है कि , अमूर्त और औपचारिक कार्य जो एक मनुष्य के लिए सबसे कठिन मानसिक उपक्रमों में से एक हैं , एक कंप्यूटर के लिए सबसे आसान है .</s>
कंप्यूटर लंबे समय से सर्वश्रेष्ठ मानव शतरंज खिलाड़ी को भी हराने में सफल रहे हैं , लेकिन केवल हाल ही में औसत मानव की वस्तुओं या बोली पहचानने की क्षमताओं में से कुछ मिलान शुरू कर दिया है .</s>
एक व्यक्ति के दैनिक जीवन के लिए विश्व के बारे में अपार ज्ञान की आवश्यकता होती है ।</s>
इस ज्ञान का बहुत कुछ व्यक्तिपरक और सहज ज्ञान युक्त है , और इसलिए इसे औपचारिक रूप से व्यक्त करना कठिन है ।</s>
कंप्यूटरों को एक बुद्धिमान तरीके से व्यवहार करने के लिए इस एक ही ज्ञान पर कब्जा करने की जरूरत है .</s>
कृत्रिम बुद्धि में एक प्रमुख चुनौती यह है कि इस अनौपचारिक ज्ञान को कंप्यूटर में कैसे प्राप्त किया जाए ।</s>
अनेक कृत्रिम बुद्धिमत्तापूर्ण परियोजनाओं ने औपचारिक भाषाओं में विश्व के बारे में कठोर ज्ञान प्राप्त करने का प्रयास किया है ।</s>
तार्किक निष्कर्ष नियमों का प्रयोग करते हुए कंप्यूटर इन औपचारिक भाषाओं में वक्तव्यों के बारे में स्वतः ही तर्क दे सकता है ।</s>
इसे कृत्रिम बुद्धिमत्ता का ज्ञान आधारित दृष्टिकोण कहा जाता है ।</s>
इनमें से किसी भी परियोजना को बड़ी सफलता नहीं मिली है ।</s>
इस तरह की सबसे प्रसिद्ध परियोजनाओं में से एक है साइक ( एलेनाट और गुहा ) , 1989 .</s>
साइक एक अनुमान इंजन है और एक भाषा में कथनों का डेटाबेस है , जिसे साइकल कहते हैं .</s>
इन बयानों को मानव पर्यवेक्षक के एक स्टाफ द्वारा दर्ज किया जाता है .</s>
यह एक अप्राप्य प्रक्रिया है ।</s>
लोग दुनिया को ठीक - ठीक बताने के लिए पर्याप्त जटिलता के साथ औपचारिक नियमों को ईजाद करने के लिए संघर्ष करते हैं ।</s>
उदाहरण के लिए , साइक सुबह फ्रेड शेव नामक व्यक्ति के बारे में एक कहानी समझने में असफल रहा ( लिनड , 1992 ) .</s>
इसके अनुमान इंजन को कहानी में एक असंगति का पता चलाः यह पता था कि लोगों के पास बिजली के पुर्जे नहीं हैं , लेकिन चूंकि फ्रेड बिजली के रेजर का काम कर रहा था , इसलिए यह मानता था कि विद्युत पुर्जे फ्रेड का काम करते थे ।</s>
इसलिए उसने पूछा कि क्या फ्रेड अभी भी एक व्यक्ति था जबकि वह शेव कर रहा था .</s>
कठोर ज्ञान पर भरोसा करने वाली प्रणालियों के सामने आने वाली कठिनाइयों से लगता है कि एआइआइ प्रणालियों के लिए अपने स्वयं के ज्ञान को प्राप्त करने की क्षमता की आवश्यकता है , 2 अध्याय 1 निष्कर्षण द्वारा .</s>
कच्चे डेटा से सूचना पैटर्न</s>
इस क्षमता को मशीन लर्निंग के नाम से जाना जाता है ।</s>
मशीनी शिक्षा शुरू करने से कंप्यूटर वास्तविक दुनिया के ज्ञान से जुड़ी समस्याओं से निपटने और व्यक्तिपरक लगने वाले निर्णय लेने में सक्षम हो गए ।</s>
एक सरल मशीन लर्निंग एल्गोरिदम , जिसे लॉजिस्टिक रीग्रेसन कहा जाता है , यह निर्धारित कर सकता है कि क्या सिजेरियन डिलीवरी ( Mor - योजफ एट अल , 1990 ) की सिफारिश करना है .</s>
एक सरल मशीन लर्निंग एल्गोरिदम जिसे नैवी बेय्स कहा जाता है , स्पैम ई - मेल से वैध ई - मेल को अलग कर सकता है ।</s>
इन सरल मशीन लर्निंग एल्गोरिदम का प्रदर्शन उन्हें दिए गए डेटा के प्रतिनिधित्व पर काफी निर्भर करता है .</s>
उदाहरण के लिए , जब सिजेरियन डिलीवरी की सिफारिश करने के लिए लॉजिस्टिक प्रतिगमन का प्रयोग किया जाता है , एआई प्रणाली रोगी की सीधे जांच नहीं करता है .</s>
इसके बजाय , चिकित्सक सिस्टम को प्रासंगिक जानकारी के कई टुकड़े बताता है , जैसे कि गर्भाशय के एक निशान की उपस्थिति या अनुपस्थिति .</s>
रोगी के प्रतिनिधित्व में शामिल जानकारी के प्रत्येक टुकड़ा एक विशेषता के रूप में जाना जाता है .</s>
लॉजिस्टिक प्रतिगमन यह सीखता है कि रोगी की इन विशेषताओं में से प्रत्येक का विभिन्न परिणामों से कैसे संबंध होता है ।</s>
हालांकि , यह कैसे सुविधाओं को किसी भी तरह से परिभाषित कर रहे हैं प्रभावित नहीं कर सकते हैं .</s>
यदि लॉजिस्टिक रीग्रेसन को मरीज का एमआरआई स्कैन दिया जाता है , बजाय चिकित्सक की औपचारिक रिपोर्ट के , यह उपयोगी भविष्यवाणियां करने में सक्षम नहीं होगा .</s>
एमआरआई स्कैन में व्यक्तिगत पिक्सल का प्रसव के दौरान होने वाली जटिलताओं के साथ नगण्य सहसंबंध होता है ।</s>
अभ्यावेदन पर यह निर्भरता एक सामान्य घटना है जो कंप्यूटर विज्ञान और यहां तक कि दैनिक जीवन भर दिखाई देती है .</s>
कंप्यूटर विज्ञान में , डेटा संग्रह की खोज जैसे कार्य घातांकीय रूप से तेजी से आगे बढ़ सकते हैं यदि कोलेक - टीन की संरचित और अनुक्रमण बुद्धिमानी से किया जाए ।</s>
लोग अरबी अंकों पर अंकगणित को आसानी से कर सकते हैं लेकिन रोमन अंकों पर अंकगणित को बहुत अधिक समय लगता है ।</s>
यह आश्चर्य की बात नहीं है कि प्रतिनिधित्व के चयन का मशीन लर्निंग एल्गोरिदम के प्रदर्शन पर भारी प्रभाव पड़ता है .</s>
एक सरल दृश्य उदाहरण के लिए , यह आंकड़ा 1 . 1 देखें ।</s>
कई कृत्रिम बुद्धि कार्यों को उस कार्य के लिए निष्कर्षण के लिए सुविधाओं के सही सेट डिजाइन करके हल किया जा सकता है , तो एक सरल मशीन सीखने एल्गोरिथ्म को इन सुविधाओं को प्रदान करते हैं .</s>
उदाहरण के लिए , ध्वनि से स्पीकर पहचान के लिए एक उपयोगी विशेषता वक्ता के मुखर पथ के आकार का अनुमान है .</s>
यह विशेषता इस बात का प्रबल संकेत देती है कि वक्ता पुरुष है , स्त्री है या संतान ।</s>
हालांकि , कई कार्यों के लिए यह जानना मुश्किल है कि किन विशेषताओं को निकाला जाना चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हम फोटो में कारों का पता लगाने के लिए एक प्रोग्राम लिखना चाहेंगे ।</s>
हम जानते हैं कि कारों पहिया है , तो हम एक विशेषता के रूप में एक पहिया की उपस्थिति का उपयोग करना चाहते हो सकता है .</s>
दुर्भाग्य से , यह वास्तव में क्या एक पहिया पिक्सेल मूल्यों के संदर्भ में की तरह लग रहा है का वर्णन करने के लिए मुश्किल है .</s>
पहिए की एक साधारण ज्यामितीय आकृति होती है , लेकिन उसकी छवि चक्र पर पड़ने वाली छायाओं से जटिल हो सकती है ।</s>
परिचय</s>
</s>
·</s>
</s>
·</s>
चित्र 1 . 1</s>
विभिन्न अभ्यावेदनों का उदाहरणः मान लीजिए कि हम डेटा की दो श्रेणियों को एक स्कैटरप्लॉट में एक रेखा खींचकर अलग करना चाहते हैं .</s>
बाईं ओर के प्लॉट में , हम कार्टेसियन निर्देशांकों का उपयोग कर कुछ डेटा का प्रतिनिधित्व करते हैं , और कार्य असंभव है .</s>
दायीं ओर के भूखंड में , हम ध्रुवीय निर्देशांकों के साथ डेटा का प्रतिनिधित्व करते हैं और एक ऊर्ध्वाधर रेखा के साथ हल करने के लिए कार्य सरल हो जाता है .</s>
डेविड वार्ड फार्ले के सहयोग से तैयार किया गया यह कपड़ा पहिये के अग्रवर्ती भाग में स्थित है ।</s>
इस समस्या का एक समाधान यह भी है कि न केवल प्रतिनिधित्व से आउटपुट तक मानचित्रण का पता लगाने के लिए मशीन शिक्षण का उपयोग किया जाए बल्कि स्वयं प्रतिनिधित्व भी किया जाए ।</s>
इस दृष्टिकोण प्रतिनिधित्व सीखने के रूप में जाना जाता है .</s>
विद्वत अभ्यावेदनों का परिणाम बहुधा हस्त - दर्शित अभ्यावेदनों की तुलना में कहीं बेहतर प्रदर्शन होता है .</s>
वे एआई प्रणालियों को भी कम से कम मानवीय हस्तक्षेप के साथ नए कार्यों के लिए तेजी से अनुकूलित करने में सक्षम बनाता है .</s>
एक अभ्यावेदन अधिगम एल्गोरिथ्म , मिनटों में एक सरल कार्य के लिए या घंटों से महीनों में एक जटिल कार्य के लिए सुविधाओं के एक अच्छे सेट का पता लगा सकता है .</s>
डिजाइन डिजाइन जटिल कार्य के लिए बहुत अधिक मानवीय समय की आवश्यकता होती है और यह प्रयास एक मैन्युअल रूप से शोधकर्ताओं के पूरे समुदाय के लिए दशकों लग सकता है .</s>
एक निरूपण अधिगम एल्गोरिथ्म का वास्तविक उदाहरण ऑ - टोनकोडर है ।</s>
एक स्व - संप्रतीक एक एनकोडर फलन का संयोजन है , जो इनपुट डेटा को एक भिन्न अभ्यावेदन , और एक विकोडक फलन में परिवर्तित करता है , जो नए निरूपण को मूल प्रारूप में वापस परिवर्तित करता है .</s>
जब किसी इनपुट को एनकोडर और उसके बाद डीकोडर के माध्यम से चलाया जाता है तो उसे यथासंभव अधिक से अधिक जानकारी सुरक्षित रखने के लिए ऑटोनोकोडर्स को प्रशिक्षित किया जाता है , लेकिन उन्हें नए प्रतिनिधित्व के विभिन्न अच्छे गुण बनाने के लिए भी प्रशिक्षित किया जाता है ।</s>
विभिन्न प्रकार के ऑटोनकोडर्स का उद्देश्य विभिन्न प्रकार की संपत्तियों को प्राप्त करना होता है ।</s>
जब सीखने की विशेषताओं के लिए सुविधाओं या एल्गोरिदम डिजाइन , हमारे लक्ष्य आमतौर पर मनाया डेटा की व्याख्या भिन्नता के कारकों को अलग करने के लिए है .</s>
इसमें 4 अध्याय 1 .</s>
आईटीरोडी संदर्भ में , हम “फैक्टर” शब्द का प्रयोग केवल अंग्रेज़ी गुणन कारकों द्वारा प्रभाव के अलग - अलग स्रोतों के लिए करते हैं ।</s>
इस तरह के कारक अक्सर ऐसी मात्राएं नहीं होती जो सीधे तौर पर देखी जाती हैं ।</s>
इसके स्थान पर , वे या तो अनपेक्षित वस्तुओं के रूप में विद्यमान हो सकते हैं या फिर स्थूल जगत् में उन अनपेक्षित शक्तियों के रूप में विद्यमान हो सकते हैं जो प्रेक्षणीय मात्राओं को प्रभावित करती हैं ।</s>
ये मानव मस्तिष्क में निर्मित वस्तुओं के रूप में भी विद्यमान हो सकते हैं जो प्रेक्षित आंकड़ों की व्याख्या या अनुमानित कारणों को सरल बनाती हैं ।</s>
वे अवधारणाओं या अमूर्त के रूप में सोचा जा सकता है कि डेटा में समृद्ध विविधता का एहसास करने में मदद करता है .</s>
जब किसी वाक् अभिलेख का विश्लेषण किया जाता है तो विभिन्नता के कारकों में वक्ता की आयु , उनके लिंग , उनके लहजे और बोलचाल के शब्द शामिल होते हैं ।</s>
किसी कार की छवि का विश्लेषण करने पर , विभिन्नता के कारकों में कार की स्थिति , उसके रंग , और सूर्य के कोण और चमक शामिल हैं .</s>
कई वास्तविक दुनिया के कृत्रिम बुद्धिमत्ता अनुप्रयोगों में कठिनाई का एक बड़ा स्रोत यह है कि विभिन्नता के कई कारक प्रत्येक डेटा के प्रत्येक टुकड़े को हम देख सकते हैं , प्रभावित करते हैं .</s>
लाल कार की एक छवि में व्यक्तिगत पिक्सल रात में काले करने के लिए बहुत करीब हो सकता है .</s>
कार के सिलहोट का आकार देखने के कोण पर निर्भर करता है ।</s>
अधिकांश अनुप्रयोगों के लिए आवश्यक है कि हम विभिन्नता के कारकों को सुलझाएं और उन कारकों को त्यागें जिनके बारे में हम परवाह नहीं करते .</s>
बेशक , इस तरह के उच्च स्तर , कच्चे डेटा से अमूर्त सुविधाओं को निकालना बहुत मुश्किल हो सकता है .</s>
विभिन्नता के इन कारकों , जैसे वक्ता के लहजे , की पहचान केवल डेटा के लगभग मानव स्तर की समझ का उपयोग करते हुए की जा सकती है ।</s>
जब मूल समस्या को हल करने के लिए प्रतिनिधित्व प्राप्त करना लगभग उतना ही कठिन होता है , तब अभ्यावेदन शिक्षण पहली नजर में हमारी सहायता नहीं करता ।</s>
गहन अधिगम , आत्म - द्वैध निरूपण द्वारा अभ्यावेदन शिक्षण में इस केंद्रीय समस्या का समाधान करता है जिसे अन्य , सरल प्रतिनिवेदनों के संदर्भ में व्यक्त किया जाता है ।</s>
गहन अध्ययन कंप्यूटर को सरल अवधारणाओं से जटिल अवधारणाओं का निर्माण करने में सक्षम बनाता है ।</s>
चित्र 1 . 2 से पता चलता है कि कैसे एक गहन शिक्षण प्रणाली सरल अवधारणाओं , जैसे कोनों और रूपरेखा , जो बारी में किनारों के संदर्भ में परिभाषित कर रहे हैं के संयोजन के द्वारा एक व्यक्ति की छवि की अवधारणा का प्रतिनिधित्व कर सकते हैं .</s>
एक गहन अधिगम मॉडल का सारभूत उदाहरण फीडेन डीप नेटवर्क या मल्टीलेयर सेप्टॉन ( एमएलपी ) है ।</s>
एक बहुप्रचलित्र , मात्र एक गणितीय फलन प्रतिचित्रण है , जो निर्गम मूल्यों के इनपुट मूल्यों के कुछ समुच्चय को प्रतिचित्रित करता है ।</s>
समारोह कई सरल प्रकार्यों की रचना करके बनता है ।</s>
हम इनपुट का एक नया प्रतिनिधित्व प्रदान करने के रूप में एक अलग गणितीय फलन के प्रत्येक आवेदन के बारे में सोच सकते हैं .</s>
डेटा के लिए सही प्रतिनिधित्व सीखने का विचार गहन अध्ययन पर एक प्रति - परिदृश्य प्रदान करता है .</s>
गहन अध्ययन का एक अन्य परिप्रेक्ष्य यह है कि गहराई कंप्यूटर को बहु - बस्ती कंप्यूटर प्रोग्राम सीखने में सक्षम बनाती है ।</s>
प्रतिनिधित्व की प्रत्येक परत को 5 अध्याय 1 के बाद कंप्यूटर की स्मृति की स्थिति के रूप में सोचा जा सकता है .</s>
अनिश्चित परत ( इनपुट पिक्सल )</s>
प्रथम प्रच्छन्न परत</s>
दूसरी छिपी हुई परत</s>
Constellation name ( optional )</s>
चित्र 1 . 2 : गहन शिक्षण मॉडल का इलस्ट्रेशन ।</s>
एक कंप्यूटर के लिए कच्चे संवेदी इनपुट डेटा के अर्थ को समझना मुश्किल है , जैसे कि यह छवि पिक्सेल मूल्यों के संग्रह के रूप में प्रतिनिधित्व करती है .</s>
किसी वस्तु की पहचान के लिए पिक्सल के सेट से फ़ंक्शन मैपिंग बहुत जटिल है .</s>
इस मानचित्रण का अध्ययन या मूल्यांकन यदि सीधे तौर पर किया जाए तो यह असंभव प्रतीत होता है ।</s>
गहन अध्ययन इस कठिनाई का समाधान अपेक्षित जटिल मानचित्रण को तोड़कर नेस्टेड सरल मानचित्रण की एक श्रृंखला में करता है , प्रत्येक मॉडल की एक अलग परत द्वारा वर्णित है .</s>
इनपुट दृश्य परत पर प्रस्तुत है , इसलिए नाम दिया है क्योंकि यह चर है कि हम निरीक्षण करने में सक्षम हैं शामिल हैं .</s>
फिर छुपी परतों की एक श्रृंखला छवि से लगातार अमूर्त विशेषताओं को निकालती है .</s>
इन परतों को " इन परतों " कहा जाता है क्योंकि उनके मान को आंकड़ों में नहीं दिया गया है क्योंकि इन पर विचार किए गए आंकड़ों की व्याख्या के लिए मॉडल को कौन से संबंध उपयोगी होने चाहिए ।</s>
यहाँ की छवियों में प्रत्येक छुपी इकाई द्वारा प्रतिनिधित्व किया विशेषता के प्रकार की कल्पना कर रहे हैं .</s>
पिक्सल को देखते हुए , पहली परत आसानी से किनारों की पहचान कर सकते हैं , पड़ोसी पिक्सेल की चमक की तुलना करके .</s>
पहली छुपी परत के किनारों के वर्णन को देखते हुए , दूसरी छुपी परत आसानी से कोनों और विस्तारित स्पर्शरेखाओं को खोज सकती है , जो किनारों के संग्रह के रूप में पहचानने योग्य हैं .</s>
कोनों और रेखाओं के संदर्भ में छवि की दूसरी छिपी पर्तों के वर्णन को देखते हुए , तीसरी छुपी पर्त विशिष्ट वस्तुओं के संपूर्ण भागों का पता लगा सकती है ।</s>
अंत में , इसमें शामिल वस्तु भागों के संदर्भ में छवि के इस वर्णन का उपयोग छवि में मौजूद वस्तुओं को पहचानने के लिए किया जा सकता है .</s>
जीइलर और फर्गस (2014 ) की अनुमति से पुनः पेश की गई छवियाँ</s>
6 अध्याय 1 .</s>
सूचना जो अनुदेशों के अन्य समुच्चय को समानांतर रूप से निष्पादित करता है ।</s>
अधिक गहराई वाले नेटवर्क अधिक अनुदेशों को अनुक्रम में निष्पादित कर सकते हैं ।</s>
अनुक्रमिक अनुदेश महान शक्ति प्रदान करते हैं क्योंकि बाद के अनुदेश पूर्व निर्देशों के परिणामों को वापस संदर्भित कर सकते हैं .</s>
एकड़</s>
डीप लर्निंग के इस दृष्टिकोण के अनुरूप , एक परत सक्रियण में सभी जानकारी आवश्यक रूप से भिन्नता के कारकों को एन्कोड करता है कि इनपुट की व्याख्या .</s>
अभ्यावेदन अवस्था सूचना को भी संग्रहित करता है जो एक प्रोग्राम को निष्पादित करने में सहायता करता है जो इनपुट का बोध करा सकता है .</s>
यह स्थिति सूचना एक पारंपरिक कंप्यूटर प्रोग्राम में एक काउंटर या सूचक के अनुरूप हो सकती है .</s>
इसका विशेष रूप से इनपुट की सामग्री से कोई संबंध नहीं है , लेकिन यह मॉडल को अपने प्रसंस्करण को व्यवस्थित करने में मदद करता है .</s>
एक मॉडल की गहराई मापने के दो मुख्य तरीके हैं .</s>
पहला दृश्य अनुक्रमिक अनुदेशों की संख्या पर आधारित है जिसे वास्तुकला का मूल्यांकन करने के लिए निष्पादित किया जाना चाहिए .</s>
हम इस के बारे में एक प्रवाह चार्ट के माध्यम से सबसे लंबे पथ की लंबाई के रूप में सोच सकते हैं जो यह वर्णन करता है कि कैसे प्रत्येक मॉडल के आउटपुट को अपने इनपुट दिया की गणना करने के लिए .</s>
जिस प्रकार दो समतुल्य कंप्यूटर प्रोग्राम की लंबाई भिन्न होती है , यह इस बात पर निर्भर करता है कि प्रोग्राम किस भाषा में लिखा गया है , उसी प्रकार समान फलन को प्रवाह संचित्र के रूप में विभिन्न गहराइयों के साथ खींचा जा सकता है , यह निर्भर करता है कि प्रवाह संचित्र में अलग - अलग चरणों के रूप में कौन से कार्य करने की अनुमति देते हैं ।</s>
चित्र 1 . 3 इस बात को स्पष्ट करता है कि किस प्रकार भाषा का यह चुनाव एक ही वास्तुकला के लिए दो भिन्न माप प्रदान कर सकता है ।</s>
</s>
×</s>
x 2</s>
x 2</s>
त् वचा</s>
2</s>
w 2 × +</s>
तत्व सेट + ×</s>
x</s>
एलिमेंट सेट लॉजिस्टिक रीग्रेसन लॉजिस्टिक रीग्रेसन चित्र 13 :</s>
संगणकीय रेखांकन का इलस्ट्रेशन , इनपुट को उस आउटपुट में प्रतिचित्रित करता है , जहाँ प्रत्येक नोड एक संक्रिया निष्पादित करता है ।</s>
गहराई इनपुट से आउटपुट के लिए सबसे लंबे पथ की लंबाई है , लेकिन क्या एक संभव कम्प्यूटेशनल चरण का गठन करता है की परिभाषा पर निर्भर करता है .</s>
गणना इन ग्राफ़िकल को दर्शाया गया गणना एक रसद मॉडल के आउटपुट , SE ( T )</s>
यदि हम अपने कंप्यूटर भाषा के तत्वों के रूप में जोड़ , गुणन और लॉजिस्टिक सिग्मिड का उपयोग करते हैं , तो इस मॉडल में गहराई तीन है ।</s>
यदि हम लॉजिस्टिक प्रतिगमन को एक तत्व के रूप में ही देखते हैं , तो इस मॉडल में गहराई एक है ।</s>
7 अध्याय 1 .</s>
InTRODUCTION एक अन्य दृष्टिकोण , जिसका प्रयोग गहन प्रोबाबिलिटी मॉडल द्वारा किया जाता है , एक मॉडल की गहराई को कम्प्यूटेशनल ग्राफ की गहराई नहीं बल्कि ग्राफ की गहराई मानता है कि कैसे अवधारणाएँ एक दूसरे से संबंधित हैं ।</s>
इस मामले में , प्रत्येक अवधारणा के निरूपण की गणना करने के लिए आवश्यक संगणनाओं के प्रवाह संचित्र की गहराई स्वयं अवधारणाओं के ग्राफ की तुलना में कहीं अधिक गहरी हो सकती है .</s>
इसका कारण यह है कि प्रणाली की सरल अवधारणाओं की समझ को अधिक जटिल अवधारणाओं के बारे में जानकारी देकर परिष्कृत किया जा सकता है ।</s>
उदाहरण के लिए , छाया में एक आंख वाले चेहरे की छवि देखने वाली एआई प्रणाली शुरू में केवल एक आंख ही देख सकती है ।</s>
यह पता लगाने के बाद कि एक चेहरा मौजूद है , प्रणाली तब अनुमान लगा सकती है कि एक दूसरी आंख शायद के रूप में अच्छी तरह से मौजूद है .</s>
इस मामले में , अवधारणाओं के ग्राफ केवल दो परत शामिल हैं - आँखों के लिए परत और चेहरों के लिए एक परत - लेकिन गणना के ग्राफ 2 एन परत शामिल हैं अगर हम अन्य एन बार दिया प्रत्येक अवधारणा के अपने अनुमान को परिष्कृत .</s>
क्योंकि यह हमेशा स्पष्ट नहीं है कि इन दो दृश्यों में से कौन सा है - कम्प्यूटेशनल ग्राफ की गहराई , या प्रोबाबिलिस्टिक मॉडलिंग ग्राफ की गहराई - सबसे अधिक प्रासंगिक है , और क्योंकि अलग - अलग लोग अलग - अलग अवयवों के अलग - अलग सेट चुनते हैं , क्योंकि एक ही आर्किटेक्चर की गहराई के लिए कोई सही मान नहीं है ।</s>
न ही इस बात पर आम सहमति है कि किसी मॉडल को “दीप” के रूप में कितनी गहराई तक स्वीकार्य करने की आवश्यकता है ।</s>
तथापि , गहन अधिगम को सुरक्षित रूप से ऐसे मॉडलों के अध्ययन के रूप में माना जा सकता है जिसमें पारंपरिक मशीन अधिगम की अपेक्षा विद्वत कार्यों या विद्वत अवधारणाओं की अधिक मात्रा सम्मिलित होती है ।</s>
संक्षेप में , गहराई से सीखने , इस पुस्तक के विषय , एआई के लिए एक दृष्टिकोण है .</s>
विशेष रूप से , यह एक प्रकार की मशीन लर्निंग , एक तकनीक है जो कंप्यूटर सिस्टम को अनुभव और डेटा के साथ बेहतर बनाने में सक्षम बनाता है .</s>
हमारा तर्क है कि ऐआइ सिस्टम बनाने के लिए मशीन लर्निंग ही एक व्यवहार्य तरीका है जो जटिल वास्तविक विश्व परिवेश में काम कर सकता है ।</s>
गहन अधिगम एक विशेष प्रकार का मशीनी अधिगम है जो विश्व को अवधारणाओं के नेस्टेड पदानुक्रम के रूप में , सरल अवधारणाओं के संबंध में परिभाषित प्रत्येक अवधारणा के साथ , और कम अमूर्त निरूपण के संदर्भ में परिकलित करके महान शक्ति और लचीलापन प्राप्त करता है ।</s>
चित्र 1 . 4 इन विभिन्न एआई विधाओं के बीच संबंधों को स्पष्ट करता है ।</s>
चित्र 1 . 5 में प्रत्येक व्यक्ति किस प्रकार कार्य करता है इसकी एक उच्च स्तर की योजना दी गई है ।</s>
1 . 1</s>
इस पुस्तक को किसने पढ़ा ?</s>
यह पुस्तक कई तरह के पाठकों के लिए उपयोगी हो सकती है , लेकिन हमने इसे दो लक्षित दर्शकों को ध्यान में रखकर लिखा ।</s>
इन लक्षित दर्शकों में से एक है विश्वविद्यालय के छात्र ( बिना स्नातक या स्नातक के मशीन लर्निंग के बारे में सीखने , उन लोगों सहित जो गहन शिक्षण और कृत्रिम बुद्धि अनुसंधान में एक कैरियर की शुरुआत कर रहे हैं .</s>
अन्य 8 अध्याय 1 .</s>
InTRODUCTION A मशीन लर्निंग प्रतिनिधित्व लर्निंग डीप लर्निंग उदाहरणः ज्ञान आधारः</s>
लॉजिस्टिक रीग्रेसन</s>
उदाहरणः</s>
शैलो ऑटोनकोडर्स</s>
उदाहरणः</s>
एमएलपी चित्र 1 . 4 :</s>
एक वेन आरेख जो दिखाता है कि कैसे गहरी अधिगम एक प्रकार का प्रतिनिधित्व अधिगम है , जो बारी में मशीन अधिगम का एक प्रकार है , जो कई के लिए प्रयोग किया जाता है लेकिन एआई के लिए सभी दृष्टिकोण नहीं है .</s>
वेन आरेख के प्रत्येक अनुभाग में एक एआई प्रौद्योगिकी का एक उदाहरण शामिल है .</s>
लक्षित श्रोता सॉफ्टवेयर इंजीनियर हैं जिनके पास मशीनी अधिगम या स्थैतिक पृष्ठभूमि नहीं है लेकिन वे तेजी से एक को प्राप्त करना चाहते हैं और अपने उत्पाद या मंच में गहन अधिगम का प्रयोग शुरू कर देते हैं ।</s>
डीप लर्निंग पहले से ही कंप्यूटर दृष्टि , भाषण और ऑडियो प्रोसेसिंग , प्राकृतिक भाषा संसाधन , रोबोटिक्स , जैव सूचना विज्ञान और रसायन विज्ञान , वीडियो गेम्स , खोज इंजन , ऑनलाइन विज्ञापन और वित्त सहित कई सॉफ्ट वेयर विधाओं में उपयोगी साबित हो चुका है ।</s>
इस पुस्तक को तीन भागों में बांटा गया है ताकि विविध पाठकों को अच्छी तरह समझा जा सके ।</s>
भाग I बुनियादी गणितीय उपकरणों और मशीन सीखने अवधारणाओं का परिचय .</s>
भाग II सबसे स्थापित गहन शिक्षण एल्गोरिदम , जो अनिवार्य रूप से हल प्रौद्योगिकियों का वर्णन करता है .</s>
भाग 3 अधिक सट्टा विचारों का वर्णन करता है जो व्यापक रूप से गहन अध्ययन में भविष्य के अनुसंधान के लिए महत्वपूर्ण माना जाता है .</s>
9 अध्याय 1 .</s>
सूचना इनपुट</s>
हैंड - डिज़ाइन प्रोग्राम</s>
आउटपुट इनपुट</s>
फीचर आउटपुट इनपुट फीचर मैपिंग से हस्तनिर्मित फीचर मैपिंग</s>
एक एआइ प्रणाली के विभिन्न भागों को विभिन्न एआइ विधाओं के भीतर एक दूसरे से किस प्रकार संबंधित किया जाता है , यह दर्शाने वाला सरल लक्षणों आउटपुट अतिरिक्त परतों से मापना नियम - बेस्ड सिस्टम क्लासिक मशीन लर्निंग रिप्रेजेंटेशन लर्निंग डीप लर्निंग अनुभव 1 . 5</s>
छायादार डिब्बे उन घटकों को सूचित करते हैं जो डेटा से सीखने में सक्षम होते हैं ।</s>
पाठकों को उन भागों को छोड़ने के लिए स्वतंत्र महसूस करना चाहिए जो उनके हितों या पृष्ठभूमि को देखते हुए प्रासंगिक नहीं हैं .</s>
रैखिक बीजगणित , प्रायिकता , और मूलभूत मशीन शिक्षण अवधारणाओं से परिचित पाठक भाग I को छोड़ सकते हैं , उदाहरण के लिए , जबकि जो लोग सिर्फ एक कार्य प्रणाली को कार्यान्वित करना चाहते हैं उन्हें भाग II से आगे पढ़ने की आवश्यकता नहीं है ।</s>
चुनने में मदद करने के लिए जो 10 अध्याय 1 .</s>
परिचय 1 .</s>
परिचय भाग 1 : अनुप्रयुक्त मठ और मशीन लर्निंग बेसिक्स 2 .</s>
रेखीय बीजगणित 3 .</s>
प्रायिकता और सूचना सिद्धांत 4 .</s>
संख्यात्मक गणना</s>
मशीन लर्निंग बेसिक्स पार्ट - 2 :</s>
डीप नेटवर्कः आधुनिक प्रथाएं 6 .</s>
डीप फीड काफी बड़ा नेटवर्क 7 .</s>
रेग्युलराइजेशन 8 .</s>
अनुकूलन 9 .</s>
सीएनएन आईबीएन</s>
आरएनएस 11</s>
व्यावहारिक पद्धति 12 .</s>
अनुप्रयोग भाग 3 :</s>
डीप लर्निंग रिसर्च</s>
रेखीय कारक मॉडल 14 .</s>
ऑटोनोकोडर्स 15</s>
प्रतिनिधित्व सीखना 16 .</s>
संरचनात्मक मॉडल 17 .</s>
मोंटे कार्लो विधियों 18</s>
पार्टीशन फंक्शन 19 .</s>
अनुमान</s>
डीप जेनरेटिव मॉडल चित्र 16 : पुस्तक का उच्चस्तर का संगठन ।</s>
एक अध्याय से दूसरे अध्याय तक के तीर से यह संकेत मिलता है कि पूर्व अध्याय को समझने के लिए पहले की सामग्री है ।</s>
11 अध्याय 1</s>
अध्यायों को पढ़ने के लिए , संख्या 1 . 6 एक फ्लोचार्ट प्रदान करता है जो पुस्तक के उच्च स्तर के संगठन को दर्शाता है ।</s>
हम यह जरूर मान लेते हैं कि सभी पाठक कंप्यूटर विज्ञान की पृष्ठभूमि से आते हैं ।</s>
हम प्रोग्रामिंग , कम्प्यूटेशनल प्रदर्शन मुद्दों की एक बुनियादी समझ , जटिलता सिद्धांत , परिचयात्मक स्तर कलन और ग्राफ सिद्धांत की कुछ शब्दावली के साथ परिचित हो लेते हैं ।</s>
1 . 2 डीप लर्निंग में ऐतिहासिक रुझान</s>
गहन अध्ययन को किसी ऐतिहासिक संदर्भ के साथ समझना सबसे आसान है ।</s>
डीप लर्निंग का विस्तृत इतिहास उपलब्ध कराने के बजाय , हम कुछ प्रमुख प्रवृत्तियों की पहचान करते हैंः</s>
गहन अध्ययन का एक लंबा और समृद्ध इतिहास रहा है , लेकिन कई नामों से चला गया है , विभिन्न दार्शनिक दृष्टिकोणों को प्रतिबिंबित , और लोकप्रियता में वृद्धि हुई है और छूट गया है .</s>
• गहन शिक्षण अधिक उपयोगी हो गया है के रूप में उपलब्ध प्रशिक्षण डेटा की राशि में वृद्धि हुई है .</s>
• गहन शिक्षण मॉडल समय के साथ आकार में वृद्धि हुई है के रूप में कंप्यूटर बुनियादी ढांचे ( गहन शिक्षण के लिए दोनोंथ हार्डवेयर और सॉफ्टवेयर में सुधार हुआ है .</s>
• गहन शिक्षण समय के साथ बढ़ती सटीकता के साथ बढ़ती जटिल अनुप्रयोगों को हल किया है .</s>
1 . 2 . 1</s>
तंत्रिका जाल के कई नाम और परिवर्तनकारी किले हैं</s>
हम उम्मीद करते हैं कि इस पुस्तक के कई पाठकों को एक रोमांचक नई प्रौद्योगिकी के रूप में गहरी सीखने के बारे में सुना है , और एक उभरते हुए क्षेत्र के बारे में एक पुस्तक में “स्वास्थ्य” का उल्लेख देख आश्चर्य है .</s>
वास्तव में गहराई से सीखने की शुरुआत 1940 के दशक से हुई है ।</s>
डीप लर्निंग केवल नया लगता है , क्योंकि यह अपनी वर्तमान लोकप्रियता से पहले कई वर्षों के लिए अपेक्षाकृत अलोकप्रिय था , और क्योंकि यह कई अलग अलग नामों से चला गया है , केवल हाल ही में “दीप लर्निंग” कहा जाता है .</s>
इस क्षेत्र का कई बार पुनर्निर्माण किया गया है , जो विभिन्न अनुसंधानकर्ताओं और विभिन्न परिप्रेक्ष्यों के प्रभाव को प्रतिबिंबित करता है ।</s>
गहन अध्ययन का एक व्यापक इतिहास इस पाठ्यपुस्तक के दायरे से बाहर है ।</s>
कुछ बुनियादी संदर्भ , तथापि , गहरी सीखने को समझने के लिए उपयोगी है .</s>
मोटे तौर पर विकास की तीन लहरें उठी हैं :</s>
1940 -1960 के दशक में साइबरनेटिक्स के रूप में जाना जाने वाला गहन ज्ञान , 12 अध्याय 1 में कनेक्शनवाद के रूप में जाना जाता है ।</s>
स्थापना १९५० १९६० १९८० १९९० २००० वर्ष</s>
0 . 000000 0 . 000050 0 .000100</s>
0 . 000200 0 . 000250 शब्द या वाक्यांश की आवृत्ति (कनेक्टिज्म + न्यूरल नेटवर्क )</s>
आंकड़े 1 . 7</s>
गूगल बुक्स के अनुसार कृत्रिम तंत्रिका जाल अनुसंधान की तीन ऐतिहासिक तरंगों में से दो , जो वाक्यांशों “सर्वनेटिक्स” और “नैतिकता” या “नैतिकता” या “नैतिक नेटवर्क” की आवृत्ति से मापी जाती है ( - तीसरी तरंग अभी हाल ही में प्रकट होने वाली है ) ।</s>
1940 के दशक में अपने निर्णयों के साथ - साथ , विकास के सिद्धांतों के साथ - साथ , पहले साइबर वेव , सन 1943 से शुरू , सन 1949 के मॉडल , एकल हेब , सन 1949 के मॉडल और क्रियान्वयन के ऐसे ही न्युएप्टनिंग द न्यूरोन लर्निंग</s>
दूसरी लहर 1980 - 1995 काल के संबंधवादी दृष्टिकोण से शुरू हुई , जिसका बैक - प्रचार ( Rumelhart एट अल , 1986 ) के साथ एक या दो प्रच्छन्न परतों वाले तंत्रिका नेटवर्क को प्रशिक्षित करने के लिए ।</s>
मौजूदा तरंग , गहरे शिक्षण की 2006 के आसपास (Hin एट अल , 2006 ) और तीसरे शुरू बेन्जियो एट</s>
अल - क़ायदा</s>
और अभी पुस्तक रूप में 2016 के रूप में प्रकट हो रहा है .</s>
इसी प्रकार की अन्य दो तरंगें भी उसी प्रकार की वैज्ञानिक गतिविधि से बहुत बाद में पुस्तकाकार रूप में प्रकट हुईं ।</s>
1980 - 1990 दशक और 2006 में डीप लर्निंग के नाम से वर्तमान पुनरुत्थान</s>
यह मात्रात्मक रूप से आंकड़ा 1 . 7 में दर्शाया गया है ।</s>
आज हम जिन प्राचीनतम शिक्षण एल्गोरिदमों को पहचान रहे हैं उनमें से कुछ का उद्देश्य जैविक अधिगम का कम्प्यूटेशनल माडल बनना था , अर्थात यह कि अधिगम कैसे होता है या मस्तिष्क में हो सकता है ।</s>
एक परिणाम के रूप में , एक नाम है कि गहरी सीखने के द्वारा चला गया है कृत्रिम तंत्रिका नेटवर्क ( एनएनएस )</s>
गहन अध्ययन मॉडलों पर तदनुरूप परिप्रेक्ष्य यह है कि वे जैविक मस्तिष्क ( चाहे वह मानव मस्तिष्क या किसी अन्य पशु के मस्तिष्क ) से प्रेरित प्रणालियों के इंजीनियर हैं ।</s>
हालांकि मशीन सीखने के लिए इस्तेमाल किया तंत्रिक नेटवर्क के प्रकार कभी कभी मस्तिष्क समारोह ( हाइटन और शैलिस , 1991 ) को समझने के लिए इस्तेमाल किया गया है , वे आम तौर पर जैविक समारोह के यथार्थवादी मॉडल नहीं बनाया गया है .</s>
गहन अध्ययन पर तंत्रिका दृष्टिकोण दो मुख्य विचारों से प्रेरित है ।</s>
एक विचार है कि मस्तिष्क उदाहरण द्वारा एक सबूत प्रदान करता है कि बुद्धिमान व्यवहार संभव है , और बुद्धि के निर्माण के लिए एक वैचारिक रूप से सीधा रास्ता है मस्तिष्क के पीछे कम्प्यूटेशनल सिद्धांतों को रिवर्स करने के लिए और उसकी कार्यक्षमता की नकल .</s>
एक और 13 अध्याय 1 .</s>
यथार्थ परिप्रेक्ष्य यह है कि मस्तिष्क और मानव बुद्धि के आधारभूत सिद्धांतों को समझना अत्यंत रोचक होगा , अतः इन मूलभूत वैज्ञानिक प्रश्नों पर प्रकाश डालने वाले यंत्र शिक्षण मॉडल इंजीनियरी अनुप्रयोगों के समाधान की अपनी क्षमता के अतिरिक्त उपयोगी होंगे ।</s>
आधुनिक शब्द “दीप अधिगम” मशीन अधिगम मॉडल की वर्तमान नस्ल पर तंत्रिका विज्ञानी परिप्रेक्ष्य से परे है .</s>
यह रचना के एकाधिक स्तरों को सीखने के एक अधिक सामान्य सिद्धांत के लिए अपील करता है , जो मशीन शिक्षण ढांचे में लागू किया जा सकता है जो आवश्यक रूप से तंत्रिका प्रेरित नहीं हैं .</s>
आधुनिक गहन अध्ययन के प्रारंभिक पूर्ववर्तियों एक तंत्रिका वैज्ञानिक परिप्रेक्ष्य से प्रेरित सरल रैखिक मॉडल थे .</s>
इन मॉडलों n इनपुट मूल्यों x 1 , 090 का एक सेट लेने के लिए डिज़ाइन किया गया था .</s>
1 . 737 . 840 शब्द 1 . 840 . 844 उच्चारण 299भाषा</s>
एक्स एन और उन्हें एक आउटपुट वाई के साथ एसोसिएट ।</s>
इन मॉडलों 1 , 099 पर वजन का एक सेट सीखना होगा .</s>
1 . 737 . 840 शब्द 1 . 840 . 844 उच्चारण 299भाषा</s>
, w n और अपने आउटपुट एफ की गणना ( x , w ) =</s>
</s>
+ ···· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·</s>
w . n .</s>
तंत्रिका नेटवर्क अनुसंधान की इस पहली लहर को साइबरनेटिक्स के रूप में जाना जाता था , जैसा कि आंकड़ा 1 . 7 में सचित्र ।</s>
मैकॉलोच - पिट्स न्यूरॉन ( मैकक्लोच और पिट्स , 1943 ) मस्तिष्क समारोह का एक प्रारंभिक मॉडल था .</s>
यह रैखिक मॉडल इनपुट की दो अलग - अलग श्रेणियों को पहचान सकता है कि एफ ( एक्स , डब्ल्यू ) धनात्मक है या ऋणात्मक ।</s>
बेशक , श्रेणियों की वांछित परिभाषा के अनुरूप करने के लिए मॉडल के लिए , सही ढंग से सेट करने के लिए आवश्यक वजन .</s>
इन भारों को मानव प्रचालक निर्धारित कर सकता था ।</s>
1950 के दशक में पेसेप्टॉन ( Rosenblogt , 1958 , 1962 ) पहला मॉडल बन गया जो प्रत्येक श्रेणी से इनपुट के उदाहरण दिए गए वर्गों को परिभाषित करने वाले भार को सीख सका .</s>
अनुकूली रैखिक तत्व ( ADALINE ) , जो लगभग एक ही समय से तिथियाँ , बस एफ का मूल्य लौटाया</s>
( एक्स ) स्वयं एक वास्तविक संख्या की भविष्यवाणी करने के लिए ( widro और Hoff , 1960 ) और डेटा से इन संख्याओं की भविष्यवाणी करना भी सीख सकता है .</s>
इन सरल शिक्षण एल्गोरिदम ने मा - चाइन लर्निंग के आधुनिक परिदृश्य को बहुत प्रभावित किया ।</s>
एडीएएलईएन के भार को अनुकूलित करने के लिए प्रयोग किया जाने वाला प्रशिक्षण एल्गोरिथ्म</s>
एक एल्गोरिथ्म का एक विशेष मामला था जिसे स्टोचिस्टिक प्रवणता अवरोहण कहा जाता है ।</s>
स्टोचेस्टिक ढाल अवरोहण एल्गोरिथ्म के सरल रूप से संशोधित संस्करण आज गहन अधिगम मॉडलों के लिए प्रमुख प्रशिक्षण एल्गोरिदम बने हुए हैं ।</s>
एफ ( x , w ) पर आधारित मॉडल , जिनका प्रयोग परसेप्टॉन और एडीएएलआईएन द्वारा किया जाता है , रैखिक मॉडल कहलाते हैं ।</s>
इन मॉडलों में कुछ सर्वाधिक व्यापक रूप से प्रयोग किए जाने वाले मशीन लर्निंग मॉडल बने रहते हैं , हालांकि कई मामलों में उन्हें मूल मॉडलों से अलग - अलग तरीकों से प्रशिक्षित किया जाता है .</s>
रेखीय मॉडल की कई सीमाएं हैं ।</s>
सबसे प्रसिद्ध , ध् को नहीं सीखते और न ही उस प्रकार के कार्य सीखते हैं ( जैसा वे कर सकते हैं ) , जहां वे कर सकते हैं ।</s>
= 1 और एफ (</s>
[1 , 0 ] Name</s>
= 1 , f (</s>
( 1 ) , 1 )</s>
= = च ( ; ) ; ( ० ) तथा ० ( ० ) ।</s>
जिन आलोचकों ने रैखिक मॉडलों में इन खामियों का पालन किया , उन्होंने सामान्य रूप से जैविक प्रेरित शिक्षण के विरुद्ध एक पृष्ठभूमि तैयार की ( एमिन्सकी एंड पेपर्ट , 1969 ) .</s>
यह तंत्रिक नेटवर्क की लोकप्रियता की पहली बड़ी डुबकी थी ।</s>
14 अध्याय 1</s>
सूचना आज , तंत्रिका विज्ञान को गहन अध्ययन अनुसंधानकर्ताओं के लिए प्रेरणा का महत्वपूर्ण स्रोत माना जाता है , लेकिन अब यह क्षेत्र का प्रमुख मार्गदर्शक नहीं है ।</s>
आज गहन अध्ययन अनुसंधान में तंत्रिका विज्ञान की घटती भूमिका का मुख्य कारण यह है कि हम बस एक गाइड के रूप में इसका उपयोग करने के लिए मस्तिष्क के बारे में पर्याप्त जानकारी नहीं है .</s>
मस्तिष्क द्वारा प्रयुक्त वास्तविक एल्गोरिदम की गहरी समझ प्राप्त करने के लिए , हमें एक साथ ( कम से कम हजारों परस्पर जुड़े हुए न्यूरॉन्स की गतिविधि की निगरानी करने में सक्षम होना होगा .</s>
क्योंकि हम ऐसा नहीं कर पा रहे हैं , हम मस्तिष्क के कुछ अत्यंत सरल और सुशिक्षित भागों को भी नहीं समझ पा रहे हैं ( ओलाशसेन एंड फील्ड , 2005 ) ।</s>
तंत्रिका विज्ञान ने हमें आशा करने का एक कारण दिया है कि एक एकल गहन शिक्षण एल्गोरिथ्म कई विभिन्न कार्यों को हल कर सकता है ।</s>
तंत्रिका विज्ञानियों ने पाया है कि अगर उनके मस्तिष्क को दृश्य संकेतों को उस क्षेत्र में भेजने के लिए प्रेरित किया जाता है तो फेरेट्स अपने मस्तिष्क के श्रवण संसाधन क्षेत्र के साथ “सी” सीख सकते हैं ( वॉन मेलर एट अल , 2000 ) ।</s>
यह सुझाव है कि स्तनधारी मस्तिष्क के बहुत सारे विभिन्न कार्यों है कि मस्तिष्क को हल करता है के अधिकांश को हल करने के लिए एक एकल एल्गोरिथ्म का उपयोग कर सकता है .</s>
इस परिकल्पना से पहले , मशीन शिक्षण अनुसंधान अधिक खंडित था , प्राकृतिक भाषा संसाधन , दृष्टि , गति नियोजन और भाषण मान्यता का अध्ययन करने वाले शोधकर्ताओं के विभिन्न समुदायों के साथ .</s>
आज , इन आवेदन समुदायों अभी भी अलग हैं , लेकिन गहरी सीखने अनुसंधान समूहों के लिए कई या यहाँ तक कि इन सभी आवेदन क्षेत्रों का एक साथ अध्ययन करने के लिए आम है .</s>
हम तंत्रिका विज्ञान से कुछ रूखे दिशा - निर्देश निकालने में सक्षम हैं ।</s>
कई कम्प्यूटेशनल इकाइयों के होने का मूल विचार जो केवल एक दूसरे के साथ उनकी बातचीत के माध्यम से बुद्धिमान बन जाते हैं , मस्तिष्क से प्रेरित होता है .</s>
निओकोगनिट्रॉन</s>
अधिकांश तंत्रिका नेटवर्क आज एक मॉडल न्यूरॉन पर आधारित हैं जिसे संशोधित रैखिक इकाई कहा जाता है ।</s>
मूल कॉग्निट्रान ( Fukushima ) , 1975 एक और अधिक जटिल संस्करण शुरू किया जो मस्तिष्क समारोह के हमारे ज्ञान से अत्यधिक प्रेरित था .</s>
सरलीकृत आधुनिक संस्करण को कई दृष्टिकोणों से विकसित किया गया जिसमें नायर और हिंटन (2010 और ग्लोरोट एट अल ) शामिल हैं ।</s>
(2011 ) तंत्रिका विज्ञान को एक प्रभाव के रूप में , और जर्रेट एट अल का हवाला देते हुए</s>
(2009 ) अधिक अभियांत्रिकी उन्मुख प्रभावों का हवाला देते हुए .</s>
जबकि तंत्रिका विज्ञान प्रेरणा का एक महत्वपूर्ण स्रोत है , यह एक कठोर गाइड के रूप में लेने की जरूरत नहीं है .</s>
हम जानते हैं कि वास्तविक न्यूरॉन्स आधुनिक संशोधित रैखिक इकाइयों की तुलना में बहुत अलग कार्यों की परिकलन करते हैं , लेकिन अधिक से अधिक तंत्रिका यथार्थवाद के कारण अभी तक मशीन शिक्षण प्रदर्शन में सुधार नहीं हुआ है ।</s>
इसके अलावा , जबकि तंत्रिका विज्ञान सफलतापूर्वक कई तंत्रिका नेटवर्क वास्तुकला को प्रेरित किया है , हम अभी तक तंत्रिका विज्ञान के लिए जैविक शिक्षण के बारे में पर्याप्त पता नहीं है सीखने एल्गोरिदम हम इन वास्तुकला का प्रशिक्षण करने के लिए उपयोग करते हैं के लिए मार्गदर्शन की पेशकश करने के लिए .</s>
15 अध्याय 1</s>
INTRODUSE मीडिया खातों अक्सर मस्तिष्क के लिए गहरी सीखने की समानता पर जोर देता है .</s>
जबकि यह सच है कि गहन अध्ययन के शोधकर्ता मस्तिष्क को अन्य मशीन लर्निंग क्षेत्रों जैसे कर्नेल मशीन या बेयसी सांख्यिकी में काम करने वाले अनुसंधानकर्ताओं की तुलना में अधिक प्रभाव के रूप में उद्धृत कर सकते हैं , हमें गहन अध्ययन को मस्तिष्क के अनुकरण के प्रयास के रूप में नहीं देखना चाहिए ।</s>
आधुनिक गहन अध्ययन कई क्षेत्रों , विशेष रूप से लागू गणित बुनियादी जैसे रैखिक बीजगणित , संभावना , सूचना सिद्धांत , और संख्यात्मक अनुकूलन से प्रेरणा लेता है .</s>
जबकि कुछ गहन अध्ययन शोधकर्ताओं तंत्रिका विज्ञान को प्रेरणा के एक महत्वपूर्ण स्रोत के रूप में उद्धृत करते हैं , दूसरों को तंत्रिका विज्ञान के साथ कतई संबंध नहीं है .</s>
गौरतलब है कि मस्तिष्क एल्गोरिथम के स्तर पर किस तरह काम करता है , इसे समझने का प्रयास जीवंत और अच्छी तरह से किया जाता है ।</s>
इस प्रयास को मुख्य रूप से “संगणक तंत्रिका विज्ञान” के रूप में जाना जाता है और यह गहन अध्ययन से अध्ययन का एक अलग क्षेत्र है ।</s>
शोधकर्ताओं के लिए यह आम है कि वे दोनों क्षेत्रों के बीच आगे - पीछे घूमें ।</s>
गहन अध्ययन का क्षेत्र मुख्य रूप से कंप्यूटर सिस्टम का निर्माण करने से संबंधित है जो बुद्धि की आवश्यकता वाले कार्यों को सफलतापूर्वक हल करने में सक्षम है , जबकि कम्प्यूटेशनल तंत्रिका विज्ञान का क्षेत्र मुख्य रूप से मस्तिष्क वास्तव में कैसे काम करता है के अधिक सटीक मॉडल के निर्माण से संबंधित है .</s>
1980 के दशक में , तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर कनेक्शनवाद , या समानांतर वितरित प्रक्रिया नामक आंदोलन के माध्यम से महान भाग में उभरा -</s>
</s>
मैक्क्ललैंड एट अल , 1995</s>
संज्ञानात्मक विज्ञान के संदर्भ में कनेक्शनवाद का उदय हुआ ।</s>
संज्ञानात्मक विज्ञान , मस्तिष्क को समझने के लिए एक अंतर्विषयक दृष्टिकोण है , जो विश्लेषण के कई विभिन्न स्तरों को संयोजित करता है .</s>
1980 के दशक के प्रारंभ में अधिकांश संज्ञानात्मक वैज्ञानिकों ने प्रतीकात्मक तर्क के मॉडलों का अध्ययन किया .</s>
उनकी लोकप्रियता के बावजूद , प्रतीकात्मक मॉडल कैसे मस्तिष्क वास्तव में न्यूरॉन्स का उपयोग कर उन्हें लागू कर सकते हैं के संदर्भ में व्याख्या करने के लिए मुश्किल थे .</s>
संबंधियों ने संज्ञान के ऐसे मॉडलों का अध्ययन करना शुरू किया जो वास्तव में तंत्रिका कार्यान्वयन में आधारित हो सकते थे ( ट्युरैट्स्की और मिंटन , 1985 ) , जो 1940 के दशक में मनोविज्ञानी डोनाल्ड हेब के काम में वापस आने वाले कई विचारों को पुनर्जीवित कर रहे थे .</s>
कनेक्शनवाद में केंद्रीय विचार है कि सरल कम्प्यूटेशनल इकाइयों की एक बड़ी संख्या एक साथ नेटवर्क करने पर बुद्धिमान व्यवहार प्राप्त कर सकते हैं .</s>
यह अंतर्दृष्टि जैविक तंत्रिका प्रणालियों में न्यूरॉन्स के लिए समान रूप से लागू होता है क्योंकि यह कम्प्यूटेशनल मॉडलों में छुपी इकाइयों के लिए करता है .</s>
1980 के दशक के संपर्क आंदोलन के दौरान कई प्रमुख अवधारणाएं उभरीं जो आज के गहन शिक्षण के लिए केंद्रीय बनी हुई हैं ।</s>
इनमें से एक अवधारणा वितरित प्रतिनिधित्व की है ( हाइनटन एट अल , 1986 ) .</s>
यह एक विचार है कि एक सिस्टम के प्रत्येक इनपुट को कई विशेषताओं द्वारा प्रतिनिधित्व किया जाना चाहिए , और प्रत्येक विशेषता को कई संभव आदानों के प्रतिनिधित्व में शामिल किया जाना चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हमारे पास एक दृष्टि प्रणाली है जो 16 अध्याय 1 को पहचान सकती है .</s>
INTRODUCTION कारों , ट्रकों , और पक्षियों , और इन वस्तुओं को प्रत्येक लाल , हरा , या नीला हो सकता है ।</s>
इन आदानों का प्रतिनिधित्व करने का एक तरीका है एक अलग न्यूरॉन या छिपा इकाई है कि नौ संभावित संयोजनों में से प्रत्येक के लिए सक्रिय करता है लाल ट्रक , लाल कार , लाल पक्षी , हरे ट्रक , और इतने पर .</s>
इसके लिए नौ विभिन्न न्यूरॉन्स की जरूरत है , और प्रत्येक न्यूरॉन स्वतंत्र रूप से रंग और वस्तु पहचान की अवधारणा सीखना होगा .</s>
इस स्थिति पर सुधार करने का एक तरीका है एक वितरित प्रतिनिधित्व का उपयोग करने के लिए , तीन न्यूरॉन्स के साथ रंग और तीन न्यूरॉन्स ऑब्जेक्ट पहचान का वर्णन करते हैं .</s>
इसके लिए नौ के स्थान पर केवल छह न्यूरॉन्स कुल की आवश्यकता होती है , और लालिमा का वर्णन करने वाला न्यूरॉन कारों , ट्रकों और पक्षियों की छवियों से लालिमा के बारे में सीखने में सक्षम है , न केवल एक विशिष्ट श्रेणी की वस्तुओं की छवियों से .</s>
वितरित प्रतिनिधित्व की अवधारणा इस पुस्तक के लिए केंद्रीय है और अध्याय 15 में अधिक विस्तार से वर्णित है .</s>
कनेक्शनवादी आंदोलन की एक और बड़ी उपलब्धि थी गहरे तंत्रिका नेटवर्क को प्रशिक्षित करने के लिए बैक - प्रोपगेशन का सफल प्रयोग जिसमें आंतरिक पुनर्विनियोजन और बैक - प्रोपगेशन एल्गोरिदम ( रॉमेलहार्ट , १९८७ , १९८६ ई० , १९८६ ई० , ए० , ए० , ए० ली० )</s>
इस एल्गोरिथ्म ने लोकप्रियता में वृद्धि और कमी की है , लेकिन इस लेखन के रूप में , गहरे मॉडलों के प्रशिक्षण का प्रमुख दृष्टिकोण है ।</s>
1990 के दशक के दौरान , शोधकर्ताओं ने तंत्रिका नेटवर्क के साथ मॉडलिंग अनुक्रम में महत्वपूर्ण प्रगति की .</s>
होक्रिटर ( १९९१ ) और बेंजियो एट अल .</s>
(1994 ) ने लंबे अनुक्रम के मॉडलिंग में कुछ मूलभूत गणितीय कठिनाइयों की पहचान की , जिनका वर्णन अनुभाग 10 .7 में किया गया है ।</s>
होफ्रीटर और श्मुबर ( १९९७ ) ने इनमें से कुछ कठिनाइयों को दूर करने के लिए दीर्घकालीन अल्पकालिक स्मृति तंत्र का सूत्रपात किया ।</s>
आज , एलएसटीएम व्यापक रूप से कई अनुक्रम मॉडलिंग कार्यों के लिए प्रयोग किया जाता है , गूगल में कई प्राकृतिक भाषा प्रसंस्करण कार्यों सहित .</s>
तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर मध्य १९९० दशक तक चली ।</s>
तंत्रिका नेटवर्क तथा अन्य एआई प्रौद्योगिकियों पर आधारित वेन - ट्यून्स ने निवेश की मांग करते समय अवास्तविक तथा अति महत्वाकांक्षी दावे करने शुरू कर दिए ।</s>
जब एआई अनुसंधान इन अनुचित अपेक्षाओं को पूरा नहीं किया , निवेशकों निराश थे .</s>
इसके साथ ही मशीनी शिक्षा के अन्य क्षेत्रों में भी प्रगति हुई ।</s>
केन् द्रीय स् त्री मशीनें ( १९९८ )</s>
इन दो कारणों से तंत्रिका नेटवर्क की लोकप्रियता में गिरावट आई जो 2007 तक चली ।</s>
इस दौरान , तंत्रिका नेटवर्क कुछ कार्यों पर प्रभावशाली प्रदर्शन प्राप्त करते रहे (</s>
लेकुन एट अल</s>
।</s>
कनाडा के उन्नत अनुसंधान संस्थान ( सीआईएफएआर ) ने अपने तंत्रिका संगणन और अनुकूलन अनुसंधान पहल के माध्यम से तंत्रिका नेटवर्कों के अनुसंधान को जीवित रखने में मदद की ।</s>
इस कार्यक्रम के टोरंटो विश्वविद्यालय के जिओफ्री हाइन्टन के नेतृत्व में मशीन शिक्षण अनुसंधान समूहों , मोंट्रियल विश्वविद्यालय में योशिआ बेंजियो , और न्यूयॉर्क विश्वविद्यालय में यान लेकन</s>
एनसीएपी अनुसंधान पहल 17 अध्याय 1</s>
सूचना में मानव और कंप्यूटर दृष्टि के तंत्रिका विज्ञानी और विशेषज्ञ भी शामिल थे ।</s>
इस बिंदु पर , डीप नेटवर्क आम तौर पर माना जाता था कि प्रशिक्षण के लिए बहुत कठिन है .</s>
अब हम जानते हैं कि एल्गोरिदम कि 1980 के दशक के बाद से अस्तित्व में है काफी अच्छी तरह से काम करते हैं , लेकिन यह स्पष्ट रूप से circa 2006 नहीं था .</s>
मुद्दा शायद यह है कि ये एल्गोरिदम उस समय उपलब्ध हार्डवेयर के साथ बहुत अधिक प्रयोग करने की अनुमति देने के लिए बहुत अधिक कम्प्यूटेशनल रूप से महंगे थे .</s>
तंत्रिका नेटवर्क अनुसंधान की तीसरी लहर 2006 में एक तोड़ के साथ शुरू हुई ।</s>
जिओफ्री हाइटन ने दिखाया कि गहन विश्वास नेटवर्क नामक एक प्रकार के तंत्रिका नेटवर्क को लालची पर्तवार पूर्वप्रदर्शन ( हाइनटन एट अल , 2006 ) नामक रणनीति का उपयोग करके कुशलतापूर्वक प्रशिक्षित किया जा सकता है , जिसका वर्णन हम अनुभाग 151 में अधिक विस्तार से करते हैं ।</s>
अन्य सफ्फित समूहों ने शीघ्रता से वही कार्यनीति दिखाई कि अनुसंधान समूहों को अन्य प्रकारों के परीक्षण हेतु अन्य प्रकारों के परीक्षण हेतु प्रशिक्षित करने के लिए वही कार्यनीति अपनाई जा सकती है ।</s>
तंत्रिका नेटवर्क अनुसंधान की इस लहर ने “दीप अधिगम” शब्द के प्रयोग को लोकप्रिय बनाया ताकि शोधकर्ता अब गहरे तंत्रिका नेटवर्क को प्रशिक्षित करने में सक्षम हो सके जो इससे पहले संभव था , और इसके लिए महत्व की ओर ध्यान दिया गया था , जो सन 2011 की गहराई ( बैंगियोट ) , एल्क्यो फोकस , एल्क्युएल 2014</s>
इस समय , गहन तंत्रिका नेटवर्क अन्य मशीन शिक्षण प्रौद्योगिकियों और साथ ही हाथ से वर्णित कार्यक्षमता के आधार पर एआई प्रणालियों को बाहर की ओर संकेत दिया .</s>
तंत्रिक नेटवर्क की लोकप्रियता की यह तीसरी लहर इस लेखन के समय तक जारी है , हालांकि इस लहर के समय के भीतर गहरे शिक्षण अनुसंधान का ध्यान नाटकीय रूप से बदल गया है .</s>
तीसरी लहर की शुरुआत नए अप्रशिक्षित अधिगम तकनीकों और छोटे डेटासेटों से अच्छा सामान्य बनाने के गहरे मॉडलों की क्षमता पर ध्यान केंद्रित करने के साथ हुई , लेकिन आज काफी पुराने पर्यवेक्षित अधिगम एल्गोरिदमों और बड़े लेबल वाले डेटासेटों की क्षमता में गहरी मॉडलों की रुचि है ।</s>
1 . 2 . 2</s>
डेटासेट आकार बढ़ाना</s>
आश्चर्य हो सकता है कि हाल ही में गहन अध्ययन को एक महत्वपूर्ण प्रौद्योगिकी के रूप में मान्यता क्यों दी गई है , हालांकि कृत्रिम तंत्रिका नेटवर्कों के साथ प्रथम प्रयोग 1950 के दशक में किए गए थे ।</s>
गहन शिक्षण वाणिज्यिक अनुप्रयोगों में 1990 के दशक के बाद से सफलतापूर्वक इस्तेमाल किया गया है , लेकिन अक्सर एक प्रौद्योगिकी और कुछ है कि केवल एक विशेषज्ञ का उपयोग कर सकता है की तुलना में एक कला के अधिक माना जाता था , हाल ही में जब तक .</s>
यह सच है कि एक गहरी सीखने एल्गोरिथ्म से अच्छा प्रदर्शन प्राप्त करने के लिए कुछ कौशल की आवश्यकता है .</s>
सौभाग्य से , आवश्यक कौशल की मात्रा कम हो जाती है क्योंकि प्रशिक्षण डेटा की मात्रा बढ़ जाती है .</s>
आज जटिल कार्यों पर मानव प्रदर्शन तक पहुंचने वाले शिक्षण एल्गोरिदम लगभग सीखने एल्गोरिदम है कि 1980 के दशक में खिलौना समस्याओं को हल करने के लिए संघर्ष किया , हालांकि मॉडल हम इन एल्गोरिदम के साथ प्रशिक्षण 18 अध्याय 1</s>
सूचना में ऐसे परिवर्तन हुए जो बहुत गहरी वास्तुकला के प्रशिक्षण को सरल बनाते हैं ।</s>
सबसे महत्वपूर्ण नया विकास है कि आज हम इन एल्गोरिदम संसाधनों के साथ वे सफल होने की जरूरत है प्रदान कर सकते हैं .</s>
चित्र 1 . 8 दिखाता है कि कैसे बेंचमार्क डाटासेट के आकार का समय के साथ उल्लेखनीय विस्तार हुआ है ।</s>
यह प्रवृत्ति समाज के बढ़ते डिजिटाइजेशन से प्रेरित है ।</s>
जैसे - जैसे हमारी अधिक से अधिक गतिविधियां कंप्यूटर पर होती हैं , अधिक से अधिक हम जो कुछ करते हैं वह दर्ज हो जाता है ।</s>
चूंकि हमारे कंप्यूटरों का एक साथ नेटवर्क बढ़ता जा रहा है , इसलिए इन अभिलेखों को केंद्रीकृत करना और उन्हें मशीन शिक्षण अनुप्रयोगों के लिए उपयुक्त आंकड़ा - सेट में ठीक करना आसान हो जाता है ।</s>
“बिग डेटा” 1900 1950 2000 2015 वर्ष 10 0 10 1 10 2 10 10 3 10 4 10 10 10 10 10 6 7 8 9 डेटासेट आकार ( असंख्य उदाहरण</s>
आईरिस मेलनसूची</s>
सार्वजनिक SVHN छविNet</s>
सीआईएफएआर - 10</s>
इमेजनेट10क आईएलएसवीआरसी 2014 स्पोर्ट्स - 1</s>
</s>
समय के साथ डेटासेट आकार में वृद्धि .</s>
1900 मापदण्डों का उपयोग करते हुए 1900 मापदण्डों का अध्ययन किया ।</s>
के दशक के दौरान , जैविक प्रेरित मशीन शिक्षण के अग्रदूतों अक्सर छोटे सिंथेटिक डेटासेट के साथ काम किया , जैसे कि कम संगणनात्मक बिटमैप , कम गणना लागत उठाने के लिए डिज़ाइन किया गया था और यह प्रदर्शित किया गया था कि न्यूरल नेटवर्क विशेष प्रकार के कार्य सीखने में सक्षम थे .</s>
के दशक में मशीनी शिक्षा अधिक सांख्यिकीय हो गई और इसने दसियों हजार उदाहरणों वाले बड़े डेटासेटों का लाभ उठाना शुरू कर दिया , जैसे कि MNIST डेटासेट ( चिह्न 19 में हस्तलिखित संख्याओं के स्कैन (LeCunaet al ) .</s>
2000 के दशक के पहले दशक में , इसी आकार के अधिक परिष्कृत डेटासेट , जैसे कि CIFAR - 10 डेटासेट ( Krizhevsky और Hinton , 2009 ) का उत्पादन जारी रहा .</s>
उस दशक के अंत में और 2010 के दशक के पूर्वार्ध में , उल्लेखनीय रूप से बड़े डेटासेट , जिनमें सैकड़ों हजारों से दसियों लाख उदाहरण हैं , पूरी तरह से बदल दिया कि गहन अध्ययन के साथ क्या संभव था .</s>
इन आँकडों में स्ट्रीट व्यू हाउस नम्बर पोर्टेट (Nzer aet al . al . al . )</s>
एम डाटासेट</s>
केरल</s>
ग्राफ के शीर्ष पर , हम देखते हैं कि अनुदित वाक्यों की डेटासेट , जैसे कि कनाडा के हंसार्ड ( ब्रान एट अल , 1990 ) से निर्मित आईबीएम के डेटासेट और डब्ल्यूएमटी 2014 इंग्लिश से फ्रेंच डेटासेट ( Schwenkset 2014 ) , अन्य डेटा से विशेष रूप से बहुत आगे हैं</s>
19 अध्याय 1</s>
परिचय 1 . 9</s>
MNIST डाटासेट से उदाहरण इनपुट</s>
“नIST” राष्ट्रीय मानक और प्रौद्योगिकी संस्थान , एजेंसी है कि मूल रूप से इस डेटा एकत्र किया है के लिए खड़ा है .</s>
चूंकि मशीन लर्निंग एल्गोरिदम के साथ डेटा का उपयोग आसान करने के लिए पूर्वप्रकल्पित किया गया है , “M” “M” “मॉडिफाइड ,” के लिए खड़ा है .</s>
MNIST डाटासेट में हस्तलिखित अंकों तथा संबद्ध लेबलों के स्कैन होते हैं जो यह बताते हैं कि प्रत्येक छवि में 0 -9 किस अंक को समाहित करता है .</s>
इस सरल वर्गीकरण समस्या गहरा सीखने अनुसंधान में सबसे सरल और सबसे व्यापक रूप से इस्तेमाल परीक्षणों में से एक है .</s>
यह आधुनिक तकनीकों को हल करने के लिए काफी आसान होने के बावजूद लोकप्रिय रहता है .</s>
जिओफ्री हाइटन ने इसे “ मशीन लर्निंग का विकार” कहा है , जिसका अर्थ है कि यह मशीन लर्निंग शोधकर्ताओं को नियंत्रित प्रयोगशाला परिस्थितियों में उनके एल्गोरिदम का अध्ययन करने के लिए सक्षम बनाता है , हालांकि जीवविज्ञानी अक्सर फल मक्खियों का अध्ययन करते हैं ।</s>
मशीनी शिक्षा को अधिक आसान बना दिया है क्योंकि सांख्यिकीय आकलन का मुख्य बोझ - आंकड़ों की एक छोटी राशि का निरीक्षण करने के बाद अच्छी तरह से नए आंकड़ों का उत्पादन - काफी हल्का किया गया है ।</s>
वर्ष 2016 तक , अंगूठे का एक मोटा नियम यह है कि पर्यवेक्षित गहन शिक्षण एल्गोरिथ्म सामान्यतः प्रति वर्ग 5 , 000 लेबल वाले उदाहरणों के साथ स्वीकार्य प्रदर्शन प्राप्त करेगा और यह 20 अध्याय 1 से मेल खाता है ।</s>
सूचना मानव प्रदर्शन से अधिक है जब एक डेटासेट के साथ प्रशिक्षित किया गया है जिसमें कम से कम 10 मिलियन लेबल उदाहरण हैं .</s>
इससे छोटा डाटासेट के साथ सफलतापूर्वक कार्य करना एक महत्वपूर्ण अनुसंधान क्षेत्र है , जिसमें विशेष रूप से इस बात पर ध्यान दिया जाता है कि हम किस प्रकार बड़ी मात्रा में अविश्वसनीय उदाहरणों का लाभ उठा सकते हैं , जिसमें अप्रशिक्षित या अर्ध - प्रसुरक्षित अधिगम है ।</s>
1 . 2 . 3</s>
बढ़ते मॉडल आकार एक और प्रमुख कारण है कि तंत्रिका नेटवर्क आज 1980 के दशक के बाद से तुलनात्मक रूप से कम सफलता का आनंद लेने के बाद बेतहाशा सफल हो रहे हैं है कि हम बहुत बड़ा मॉडल आज चलाने के लिए कम्प्यूटेशनल संसाधन है .</s>
कनेक्शन - इस्म की एक मुख्य अंतर्दृष्टि है कि जानवर बुद्धिमान हो जाते हैं जब उनके कई न्यूरॉन्स एक साथ काम करते हैं .</s>
एक व्यक्ति न्यूरॉन या न्यूरॉन्स के छोटे संग्रह विशेष रूप से उपयोगी नहीं है .</s>
जैविक न्यूरॉन्स विशेष रूप से सघन रूप से जुड़े नहीं हैं .</s>
जैसा कि 1 . 10 अंक में देखा गया है , हमारे मशीनी सीखने के मॉडलों में दशकों से स्तनधारियों के मस्तिष्क के परिमाण के एक क्रम के भीतर प्रति न्यूरॉन कई कनेक्शन थे .</s>
न्यूरॉन्स की कुल संख्या के संदर्भ में , तंत्रिका नेटवर्क आश्चर्यजनक रूप से काफी हाल ही में , के रूप में आंकड़ा 1 . 11 में दिखाया गया है .</s>
छुपी इकाइयों के शुरू होने के बाद से , कृत्रिम तंत्रिका नेटवर्क का आकार मोटे तौर पर हर 2 . 4 वर्ष में दोगुना हो गया है ।</s>
यह वृद्धि बड़ी स्मृति वाले तेज कंप्यूटरों और बड़े डेटासेटों की उपलब्धता से प्रेरित है ।</s>
बड़े नेटवर्क अधिक जटिल कार्यों पर उच्च सटीकता प्राप्त करने में सक्षम होते हैं .</s>
ऐसा लगता है कि यह सिलसिला दशकों तक चलता रहेगा ।</s>
जब तक नई प्रौद्योगिकियां तेजी से स्केलिंग को सक्षम नहीं करतीं , तब तक कृत्रिम तंत्रिका नेटवर्कों में कम से कम 2050 के दशक तक न्यूरॉन्स की संख्या उतनी नहीं होगी ।</s>
जैविक न्यूरॉन्स वर्तमान कृत्रिम न्यूरॉन्स की तुलना में अधिक जटिल कार्यों का प्रतिनिधित्व कर सकते हैं , तो जैविक तंत्रिका नेटवर्क इस साजिश चित्रण से भी बड़ा हो सकता है .</s>
पुनर्निरीक्षण में , यह विशेष रूप से आश्चर्य की बात नहीं है कि जोंक की तुलना में कम न्यूरॉन वाले तंत्रिका नेटवर्क परिष्कृत कृत्रिम बुद्धिमत्ता प्रोब - लेम को हल करने में असमर्थ थे .</s>
आज के नेटवर्क , जिन्हें हम कम्प्यूटेशनल सिस्टम की दृष्टि से काफी बड़े मानते हैं , मेंढ़क जैसे आदिम कशेरुकी प्राणियों के तंत्रिका तंत्र से भी छोटे हैं ।</s>
समय के साथ मॉडल आकार में वृद्धि , तीव्र सीपीयू की उपलब्धता के कारण , सामान्य प्रयोजन जी पी यू ( खंड 12 . 1 . 2 में निहित ) , तीव्र नेटवर्क संयोजकता और वितरित कंप्यूटिंग के लिए बेहतर सॉफ्टवेयर मूल संरचना , गहन अधिगम के इतिहास की सबसे महत्वपूर्ण प्रवृत्तियों में से एक है ।</s>
यह प्रवृत्ति आमतौर पर भविष्य में भी अच्छी तरह से जारी रहने की उम्मीद है ।</s>
21 अध्याय 1</s>
सूचना 1950 सन 2000 2015 वर्ष 10 1 10 2 10 10 3 10 4 4 कनेक्शन प्रति न्यूरॉन 1 2 3 4 5 6 7 8 9 10 फल मक्खी माउस बिल्ली मानव चित्र 110</s>
: समय के साथ प्रति न्यूरॉन कनेक्शन की संख्या .</s>
प्रारंभ में , कृत्रिम तंत्रिका नेटवर्क में न्यूरॉन्स के बीच होने वाले मिलन की संख्या हार्डवेयर क्षमताओं द्वारा सीमित थी .</s>
आज , न्यूरॉन्स के बीच कनेक्शन की संख्या ज्यादातर एक डिजाइन विचार है .</s>
कुछ कृत्रिम तंत्रिका नेटवर्क एक बिल्ली के रूप में न्यूरॉन प्रति कई कनेक्शन के रूप में लगभग है , और यह अन्य तंत्रिका नेटवर्क के लिए काफी आम है कि चूहों जैसे छोटे स्तनधारियों के रूप में न्यूरॉन प्रति कई कनेक्शन है .</s>
मानव मस्तिष्क में भी प्रति न्यूरॉन कनेक्शन की अनाप - शनाप मात्रा नहीं होती है ।</s>
जैव तंत्रिका नेटवर्क का आकार विकिपीडिया से (2015 )</s>
1 .</s>
अनुकूली रैखिक तत्व (Widro and Hoff , 1960 2 .</s>
नियोकोजनित्र ( Fukushima )</s>
Name</s>
( chellapillaet al 2006 ) 4 .</s>
डीप बॉटसन मशीन (Salakhutdinov और Hinton , 2009 ) 5 .</s>
अनर्जक कनवल्युशनल नेटवर्क (</s>
ज्रेट एट अल , 2009 6 .</s>
जीपीयू - एसीर सेप्टॉन</s>
(Ciresan . kgm</s>
२०१०</s>
वितरित autoencoder (</s>
ली एट अल</s>
अगस् त 2012</s>
बहु - जीपीयू संवलनीय नेटवर्क (</s>
किरिज़ेव्स्की एत अल</s>
2012</s>
COTS HPC अपरिभाषित संवलित नेटवर्क (</s>
कोट्स एट अल , 2013 10 .</s>
गोग्लिनेट</s>
1 . 2 . 4</s>
बढ़ती परिशुद्धता , जटिलता और वास्तविक - वर्ल्ड प्रभाव</s>
1980 के दशक के बाद से , गहरी सीखने लगातार सटीक मान्यता और भविष्यवाणी प्रदान करने की क्षमता में सुधार किया है .</s>
इसके अलावा , गहरे सीखने लगातार सफलता के साथ आवेदन के व्यापक और व्यापक सेट करने के लिए लागू किया गया है .</s>
सबसे पहले गहरे मॉडल का उपयोग व्यक्तिगत वस्तुओं को कसकर फसल में पहचानने के लिए किया गया , अत्यंत छोटे छवियों (</s>
रेमुल्ट एट अल , 1986</s>
तब से छवियों के आकार में क्रमिक वृद्धि हुई है तंत्रिका नेटवर्क प्रक्रिया कर सकता है .</s>
आधुनिक वस्तु अभिज्ञान नेटवर्क समृद्ध उच्च समाधान फोटो प्रक्रिया और नहीं 22 अध्याय 1 .</s>
स्थापना १९५० सन १९८५ २००० वर्ष २५६ वर्ष १० - २ १० - १</s>
10 0 10 1 10 10 10 2 10 3 10 4 10 10 5 10 10 6 10 10 10 8 10 9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10</s>
न्यूरॉन्स की 10 11 संख्या</s>
1</s>
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 स्पंज गोलकृमि लीच चींटी 20 20 20 स्पंज ऑक्टोपस मानव चित्र 111 बढ़ती तंत्रिका नेटवर्क आकार समय के साथ</s>
छुपी इकाइयों के शुरू होने के बाद से , कृत्रिम तंत्रिका नेटवर्क का आकार मोटे तौर पर हर 2 . 4 वर्ष में दोगुना हो गया है ।</s>
जैव तंत्रिका नेटवर्क का आकार विकिपीडिया से (2015 )</s>
1 .</s>
पेर्सेफ्रॉन</s>
अनुकूली रैखिक तत्व (Widro and Hoff , 1960 3 .</s>
नियोकोजनित्र ( फुकुशिमा , 1980 4 .</s>
अर्ली बैक - प्रोपगेशन नेटवर्क ( रुमेलहार्ट एट अल , 1986बीडी 5 .</s>
भाषण पहचान के लिए पुनरावर्ती तंत्रिका नेटवर्क ( रॉबिन्सन एंड फाल्सड ) , 1991</s>
6 .</s>
भाषण पहचान के लिए मल्टीलेयर परसेप्टॉन ( Bengio एट अल )</s>
१९९१ - ७</s>
मीन फील्ड अवग्रह नेटवर्क ( सऊल एट अल . , 1996 )</s>
8 .</s>
लिनेट - ५</s>
( लेट - कुन एट अल )</s>
१९९८बी</s>
इको स्टेट नेटवर्क (जागर और हास , 2004 )</s>
१०</s>
गहरा विश्वास नेटवर्क</s>
Name</s>
(Chellapilla एट अल , 2006 12 .</s>
डीप बॉटसन मशीन</s>
जीपीयू - चेल्सीकृत गहरा विश्वास नेटवर्क (</s>
रैना एट अल</s>
१४ , २००९</s>
अनर्जक कनवल्युशनल नेटवर्क (</s>
ज्रेट एट अल , 2009 15 .</s>
जीपीयू - एसीर सेप्टॉन</s>
समाप्तप्रायः २०१०</s>
१६</s>
मिशन मोडैट्स एंड एनजी , 2011 17</s>
वितरित autoencoder (</s>
लेअई एट अल , 2012 18 .</s>
बहु - जीपीयू संवलनीय नेटवर्क (</s>
किरिज़ेव्स्की एत अल</s>
2012</s>
COTS HPC अपरिभाषित संवलित नेटवर्क (</s>
कोट्स एट अल , 2013 20 .</s>
गोग्लिनेत</s>
इसी प्रकार , प्रारंभिक नेटवर्क केवल दो प्रकार की वस्तुओं को पहचान सकता था ( कुछ मामलों में , एक ही प्रकार की वस्तु की अनुपस्थिति या उपस्थिति , जबकि ये आधुनिक नेटवर्क आमतौर पर वस्तुओं की कम से कम 1 , 000 विभिन्न श्रेणियों को पहचानते हैं .</s>
वस्तु पहचान में सबसे बड़ा मुकाबला छवि - सूत्र 23 अध्याय 1 है .</s>
परिचय</s>
प्रत्येक वर्ष बड़े पैमाने पर दृश्य अभिज्ञान चैलेंज आयोजित किया जाता है ।</s>
गहन अध्ययन के उल्कापिंड में नाटकीय क्षण आया जब एक संवलित नेटवर्क ने यह चुनौती पहली बार प्राप्त की और एक व्यापक मार्जिन से , राज्य - बाहर - 5 त्रुटि दर से 15 . 1 प्रतिशत )</s>
तब से , इन प्रतियोगिताओं को लगातार गहरे संवलित जालों द्वारा जीता जाता है , और इस लेखन के रूप में , गहन अध्ययन में अग्रिम इस प्रतियोगिता में नवीनतम शीर्ष - 5 त्रुटि दर को 36 प्रतिशत तक ले आया है , जैसा कि अंक 1 .12 में दर्शाया गया है .</s>
गहन अध्ययन का भाषण मान्यता पर भी नाटकीय प्रभाव पड़ा है ।</s>
1990 के दशक में सुधार के बाद , भाषण पहचान के लिए त्रुटि दरों में लगभग 2000 में शुरू कर दिया .</s>
< s > प्रस्तावना । < s > प्रस्तुतकर्ता । < s > 2010 बेशक कोई गलती दरों में गलती की कमी के कारण गलती दरों में गलती से गलती से गलती की दरों में गलती से गलती की कमी के कारण</s>
हम इस इतिहास को अधिक विस्तार से देखें अनुभाग 12 . 3 में ।</s>
डीप नेटवर्क ने पैदल यात्री पहचान और छवि खंडन के लिए भी शानदार सफलता हासिल की है ।</s>
अल - अल - 2013 अल - क़ुरिब - अल - क़ुरिब - असहाब - अल - क़ुरैश क़स्बे और क़स्बे - अल - क़ुरैश क़स्बे में</s>
सेरेसन एट अल , 2012</s>
इसी के साथ कि गहरे नेटवर्कों के पैमाने और सटीकता में वृद्धि हुई है , 2010 2011 2012 2013 2014 2015 वर्ष 000005 0 .10 015</s>
0 . 2025 0 .30 ILSVRC वर्गीकरण त्रुटि दर चित्र 1 . 12 : समय के साथ त्रुटि दर घटाना</s>
चूंकि डीप नेटवर्क इमेजनेट बड़े पैमाने पर दृश्य पहचान चैलेंज में प्रतिस्पर्धा करने के लिए आवश्यक पैमाने पर पहुँच गया , उन्होंने लगातार प्रति वर्ष प्रतियोगिता जीती है , जिससे हर बार कम और कम त्रुटि दर प्राप्त होती है .</s>
रसकोव्स्की वगैरह का डाटा</s>
पूरा</s>
और एट अल</s>
१९१५</s>
24 अध्याय 1</s>
INTRODUCTION में कार्य की जटिलता है जिसे वे हल कर सकते हैं ।</s>
गुडफेलो एट अल</s>
(2014 में पता चला कि तंत्रिका नेटवर्क किसी एक वस्तु की पहचान करने के बजाय , किसी छवि से प्रतिलेखन किए गए अक्षरों के एक पूरे अनुक्रम का आउटपुट सीख सकते हैं .</s>
पहले , यह व्यापक रूप से माना जाता था कि इस प्रकार के ज्ञान की आवश्यकता अनुक्रम के अलग - अलग तत्वों (Gülçhre और बेंजियो , 2013 ) की लेबलिंग है .</s>
पुनरावर्ती तंत्रिका नेटवर्क , जैसे कि ऊपर उल्लिखित LSTM अनुक्रम मॉडल , का प्रयोग अब केवल नियत इनपुट के बजाय अनुक्रम और अन्य अनुक्रमों के बीच संबंधों को मॉडल करने के लिए किया जाता है .</s>
यह अनुक्रम - से - अनुक्रम शिक्षण दूसरे अनुप्रयोग के क्रांतिकरण के आधार पर प्रतीत होता हैः मशीनी अनुवाद</s>
बीत्तनम</s>
बढ़ती जटिलता की इस प्रवृत्ति को तार्किक परिणति की ओर धकेल दिया गया है तंत्रिक ट्यूरिंग मशीन (Graves एट अल , 2014 ) जो स्मृति कोशिकाओं से पढ़ना और स्मृति कोशिकाओं के लिए मनमाना सामग्री लिखना सीखते हैं ।</s>
ऐसे तंत्रिक नेटवर्क वांछित व्यवहार के उदाहरणों से सरल प्रोग्राम सीख सकते हैं ।</s>
उदाहरण के लिए , वे स्केम्बल्ड और अनुक्रमों के उदाहरण दिए गए संख्याओं की सूचियों को छांटना सीख सकते हैं .</s>
यह स्व - प्रोग्रामन प्रौद्योगिकी अपनी शैशवावस्था में है , लेकिन भविष्य में इसे सिद्धांत रूप में लगभग किसी भी कार्य पर लागू किया जा सकता है ।</s>
गहन अध्ययन की एक अन्य महत्वपूर्ण उपलब्धि है इसका विस्तार , प्रवर्तन शिक्षण के क्षेत्र में है ।</s>
प्रवर्तन शिक्षण के संदर्भ में , एक स्वायत्त एजेंट को मानव प्रचालक से किसी मार्गदर्शन के बिना परीक्षण और त्रुटि द्वारा कार्य करना सीखना चाहिए ।</s>
दीपमण्ड ने यह प्रदर्शित किया कि गहन शिक्षण पर आधारित एक मजबूत शिक्षण प्रणाली , अतुल्य वीडियो गेम खेलने के लिए सीखने में सक्षम है , जो कई कार्यों पर मानव स्तर के प्रदर्शन तक पहुंचता है ।</s>
( Mnih एट अल , 2015 )</s>
डीप लर्निंग ने रोबोटिक्स (फिन एट अल , 2015 ) के लिए प्रवर्तन शिक्षण के प्रदर्शन में भी महत्वपूर्ण सुधार किया है ।</s>
गहन अध्ययन के इन अनुप्रयोगों में से कई अत्यधिक लाभदायक हैं .</s>
डीप लर्निंग अब गूगल , माइक्रोसॉफ्ट , फेसबुक , आईबीएम , बैदु , एप्पल , एडोब , नेटफ्लिक्स , एनवीआईए , और एनईसी सहित कई शीर्ष प्रौद्योगिकी कंपनियों द्वारा इस्तेमाल किया जाता है</s>
गहन अध्ययन में प्रगति सॉफ्टवेयर मूल संरचना में हुई प्रगति पर भी काफी निर्भर करती है ।</s>
साफ्टवेयर पुस्तकालय</s>
↑ 2013ेक , टोर्च ( कोलोबर्ट एट अल )</s>
</s>
टेन्सरफ्लो ( आबादी एत अल , 2015 ) सबने महत्वपूर्ण अनुसंधान परियोजनाओं या वाणिज्यिक उत्पादों का समर्थन किया है ।</s>
गहन अध्ययन ने अन्य विज्ञानों में भी योगदान दिया है ।</s>
ऑब्जैक्ट अभिज्ञान के लिए आधुनिक ऐक्शनल नेटवर्क दृश्य प्रक्रमण का एक मॉडल उपलब्ध कराते हैं जिसका अध्ययन तंत्रिका विज्ञानी कर सकते हैं (डी - कार्लो , 2013 ) ।</s>
गहन अध्ययन भारी मात्रा में डेटा के प्रसंस्करण और वैज्ञानिक 25 अध्याय 1 में उपयोगी भविष्यवाणियां करने के लिए उपयोगी उपकरण भी प्रदान करता है ।</s>
परिचय क्षेत्र</s>
इसका सफल प्रयोग यह भविष्यवाणी करने में किया गया है कि किस प्रकार अणुओं के साथ अन्योन्य क्रिया की जाएगी ताकि दवा कम्पनियों को नई दवाओं के निर्माण में सहायता मिल सके</s>
हम भविष्य में अधिक से अधिक वैज्ञानिक क्षेत्रों में गहरी शिक्षा के प्रकट होने की अपेक्षा करते हैं ।</s>
सारांश में , गहन अधिगम मशीनी अधिगम का एक दृष्टिकोण है जिसने पिछले कई दशकों के दौरान विकसित मानव मस्तिष्क , सांख्यिकी और अनुप्रयुक्त गणित के हमारे ज्ञान पर काफी प्रभाव डाला है ।</s>
हाल के वर्षों में गहन अध्ययन ने अपनी लोकप्रियता और उपादेयता में अत्यधिक वृद्धि देखी है , मोटे तौर पर अधिक शक्तिशाली कंप्यूटरों , बड़े डेटासेटों और गहन नेटवर्कों को प्रशिक्षित करने की तकनीकों के परिणाम स्वरूप ।</s>
आने वाले वर्षों में गहन ज्ञान में और सुधार लाने तथा इसे नए मोर्चों पर लाने के लिए चुनौतियों और अवसरों का सामना करना पड़ रहा है ।</s>
26</s>
