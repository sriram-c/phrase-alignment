अध्याय 1 परिचय अन्वेषकों लंबे समय से मशीनों है कि लगता है बनाने का सपना देखा है .</s>
यह इच्छा कम से कम प्राचीन ग्रीस के समय के लिए वापस तिथियाँ .</s>
मिथकों के रूप में कहें तो पिगमैलस डेफिसालस और हिफेस्सा के रूप में प्रयुक्त होने वाले मिथकों को छोड़कर यदि साबित कर दिया जाए तो गैलेक्टिक गैलेक्टिक टैंटोस टैंटोस आदि को कृत्रिम जीवनी * * * * * * * * * * * * * टाँ और मार्टिन . १९९७ .</s>
जब प्रोग्राम योग्य कंप्यूटरों की पहली कल्पना की गई तो लोगों ने सोचा कि क्या ऐसी मशीनें किसी के निर्माण से सौ वर्ष पहले बुद्धिमान बन सकती हैं ( लॉवेलेलिस , 1842 ) ।</s>
आज , कृत्रिम बुद्धिमत्ता अनेक व्यावहारिक अनुप्रयोगों और सक्रिय अनुसंधान विषयों के साथ एक फलता - फूलता क्षेत्र है ।</s>
हम रुटीन प्रसव के लिए बुद्धिमत्तापूर्ण सॉफ़्टवेयर की ओर देखते हैं , भाषण या तस्वीरों को समझते हैं , चिकित्सा में निदान करते हैं और बुनियादी वैज्ञानिक अनुसंधान को समर्थन देते हैं ।</s>
कृत्रिम बुद्धि के प्रारंभिक दिनों में , क्षेत्र तेजी से निपटने और समाधान समस्याओं है कि मानव के लिए बौद्धिक रूप से मुश्किल है , लेकिन कंप्यूटर के लिए अपेक्षाकृत सीधे अग्रगामी -</s>
समस्याओं है कि औपचारिक , गणित - विषयक नियमों की एक सूची द्वारा वर्णित किया जा सकता है .</s>
कृत्रिम बुद्धि के लिए सच्ची चुनौती उन कार्यों को हल करने में सिद्ध हुई जो लोगों के लिए आसान हैं , लेकिन लोगों के लिए औपचारिक रूप से वर्णन करने के लिए कठिन हैं - उन संकेतों को , जिन्हें हम अंतर्ज्ञानात्मक रूप से हल करते हैं , जो बोलचाल के शब्दों को पहचानने की तरह महसूस करते हैं या छवियों में चेहरे ।</s>
यह पुस्तक इन अधिक सहज समस्याओं के एक समाधान के बारे में है .</s>
इस समाधान के लिए कंप्यूटर अनुभव से सीखने और अवधारणाओं के एक पदानुक्रम के संदर्भ में दुनिया को समझने की अनुमति है , सरल अवधारणाओं के साथ अपने संबंध के माध्यम से परिभाषित प्रत्येक अवधारणा के साथ .</s>
अनुभव से ज्ञान को इकट्ठा करके , यह दृष्टिकोण उन सभी ज्ञान को औपचारिक रूप से निर्दिष्ट करने के लिए मानव ऑपरेटरों की आवश्यकता से बचता है जो कंप्यूटर की जरूरत है .</s>
अवधारणाओं का पदानुक्रम कंप्यूटर को सरल अवधारणाओं के निर्माण द्वारा जटिल अवधारणाओं को सीखने में सक्षम बनाता है ।</s>
यदि हम एक ग्राफ को दिखाने कैसे इन अवधारणाओं 1 अध्याय 1 आकर्षित ।</s>
INTRODUCTION एक दूसरे के ऊपर बने हैं , ग्राफ गहरा है , कई परतों के साथ ।</s>
इस कारण के लिए , हम एआई गहन सीखने के लिए इस दृष्टिकोण कहते हैं ।</s>
एआई की कई प्रारंभिक सफलताएं अपेक्षाकृत बंध्य और औपचारिक वातावरण में हुई और इसके लिए कंप्यूटरों को विश्व के बारे में अधिक जानकारी की आवश्यकता नहीं थी .</s>
उदाहरण के लिए , आईबीएम की डीप ब्लू शतरंज खेलने की प्रणाली ने 1997 ( हासू , 2002 ) में विश्व चैंपियन गैरी कास्परोव को हराया ।</s>
शतरंज निस्संदेह एक बहुत ही सरल संसार है , जिसमें केवल चौंसठ स्थान और चौंतीस टुकड़े हैं , जो केवल कठोर परिधि में घूम सकते हैं ।</s>
शतरंज की सफल रणनीति का कारण शतरंज के ढेर को वर्णित करने की कठिनाई नहीं है ।</s>
शतरंज पूरी तरह से औपचारिक नियमों की एक बहुत ही संक्षिप्त सूची द्वारा वर्णित किया जा सकता है , आसानी से प्रोग्रामर द्वारा समय से पहले प्रदान की .</s>
विडंबना यह है कि एक मानव के लिए सबसे कठिन मानसिक उपक्रमों में से एक हैं , अमूर्त और औपचारिक कार्य कंप्यूटर के लिए सबसे आसान में हैं .</s>
कंप्यूटर लंबे समय से सर्वश्रेष्ठ मानव शतरंज खिलाड़ी को भी हराने में सफल रहे हैं लेकिन हाल ही में औसत मानव की वस्तुओं या बोली को पहचानने की क्षमताओं में से कुछ का मिलान शुरू कर दिया है .</s>
एक व्यक्ति के दैनिक जीवन में विश्व के बारे में अपार ज्ञान की आवश्यकता होती है ।</s>
इस ज्ञान का अधिकांश व्यक्तिपरक और सहज ज्ञान युक्त है , और इसलिए एक औपचारिक तरीके से मुखर करने के लिए मुश्किल है .</s>
कम्प्यूटरों को बुद्धिमानी से व्यवहार करने के लिए इसी ज्ञान को हथियाने की आवश्यकता होती है ।</s>
कृत्रिम बुद्धि में एक प्रमुख चुनौती यह है कि इस अनौपचारिक ज्ञान को कम्प्यूटर में कैसे प्राप्त किया जाए ।</s>
अनेक कृत्रिम बुद्धि परियोजनाओं ने औपचारिक भाषाओं में विश्व के बारे में ज्ञान प्राप्त करने का प्रयास किया है ।</s>
एक कंप्यूटर इन औपचारिक भाषाओं में तार्किक अनुमान नियमों का उपयोग करते हुए बयानों के बारे में स्वचालित रूप से कारण हो सकता है .</s>
इसे कृत्रिम बुद्धिमत्ता का ज्ञानाधार दृष्टिकोण कहते हैं ।</s>
इनमें से किसी भी परियोजना से बड़ी सफलता नहीं मिली है ।</s>
इन परियोजनाओं में से एक सबसे प्रसिद्ध है साइक ( एलेनैट और गुहा , 1989 ) .</s>
साइक एक अनुमान इंजन है और एक साइट ( CycL ) नामक भाषा में कथनों का डेटाबेस है .</s>
इन बयानों मानव पर्यवेक्षक के एक स्टाफ द्वारा दर्ज कर रहे हैं .</s>
यह एक दुर्वह प्रक्रिया है ।</s>
लोग दुनिया को सही ढंग से बताने के लिए पर्याप्त जटिलता के साथ औपचारिक नियम बनाने के लिए संघर्ष करते हैं ।</s>
उदाहरण के लिए , साइक सुबह फ्रेड शेव नामक व्यक्ति के बारे में एक कहानी समझने में असफल रहा ( १९९२ ) .</s>
इसके अनुमान इंजन को कहानी में असंगति का पता चलाः यह पता था कि लोगों के पास बिजली के पुर्जे नहीं हैं , लेकिन फ्रेड के पास बिजली का रेज़र था , इसलिए उसने विद्युत पुर्जों की फर्म का विश्वास किया ।</s>
इसलिए पूछा कि क्या फ्रेड अभी भी एक व्यक्ति था जबकि वह शेव कर रहा था .</s>
हार्ड कोडिंग ज्ञान पर भरोसा करने वाली प्रणालियों के सामने आने वाली मुश्किलें यह बताती हैं कि एआइएस सिस्टम को 2 अध्याय 1 निकालकर खुद का ज्ञान हासिल करने की क्षमता चाहिए ।</s>
कच्चे डेटा से सूचना पैटर्न</s>
इस क्षमता को मशीन लर्निंग के नाम से जाना जाता है ।</s>
मशीनी सीखने की शुरूआत ने कंप्यूटरों को वास्तविक दुनिया की जानकारी से जुड़ी समस्याओं को हल करने और ऐसे निर्णय लेने में सक्षम बनाया जो व्यक्तिपरक प्रतीत होते हैं ।</s>
एक सरल मशीनी अधिगम एल्गोरिथ्म जिसे लॉजिस्टिक प्रतिगमन कहा जाता है , यह निर्धारित कर सकता है कि सिजेरियन डिलीवरी ( मोर - योजफ एट अल , 1990 ) की सिफारिश करें या नहीं ।</s>
एक साधारण मशीन लर्निंग एल्गोरिदम जिसे भोले बेयॉ कहा जाता है , स्पैम ई - मेल से वैध ई - मेल को अलग कर सकता है ।</s>
इन सरल मशीन सीखने एल्गोरिदम का प्रदर्शन उन्हें दिए गए डेटा के प्रतिनिधित्व पर काफी निर्भर करता है .</s>
उदाहरण के लिए , जब सीजेरियन डिलीवरी की सिफारिश के लिए लॉजिस्टिक प्रतिगमन का प्रयोग किया जाता है , एआई प्रणाली रोगी की सीधे जांच नहीं करती है .</s>
इसके बजाय , चिकित्सक सिस्टम को प्रासंगिक जानकारी के कई टुकड़े बताता है , जैसे गर्भाशय के निशान की उपस्थिति या अनुपस्थिति .</s>
रोगी के प्रतिनिधित्व में शामिल जानकारी के प्रत्येक टुकड़ा एक विशेषता के रूप में जाना जाता है .</s>
लॉजिस्टिक प्रतिगमन यह सीखता है कि रोगी की इन विशेषताओं का किस प्रकार विभिन्न परिणामों से संबंध होता है ।</s>
हालांकि , यह कैसे सुविधाओं को किसी भी तरह से परिभाषित कर रहे हैं प्रभावित नहीं कर सकते हैं .</s>
यदि लॉजिस्टिक प्रतिगमन मरीज का एमआरआई स्कैन दिया जाता , बजाय चिकित्सक की औपचारिक रिपोर्ट के , तो यह उपयोगी भविष्यवाणियां करने में सक्षम नहीं होगा .</s>
एमआरआई स्कैन में व्यक्तिगत पिक्सल का प्रसव के दौरान होने वाली जटिलताओं के साथ नगण्य सहसंबंध होता है ।</s>
प्रतिनिधित्वों पर यह निर्भरता एक सामान्य घटना है जो पूरे कंप्यूटर विज्ञान और यहां तक कि दैनिक जीवन में प्रकट होती है .</s>
कंप्यूटर विज्ञान में , डेटा संग्रह की खोज करने जैसी संक्रियाएं घातीय रूप से तेजी से आगे बढ़ सकती हैं , यदि कोलेक - टीन की संरचना और अनुक्रमण समझदारी से हो ।</s>
लोग अरबी अंकों पर अंकगणित आसानी से कर सकते हैं लेकिन रोमन अंकों पर अंकगणित को बहुत अधिक समय लेने वाला पाते हैं ।</s>
यह आश्चर्य की बात नहीं है कि प्रतिनिधित्व के चयन का मशीन सीखने एल्गोरिदम के प्रदर्शन पर भारी प्रभाव पड़ता है .</s>
एक सरल दृश्य उदाहरण के लिए , देखें आंकड़ा 1 .1</s>
कई कृत्रिम बुद्धि कार्यों उस कार्य के लिए निकालने के लिए सुविधाओं के सही सेट डिजाइन करने के द्वारा हल किया जा सकता है , तो एक सरल मशीन सीखने एल्गोरिथ्म के लिए इन सुविधाओं को प्रदान .</s>
उदाहरण के लिए , ध्वनि से वक्ता की पहचान के लिए एक उपयोगी विशेषता वक्ता के स्वर तंत्र के आकार का अनुमान है ।</s>
यह विशेषता इस बात का मजबूत संकेत देती है कि वक्ता पुरुष है या स्त्री , या बच्चा ।</s>
कई कार्यों के लिए , तथापि , यह जानना मुश्किल है कि किन सुविधाओं को निकाला जाना चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हम फोटो में कारों का पता लगाने का प्रोग्राम लिखना चाहेंगे ।</s>
हम जानते हैं कि कारों पहिया है , तो हम एक विशेषता के रूप में एक पहिया की उपस्थिति का उपयोग करना चाहते हो सकता है .</s>
दुर्भाग्य से , यह वास्तव में क्या एक पहिया पिक्सेल मूल्यों के संदर्भ में की तरह लग रहा है का वर्णन करने के लिए मुश्किल है ।</s>
पहिए की एक साधारण ज्यामितीय आकृति होती है , लेकिन उसकी छवि पहिए पर पड़ने वाली छाया , पहिए की धातु के हिस्सों को चमकते हुए सूर्य , कार का लिंग या किसी वस्तु को 3 अध्याय 1 में जटिल बना देती है ।</s>
परिचय</s>
</s>
।</s>
</s>
।</s>
आकृति 1 . 1</s>
विभिन्न अभ्यावेदनों का उदाहरणः मान लीजिए कि हम एक स्कैटरपॉट में उनके बीच एक रेखा खींचकर दो श्रेणियों के डेटा को अलग करना चाहते हैं .</s>
बाईं ओर के भूखंड में , हम कार्टेसियन निर्देशांकों का उपयोग कर कुछ डेटा का प्रतिनिधित्व करते हैं , और कार्य असंभव है .</s>
दाईं ओर के भूखंड में , हम ध्रुवीय निर्देशांकों के साथ डेटा का प्रतिनिधित्व करते हैं और कार्य एक ऊर्ध्वाधर रेखा के साथ हल करने के लिए सरल हो जाता है .</s>
।</s>
इस समस्या का एक समाधान यह है कि न केवल प्रतिनिधित्व से आउटपुट तक मानचित्रण का पता लगाने के लिए मशीन लर्निंग का उपयोग किया जाए बल्कि स्वयं प्रतिनिधित्व भी किया जाए ।</s>
इस दृष्टिकोण प्रतिनिधित्व सीखने के रूप में जाना जाता है .</s>
विद्वत अभ्यावेदन का परिणाम प्रायः हस्तपरिभाषित अभ्यावेदनों से प्राप्त प्रदर्शनों से कहीं बेहतर होता है ।</s>
वे एआई प्रणालियों को भी तेजी से नए कार्यों के लिए अनुकूलित करने के लिए सक्षम बनाता है , न्यूनतम मानव हस्तक्षेप के साथ .</s>
एक निरूपण अधिगम एल्गोरिथ्म मिनटों में एक सरल कार्य के लिए या घंटों से महीनों में एक जटिल कार्य के लिए विशेषताओं के एक अच्छे सेट की खोज कर सकती है ।</s>
डिजाइन जटिल जटिल कार्य की आवश्यकता है मानव समय की विशेषताओं और प्रयास के लिए एक महान सौदा हस्तचालित सौदा के लिए दशकों ले सकता है शोधकर्ताओं के पूरे समुदाय</s>
एक निरूपण अधिगम एल्गोरिथ्म का महत्वपूर्ण उदाहरण ऑ - टोनकोडर है ।</s>
स्वतः - कोडक एक एनकोडर फलन का संयोजन है , जो इनपुट डेटा को भिन्न अभ्यावेदन , और एक विकोडक फलन में परिवर्तित करता है , जो नए निरूपण को वापस मूल स्वरूप में परिवर्तित करता है ।</s>
जब किसी इनपुट को एनकोडर और उसके बाद विकोडक के माध्यम से चलाया जाता है तो स्वचालितकोडर्स को यथासंभव अधिक से अधिक जानकारी सुरक्षित रखने के लिए प्रशिक्षित किया जाता है , लेकिन उन्हें नए निरूपण को विभिन्न अच्छे गुण बनाने के लिए भी प्रशिक्षित किया जाता है ।</s>
विभिन्न प्रकार के ऑटोनकोडर्स का उद्देश्य विभिन्न प्रकार की संपत्तियों को प्राप्त करना होता है ।</s>
जब अधिगम विशेषताओं या एल्गोरिदम डिजाइनिंग , हमारा लक्ष्य आम तौर पर भिन्नता के कारकों को अलग करने के लिए है जो मनाया डेटा की व्याख्या .</s>
इसमें 4 अध्याय 1 .</s>
आईटीआरओ संदर्भ , हम का उपयोग करें “उपयोगकर्ता” का उपयोग केवल “उपयोगकर्ता के संदर्भ में अलग स्रोतों को संदर्भित करता है” आमतौर पर गुणन के द्वारा प्रभाव को प्रभावित नहीं कर रहे हैं .</s>
इस तरह के कारक अक्सर वे मात्राएं नहीं होती जो प्रत्यक्ष रूप से देखी जाती हैं ।</s>
इसके स्थान पर वे पदार्थ या तो अनपेक्षित वस्तुओं के रूप में विद्यमान हो सकते हैं अथवा प्रेक्षण योग्य मात्राओं को प्रभावित करने वाली भौतिक जगत की अनपेक्षित शक्तियों के रूप में भी विद्यमान हो सकते हैं ।</s>
ये मानव मस्तिष्क में निर्मित संरचनाओं के रूप में भी विद्यमान हो सकते हैं जो अवलोकन किए गए आंकड़ों की व्याख्याओं या अनुमानित कारणों का उपयोगी सरलीकरण प्रदान करते हैं ।</s>
वे अवधारणाओं या अमूर्तनों के रूप में सोचा जा सकता है जो हमें डेटा में समृद्ध विविधता का एहसास कराने में मदद करते हैं .</s>
जब किसी वाक् अभिलेखन का विश्लेषण किया जाता है तो विभिन्नता के कारकों में वक्ता की आयु , उनके लिंग , उनके लहजे और वे बोल रहे शब्द शामिल होते हैं ।</s>
किसी कार की छवि का विश्लेषण करने पर , विभिन्नता के कारक कार की स्थिति , उसके रंग , और सूर्य के कोण और चमक शामिल हैं .</s>
अनेक रीयल - वर्ल्ड कृत्रिम बुद्धिमता अनुप्रयोगों में कठिनाई का एक बड़ा स्रोत यह है कि विभिन्नता के अनेक कारक , डेटा के प्रत्येक टुकड़े पर प्रभाव डालते हैं जिन्हें हम देख सकते हैं .</s>
लाल कार की एक छवि में व्यक्तिगत पिक्सल रात में काले के बहुत करीब हो सकता है .</s>
कार के सिलहोट का आकार देखने के कोण पर निर्भर करता है ।</s>
अधिकांश अनुप्रयोगों के लिए यह आवश्यक होता है कि हम विभिन्नता के कारकों को सुलझाएं और उन कारकों को त्यागें जिनके बारे में हमें परवाह नहीं है ।</s>
बेशक , कच्चे डेटा से इस तरह के उच्च स्तर , अमूर्त सुविधाओं को निकालना बहुत मुश्किल हो सकता है .</s>
विभिन्नता के इन कारकों , जैसे कि वक्ता के लहजे , की पहचान केवल डेटा के लगभग मानव स्तर की समझ का उपयोग करते हुए की जा सकती है ।</s>
जब मूल समस्या को हल करने के लिए प्रतिनिधित्व प्राप्त करना लगभग उतना ही कठिन होता है , तो अभ्यावेदन शिक्षण पहली नजर में हमारी सहायता करता प्रतीत होता है ।</s>
गहन अधिगम प्रतिनिधित्व शिक्षण में इस केंद्रीय समस्या का समाधान आत्म - दुभाषिए द्वारा करता है जो अन्य , सरल निरूपण के संदर्भ में व्यक्त किए जाते हैं ।</s>
गहन अधिगम कंप्यूटर को सरल विचारों से जटिल संकल्पनाओं का निर्माण करने में सक्षम बनाता है ।</s>
चित्र 1 . 2 में दिखाया गया है कि किस प्रकार एक गहन शिक्षण प्रणाली सरल अवधारणाओं , जैसे कोनों और रेखाओं , जो बारी - बारी से किनारों के संदर्भ में परिभाषित किया गया है , के संयोजन द्वारा एक व्यक्ति की छवि की अवधारणा का प्रतिनिधित्व कर सकती है ।</s>
डीप लर्निंग मॉडल का विन्यस्त उदाहरण फीड काफी डीप नेटवर्क या मल्टीलेयर पेप्ट्रॉन है ।</s>
एक मल्टीलेर परसेप्ट्रण , मात्र एक गणितीय फलन है , जो इनपुट मानों के कुछ समुच्चय को आउटपुट मानों के लिए प्रतिचित्रित करता है ।</s>
समारोह कई सरल कार्यों की रचना करके बनता है .</s>
हम इनपुट का एक नया प्रतिनिधित्व प्रदान करने के रूप में एक अलग गणितीय समारोह के प्रत्येक आवेदन के बारे में सोच सकते हैं .</s>
डेटा के लिए सही प्रतिनिधित्व सीखने का विचार गहन अध्ययन पर एक प्रति - स्पेक्ट्रोस्कोपी प्रदान करता है .</s>
डीप लर्निंग का एक अन्य परिप्रेक्ष्य यह है कि डीप कंप्यूटर प्रोग्राम को सीखने में सक्षम बनाता है ।</s>
प्रतिनिधित्व की प्रत्येक परत को 5 अध्याय 1 के बाद कंप्यूटर की स्मृति की स्थिति के रूप में सोचा जा सकता है .</s>
दृश्य परत ( इनपुट पिक्सेल्स )</s>
प्रथम प्रच्छन्न परत</s>
द्वितीय प्रच्छन्न परत</s>
CAR PERSON ANIMAL आउटपुट ( विषयगत पहचान )</s>
चित्र 1 . 2 . गहन शिक्षण मॉडल का चित्रण ।</s>
कच्चे संवेदी इनपुट डेटा के अर्थ को समझना कंप्यूटर के लिए कठिन है , जैसे यह छवि पिक्सेल मूल्यों के संग्रह के रूप में प्रतिनिधित्व करती है .</s>
किसी वस्तु पहचान के लिए पिक्सल के समुच्चय से फलन प्रतिचित्रण बहुत जटिल होता है ।</s>
इस मानचित्रण का ज्ञान या मूल्यांकन यदि सीधे ही किया जाए तो यह अकाट्य प्रतीत होता है ।</s>
गहन अधिगम नेस्टेड सरल मानचित्रण की एक श्रृंखला में इच्छित जटिल मानचित्रण को तोड़कर , प्रत्येक मॉडल की एक अलग परत द्वारा वर्णित इस कठिनाई को दूर करता है .</s>
इनपुट दृश्य परत पर प्रस्तुत है , इसलिए नाम दिया है क्योंकि यह चर है कि हम निरीक्षण करने में सक्षम हैं शामिल हैं .</s>
फिर छिपा परतों की एक श्रृंखला छवि से लगातार सार सार .</s>
इन परतों को कहते हैं & # 44 ; क्योंकि इन परतों को नहीं दिया जाता है & # 44 ; क्योंकि इन परतों को वास्तव में डेटा कहते हैं & # 44 ; जो मॉडल के रूप में निर्धारित किया जाता है वो निर्धारित करने के लिए निर्धारित किया जाना चाहिए & # 44 ; मनाया जाने वाला डेटा .</s>
यहाँ छवियों प्रत्येक छिपा इकाई द्वारा प्रतिनिधित्व विशेषता की तरह की कल्पना कर रहे हैं .</s>
पिक्सल को देखते हुए , पहली परत पड़ोसी पिक्सल की चमक की तुलना करके किनारों को आसानी से पहचान सकती है .</s>
पहली छुपी परत के किनारों के वर्णन को देखते हुए , दूसरी छुपी परत आसानी से कोनों को खोज सकती है और विस्तारित कंटूर , जो किनारों के संग्रह के रूप में पहचानी जा सकती है .</s>
कोनों और रेखाओं के संदर्भ में छवि की दूसरी छुपी पर्त के वर्णन को देखते हुए , तीसरी छुपी पर्त विशिष्ट वस्तुओं के संपूर्ण भागों का पता लगा सकती है , कॉनूरों और कोनों के विशिष्ट संग्रहों का पता लगाकर .</s>
अंत में , छवि के इस विवरण को उसमें रखे गए ऑब्जेक्ट पार्ट्स के संदर्भ में , छवि में मौजूद वस्तुओं को पहचानने के लिए इस्तेमाल किया जा सकता है .</s>
ज़िलर और फर्गस ( २०१४ ) की अनुमति से छवियाँ तैयार की गईं .</s>
6 अध्याय 1 .</s>
आईटीआरओडीयूसीएसई ( समानांतर निर्देशों के एक अन्य सेट का निष्पादन )</s>
अधिक गहराई वाले नेटवर्क , अनुक्रम में अधिक अनुदेशों को निष्पादित कर सकते हैं ।</s>
अनुक्रमिक अनुदेश महान शक्ति प्रदान करते हैं क्योंकि बाद के अनुदेश पूर्व अनुदेशों के परिणामों को वापस संदर्भित कर सकते हैं .</s>
ऐक -</s>
गहन अधिगम के इस दृष्टिकोण को मजबूत करते हुए , एक परत सक्रियण में सभी जानकारी आवश्यक रूप से विभिन्नता के कारकों जो इनपुट की व्याख्या करते हैं एन्कोडिंग .</s>
प्रतिनिधित्व अवस्था सूचना को भी संग्रहित करता है जो एक प्रोग्राम को निष्पादित करने में सहायता करता है जो इनपुट का बोध करा सकता है .</s>
यह अवस्था सूचना परंपरागत कंप्यूटर प्रोग्राम में एक काउंटर या सूचक के अनुरूप हो सकती है ।</s>
इसका विशेष रूप से इनपुट की सामग्री से कोई संबंध नहीं है , लेकिन यह मॉडल को अपने प्रसंस्करण को व्यवस्थित करने में मदद करता है .</s>
एक मॉडल की गहराई मापने के दो मुख्य तरीके हैं .</s>
पहला दृश्य अनुक्रमिक अनुदेशों की संख्या पर आधारित है जिसे वास्तुकला का मूल्यांकन करने के लिए निष्पादित किया जाना चाहिए .</s>
हम इसके बारे में प्रवाह चार्ट के माध्यम से सबसे लंबे पथ की लंबाई के रूप में सोच सकते हैं जो यह बताता है कि कैसे प्रत्येक मॉडल के आउटपुट अपने इनपुट दिया गणना करने के लिए .</s>
जिस प्रकार दो समतुल्य कंप्यूटर प्रोग्रामों की लंबाई अलग - अलग होती है , यह निर्भर करता है कि प्रोग्राम किस भाषा में लिखा गया है , उसी प्रकार एक ही फलन को विभिन्न गहराइयों के साथ फ्लो चार्ट के रूप में तैयार किया जा सकता है , जिस पर निर्भर करता है कि हम प्रवाह संचित्र में अलग - अलग - अलग चरणों के रूप में प्रयोग की अनुमति देते हैं ।</s>
चित्र 1 . 3 यह स्पष्ट करता है कि किस प्रकार भाषा का यह चयन एक ही वास्तुकला के लिए दो भिन्न माप प्रदान कर सकता है ।</s>
</s>
×</s>
x 2</s>
x 2</s>
त्यौरी</s>
2</s>
w 2 × +</s>
तत्व सेट + ×</s>
x</s>
तत्व सेट लॉजिस्टिक रीग्रेसन लॉजिस्टिक रीग्रेसन चित्र 1 . 3</s>
एक आउटपुट के लिए जहाँ प्रत्येक नोड एक ऑपरेशन निष्पादित करता है कम्प्यूटेशनल ग्राफों की व्याख्या ।</s>
गहराई , इनपुट से आउटपुट तक के सबसे लंबे पथ की लंबाई है , लेकिन यह संभावित संगणकीय चरण का गठन करने की परिभाषा पर निर्भर करता है ।</s>
गणना इन गणना इन ग्राफ की गणना जब यह लॉगिस्टिक मॉडल की गणना है , लॉग इन करने के लिए लॉगिस्टिक मॉडल , लॉग s है x ) के लॉग इन दर्शाया गया है जहाँ repressmo T मॉडल में निर्गत है ।</s>
यदि हम अपने कंप्यूटर भाषा के तत्वों के रूप में जोड़ , गुणन और रसद अवग्रहों का उपयोग करें , तो इस मॉडल में गहराई तीन है ।</s>
यदि हम लॉजिस्टिक प्रतिगमन को एक तत्व के रूप में ही देखते हैं , तो इस मॉडल में गहराई एक है .</s>
7 अध्याय 1 .</s>
INTRODUCTION एक अन्य दृष्टिकोण , जिसका प्रयोग गहन प्रायैब्लिस्टिक मॉडल द्वारा किया जाता है , एक मॉडल की गहराई को कम्प्यूटेशनल ग्राफ की गहराई नहीं बल्कि ग्राफ की गहराई मानती है कि अवधारणाएँ कैसे एक दूसरे से संबंधित हैं ।</s>
इस मामले में , प्रत्येक अवधारणा के निरूपण की गणना के लिए आवश्यक संगणनाओं के प्रवाह संचित्र की गहराई स्वयं अवधारणाओं के ग्राफ से कहीं अधिक गहरी हो सकती है .</s>
इसका कारण यह है कि प्रणाली की सरल अवधारणाओं की समझ को अधिक जटिल अवधारणाओं के बारे में जानकारी देकर परिष्कृत किया जा सकता है ।</s>
उदाहरण के लिए , छाया में एक आंख वाले चेहरे के प्रतिबिंब को देखने वाली एआई प्रणाली प्रारंभ में केवल एक आंख ही देख सकती है ।</s>
यह पता लगाने के बाद कि कोई चेहरा मौजूद है , प्रणाली तब अनुमान लगा सकती है कि शायद दूसरी आँख भी मौजूद है .</s>
इस मामले में , अवधारणाओं के ग्राफ में केवल दो परतें शामिल हैं - आँखों के लिए एक परत और चेहरों के लिए एक परत - गणना के ग्राफ 2 n परतों शामिल हैं अगर हम अन्य n समय दिया प्रत्येक अवधारणा के अपने अनुमान को परिष्कृत .</s>
क्योंकि यह हमेशा स्पष्ट नहीं है कि इन दो दृश्यों में से कौन सा है - कम्प्यूटेशनल ग्राफ की गहराई , या प्रोबाबिलिस्टिक मॉडलिंग ग्राफ की गहराई - सबसे अधिक प्रासंगिक है , और क्योंकि विभिन्न लोग अपने ग्राफों का निर्माण करने के लिए सबसे छोटे तत्वों के विभिन्न सेट चुनते हैं , एक वास्तुकला की गहराई के लिए कोई एकल सही मूल्य नहीं है ,</s>
न ही इस बात पर आम सहमति है कि किसी मॉडल को “दीपावली” के रूप में कितनी गहराई से योग्य बनाने की आवश्यकता है ।</s>
तथापि , गहन अधिगम को सुरक्षित रूप से ऐसे मॉडलों का अध्ययन माना जा सकता है जिसमें पारंपरिक मशीन अधिगम की अपेक्षा अधिक मात्रा में विद्वत प्रकार्यों या विद्वत अवधारणाओं की रचना निहित होती है ।</s>
संक्षेप करने के लिए , गहरी सीखने , इस पुस्तक के विषय एआई के लिए एक दृष्टिकोण है .</s>
विशेष रूप से , यह मशीन अधिगम का एक प्रकार है , एक तकनीक है जो अनुभव और डेटा के साथ कंप्यूटर सिस्टम को बेहतर बनाने में सक्षम बनाता है .</s>
हमारा तर्क है कि एआइ सिस्टम बनाने के लिए मशीन लर्निंग ही एक व्यवहार्य उपाय है जो जटिल वास्तविक दुनिया के वातावरण में काम कर सकता है .</s>
गहन अधिगम एक विशेष प्रकार का मशीनी अधिगम है जो सामान्य अवधारणाओं के संबंध में परिभाषित प्रत्येक अवधारणा के साथ अवधारणाओं के एक नेस्टेड पदानुक्रम के रूप में विश्व का प्रतिनिधित्व करते हुए महान शक्ति और लचीलापन प्राप्त करता है , और कम अमूर्त निरूपण के संदर्भ में गणना करता है ।</s>
चित्र 1 . 4 इन विभिन्न एआइ विधाओं के बीच संबंध स्पष्ट करता है ।</s>
चित्र 1 . 5 में प्रत्येक कैसे कार्य करता है , इसकी एक उच्च स्तरीय व्यवस्था दी गई है ।</s>
1 . 1</s>
इस पुस्तक को किसने पढ़ा है ?</s>
यह पुस्तक कई तरह के पाठकों के लिए उपयोगी हो सकती है , लेकिन हमने इसे दो लक्षित दर्शकों को ध्यान में रखकर लिखा ।</s>
इन लक्षित दर्शकों में से एक है विश्वविद्यालय के छात्र ( स्नातक या स्नातकोत्तर ) मशीन लर्निंग के बारे में , उन लोगों सहित जो गहन अध्ययन और कृत्रिम बुद्धि अनुसंधान में एक कैरियर की शुरुआत कर रहे हैं .</s>
अन्य 8 अध्याय 1 .</s>
INTRODUCTION A मशीन लर्निंग प्रतिनिधित्व लर्निंग डीप लर्निंग उदाहरणः ज्ञान आधार</s>
लॉजिस्टिक प्रतिगमन</s>
उदाहरणः</s>
शैलो ऑटोनकोडर्स</s>
उदाहरणः</s>
एमएलपी चित्र 1 . 4</s>
एक वेन आरेख दिखाता है कि कैसे गहरी अधिगम एक प्रकार का प्रतिनिधित्व अधिगम है , जो बारी में मशीन अधिगम , जो कई के लिए प्रयोग किया जाता है लेकिन एआई के लिए सभी दृष्टिकोण नहीं है .</s>
वेन आरेख के प्रत्येक अनुभाग में एक एआई प्रौद्योगिकी का एक उदाहरण शामिल है .</s>
लक्षित श्रोता उन सॉफ्टवेयर इंजीनियरों को कहते हैं , जिनके पास मशीनी शिक्षा या स्थैतिक पृष्ठभूमि नहीं है लेकिन वे तेजी से एक को प्राप्त करना चाहते हैं और अपने उत्पाद या मंच में गहन अधिगम का प्रयोग शुरू कर देते हैं ।</s>
कंप्यूटर दृष्टि , भाषण और ऑडियो प्रोसेसिंग , प्राकृतिक भाषा संसाधन , रोबोटिक्स , जैव सूचना विज्ञान और रसायन , वीडियो गेम्स , ऑनलाइन विज्ञापन और वित्त सहित कई सॉफ्ट वेयर विधाओं में डीप लर्निंग पहले से ही उपयोगी साबित हुई है ।</s>
इस पुस्तक को तीन भागों में बांटा गया है ताकि विभिन्न प्रकार के पाठकों का आदान - प्रदान हो सके ।</s>
भाग 1 बुनियादी गणितीय उपकरणों और मशीन सीखने अवधारणाओं परिचय .</s>
भाग द्वितीय सबसे स्थापित गहन शिक्षण एल्गोरिदम , जो अनिवार्य रूप से हल प्रौद्योगिकियों का वर्णन करता है .</s>
भाग 3 अधिक सट्टा विचारों का वर्णन करता है जो व्यापक रूप से माना जाता है कि गहन अध्ययन में भविष्य के अनुसंधान के लिए महत्वपूर्ण है .</s>
9 अध्याय 1 .</s>
परिचय</s>
हस्तनिर्मित प्रोग्राम</s>
आउटपुट इनपुट</s>
फीचर आउटपुट इनपुट से हस्तनिर्मित फीचर मैपिंग</s>
सुविधाओं आउटपुट अतिरिक्त परतों से मैपिंग अधिक अमूर्त सुविधाओं नियम - बेस्ड सिस्टम्स क्लासिक मशीन लर्निंग रीप्रेजेंटेशन लर्निंग डीप लर्निंग चित्र 1 . 5 एक एआइ प्रणाली के विभिन्न भागों को विभिन्न एआइ विधाओं के भीतर कैसे एक दूसरे से संबंधित दिखाता है ।</s>
छायांकित बक्से उन घटकों को सूचित करते हैं जो डेटा से सीखने में सक्षम होते हैं ।</s>
पाठकों को उन भागों को छोड़ने के लिए स्वतंत्र महसूस होना चाहिए जो उनके हितों या पृष्ठभूमि को देखते हुए प्रासंगिक नहीं हैं .</s>
रैखिक बीजगणित , संभावना , और मौलिक मशीन सीखने अवधारणाओं से परिचित पाठक भाग I , उदाहरण के लिए छोड़ सकते हैं , जबकि जो लोग सिर्फ एक कार्य प्रणाली को लागू करना चाहते हैं उन्हें भाग II से आगे पढ़ने की जरूरत नहीं है ।</s>
चुनने में मदद करने के लिए जो 10 अध्याय 1 .</s>
परिचय 1 .</s>
परिचय भाग - 1 : अनुप्रयुक्त मठ और मशीन लर्निंग बेसिक्स 2 .</s>
रेखीय बीजगणित 3 .</s>
संभावना और सूचना सिद्धांत 4 .</s>
संख्यात्मक गणना 5 .</s>
मशीन लर्निंग बेसिक्स पार्ट - 2 :</s>
डीप नेटवर्क्सः आधुनिक पद्धतियाँ 6 .</s>
डीप फीडवर्ड संजाल 7 .</s>
नियमितीकरण ८ .</s>
अनुकूलन 9 .</s>
सीएनएन १०</s>
आरएनएस 11</s>
व्यावहारिक पद्धति 12 .</s>
अनुप्रयोग भाग -</s>
गहन अध्ययन अनुसंधान 13 .</s>
रेखीय फैक्टर मॉडल्स 14 .</s>
आटोन्कोडर्स 15</s>
प्रतिनिधित्व सीखना 16 .</s>
संरचित सम्भाव्य मॉडल 17 .</s>
मोंटे कार्लो विधियाँ</s>
पार्टीशन फंक्शन 19 .</s>
अनुमान 20 .</s>
डीप जेनरेशनेटिव मॉडल चित्र 1 . 6 : पुस्तक का उच्च स्तरीय संगठन ।</s>
एक अध्याय से दूसरे अध्याय तक तीर यह इंगित करता है कि पहले अध्याय को समझने के लिए पूर्वापेक्षित सामग्री है ।</s>
11 अध्याय 1 .</s>
पढ़ने के लिए सूचना अध्याय , 1 . 6 में एक फ्लोचार्ट दिया गया है जो पुस्तक के उच्च स्तर के संगठन को दर्शाता है ।</s>
हम यह जरूर मान लेते हैं कि सभी पाठक कंप्यूटर विज्ञान की पृष्ठभूमि से आते हैं ।</s>
हम प्रोग्रामिंग , कम्प्यूटेशनल प्रदर्शन मुद्दों की एक बुनियादी समझ , जटिलता सिद्धांत , परिचयात्मक स्तर पथरी और ग्राफ सिद्धांत की कुछ शब्दावली के साथ परिचितता ग्रहण करते हैं .</s>
1 . 2 डीप लर्निंग में ऐतिहासिक रुझान</s>
गहन अध्ययन को किसी ऐतिहासिक संदर्भ के साथ समझना सबसे आसान है ।</s>
डीप लर्निंग का एक विस्तृत इतिहास प्रदान करने के बजाय , हम कुछ प्रमुख प्रवृत्तियों की पहचानः</s>
• गहन अध्ययन का एक लंबा और समृद्ध इतिहास रहा है , लेकिन कई नामों से चला गया है , विभिन्न दार्शनिक दृष्टिकोणों को प्रतिबिंबित , और लोकप्रियता में वृद्धि हुई है और कम हो गया है .</s>
• गहन शिक्षण अधिक उपयोगी हो गया है के रूप में उपलब्ध प्रशिक्षण डेटा की राशि में वृद्धि हुई है .</s>
• गहन अधिगम मॉडल समय के साथ आकार में वृद्धि हुई है के रूप में कंप्यूटर बुनियादी ढांचे ( दोनोंथ हार्डवेयर और गहरी अधिगम के लिए सॉफ्टवेयर में सुधार हुआ है .</s>
• गहन अधिगम समय के साथ बढ़ती सटीकता के साथ लगातार जटिल अनुप्रयोगों हल किया है .</s>
1 . 2 . 1</s>
तंत्रिका नेट के अनेक नाम और परिवर्तन के किले</s>
हम उम्मीद करते हैं कि इस पुस्तक के कई पाठकों को एक रोमांचक नई प्रौद्योगिकी के रूप में गहरी सीखने के बारे में सुना है , और एक उभरते क्षेत्र के बारे में एक पुस्तक में “स्वास्थ्य” का उल्लेख देख आश्चर्य है .</s>
वास्तव में , गहरे अध्ययन की शुरुआत 1940 के दशक में हुई थी ।</s>
गहन अधिगम केवल नया प्रतीत होता है , क्योंकि यह अपनी वर्तमान लोकप्रियता से पहले कई वर्षों के लिए अपेक्षाकृत अलोकप्रिय था , और क्योंकि यह कई अलग अलग नामों से चला गया है , केवल हाल ही में “दीप अधिगम” कहा जा रहा है .</s>
क्षेत्र का कई बार पुनर्गठन किया गया है , विभिन्न शोधकर्ताओं और विभिन्न परिप्रेक्ष्य के प्रभाव को प्रतिबिंबित .</s>
गहन अध्ययन का व्यापक इतिहास इस पाठ्यपुस्तक के दायरे से बाहर है ।</s>
कुछ बुनियादी संदर्भ , तथापि , गहरी सीखने को समझने के लिए उपयोगी है .</s>
मोटे तौर पर विकास की तीन लहरें रही हैं :</s>
1940 - 1960 के दशक में साइबरनेटिक्स के नाम से जाना जाने वाला गहरा अध्ययन , 12 अध्याय 1 में कनेक्शनवाद के रूप में जाना जाता था ।</s>
परिचय 1940 1950 दशक 1970 2000 वर्ष</s>
</s>
0 . 000200 0 . 000250 शब्द या वाक्यांशों के साइबरनेटिक्स ( कनेक्टिविटी + न्यूरल नेटवर्क ) की आवृत्ति</s>
आंकड़े 1 . 7</s>
गूगल बुक्स के अनुसार कृत्रिम तंत्रिका जाल अनुसंधान की तीन ऐतिहासिक तरंगों में से दो , वाक्यों “व्यक्तित्व” और “संबंध” या “न्यूरल नेटवर्क” की आवृत्ति से मापी जाती है ( - तीसरी तरंग बहुत हाल ही में प्रकट होती है ।</s>
१९४० दशक में वायुविज्ञान के साथ वायुविज्ञान , पर्यावरण सिद्धांत के विकास , १९५८ - आगमन की पहली वेव - फिलॉस्फे , १९४६ में वायू - वायू एलोयोस , १९४६ में वायू - संबंधी मॉडलों के आरंभ , वायू - क्रियान्वयन के रूप में ,</s>
दूसरी लहर 1980 -1995 अवधि के कनेक्शनवादी दृष्टिकोण , बैक - प्रोपेगंडा (Rumelhart एट अल , 1986 ) से शुरू हुई ताकि एक या दो गुप्त परतों वाले तंत्रिका नेटवर्क को प्रशिक्षित किया जा सके ।</s>
वर्तमान में & # 44 ; गहरा अधिगम 2006 ( अल . अल . ) & # 44 ; और तृतीय & # 44 ; बेन्जियो एट</s>
albania _ districts . kgm</s>
और अभी 2016 के रूप में पुस्तक के रूप में प्रकट हो रहा है .</s>
इसी प्रकार की अन्य दो तरंगें पुस्तकाकार में उस वैज्ञानिक गतिविधि से बहुत बाद में प्रकट हुईं ।</s>
1980 -1990 दशक , और 2006 में गहन अध्ययन की शुरुआत के नाम से वर्तमान पुनरुत्थान .</s>
यह संख्या 1 . 7 है ।</s>
आज हम जिस आरंभिक शिक्षण एल्गोरिदम को मान्यता दे रहे हैं , उसका उद्देश्य जैविक अधिगम का संगणकीय मॉडल बनना था , अर्थात अधिगम कैसे होता है या मस्तिष्क में हो सकता है , उसका मॉडल ।</s>
इसके परिणामस्वरूप , गहराई से सीखने वाले नामों में से एक कृत्रिम तंत्रिका नेटवर्क है ।</s>
गहन अध्ययन मॉडलों पर तदनुरूप परिप्रेक्ष्य यह है कि वे जैविक मस्तिष्क ( चाहे मानव मस्तिष्क हो या किसी अन्य पशु के मस्तिष्क ) से प्रेरित प्रणालियां निर्मित करते हैं ।</s>
हालांकि मशीन लर्निंग के लिए प्रयुक्त किस्म के तंत्रिका नेटवर्क कभी - कभी मस्तिष्क समारोह ( हाइटन एंड शल्किस ) , 1991 को समझने के लिए इस्तेमाल किया गया है , वे आम तौर पर जैविक प्रकार्य के यथार्थवादी मॉडल नहीं बनने के लिए डिज़ाइन कर रहे हैं .</s>
गहन अध्ययन पर तंत्रिका संबंधी नजरिया दो मुख्य विचारों से प्रेरित है ।</s>
एक विचार यह है कि मस्तिष्क उदाहरण द्वारा एक प्रमाण प्रदान करता है कि बुद्धिमत्तापूर्ण व्यवहार संभव है , और बुद्धि के निर्माण के लिए वैचारिक रूप से सीधा मार्ग यह है कि मस्तिष्क के पीछे कम्प्यूटेशनल सिद्धांतों को अभियांत्रिक बनाया जाए और उसकी कार्यक्षमता को दोहराया जाए ।</s>
एक और 13 अध्याय 1 .</s>
इंट्रोड्यूसिंग परिप्रेक्ष्य यह है कि मस्तिष्क और मानव बुद्धि का आकलन करने वाले सिद्धांतों को समझना अत्यंत रोचक होगा , इसलिए ऐसे मशीनी शिक्षण मॉडल जो इन बुनियादी वैज्ञानिक प्रश्नों पर प्रकाश डालते हैं , इंजीनियरी अनुप्रयोगों को हल करने की उनकी क्षमता के अलावा उपयोगी होंगे ।</s>
आधुनिक शब्द “दीप्त शिक्षण” मशीन शिक्षण मॉडल की वर्तमान नस्ल पर तंत्रिका विज्ञानी परिप्रेक्ष्य से परे चला जाता है .</s>
यह रचना के एकाधिक स्तरों सीखने के एक अधिक सामान्य सिद्धांत के लिए अपील करता है , जो मशीन सीखने के ढांचे में लागू किया जा सकता है कि आवश्यक रूप से तंत्रिका प्रेरित नहीं हैं .</s>
आधुनिक गहन अध्ययन के सबसे पहले पूर्ववर्तियों सरल रैखिक मॉडल एक तंत्रिका विज्ञानी परिप्रेक्ष्य से प्रेरित थे .</s>
इन मॉडलों n इनपुट मूल्यों का एक सेट x 1 , 999 लेने के लिए डिज़ाइन किया गया था .</s>
1 . 778 . 793 शब्द 1 . 893 . 893 उच्चारण 306भाषा</s>
X n और उन्हें एक आउटपुट वाई के साथ सहयोगी ।</s>
इन मॉडलों को 1 , 499 पर वजन का एक सेट सीखना होगा .</s>
1 . 778 . 793 शब्द 1 . 893 . 893 उच्चारण 306भाषा</s>
, w n और अपने आउटपुट एफ की गणना ( x , w ) =</s>
</s>
+ ·· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·</s>
w . n .</s>
तंत्रिका नेटवर्क अनुसंधान की इस पहली लहर को साइबरनेटिक्स के नाम से जाना जाता था ।</s>
मैकलॉच - पिट्स न्यूरॉन ( मैकक्लोच और पिट्स ) , 1943 मस्तिष्क की कार्यप्रणाली का प्रारंभिक मॉडल था ।</s>
इस रैखिक मॉडल आदानों के दो अलग - अलग वर्गों को पहचान सकता है परीक्षण के द्वारा कि क्या एफ ( एक्स , डब्ल्यू ) सकारात्मक है या नकारात्मक .</s>
बेशक , श्रेणियों के वांछित परिभाषा के अनुरूप करने के लिए मॉडल के लिए , सही ढंग से सेट किया जा करने के लिए आवश्यक वजन .</s>
इन वजन मानव ऑपरेटर द्वारा सेट किया जा सकता है .</s>
1950 के दशक में , पेसेप्टॉन ( Rosenblatt , 1958 , 1962 ) पहला मॉडल बन गया जो प्रत्येक श्रेणी से इनपुट के उदाहरण प्रदान की श्रेणियों को परिभाषित करने वाले वजन को सीख सका .</s>
अनुकूली रैखिक तत्व ( ADALINE ) , जो लगभग उसी समय से तिथियाँ करता था , बस च का मान वापस कर दिया .</s>
( x ) स्वयं वास्तविक संख्या की भविष्यवाणी करना (Widrow and Hoff , 1960 ) और इन संख्याओं की भविष्यवाणी करना भी डेटा से सीख सकता है ।</s>
इन सरल शिक्षण एल्गोरिदम ने मा - चाइन लर्निंग के आधुनिक परिदृश्य को बहुत प्रभावित किया ।</s>
प्रशिक्षण एल्गोरिथ्म एडीएएलआई के वजन को अनुकूलित करने के लिए इस्तेमाल किया</s>
स्टोचिस्टिक प्रवणता नामक एल्गोरिथ्म का एक विशेष मामला था ।</s>
स्टोचास्टिक ढाल अवरोहण एल्गोरिथ्म के थोड़ा संशोधित संस्करण आज गहन शिक्षण मॉडलों के लिए प्रमुख प्रशिक्षण एल्गोरिदम बने हुए हैं ।</s>
पेसेप्टॉन और ADALINE द्वारा प्रयुक्त f ( x , w ) पर आधारित मॉडलों को रैखिक मॉडल कहा जाता है ।</s>
इन मॉडलों में कुछ सबसे व्यापक रूप से इस्तेमाल किया मशीन सीखने मॉडल बने हुए हैं , हालांकि कई मामलों में उन्हें मूल मॉडलों से अलग तरीके से प्रशिक्षित किया जाता है .</s>
रेखीय माडलों की अनेक सीमाएं होती हैं ।</s>
अधिक - से - अधिक प्रसिद्ध , और अधिक - से - अधिक प्रचलित , ध् ।</s>
= 1 और च (</s>
</s>
= 1 लेकिन च (</s>
1 , 1 )</s>
= च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; तथा च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ; च ;
जिन आलोचकों ने रैखिक मॉडलों में इन खामियों का पालन किया , उन्होंने सामान्य रूप से जैविक प्रेरित शिक्षण ( Minsky and Redt , 1969 ) के खिलाफ एक पृष्ठभूमि तैयार की .</s>
यह तंत्रिका नेटवर्क की लोकप्रियता की पहली बड़ी डुबकी थी ।</s>
14 अध्याय 1</s>
इंट्राऑडयूसीशन आज , तंत्रिका विज्ञान को गहन अध्ययन करने वाले अनुसंधानकर्ताओं के लिए एक महत्वपूर्ण प्रेरणा स्रोत माना जाता है , परंतु अब यह क्षेत्र का प्रमुख मार्गदर्शक नहीं है ।</s>
आज गहन शिक्षण अनुसंधान में तंत्रिका विज्ञान की घटती भूमिका का मुख्य कारण यह है कि हम बस एक गाइड के रूप में इसका उपयोग करने के लिए मस्तिष्क के बारे में पर्याप्त जानकारी नहीं है .</s>
मस्तिष्क द्वारा प्रयुक्त वास्तविक एल्गोरिदम की गहरी समझ प्राप्त करने के लिए , हम की गतिविधि पर नजर रखने के लिए सक्षम होना करने की आवश्यकता होगी ( कि एक साथ जुड़े हजारों अंतर्संबंधित न्यूरॉन्स के बहुत कम हजारों की .</s>
क्योंकि हम ऐसा नहीं कर पा रहे हैं , हम मस्तिष्क के कुछ अत्यंत सरल और सुशिक्षित भागों को भी नहीं समझ पा रहे हैं ( Olशसन एंड फील्ड , 2005 ) .</s>
तंत्रिका विज्ञान ने हमें आशा करने का एक कारण दिया है कि एक एकल गहन शिक्षण एल्गोरिथ्म कई अलग अलग कार्यों को हल कर सकता है ।</s>
तंत्रिका विज्ञानियों ने पाया है कि यदि उनके मस्तिष्क को दृश्य संकेतों को उस क्षेत्र में भेजने के लिए पुनर्नवा किया जाता है तो उनके मस्तिष्क के श्रवण संसाधन क्षेत्र के साथ निषेचन करना सीख सकते हैं ( वोन मेल्कर एट अल , 2000 ) ।</s>
यह सुझाव है कि स्तनधारी मस्तिष्क का बहुत कुछ अलग कार्यों के अधिकांश को हल करने के लिए एक एकल एल्गोरिथ्म का उपयोग कर सकता है कि मस्तिष्क हल करता है .</s>
इस परिकल्पना से पहले , मशीनी अधिगम अनुसंधान अधिक विखंडित था , प्राकृतिक भाषा संसाधन , दृष्टि , गति नियोजन और भाषण मान्यता का अध्ययन कर रहे शोधकर्ताओं के विभिन्न समुदायों के साथ .</s>
आज , इन आवेदन समुदायों अभी भी अलग है , लेकिन यह गहरी सीखने अनुसंधान समूहों के लिए एक साथ कई या यहाँ तक कि इन सभी आवेदन क्षेत्रों का अध्ययन करने के लिए आम है .</s>
हम तंत्रिका विज्ञान से कुछ रूखे दिशा - निर्देश बनाने में सक्षम हैं ।</s>
कई कम्प्यूटेशनल इकाइयों है कि केवल एक दूसरे के साथ उनके बातचीत के माध्यम से बुद्धिमान बन का मूल विचार मस्तिष्क से प्रेरित है .</s>
नियोनिट्रॉन</s>
अधिकांश तंत्रिका नेटवर्क आज परिशोधित रैखिक इकाई नामक मॉडल न्यूरॉन पर आधारित हैं .</s>
मूल संज्ञानित्र ( एफुकुशिमा , 1975 ) ने एक अधिक जटिल संस्करण प्रस्तुत किया जो मस्तिष्क कार्यप्रणाली के हमारे ज्ञान से अत्यधिक प्रेरित था ।</s>
नायर और हिंटन (2010 ) और ग्लोोटा एट अल के साथ कई दृष्टिकोणों से सरलीकृत आधुनिक संस्करण विकसित किया गया .</s>
(2011 ) तंत्रिका विज्ञान को एक प्रभाव के रूप में उद्धृत करते हुए , और ज्रेट एट अल .</s>
(2009 - अधिक इंजीनियरी उन्मुख प्रभावों का हवाला देते हुए ।</s>
जबकि तंत्रिका विज्ञान प्रेरणा का एक महत्वपूर्ण स्रोत है , यह एक कठोर गाइड के रूप में लेने की जरूरत नहीं है .</s>
हम जानते हैं कि वास्तविक न्यूरॉन्स आधुनिक संशोधित रैखिक इकाइयों की तुलना में बहुत अलग कार्यों की गणना करते हैं , लेकिन अधिक तंत्रिका यथार्थवाद अभी तक मशीन सीखने के प्रदर्शन में सुधार का नेतृत्व नहीं किया है .</s>
इसके अलावा , जबकि तंत्रिका विज्ञान सफलतापूर्वक कई तंत्रिका नेटवर्क वास्तुकलाओं को प्रेरित किया है , हम अभी तक तंत्रिका विज्ञान के लिए जैविक सीखने के बारे में पर्याप्त जानकारी नहीं है कि हम इन वास्तुकलाओं को प्रशिक्षित करने के लिए इस्तेमाल सीखने एल्गोरिदम के लिए बहुत मार्गदर्शन की पेशकश .</s>
15 अध्याय 1 .</s>
इंट्राऑडयूसीशन मीडिया खातों में अक्सर मस्तिष्क के लिए गहरे शिक्षण की समानता पर जोर दिया जाता है ।</s>
जबकि यह सच है कि गहन अध्ययन करने वाले शोधकर्ताओं के मस्तिष्क को अन्य मशीन सीखने के क्षेत्रों जैसे कर्नेल मशीन या बैसीशियन सांख्यिकी में काम करने वाले शोधकर्ताओं की तुलना में अधिक प्रभाव के रूप में उद्धृत करने की संभावना होती है , व्यक्ति को गहन अध्ययन को मस्तिष्क अनुकरण करने के प्रयास के रूप में नहीं देखना चाहिए ।</s>
आधुनिक गहन शिक्षण कई क्षेत्रों , विशेष रूप से लागू गणित बुनियादी जैसे रैखिक बीजगणित , संभावना , सूचना सिद्धांत , और संख्यात्मक अनुकूलन से प्रेरणा ग्रहण करता है .</s>
जबकि कुछ गहन शिक्षण शोधकर्ताओं तंत्रिका विज्ञान को प्रेरणा का एक महत्वपूर्ण स्रोत के रूप में उद्धृत करते हैं , दूसरों को तंत्रिका विज्ञान के साथ बिल्कुल भी चिंतित नहीं है .</s>
यह ध्यान देने योग्य है कि मस्तिष्क कैसे एल्गोरिथम स्तर पर काम करता है समझने की कोशिश जीवित और अच्छी तरह से है .</s>
इस प्रयास को मुख्य रूप से “ निर्विवाद तंत्रिका विज्ञान” के रूप में जाना जाता है और गहन अध्ययन से एक अलग क्षेत्र है .</s>
शोधकर्ताओं के लिए दोनों क्षेत्रों के बीच आगे - पीछे जाना आम बात है ।</s>
गहन अधिगम का क्षेत्र मुख्य रूप से इस बात से संबंधित है कि बुद्धि की आवश्यकता वाले कार्यों को सफलतापूर्वक हल करने में सक्षम कंप्यूटर सिस्टम का निर्माण कैसे किया जाए , जबकि संगणकीय तंत्रिका विज्ञान का क्षेत्र मुख्य रूप से इस बात से संबंधित है कि मस्तिष्क वास्तव में कैसे कार्य करता है के अधिक सही मॉडलों का निर्माण कैसे किया जाए ।</s>
1980 के दशक में , तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर कनेक्शनवाद , या समानांतर वितरण प्रक्रिया के माध्यम से एक आंदोलन के माध्यम से महान भाग में उभरा -</s>
</s>
मेक्सक्ललैंड एट अल . , 1995</s>
संज्ञानात्मक विज्ञान के संदर्भ में कनेक्शनवाद का उदय हुआ ।</s>
संज्ञानात्मक विज्ञान मन को समझने के लिए एक अंतर्विधात्मक दृष्टिकोण है , जिसमें विश्लेषण के कई अलग - अलग स्तरों को संयोजित किया जाता है .</s>
1980 के दशक के प्रारंभ में , अधिकांश संज्ञानात्मक वैज्ञानिकों ने प्रतीकात्मक तर्क के मॉडलों का अध्ययन किया .</s>
उनकी लोकप्रियता के बावजूद , प्रतीकात्मक मॉडल कैसे मस्तिष्क वास्तव में न्यूरॉन्स का उपयोग कर उन्हें लागू कर सकता है के संदर्भ में व्याख्या करने के लिए मुश्किल थे .</s>
कनेक्शन वालों ने संज्ञान के मॉडलों का अध्ययन करना शुरू किया जो वास्तव में तंत्रिका क्रियान्वयन में आधारित हो सकते हैं ( टौरेट्ज़की और मिंटन , 1985 ) , 1940 के दशक में मनोविज्ञानी डोनाल्ड हेब के काम के लिए कई विचारों को पुनर्जीवित कर रहा था .</s>
कनेक्शनवाद में केंद्रीय विचार है कि सरल कम्प्यूटेशनल इकाइयों की एक बड़ी संख्या में एक साथ नेटवर्क करने पर बुद्धिमान व्यवहार प्राप्त कर सकते हैं .</s>
यह अंतर्दृष्टि जैविक तंत्रिका प्रणालियों में न्यूरॉन्स के लिए समान रूप से लागू होता है , जैसा कि यह कम्प्यूटेशनल मॉडल में छिपा इकाइयों के लिए करता है .</s>
1980 के दशक के कनेक्शनवाद आंदोलन के दौरान कई प्रमुख अवधारणाएं उभरी जो आज के गहन अध्ययन का केंद्र बनी हुई हैं ।</s>
इनमें से एक अवधारणा वितरित प्रतिनिधित्व की है ( हैटन एट अल , 1986 ) .</s>
यह एक विचार है कि एक सिस्टम के प्रत्येक इनपुट को कई विशेषताओं द्वारा प्रतिनिधित्व किया जाना चाहिए , और प्रत्येक विशेषता को कई संभावित इनपुट के प्रतिनिधित्व में शामिल किया जाना चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हमारे पास एक दृष्टि प्रणाली है जो 16 अध्याय 1 को पहचान सकती है ।</s>
InTRODUCTION कारें , ट्रक , और पक्षी , और ये चीज़ें हर लाल , हरा , या नीला हो सकती हैं ।</s>
इन आदानों का प्रतिनिधित्व करने का एक तरीका एक अलग न्यूरॉन या छिपा इकाई है कि नौ संभावित संयोजनों में से प्रत्येक के लिए सक्रिय करता हैः लाल ट्रक , लाल कार , लाल पक्षी , हरे ट्रक , और इतने पर .</s>
यह नौ अलग न्यूरॉन्स की आवश्यकता है , और प्रत्येक न्यूरॉन स्वतंत्र रूप से रंग और वस्तु पहचान की अवधारणा सीखना होगा .</s>
इस स्थिति पर सुधार करने का एक तरीका है एक वितरित प्रतिनिधित्व का उपयोग करने के लिए , तीन न्यूरॉन्स के साथ रंग का वर्णन और तीन न्यूरॉन्स ऑब्जेक्ट पहचान का वर्णन .</s>
इसके लिए नौ के बजाय केवल छह न्यूरॉन्स कुल की आवश्यकता होती है , और लालिमा का वर्णन करने वाले न्यूरॉन कारों , ट्रकों और पक्षियों की छवियों से लालिमा के बारे में सीखने में सक्षम है , न केवल वस्तुओं की एक विशिष्ट श्रेणी की छवियों से .</s>
वितरित प्रतिनिधित्व की अवधारणा इस पुस्तक के लिए केंद्रीय है और अध्याय 15 में अधिक विस्तार से वर्णित है .</s>
कनेक्शनवादी आंदोलन का एक अन्य प्रमुख परिणाम यह हुआ कि इसने आंतरिक पुनः प्रचार के साथ तंत्रिका नेटवर्क को प्रशिक्षित करने के लिए बैक प्रोपोग्रेशन एल्गोरिदम का सफल प्रयोग किया ।</s>
यह एल्गोरिथ्म लोकप्रियता में घटती - बढ़ती है , लेकिन इस लेखन के रूप में गहन मॉडलों के प्रशिक्षण का प्रमुख दृष्टिकोण है ।</s>
1990 के दशक के दौरान , शोधकर्ताओं ने तंत्रिका नेटवर्क के साथ मॉडलिंग अनुक्रम में महत्वपूर्ण अग्रिम किया .</s>
हॉकरेइटर ( १९९१ और बेंजिओ एट अल )</s>
(1994 ) लंबे अनुक्रम मॉडलिंग में कुछ मूलभूत गणितीय कठिनाइयों की पहचान की , जो अनुभाग 10 .7 में वर्णित है .</s>
हॉकर और शम्मीदबर (1997 ) ने इनमें से कुछ कठिनाइयों को दूर करने के लिए लघु - स्मृति नेटवर्क शुरू किया ।</s>
आज , एलएसटीएम व्यापक रूप से कई अनुक्रम मॉडलिंग कार्यों के लिए प्रयोग किया जाता है , गूगल में कई प्राकृतिक भाषा प्रसंस्करण कार्यों सहित .</s>
तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर १९९० दशक के मध्य तक चली ।</s>
निवेश की मांग करते समय वेन - ट्यूनर , न्यूरल नेटवर्क और अन्य एआइ तकनीक के आधार पर अयथार्थवादी - सुज्हव देने लगे .</s>
जब एआई अनुसंधान इन अनुचित अपेक्षाओं को पूरा नहीं किया , निवेशकों को निराश थे .</s>
इसके साथ ही मशीनी शिक्षा के अन्य क्षेत्रों में भी प्रगति हुई ।</s>
१९९९ में ऍन्टर डैन कई कार्यों को सामने लाया और कई कार्यों को सामने लाया । इसके अलावा कई अन्य कई कार्यों को करने के लिए इथियोपियाई मॉडलों और मॉडलों को १९९९ में उठाया ।</s>
इन दो कारणों से तंत्रिका नेटवर्कों की लोकप्रियता में गिरावट आई जो 2007 तक चली ।</s>
इस दौरान , तंत्रिका नेटवर्क कुछ कार्यों पर प्रभावशाली प्रदर्शन प्राप्त करते रहे (</s>
लेकुन एट अल</s>
↑ अल .</s>
कनाडा के उन्नत अनुसंधान संस्थान ने अपने तंत्रिका संधान तथा अनुकूलन अनुसंधान पहल के माध्यम से तंत्रिका नेटवर्क अनुसंधान को जीवित रखने में मदद की ।</s>
इस कार्यक्रम के टोरंटो के विश्वविद्यालय , मॉन्ट्रियल में योशिआ बेंजियो , और न्यूयॉर्क विश्वविद्यालय में यिन लेकून के नेतृत्व में मशीन सीखने अनुसंधान समूहों .</s>
बहुविध संकाय एनसीएपी अनुसंधान पहल 17 अध्याय 1 .</s>
इनTRODUCKKCharselect unicode block name</s>
इस बिंदु पर , डीप नेटवर्क आम तौर पर माना जाता था कि प्रशिक्षण के लिए बहुत मुश्किल है .</s>
हम अब जानते हैं कि एल्गोरिदम कि 1980 के दशक के बाद से अस्तित्व में है काफी अच्छी तरह से काम करते हैं , लेकिन यह जाहिरा तौर पर नहीं था 2006 .</s>
मुद्दा शायद बस यह है कि इन एल्गोरिदम उस समय उपलब्ध हार्डवेयर के साथ बहुत प्रयोग की अनुमति देने के लिए बहुत अधिक कम्प्यूटेशनल रूप से महंगे थे .</s>
तंत्रिका नेटवर्क अनुसंधान की तीसरी लहर 2006 में एक सफलता के साथ शुरू हुई ।</s>
जिओफ्री हाइटन ने दिखाया कि गहन विश्वास नेटवर्क नामक एक प्रकार के तंत्रिका नेटवर्क को लालची पर्तवार पूर्वसंरचना ( हीटन एट अल , 2006 ) नामक व्यूह का प्रयोग करके कुशलतापूर्वक प्रशिक्षित किया जा सकता है , जिसका वर्णन हम अधिक विस्तार से 151 में करते हैं ।</s>
अन्य प्रभावकारी अनुसंधान समूहों ने शीघ्रता से वही कार्यनीति दर्शाई ताकि परीक्षण के आधार पर परीक्षण में सुधार लाने में सहायता प्राप्त करने के लिए अनेक अन्यान्य वैज्ञानिक अनुसंधान समूहों को भी प्रशिक्षित किया जा सके ।</s>
यह शब्द इंटरनेट पर इसलिए लोकप्रिय हुआ है कि शोधकर्ताओं ने पहले से ही न्यूट्रल नेटवर्क का प्रशिक्षण ले लिया है ।</s>
इस समय , गहन तंत्रिका नेटवर्क अन्य मशीनी शिक्षण प्रौद्योगिकियों के साथ - साथ हाथ की ओर इशारा कार्यक्षमता पर आधारित एआई प्रणालियों को आउटर दिया .</s>
तंत्रिका नेटवर्क की लोकप्रियता की यह तीसरी लहर इस लेखन के समय तक जारी है , हालांकि गहन शिक्षण अनुसंधान का ध्यान इस लहर के समय के भीतर नाटकीय रूप से बदल गया है .</s>
तीसरी लहर की शुरुआत नई अप्रशिक्षित अधिगम तकनीकों और छोटे डेटासेटों से अच्छे को सामान्य बनाने के लिए गहरे मॉडलों की क्षमता पर ध्यान केंद्रित करने के साथ हुई थी , लेकिन आज काफी पुराने पर्यवेक्षित अधिगम एल्गोरिदम और बड़े लेबल किए गए डेटासेटों को जुटाने में गहन मॉडलों की क्षमता में अधिक रुचि है ।</s>
1 . 2 . 2</s>
बढ़ते डाटासेट आकार</s>
कोई भी यह सोच सकता है कि हाल ही में गहन अध्ययन को महत्वपूर्ण प्रौद्योगिकी के रूप में क्यों स्वीकार किया गया है यद्यपि 1950 के दशक में कृत्रिम तंत्रिका नेटवर्कों के साथ पहला प्रयोग किया गया था ।</s>
गहन अधिगम 1990 के दशक से वाणिज्यिक अनुप्रयोगों में सफलतापूर्वक इस्तेमाल किया गया है , लेकिन अक्सर एक प्रौद्योगिकी और कुछ है कि केवल एक विशेषज्ञ का उपयोग कर सकता है की तुलना में एक कला के अधिक माना जाता था , हाल ही तक .</s>
यह सच है कि एक गहरी सीखने एल्गोरिथ्म से अच्छा प्रदर्शन प्राप्त करने के लिए कुछ कौशल की आवश्यकता है .</s>
सौभाग्य से , आवश्यक कौशल की मात्रा को कम कर देता है के रूप में प्रशिक्षण डेटा की मात्रा बढ़ जाती है .</s>
आज जटिल कार्यों पर मानव प्रदर्शन तक पहुँचने सीखने एल्गोरिदम लगभग सीखने एल्गोरिदम है कि 1980 के दशक में खिलौना समस्याओं को हल करने के लिए संघर्ष किया , हालांकि मॉडल हम इन एल्गोरिदम के साथ 18 अध्याय 1 है .</s>
इनट्रोडयूसीशन में ऐसे परिवर्तन हुए जो बहुत गहरे आर्किटेक्चर के प्रशिक्षण को सरल बनाते हैं ।</s>
सबसे महत्वपूर्ण नया विकास है कि आज हम इन एल्गोरिदम संसाधनों वे सफल होने की जरूरत के साथ प्रदान कर सकते हैं .</s>
चित्र 1 . 8 दिखाता है कि कैसे बेंचमार्क डेटासेटों के आकार का समय के साथ उल्लेखनीय विस्तार हुआ है ।</s>
यह प्रवृत्ति समाज के बढ़ते डिजिटाइजेशन से प्रेरित है ।</s>
जैसे - जैसे हमारी ज्यादा से ज्यादा गतिविधियां कंप्यूटर पर होती हैं , ज्यादा से ज्यादा जो हम करते हैं वह दर्ज हो जाता है ।</s>
चूंकि हमारे कंप्यूटर लगातार एक साथ नेटवर्क में लगे रहते हैं , इसलिए इन रिकॉर्डों को केंद्रीकृत करना और उन्हें मशीनी सीखने के अनुप्रयोगों के लिए उपयुक्त डेटासेट में रूपांतरित करना आसान हो जाता है ।</s>
“बिग डेटा” १९०० १९५० १९५० १९८५ २००० वर्ष १० ० १० १ १० २ १० १० १० १० ३ १० ४ १० १० १० ५ १० १० ७ १० ८ १० ९ डाटासेट साइज़ ( अनगिनत उदाहरण )</s>
आईरिस एमनैट</s>
सार्वजनिक SVHN छविNet</s>
सीआईएफएआर - १०</s>
इमेजनेट10क आईएलएसवीआरसी 2014 स्पोर्ट्स - 1</s>
</s>
समय के साथ डेटासेट आकार बढ़ाना .</s>
में शुरू में मैन्युअल रूप से मैन्युअल रूप से मैन्युअल रूप से तैयार किया डेटा का अध्ययन किया , ढेर सारा संकलन 1908 संकलन , ढेर सारे माप का अध्ययन किया , सांख्यिकीय माप के साथ 1935 में सांख्यिकीविद् या सांख्यिकीय माप के प्रयोग से 1935 में सैकड़ों</s>
के दशक में 1980 के दशक में , जैविक प्रेरित मशीन सीखने के अग्रणी अक्सर छोटे सिंथेटिक डाटासेट के साथ काम किया , जैसे कि अक्षरों की लो - रिडसॉल्ट बिटमैप , लो - कम्प्यूटेशनल लागत उठाने के लिए डिज़ाइन किया गया और यह प्रदर्शित किया गया कि न्यूरल नेटवर्क प्रकार के प्रकार सीखने में सक्षम थे .</s>
वर्ष 1980 और 1990 के दशक में मशीनी अधिगम अधिक सांख्यिकीय बन गया और हजारों उदाहरणों के दसियों से युक्त बड़े डेटासेटों को बढ़ाना शुरू किया , जैसे कि MNIST डेटासेट ( गिनती में हस्तलिखित संख्या के 19 के आंकड़े ) , 1998</s>
2000 के दशक के पहले दशक में इसी आकार के अधिक परिष्कृत डेटासेट , जैसे कि सीआईएफएआर - 10 डेटासेट ( Krizhevsky और Hinton , 2009 ) का उत्पादन जारी रहा .</s>
उस दशक के अंत में और 2010 के दशक के पूर्वार्ध में , उल्लेखनीय रूप से बड़े डेटासेट , लाखों उदाहरणों के सैकड़ों हजारों से दसियों , पूरी तरह से बदल गया था कि गहन अध्ययन के साथ क्या संभव था .</s>
</s>
एम डाटासेट</s>
व्यापक</s>
ग्राफ के शीर्ष पर , हम देख रहे हैं कि आईबीएम के अनुवादित वाक्यों के डेटासेट , जैसे कि कनाडा के हेन्सर्ड ( बॉउन एट अल , 1990 और डब्ल्यूएमटी 2014 इंग्लिश से फ्रेंच डेटासेट ( schwenkset , 2014 ) , आमतौर पर अन्य डेटासेट से काफी आगे हैं ।</s>
19 अध्याय 1 .</s>
परिचय चित्र 1 . 9</s>
MNIST डेटासेट से उदाहरण इनपुट</s>
राष्ट्रीय मानक और प्रौद्योगिकी संस्थान , एजेंसी है कि मूल रूप से इस डेटा को इकट्ठा किया .</s>
चूंकि मशीन सीखने एल्गोरिदम के साथ डेटा आसानी से उपयोग के लिए preprocess किया गया है “M” “moded ,” के लिए खड़ा है .</s>
MNIST डेटासेट में हस्तलिखित अंकों तथा संबद्ध लेबलों के स्कैन होते हैं , जो बताता है कि प्रत्येक छवि में 0 - 9 किस अंक को समाहित किया गया है ।</s>
इस सरल वर्गीकरण समस्या गहरी सीखने अनुसंधान में सबसे सरल और सबसे व्यापक रूप से इस्तेमाल परीक्षणों में से एक है .</s>
यह आधुनिक तकनीकों को हल करने के लिए काफी आसान होने के बावजूद लोकप्रिय बना हुआ है .</s>
जेफ्री हिल्टन ने इसे मशीन लर्निंग का नीरस कहा है , जिसका अर्थ है कि यह मशीन लर्निंग शोधकर्ताओं को नियंत्रित प्रयोगशाला परिस्थितियों में उनके एल्गोरिदम का अध्ययन करने में सक्षम बनाता है , जैसे कि जीवविज्ञानी अक्सर फल मक्खियों का अध्ययन करते हैं ।</s>
मशीनी शिक्षा को और अधिक आसान बना दिया है क्योंकि सांख्यिकीय आकलन का प्रमुख भार - आंकड़ों की एक छोटी राशि का अवलोकन करने के बाद - काफी हद तक हल्का हो गया है ।</s>
2016 के रूप में , अंगूठे का एक मोटा नियम है कि एक पर्यवेक्षित गहरी सीखने एल्गोरिथ्म आम तौर पर प्रति वर्ग के लगभग 5 , 000 लेबल उदाहरण के साथ स्वीकार्य प्रदर्शन प्राप्त होगा और मेल या 20 अध्याय 1 .</s>
आईटीआरओडीयूसी मानव कार्य निष्पादन से अधिक है जब एक डेटासेट के साथ प्रशिक्षित किया जाता है जिसमें कम से कम 10 मिलियन लेबल वाले उदाहरण होते हैं .</s>
डेटासेट से छोटे डेटासेट के साथ सफलतापूर्वक कार्य करना एक महत्वपूर्ण अनुसंधान क्षेत्र है , विशेष रूप से इस बात पर ध्यान केंद्रित करते हुए कि हम कैसे बड़ी मात्रा में अविश्वसनीय उदाहरणों का लाभ उठा सकते हैं , जिसमें अप्रशिक्षित या अर्द्धसंयोजित अधिगम होता है .</s>
1 . 2 . 3</s>
बढ़ती मॉडल आकार एक और प्रमुख कारण है कि तंत्रिका नेटवर्क आज अपेक्षाकृत कम सफलता का आनंद लेने के बाद बेतहाशा सफल रहे हैं 1980 के दशक से है कि हम आज बहुत बड़े मॉडल चलाने के लिए कम्प्यूटेशनल संसाधनों है .</s>
कनेक्शन - इस्म की एक मुख्य अंतर्दृष्टि है कि जानवर बुद्धिमान हो जाते हैं जब उनके कई न्यूरॉन्स एक साथ काम करते हैं .</s>
एक व्यक्ति न्यूरॉन या न्यूरॉन्स का छोटा संग्रह विशेष रूप से उपयोगी नहीं है .</s>
जैविक न्यूरॉन्स विशेष रूप से सघन रूप से जुड़े नहीं हैं .</s>
जैसा कि आंकड़ा 1 . 10 में देखा गया है , हमारे मशीनी शिक्षण मॉडलों में कई दशकों से स्तनधारियों के मस्तिष्क तक के परिमाण के एक वर्ग के भीतर न्यूरॉन प्रति कनेक्शन रहे हैं ।</s>
न्यूरॉन्स की कुल संख्या के संदर्भ में , तंत्रिका नेटवर्क आश्चर्यजनक रूप से काफी हाल ही में छोटे रहे हैं , जैसा कि आंकड़ा 1 . 11 में दिखाया गया है ।</s>
प्रच्छन्न इकाइयों के आरंभ होने के बाद से कृत्रिम तंत्रिका नेटवर्कों का आकार लगभग हर 2 . 4 वर्ष में दुगुना हो गया है ।</s>
यह वृद्धि अधिक स्मृति वाले तीव्र कंप्यूटरों तथा बड़े डेटासेटों की उपलब्धता से संचालित होती है ।</s>
बड़े नेटवर्क अधिक जटिल कार्यों पर उच्च सटीकता प्राप्त करने में सक्षम हैं .</s>
ऐसा लगता है कि यह प्रवृत्ति दशकों तक बनी रहेगी ।</s>
जब तक नई प्रौद्योगिकियां तेजी से स्केलिंग सक्षम नहीं करतीं , कृत्रिम तंत्रिका नेटवर्कों में कम से कम 2050 के दशक तक न्यूरॉन्स की संख्या उतनी नहीं होगी ।</s>
जैविक न्यूरॉन्स वर्तमान कृत्रिम न्यूरॉन्स की तुलना में अधिक जटिल कार्यों का प्रतिनिधित्व कर सकते हैं , इसलिए जैविक तंत्रिका नेटवर्क इस साजिश चित्रण से भी बड़ा हो सकता है .</s>
आत्मनिरीक्षण में , यह विशेष आश्चर्य की बात नहीं है कि जोंक की तुलना में कम न्यूरॉन्स वाले तंत्रिका नेटवर्क परिष्कृत कृत्रिम बुद्धिमत्ता प्रोब - लेम को हल करने में असमर्थ थे .</s>
आज के नेटवर्क , जिन्हें हम कम्प्यूटेशनल सिस्टम की दृष्टि से काफी बड़े मानते हैं , मेंढ़क जैसे अपेक्षाकृत आदिम कशेरुकी जंतुओं के तंत्रिका तंत्र से भी छोटे हैं ।</s>
समय के साथ मॉडल के आकार में वृद्धि , तेजी से सीपीयू की उपलब्धता के कारण , सामान्य उद्देश्य GPUs ( धारा 12 . 1 . 2 में निहित ) के आगमन , तीव्र नेटवर्क कनेक्टिविटी और वितरित कंप्यूटिंग के लिए बेहतर सॉफ्टवेयर मूल संरचना , गहन शिक्षण के इतिहास में सबसे महत्वपूर्ण रुझानों में से एक है .</s>
यह प्रवृत्ति आमतौर पर भविष्य में भी अच्छी तरह जारी रहने की उम्मीद है ।</s>
21 अध्याय 1</s>
सूचना प्रौद्योगिकी 1950 1985 2000 2015 वर्ष 10 1 10 2 10 3 10 10 4 कनेक्शन प्रति न्यूरॉन 1 2 3 4 5 6 7 8 9 10 फल मक्खी माउस बिल्ली मानव चित्र 1 . 10</s>
: समय के साथ प्रति न्यूरॉन कनेक्शन की संख्या .</s>
प्रारंभ में , कृत्रिम तंत्रिका नेटवर्कों में न्यूरॉन्स के बीच होने वाले संकुचन की संख्या हार्डवेयर क्षमताओं द्वारा सीमित थी .</s>
आज , न्यूरॉन्स के बीच कनेक्शन की संख्या ज्यादातर एक डिजाइन विचार है .</s>
कुछ कृत्रिम तंत्रिका नेटवर्कों में एक बिल्ली के रूप में न्यूरॉन पर लगभग कई कनेक्शन है , और यह अन्य तंत्रिका नेटवर्क के लिए काफी आम है चूहों की तरह छोटे स्तनधारियों के रूप में न्यूरॉन प्रति कनेक्शन है .</s>
यहां तक कि मानव मस्तिष्क में प्रति न्यूरॉन कनेक्शन की अत्यधिक मात्रा नहीं होती है .</s>
विकीपीडिया से जैवीय तंत्रिका नेटवर्क आकार (2015 )</s>
१ .</s>
अनुकूली रैखिक तत्व (Widro and Hoff , 1960 2 .</s>
नियोकॉग्नीट्रान ( एफुकुशिमा ) १९८० ३</s>
Comment</s>
कोलैपिला एट अल . , २००६ ४ .</s>
डीप बोल्त्समैन मशीन (Salakhutdinov और हिंटन , 2009 ) 5 .</s>
अविकसित संवलनीय नेटवर्क (</s>
जेरेट एट अल . , 2009 6 .</s>
Comment</s>
( Ciresan एट अल )</s>
२०१० वर्ष २०१० का २०१० वर्ष २०१० का २०१० वर्ष</s>
वितरित autoencoder (</s>
ली एट अल</s>
अन्तिम परिवर्तन २०१२</s>
बहु - जीपीयू संवलन नेटवर्क (</s>
किरिज़ेव्स्की</s>
२०१२ इ० , ९</s>
सीओटीएस एचपीसी अविकसित संवलनीय नेटवर्क (</s>
कोट्स एट अल , 2013</s>
गूगलनेट</s>
1 . 2 . 4</s>
बढ़ती परिशुद्धता , जटिलता और वास्तविक - वर्ल्ड प्रभाव</s>
1980 के दशक के बाद से , गहरी सीखने लगातार सटीक मान्यता और भविष्यवाणी प्रदान करने की क्षमता में सुधार हुआ है .</s>
इसके अलावा , गहरे सीखने लगातार सफलता के साथ अनुप्रयोगों के व्यापक और व्यापक सेट करने के लिए लागू किया गया है .</s>
सबसे पहले गहरे मॉडलों का उपयोग व्यक्तिगत वस्तुओं को कसकर फसली , अत्यंत छोटे चित्रों में पहचानने के लिए किया गया था (</s>
रेमेलहार्ट एट अल , 1986</s>
तब से वहाँ छवियों तंत्रिका नेटवर्क प्रक्रिया कर सकता है के आकार में क्रमिक वृद्धि हुई है .</s>
आधुनिक वस्तु अभिज्ञान नेटवर्क प्रक्रिया समृद्ध उच्च - निम्न फोटो युक्त और नहीं 22 अध्याय 1 .</s>
आगमन १९५० सन् २००० वर्ष २५६ वर्ष १० - २</s>
10 0 10 1 10 10 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10</s>
न्यूरॉन्स की 10 11 संख्या</s>
1</s>
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 स्पंज गोलकृमि ली ची ची ची ची ची ची ची ची ची चीं चीं फ्रक्टोपस मानव चित्र 111 : समय के साथ तंत्रिका नेटवर्क आकार</s>
प्रच्छन्न इकाइयों के आरंभ होने के बाद से कृत्रिम तंत्रिका नेटवर्कों का आकार लगभग हर 2 . 4 वर्ष में दुगुना हो गया है ।</s>
विकीपीडिया से जैवीय तंत्रिका नेटवर्क आकार (2015 )</s>
१ .</s>
पेसेप्टॉन ( १९५८ )</s>
अनुकूली रैखिक तत्व (Widro and Hoff , 1960 3 .</s>
नियोकॉग्नीट्रान ( एफुकुशिमा ) १९८० ४ .</s>
शीघ्र बैक - प्रोपोग्रेशन नेटवर्क</s>
भाषण पहचान के लिए पुनरावर्ती तंत्रिका नेटवर्क ( रिबिन्सन एंड फालसाइड ) , 1991</s>
6 .</s>
भाषण पहचान के लिए मल्टीलेयर परसेप्टरॉन ( Bengio एट अल )</s>
१९९१ - ७</s>
अर्थ फील्ड अवग्रहीत विश्वास नेटवर्क ( सॉल एट अल , 1996 )</s>
8 .</s>
लेनेट - ५</s>
Name</s>
१९९८ वाला</s>
इको स्टेट नेटवर्क ( जेजर एंड हास , 2004 )</s>
10</s>
गहरा विश्वास नेटवर्क</s>
Comment</s>
कोलैपिला वगैरह</s>
डीप बोल्ज़मैन मशीन</s>
GPU - ध्वनित गहरा विश्वास नेटवर्क (</s>
रैना एट अल</s>
↑ 2009 स्ट्रैटेजी ( एच . सी . ) 14 .</s>
अविकसित संवलनीय नेटवर्क (</s>
जेरेट एत अल , २००९ आ १५ .</s>
Comment</s>
( प्राचीन एट अल , २०१० )</s>
16 .</s>
ओएमपी - 1 नेटवर्क ( कोट्स एंड एनजी ) , 2011 17 .</s>
वितरित autoencoder (</s>
ले आयल</s>
बहु - जीपीयू संवलन नेटवर्क (</s>
किरिज़ेव्स्की</s>
वर्ष 2012 · जनवरी 2012 · जनवरी 2012 ·</s>
सीओटीएस एचपीसी अविकसित संवलनीय नेटवर्क (</s>
कोट्स एट अल , 2013</s>
गूगल - नेट</s>
इसी प्रकार , सबसे प्राचीन नेटवर्क केवल दो प्रकार की वस्तुओं को पहचान सकता था ( जैसे कि कुछ मामलों में , एक ही प्रकार की वस्तु की अनुपस्थिति या उपस्थिति , जबकि ये आधुनिक नेटवर्क आमतौर पर कम से कम 1 , 000 विभिन्न श्रेणियों की वस्तुओं को पहचानते हैं .</s>
वस्तु अभिज्ञान में सबसे बड़ी प्रतियोगिता इमेजनेट 23 अध्याय 1 है .</s>
परिचय</s>
प्रत्येक वर्ष बड़े पैमाने पर दृश्य अभिज्ञान चैलेंज आयोजित किया जाता है ।</s>
की एक नाटकीय वृद्धि हुई , गहन अध्ययन की जब एक संवलित नेटवर्क पहली बार के लिए इस चुनौती को जीत लिया और एक व्यापक मार्जिन से , नीचे ले आता है 26 . 1 प्रतिशत से 15 प्रतिशत</s>
तब से , ये प्रतियोगिताएं लगातार गहरे संवलित जालों द्वारा जीती जाती हैं , और इस लेखन के रूप में , गहन अध्ययन में अग्रिम ने इस प्रतियोगिता में नवीनतम टॉप - 5 त्रुटि दर को 3 . 6 प्रतिशत तक ला दिया है , जैसा कि अंक 1 . 12 में दर्शाया गया है ।</s>
गहन अध्ययन का वाचिक अभिज्ञान पर भी नाटकीय प्रभाव पड़ा है ।</s>
1990 के दशक भर में सुधार के बाद , भाषण मान्यता के लिए त्रुटि दरों के बारे में 2000 में शुरू रुका .</s>
नहीं</s>
हम इस इतिहास का अन्वेषण अनुभाग 12 . 3 में और अधिक विस्तार से करते हैं .</s>
डीप नेटवर्क को पैदल पता लगाने और प्रतिबिंब विभाजन के लिए भी शानदार सफलता मिली है ।</s>
align = “left” title</s>
ख़ते अल क़तर</s>
इसी के साथ डीप नेटवर्क के पैमाने और सटीकता में वृद्धि हुई है , 2010 2011 2012 2013 2014 2015 वर्ष 000 005 0 .10 015</s>
0 . 20 0 .25 0 .30 आईएलएसवीआरसी वर्गीकरण त्रुटि दर चित्र 1 . 12 : समय के साथ त्रुटि दर घटाना</s>
चूंकि डीप नेटवर्क इमेजनेट बड़े पैमाने पर दृश्य अभिज्ञान चैलेंज में प्रतिस्पर्धा करने के लिए आवश्यक पैमाने पर पहुँच गया , वे लगातार प्रतिवर्ष प्रतियोगिता जीता है , और प्रत्येक बार कम और कम त्रुटि दरों का उत्पादन किया है .</s>
रसकोव्स्की वगैरह का डेटा</s>
( २०१४ )</s>
और एट अल</s>
( १२०५ )</s>
24 अध्याय 1</s>
InTRODUCTION में कार्य की जटिलता है जिससे वे हल कर सकते हैं ।</s>
गुडफेलो एट अल .</s>
(2014 में प्रदर्शित किया गया कि न्यूरल नेटवर्क किसी छवि से प्रतिलेखन किए गए अक्षरों के एक पूरे अनुक्रम के आउटपुट को सीख सकता है , बजाय इसके कि केवल एक वस्तु की पहचान की जाए .</s>
पहले , यह व्यापक रूप से माना जाता था कि इस प्रकार के सीखने के लिए अनुक्रम के व्यक्तिगत तत्वों की लेबलिंग की आवश्यकता होती है ( Gülçhre and बंजियो , 2013 .</s>
पुनरावर्ती तंत्रिका नेटवर्क , जैसे कि ऊपर उल्लिखित LSTM अनुक्रम मॉडल , अब केवल नियत इनपुट के बजाय अनुक्रम और अन्य अनुक्रमों के बीच संबंधों को मॉडल करने के लिए प्रयोग किया जाता है .</s>
यह अनुक्रम - से - अनुक्रम शिक्षण एक अन्य अनुप्रयोग के क्रांतिकरण के मुहाने पर प्रतीत होता हैः मशीन अनुवाद (Sutskever एट अल ) .</s>
वैश्विक व्यापार > अंतरराष्ट्रीय व्यापार मे म . एस . सी > ग्रेट ब्रिटन ( यूके ) > ग्लासगो</s>
बढ़ती जटिलता की इस प्रवृत्ति को , न्युरल ट्यूरिंग मशीन (Graves एट अल , 2014 ) जो स्मृति कोशिकाओं से पढ़ने और स्मृति कोशिकाओं के लिए मनमाना सामग्री लिखने के लिए सीखने के साथ अपनी तार्किक परिणति की ओर धकेल दिया है .</s>
ऐसे तंत्रिक नेटवर्क सरल प्रोग्रामों को वांछित व्यवहार के उदाहरणों से सीख सकते हैं ।</s>
उदाहरण के लिए , वे क्रंबलित और अनुक्रमित अनुक्रमों के उदाहरण दिए गए संख्याओं की सूचियों को छांटना सीख सकते हैं .</s>
यह आत्म - प्रोग्रामन प्रौद्योगिकी अपनी शैशवावस्था में है , लेकिन भविष्य में सिद्धांत रूप में इसे लगभग किसी भी कार्य के लिए लागू किया जा सकता है ।</s>
गहन अध्ययन की एक और सर्वोपरि उपलब्धि है इसे प्रवर्तन शिक्षण के क्षेत्र में विस्तार देना ।</s>
प्रवर्तन शिक्षण के संदर्भ में , एक स्वायत्त एजेंट को मानव ऑपरेटर से किसी मार्गदर्शन के बिना परीक्षण और त्रुटि के द्वारा कार्य करना सीखना चाहिए .</s>
दीपमण्ड ने प्रदर्शित किया कि गहन अधिगम पर आधारित एक प्रवर्तन शिक्षण प्रणाली अटारी वीडियो गेम्स खेलना सीखने में सक्षम है , जो कई कार्यों पर मानव स्तर के प्रदर्शन तक पहुंचता है .</s>
( प्रमुख अल , 2015 )</s>
डीप लर्निंग ने रोबोटिक्स (फिन एट अल , 2015 ) के लिए प्रवर्तन सीखने के कार्य में भी महत्वपूर्ण सुधार किया है ।</s>
गहन अध्ययन के इन अनुप्रयोगों में से कई अत्यधिक लाभदायक हैं .</s>
गहन अधिगम का उपयोग अब कई शीर्ष प्रौद्योगिकी कंपनियों द्वारा किया जाता है , जिनमें गूगल , माइक्रोसॉफ्ट , फेसबुक , आईबीएम , बैदू , एप्पल , एडोब , नेटफ्लिक्स , एनवीआईए , और एनईसी</s>
गहन शिक्षा में प्रगति सॉफ्टवेयर अवसंरचना में हुई प्रगति पर भी काफी निर्भर करती है ।</s>
सॉफ्टवेयर लाइब्रेरीज़ जैसे कि - बर्केट अल .</s>
FARRUKHABAD ( Cotobert )</s>
Written , DistBelief ( Distef )</s>
TensorFlow ( आबादी एट अल , 2015 ) सबने महत्वपूर्ण अनुसंधान परियोजनाओं या वाणिज्यिक उत्पादों का समर्थन किया है ।</s>
गहन अध्ययन ने अन्य विज्ञानों में भी योगदान दिया है ।</s>
वस्तु अभिज्ञान हेतु आधुनिक नाभिकीय नेटवर्क दृश्य प्रक्रमण का एक मॉडल प्रदान करता है जिसका अध्ययन तंत्रिका विज्ञानी ( डि - कार्लो , 2013 ) कर सकते हैं ।</s>
गहन अधिगम भारी मात्रा में डेटा के प्रसंस्करण और वैज्ञानिक 25 अध्याय 1 में उपयोगी भविष्यवाणियां करने के लिए उपयोगी उपकरण भी प्रदान करता है ।</s>
परिचय क्षेत्र</s>
इसका सफल प्रयोग इस बात की भविष्यवाणी करने में किया गया है कि दवा कंपनियों को नई दवाओं का निर्माण करने में मदद करने के लिए कैसे अणु आपस में विचार - विमर्श करेंगे ( थमाल एट अल , 2014 , सबट्रेटिक कणों की खोज में )</s>
हम भविष्य में अधिक से अधिक वैज्ञानिक क्षेत्रों में गहरी सीखने की उम्मीद है .</s>
सारांश में , गहन अधिगम मशीन अधिगम का एक दृष्टिकोण है जो पिछले कई दशकों के दौरान विकसित मानव मस्तिष्क , सांख्यिकी और अनुप्रयुक्त गणित के हमारे ज्ञान पर भारी पड़ा है ।</s>
हाल के वर्षों में गहन अधिगम ने अपनी लोकप्रियता और उपयोगिता में अत्यधिक वृद्धि देखी है , मोटे तौर पर अधिक शक्तिशाली कंप्यूटरों , अधिक बड़े डेटासेटों और गहन नेटवर्कों को प्रशिक्षित करने की तकनीकों के परिणामस्वरूप ।</s>
आने वाले वर्षों में गहन शिक्षा में सुधार और इसे नए मोर्चों पर लाने की चुनौतियों और अवसरों का सामना करना पड़ रहा है ।</s>
छब्बीस</s>
