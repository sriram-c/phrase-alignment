अध्याय 1 परिचय निवेशकों ने लंबे समय से ऐसी मशीनों के निर्माण का सपना देखा है जो सोचती हैं ।</s>
यह इच्छा कम से कम प्राचीन यूनान के समय की है ।</s>
मिथकीय</s>
जब प्रोग्राम योग्य कंप्यूटरों की पहली कल्पना की गई तो लोगों को आश्चर्य हुआ कि क्या ऐसी मशीनें बुद्धिमान हो सकती हैं , एक के निर्माण से सौ वर्ष पहले ( लोवेलेस , 1842 )</s>
आज , कृत्रिम बुद्धि ( एआईआई ) एक फलता - फूलता क्षेत्र है जिसमें कई व्यावहारिक अनुप्रयोग और सक्रिय अनुसंधान विषय हैं ।</s>
हम सामान्य श्रम को स्वचालित करने , भाषण या छवियों को समझने , चिकित्सा में निदान करने और बुनियादी वैज्ञानिक अनुसंधान का समर्थन करने के लिए बुद्धिमान सॉफ्टवेयर की ओर देखते हैं .</s>
कृत्रिम बुद्धि के आरंभिक दिनों में इस क्षेत्र ने तेजी से उन समस्याओं का समाधान और समाधान किया जो मनुष्यों के लिए बौद्धिक रूप से कठिन हैं लेकिन कंप्यूटर के लिए अपेक्षाकृत सीधे - सीधे आगे हैं ।</s>
समस्याओं है कि औपचारिक , गणित विषयगत नियमों की एक सूची द्वारा वर्णित किया जा सकता है .</s>
कृत्रिम बुद्धि के लिए सच्ची चुनौती उन कार्यों को हल करने में सिद्ध हुई जो लोगों के लिए करना आसान है लेकिन लोगों के लिए औपचारिक रूप से वर्णन करना कठिन है - वे ऐसे प्रतीक हैं जिनका हम सहज ही समाधान करते हैं , जो बोलचाल के शब्दों या चेहरों को पहचानने की तरह स्वचालित महसूस करते हैं ।</s>
यह पुस्तक इन अधिक सहज ज्ञान युक्त समस्याओं के समाधान के बारे में है ।</s>
यह समाधान कंप्यूटर को अनुभव से सीखने और अवधारणाओं के एक पदानुक्रम के संदर्भ में दुनिया को समझने की अनुमति देने के लिए है , सरल अवधारणाओं के साथ अपने संबंध के माध्यम से परिभाषित प्रत्येक अवधारणा के साथ .</s>
अनुभव से ज्ञान इकट्ठा करके , यह दृष्टिकोण कंप्यूटर की आवश्यकता के सभी ज्ञान को औपचारिक रूप से निर्दिष्ट करने के लिए मानव ऑपरेटरों की आवश्यकता से बचता है .</s>
अवधारणाओं का पदानुक्रम कंप्यूटर को सरल से सरल अवधारणाओं के निर्माण द्वारा जटिल अवधारणाओं को सीखने में सक्षम बनाता है .</s>
यदि हम एक ग्राफ आकर्षित कैसे इन अवधारणाओं 1 CHAPTER 1</s>
INTRODUCING एक दूसरे के शीर्ष पर बनाया गया है , ग्राफ गहरा है , कई परतों के साथ ।</s>
इस कारण से , हम एआई गहरी सीखने के लिए इस दृष्टिकोण कहते हैं .</s>
एआई की कई प्रारंभिक सफलताएं अपेक्षाकृत बंध्य और औपचारिक वातावरण में हुईं और उन्हें दुनिया के बारे में अधिक जानकारी रखने के लिए कंप्यूटर की आवश्यकता नहीं थी .</s>
उदाहरण के लिए , आईबीएम की डीप ब्लू शतरंज खेलने की प्रणाली ने 1997 में विश्व चैंपियन गैरी कास्परोव को हराया ( हासु , 2002 )</s>
शतरंज निस्संदेह एक बहुत ही सरल संसार है , जिसमें केवल साठ - चौंसठ स्थान और चौंसठ टुकड़े हैं जो केवल कठोर रूप से परिक्रमा कर सकते हैं ।</s>
एक सफल शतरंज रणनीति का देवीकरण एक जबरदस्त उपलब्धि है , लेकिन चुनौती शतरंज के टुकड़ों के सेट का वर्णन करने और कंप्यूटर के लिए अनुमति योग्य चाल की कठिनाई के कारण नहीं है .</s>
शतरंज पूरी तरह से औपचारिक नियमों की एक बहुत ही संक्षिप्त सूची द्वारा वर्णित किया जा सकता है , आसानी से प्रोग्रामर द्वारा समय से पहले प्रदान की जाती है .</s>
विडंबना यह है कि , अमूर्त और औपचारिक कार्य जो एक मानव के लिए सबसे कठिन मानसिक उपक्रमों में से हैं एक कंप्यूटर के लिए सबसे आसान में से हैं .</s>
कंप्यूटर लंबे समय से सर्वश्रेष्ठ मानव शतरंज खिलाड़ी को भी हराने में सफल रहे हैं , लेकिन अभी हाल ही में वस्तुओं या भाषण को पहचानने के लिए औसत मानव की कुछ क्षमताओं से मेल खाना शुरू कर दिया है .</s>
व्यक्ति के दैनिक जीवन के लिए विश्व के बारे में अपार ज्ञान की आवश्यकता होती है ।</s>
इस ज्ञान का अधिकांश व्यक्तिपरक और अन्तर्ज्ञानात्मक है , और इसलिए इसे औपचारिक रूप से व्यक्त करना कठिन है ।</s>
कंप्यूटर एक बुद्धिमान तरीके से व्यवहार करने के लिए इसी ज्ञान पर कब्जा करने की जरूरत है .</s>
कृत्रिम बुद्धि में एक प्रमुख चुनौती यह है कि इस अनौपचारिक ज्ञान को कंप्यूटर में कैसे प्राप्त किया जाए ।</s>
कई कृत्रिम आसूचना परियोजनाओं ने औपचारिक भाषाओं में विश्व के बारे में ज्ञान को कठोर बनाने का प्रयास किया है ।</s>
एक कंप्यूटर तार्किक अनुमान नियमों का उपयोग कर इन औपचारिक भाषाओं में बयानों के बारे में स्वतः तर्क कर सकता है .</s>
इसे कृत्रिम बुद्धि का ज्ञान आधार दृष्टिकोण कहा जाता है ।</s>
इनमें से किसी भी परियोजना को बड़ी सफलता नहीं मिली है ।</s>
ऐसी सबसे प्रसिद्ध परियोजनाओं में से एक है सीक ( Lyc ) और गुहा , 1989 .</s>
Cyc एक अनुमान इंजन और CycL नामक भाषा में कथनों का डेटाबेस है .</s>
इन बयानों को मानव पर्यवेक्षकों के एक स्टाफ द्वारा दर्ज किया जाता है .</s>
यह एक अपरिच्छिन्न प्रक्रिया है ।</s>
लोग दुनिया का सही वर्णन करने के लिए पर्याप्त जटिलता के साथ औपचारिक नियमों को ईजाद करने के लिए संघर्ष करते हैं .</s>
उदाहरण के लिए , Cyc सुबह ( Linde , 1992 ) में फ्रेड शेव नामक व्यक्ति के बारे में एक कहानी समझने में विफल रहा .</s>
इसके अनुमान इंजन ने कहानी में एक असंगति का पता लगायाः यह जानता था कि लोगों के पास विद्युत पुर्जे नहीं हैं , लेकिन चूंकि फ्रेड एक विद्युत रज़र था , इसलिए उसका मानना था कि एंटिटी “फ़्रेडवाहिलशिंग” में विद्युत पुर्जे थे .</s>
इसलिए यह पूछा गया कि क्या फ्रेड अभी भी एक व्यक्ति था जबकि वह शेव कर रहा था .</s>
कठोर ज्ञान पर भरोसा करने वाली प्रणालियों के सामने आने वाली कठिनाइयों से यह संकेत मिलता है कि एआई प्रणालियों को अपना ज्ञान प्राप्त करने की क्षमता की आवश्यकता है , 2 सीएचएपी 1 निकालकर .</s>
कच्चे डेटा से INTRODUCING पैटर्न</s>
इस क्षमता को मशीन लर्निंग के नाम से जाना जाता है ।</s>
मशीन अधिगम की शुरुआत ने कंप्यूटरों को वास्तविक दुनिया के ज्ञान से संबंधित समस्याओं से निपटने और व्यक्तिपरक दिखने वाले निर्णय लेने में सक्षम बनाया .</s>
एक सरल मशीन अधिगम एल्गोरिथ्म जिसे संभार प्रतिगमन कहा जाता है , यह निर्धारित कर सकता है कि क्या सीजेरियन डिलीवरी की सिफारिश की जाए ( Mor -Ysef एट अल , 1990 )</s>
एक सरल मशीन लर्निंग एल्गोरिदम जिसे नैव बेयस कहा जाता है , स्पैम ई - मेल से वैध ई - मेल को अलग कर सकता है ।</s>
इन सरल मशीन अधिगम एल्गोरिदम का निष्पादन उनके द्वारा दिए गए डेटा के निरूपण पर अत्यधिक निर्भर करता है .</s>
उदाहरण के लिए , जब संभारिक प्रतिगमन का प्रयोग सीजेरियन डिलीवरी की सिफारिश के लिए किया जाता है , एआई प्रणाली सीधे रोगी की जांच नहीं करती है .</s>
इसके बजाय , चिकित्सक प्रणाली को प्रासंगिक जानकारी के कई टुकड़े बताता है , जैसे कि गर्भाशय के निशान की उपस्थिति या अनुपस्थिति .</s>
रोगी के प्रतिनिधित्व में शामिल जानकारी के प्रत्येक टुकड़े एक विशेषता के रूप में जाना जाता है .</s>
लॉजिस्टिक प्रतिगमन सीखता है कि कैसे रोगी की इन विशेषताओं में से प्रत्येक विभिन्न परिणामों के साथ सहसंबंधी है .</s>
हालांकि , यह कैसे सुविधाओं को किसी भी तरह से परिभाषित कर रहे हैं प्रभावित नहीं कर सकते हैं .</s>
यदि लॉजिस्टिक प्रतिगमन को रोगी का एमआरआई स्कैन दिया जाता , बजाय डॉक्टर की औपचारिक रिपोर्ट के , तो यह उपयोगी भविष्यवाणियां नहीं कर पाता .</s>
एमआरआई स्कैन में व्यक्तिगत पिक्सल का प्रसव के दौरान होने वाली किसी भी जटिलताओं के साथ नगण्य सहसंबंध होता है ।</s>
अभ्यावेदन पर यह निर्भरता एक सामान्य घटना है जो पूरे कंप्यूटर विज्ञान और यहां तक कि दैनिक जीवन में प्रकट होती है .</s>
कंप्यूटर विज्ञान में , डेटा संग्रह की खोज जैसे संक्रियाएं घातीय रूप से तेज गति से आगे बढ़ सकती हैं यदि कोलेक - टीन संरचित और बुद्धिमानी से अनुक्रमित हो .</s>
लोग अरबी अंकों पर अंकगणित आसानी से कर सकते हैं लेकिन रोमन अंकों पर अंकगणित कहीं अधिक समय लेने वाले होते हैं ।</s>
यह आश्चर्य की बात नहीं है कि प्रतिनिधित्व के चयन मशीन सीखने एल्गोरिदम के प्रदर्शन पर एक विशाल प्रभाव है .</s>
एक साधारण दृश्य उदाहरण के लिए , देखें आंकड़ा 1 . 1</s>
कई कृत्रिम बुद्धि कार्यों को उस कार्य के लिए निकालने के लिए सुविधाओं के सही सेट को डिजाइन करके हल किया जा सकता है , फिर एक सरल मशीन सीखने एल्गोरिथ्म के लिए इन सुविधाओं को प्रदान करता है .</s>
उदाहरण के लिए , ध्वनि से वक्ता की पहचान के लिए एक उपयोगी विशेषता वक्ता के मुखर पथ के आकार का अनुमान है .</s>
इस विशेषता से इस बात का पुख्ता सुराग मिलता है कि वक्ता पुरुष है , स्त्री है या बच्चा ।</s>
कई कार्यों के लिए , तथापि , यह जानना कठिन है कि कौन सी विशेषताएं निकाली जानी चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हम तस्वीरों में कारों का पता लगाने के लिए एक प्रोग्राम लिखना चाहेंगे .</s>
हम जानते हैं कि कारों के पहिए होते हैं , इसलिए हम एक विशेषता के रूप में एक पहिया की उपस्थिति का उपयोग करना पसंद हो सकता है .</s>
दुर्भाग्य से , यह बिल्कुल क्या एक पहिया पिक्सेल मूल्यों के संदर्भ में की तरह लग रहा है का वर्णन करने के लिए मुश्किल है .</s>
एक पहिये का आकार सरल ज्यामितीय होता है , लेकिन इसकी छवि पहिये पर पड़ने वाली छायाओं से जटिल हो सकती है , सूर्य पहिये के धातु भागों से बाहर निकलता है , कार का लिंग या वस्तु 3 सीएचएपीटर 1 में</s>
INTRODUC सुविधाएँ</s>
. .</s>
</s>
. .</s>
</s>
चित्र 1 . 1 :</s>
विभिन्न अभ्यावेदनों का उदाहरणः मान लीजिए कि हम डेटा की दो श्रेणियों को एक प्रकीर्णन में उनके बीच एक रेखा खींचकर अलग करना चाहते हैं .</s>
बाईं ओर के भूखंड में , हम कार्टेसियन निर्देशांक का उपयोग कर कुछ डेटा का प्रतिनिधित्व करते हैं , और कार्य असंभव है .</s>
दाहिनी ओर के भूखंड में , हम ध्रुवीय निर्देशांक के साथ डेटा का प्रतिनिधित्व करते हैं और एक ऊर्ध्वाधर रेखा के साथ हल करने के लिए कार्य सरल हो जाता है .</s>
</s>
इस समस्या का एक समाधान यह है कि न केवल प्रतिनिधित्व से आउटपुट तक मानचित्रण का पता लगाने के लिए मशीन अधिगम का उपयोग किया जाए बल्कि स्वयं निरूपण भी किया जाए ।</s>
इस दृष्टिकोण प्रतिनिधित्व सीखने के रूप में जाना जाता है .</s>
विद्वत अभ्यावेदन अक्सर हस्तलेखित अभ्यावेदनों के साथ प्राप्त किए जा सकने की तुलना में कहीं बेहतर प्रदर्शन में परिणामित होते हैं .</s>
वे एआई सिस्टम को नए कार्यों के लिए तेजी से अनुकूलित करने में सक्षम बनाते हैं , जिसमें कम से कम मानवीय हस्तक्षेप होता है .</s>
एक प्रतिनिधित्व सीखने एल्गोरिथ्म मिनट में एक सरल कार्य के लिए सुविधाओं के एक अच्छे सेट की खोज कर सकते हैं , या घंटे से महीनों में एक जटिल कार्य के लिए .</s>
मैनुअल डिजाइन कार्य के लिए मानव समय की विशेषताओं की आवश्यकता होती है और एक महान प्रयास के लिए कई दशकों तक शोधकर्ताओं के समुदाय के लिए एक महान प्रयास की आवश्यकता होती है ।</s>
अभ्यावेदन अधिगम एल्गोरिथ्म का सारभूत उदाहरण ऑ - टॉनकोडर है ।</s>
एक ऑटोनकोडर , एक एनकोडर फलन का संयोजन होता है , जो इनपुट डेटा को भिन्न निरूपण में परिवर्तित करता है , और एक विकोडक फलन , जो नए निरूपण को वापस मूल प्रारूप में परिवर्तित करता है .</s>
ऑटोनकोडर्स को अधिक से अधिक जानकारी को सुरक्षित रखने के लिए प्रशिक्षित किया जाता है जब एक इनपुट को एनकोडर के माध्यम से चलाया जाता है और फिर डिकोडर , लेकिन उन्हें यह भी प्रशिक्षित किया जाता है कि नए प्रतिनिधित्व में विभिन्न अच्छे गुण हों .</s>
विभिन्न प्रकार के ऑटोनकोडर्स का उद्देश्य विभिन्न प्रकार के गुणों को प्राप्त करना होता है ।</s>
जब सुविधाओं या एल्गोरिदम सीखने सुविधाओं के लिए डिजाइन कर रहे हैं , हमारा लक्ष्य आमतौर पर विभिन्नता के कारकों को अलग करने के लिए है कि मनाया डेटा की व्याख्या .</s>
इसमें 4 CHAPTER 1 .</s>
इनट्राओडी ( INTROD )</s>
इस तरह के कारक अक्सर ऐसी मात्राएं नहीं होती जो सीधे देखी जाती हैं .</s>
इसके बजाय , वे भौतिक जगत में या तो अपरिच्छिन्न वस्तुओं या अपरिच्छिन्न शक्तियों के रूप में विद्यमान हो सकते हैं जो वेध योग्य मात्राओं को प्रभावित करती हैं .</s>
वे मानव मस्तिष्क में निर्माण के रूप में भी मौजूद हो सकते हैं जो अवलोकन डेटा की उपयोगी सरल व्याख्या या अनुमानित कारणों को प्रदान करते हैं .</s>
उन्हें अवधारणाओं या अमूर्तनों के रूप में सोचा जा सकता है जो हमें आंकड़ों में समृद्ध विविधता का बोध कराने में मदद करते हैं ।</s>
जब किसी भाषण रिकॉर्डिंग का विश्लेषण किया जाता है तो विभिन्नता के कारकों में वक्ता की आयु , उनका लिंग , उनके लहजे और वे बोल रहे शब्द शामिल हैं .</s>
जब किसी कार की छवि का विश्लेषण किया जाता है तो विभिन्नता के कारकों में कार की स्थिति , उसका रंग , और सूर्य का कोण और चमक शामिल हैं .</s>
कई वास्तविक दुनिया कृत्रिम बुद्धि अनुप्रयोगों में कठिनाई का एक प्रमुख स्रोत यह है कि विभिन्नता के कई कारक प्रत्येक डेटा के एक टुकड़े को प्रभावित करते हैं जिसे हम देख सकते हैं .</s>
एक लाल कार की छवि में व्यक्तिगत पिक्सल रात में काले के बहुत करीब हो सकता है .</s>
कार के सिलोयूट का आकार देखने के कोण पर निर्भर करता है ।</s>
अधिकांश अनुप्रयोगों के लिए हमें विभिन्नता के कारकों को अलग करने और उन कारकों को त्यागने की आवश्यकता होती है जिनकी हमें परवाह नहीं है .</s>
बेशक , कच्चे डेटा से इस तरह के उच्च स्तर , अमूर्त विशेषताओं को निकालना बहुत मुश्किल हो सकता है .</s>
विभिन्नता के इन कारकों में से कई , जैसे कि वक्ता के लहजे , की पहचान केवल परिष्कृत , लगभग मानव - स्तर की समझ के उपयोग से की जा सकती है .</s>
जब मूल समस्या को हल करने के लिए प्रतिनिधित्व प्राप्त करना लगभग उतना ही कठिन होता है , तो पहली नजर में , अभ्यावेदन विद्या हमारी सहायता करती प्रतीत होती है ।</s>
गहन अधिगम इस केंद्रीय समस्या का समाधान अन्तर्निहित अभ्यावेदनों द्वारा करता है जो अन्य , सरल अभ्यावेदनों के संदर्भ में व्यक्त किए जाते हैं ।</s>
गहन अधिगम कंप्यूटर को सरल संग्राहकों में से जटिल अवधारणाओं के निर्माण में सक्षम बनाता है ।</s>
चित्र 1 . 2 से पता चलता है कि कैसे एक गहरी सीखने की प्रणाली सरल अवधारणाओं , जैसे कोनों और contours , जो बारी में किनारों के संदर्भ में परिभाषित कर रहे हैं के संयोजन के द्वारा एक व्यक्ति की छवि की अवधारणा का प्रतिनिधित्व कर सकते हैं .</s>
एक गहन अधिगम मॉडल का सारभूत उदाहरण फ़ीड गहन नेटवर्क , या मल्टीलेयर पर्सेप्टॉन (</s>
एक मल्टीलेयर पर्सेप्टॉन सिर्फ एक गणितीय फलन प्रतिचित्रण है जो इनपुट मूल्यों के कुछ सेट को आउटपुट मूल्यों के लिए प्रतिचित्रित करता है .</s>
समारोह कई सरल कार्यों की रचना के द्वारा निर्मित है .</s>
हम इनपुट का एक नया प्रतिनिधित्व प्रदान करने के रूप में एक अलग गणितीय समारोह के प्रत्येक अनुप्रयोग के बारे में सोच सकते हैं .</s>
डेटा के लिए सही प्रतिनिधित्व सीखने का विचार गहन शिक्षण पर एक प्रति - विशेष प्रदान करता है .</s>
गहन अधिगम पर एक अन्य परिप्रेक्ष्य यह है कि गहराई कंप्यूटर को बहुसंकेत कंप्यूटर प्रोग्राम सीखने में सक्षम बनाती है ।</s>
प्रतिनिधित्व की प्रत्येक परत को 5 CHAPTER 1 के बाद कंप्यूटर की स्मृति की स्थिति के रूप में सोचा जा सकता है .</s>
वस्तु दृश्य परत ( इनपुट पिक्सल )</s>
प्रथम प्रच्छन्न परत</s>
द्वितीय प्रच्छन्न परत</s>
CAR PERSONANIMAL आउटपुट ( विषयगत पहचान )</s>
चित्र 1 . 2 : एक गहन शिक्षण मॉडल का प्रदर्शन</s>
एक कंप्यूटर के लिए कच्चे संवेदी इनपुट डेटा के अर्थ को समझना कठिन होता है , जैसे कि यह छवि पिक्सेल मूल्यों के संग्रह के रूप में प्रतिनिधित्व करती है .</s>
पिक्सल के सेट से ऑब्जेक्ट पहचान के लिए फंक्शन मैपिंग बहुत जटिल है .</s>
यदि सीधे मुकाबला किया जाए तो इस मानचित्रण को सीखना या मूल्यांकन करना असंदिग्ध प्रतीत होता है ।</s>
गहन अधिगम इस कठिनाई का समाधान वांछित जटिल प्रतिचित्रण को नीड़ित सरल प्रतिचित्रणों की श्रृंखला में तोड़कर करता है , प्रत्येक मॉडल की एक अलग परत द्वारा वर्णित है ।</s>
इनपुट दृश्य परत पर प्रस्तुत किया जाता है , इसलिए नाम दिया गया है क्योंकि यह चर है कि हम निरीक्षण करने में सक्षम हैं शामिल हैं .</s>
फिर छिपा परतों की एक श्रृंखला छवि से लगातार अमूर्त विशेषताओं को निकालता है .</s>
इन परतों को " इन परतों को " नहीं दिया जाता है क्योंकि इन परतों को " इन परतों को " नहीं दिया जाता है क्योंकि इनके मूल्यों को " इन पर आधारित डेटा " के रूप में निर्धारित किया जाना चाहिए ताकि प्रेक्षित डेटा की व्याख्या की जा सके ।</s>
यहाँ छवियों प्रत्येक छिपा इकाई द्वारा प्रतिनिधित्व विशेषता के प्रकार की कल्पना कर रहे हैं .</s>
पिक्सेल को देखते हुए , पहली परत आसानी से किनारों की पहचान कर सकते हैं , पड़ोसी पिक्सेल की चमक की तुलना करके .</s>
किनारों के पहले छिपा परत के वर्णन को देखते हुए , दूसरी छिपा परत आसानी से कोनों और विस्तारित contours की खोज कर सकते हैं , जो कि किनारों के संग्रह के रूप में पहचानने योग्य हैं .</s>
कोनों और संचरों के संदर्भ में छवि के दूसरे छिपे हुए परत के वर्णन को देखते हुए , तीसरी छुपी परत विशिष्ट वस्तुओं के संपूर्ण भागों का पता लगा सकती है , जिसमें संचरों और कोनों का विशिष्ट संग्रह पाया जा सकता है .</s>
अंत में , वस्तु भागों के संदर्भ में छवि के इस वर्णन का उपयोग छवि में मौजूद वस्तुओं को पहचानने के लिए किया जा सकता है .</s>
ज़ाइलर और फर्गस (2014 ) से अनुमति के साथ छवियों का पुनरुत्पादन किया गया</s>
6 CHAPTER 1 .</s>
निर्देश के एक अन्य समुच्चय को समानांतर रूप से निष्पादित करना ।</s>
अधिक गहराई वाले नेटवर्क अनुक्रम में अधिक अनुदेश निष्पादित कर सकते हैं ।</s>
अनुक्रमिक निर्देश महान शक्ति की पेशकश करते हैं क्योंकि बाद के निर्देश पहले के निर्देशों के परिणामों को वापस संदर्भित कर सकते हैं .</s>
</s>
गहन अधिगम के इस दृष्टिकोण के अनुरूप , एक परत के सक्रियण में सभी सूचनाएं इनपुट की व्याख्या करने वाले विभिन्नता के कारकों को अनिवार्य रूप से एन्कोड करती हैं .</s>
अभ्यावेदन राज्य सूचना को भी संग्रहित करता है जो इनपुट का बोध कर सकने वाले प्रोग्राम को निष्पादित करने में सहायता करता है .</s>
यह राज्य सूचना एक पारंपरिक कंप्यूटर प्रोग्राम में एक काउंटर या सूचक के अनुरूप हो सकती है .</s>
इसका इनपुट की सामग्री से विशेष रूप से कोई संबंध नहीं है , लेकिन यह मॉडल को अपने प्रसंस्करण को व्यवस्थित करने में मदद करता है .</s>
एक मॉडल की गहराई मापने के दो मुख्य तरीके हैं .</s>
पहला दृश्य अनुक्रमिक अनुदेशों की संख्या पर आधारित है जिसे वास्तुकला के मूल्यांकन के लिए निष्पादित किया जाना चाहिए ।</s>
हम इसे प्रवाह चार्ट के माध्यम से सबसे लंबे पथ की लंबाई के रूप में सोच सकते हैं जो यह वर्णन करता है कि कैसे मॉडल के आउटपुट में से प्रत्येक की गणना करने के लिए अपने इनपुट दिया .</s>
जिस प्रकार दो समतुल्य कंप्यूटर प्रोग्रामों की लंबाई अलग - अलग होगी , इस आधार पर कि प्रोग्राम किस भाषा में लिखा जाता है , एक ही फंक्शन को फ्लो चार्ट के रूप में अलग - अलग गहराई के साथ खींचा जा सकता है , यह निर्भर करता है कि हम फ्लो चार्ट में अलग - अलग चरणों के रूप में प्रयोग करने की अनुमति देते हैं ।</s>
चित्र 1 . 3 स्पष्ट करता है कि कैसे भाषा का यह चुनाव एक ही वास्तुकला के लिए दो अलग - अलग माप दे सकता है ।</s>
x 1 x 1 x 1 w 1 w</s>
× × × × × × × ×</s>
x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x 2 x</s>
x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x 2 x</s>
w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w</s>
2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</s>
w 2 × +</s>
तत्व सेट +</s>
x</s>
तत्व सेट लॉजिस्टिक प्रतिगमन लॉजिस्टिक प्रतिगमन चित्र 1 . 3 :</s>
कम्प्यूटेशनल रेखांकन का परिवर्धन एक इनपुट को आउटपुट में प्रतिचित्रित करता है , जहां प्रत्येक नोड एक संक्रिया निष्पादित करता है ।</s>
गहराई इनपुट से आउटपुट तक सबसे लंबे पथ की लंबाई है , लेकिन क्या एक संभावित कम्प्यूटेशनल कदम का गठन की परिभाषा पर निर्भर करता है .</s>
अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन अंकन</s>
यदि हम परिवर्धन , गुणन और रसद सिग्मिड का उपयोग हमारे कंप्यूटर भाषा के तत्वों के रूप में करते हैं , तो इस मॉडल की गहराई तीन</s>
यदि हम रसद प्रतिगमन को स्वयं एक तत्व के रूप में देखते हैं , तो इस मॉडल की गहराई एक है .</s>
7 CHAPTER 1 .</s>
INTRODUCTER एक अन्य दृष्टिकोण , जिसका प्रयोग गहरे व्यावहारिक मॉडलों द्वारा किया जाता है , मॉडल की गहराई को कम्प्यूटेशनल ग्राफ की गहराई नहीं , बल्कि ग्राफ की गहराई को यह वर्णित करता है कि अवधारणाएं एक दूसरे से कैसे संबंधित हैं .</s>
इस मामले में , प्रत्येक अवधारणा के प्रतिनिधित्व की गणना करने के लिए आवश्यक संगणना के प्रवाह संचित्र की गहराई स्वयं अवधारणाओं के ग्राफ से कहीं अधिक गहरी हो सकती है .</s>
इसका कारण यह है कि सरल अवधारणाओं की प्रणाली की समझ को अधिक जटिल अवधारणाओं के बारे में जानकारी देते हुए परिष्कृत किया जा सकता है ।</s>
उदाहरण के लिए , एआई प्रणाली , जो छाया में एक आंख वाले चेहरे की छवि का अवलोकन करती है , प्रारंभ में केवल एक आंख देख सकती है .</s>
यह पता लगाने के बाद कि चेहरा मौजूद है , तंत्र तब यह अनुमान लगा सकता है कि दूसरी आंख शायद भी मौजूद है ।</s>
इस मामले में , अवधारणाओं के ग्राफ में केवल दो परतें शामिल हैं - एक आंख के लिए परत और चेहरों के लिए एक परत - लेकिन गणना के ग्राफ में 2 n परतें शामिल हैं अगर हम अन्य n समय दिए गए प्रत्येक अवधारणा के अपने अनुमान को परिष्कृत .</s>
क्योंकि यह हमेशा स्पष्ट नहीं होता कि इन दो दृश्यों में से कौन सा कम्प्यूटेशनल ग्राफ की गहराई , या प्रोबाबिलिस्टिक मॉडलिंग ग्राफ की गहराई , सबसे अधिक प्रासंगिक है , और क्योंकि विभिन्न लोग सबसे छोटे तत्वों के विभिन्न सेट चुनते हैं , जिससे उनके रेखांकन का निर्माण किया जा सके , एक वास्तुकला की गहराई के लिए केवल एक ही सही मूल्य नहीं है ।</s>
न ही इस बारे में आम सहमति है कि किसी मॉडल को “देप” के रूप में कितनी गहराई से अर्हता प्राप्त करने की आवश्यकता होती है .</s>
हालांकि , गहन अधिगम को सुरक्षित रूप से मॉडलों के अध्ययन के रूप में माना जा सकता है जिसमें पारंपरिक मशीन अधिगम की तुलना में विद्वत कार्यों या विद्वत अवधारणाओं की अधिक मात्रा शामिल होती है .</s>
संक्षेप में , गहन अध्ययन , इस पुस्तक का विषय , एआई के लिए एक दृष्टिकोण है .</s>
विशेष रूप से , यह मशीन अधिगम का एक प्रकार है , एक तकनीक है जो कंप्यूटर प्रणालियों को अनुभव और डेटा के साथ सुधार करने में सक्षम बनाती है .</s>
हम प्रतिवाद करते हैं कि मशीन अधिगम ही एआई सिस्टम के निर्माण का एकमात्र व्यवहार्य दृष्टिकोण है जो जटिल वास्तविक दुनिया के वातावरण में कार्य कर सकता है .</s>
गहन अधिगम एक विशेष प्रकार का मशीन अधिगम है जो विश्व को अवधारणाओं के नीड़ित पदानुक्रम के रूप में निरूपित करके महान शक्ति और लचीलापन प्राप्त करता है , जिसमें प्रत्येक अवधारणा सरल अवधारणाओं के संबंध में परिभाषित होती है , और कम अमूर्त निरूपण के संदर्भ में परिकलित अधिक अमूर्त निरूपण होता है ।</s>
चित्र 1 . 4 इन विभिन्न एआई विधाओं के बीच संबंधों को स्पष्ट करता है ।</s>
चित्र 1 . 5 प्रत्येक कृति के बारे में उच्च स्तर की योजना बनाता है ।</s>
1 . 1</s>
यह पुस्तक किसने पढ़ी ?</s>
यह पुस्तक कई तरह के पाठकों के लिए उपयोगी हो सकती है , लेकिन हमने इसे दो लक्षित दर्शकों को ध्यान में रखकर लिखा ।</s>
इन लक्षित दर्शकों में से एक है विश्वविद्यालय के छात्र ( मशीन सीखने के बारे में स्नातक या स्नातक</s>
अन्य 8 CHAPTER 1 .</s>
INTRODUCING AI मशीन अधिगम प्रतिनिधित्व सीखने डीप लर्निंग उदाहरणः ज्ञान आधार उदाहरणः</s>
लॉजिस्टिक प्रतिगमन</s>
उदाहरणः उदाहरणः उदाहरणः उदाहरणः</s>
Name</s>
उदाहरणः उदाहरणः उदाहरणः उदाहरणः</s>
MLP चित्र 1 . 4 :</s>
एक वेन आरेख , जो दिखाता है कि कैसे गहरी सीखने एक प्रकार का प्रतिनिधित्व सीखने , जो बारी में मशीन सीखने की एक तरह है , जो कई के लिए इस्तेमाल किया जाता है , लेकिन सभी एआई के लिए दृष्टिकोण नहीं है .</s>
वेन आरेख के प्रत्येक अनुभाग में एक एआई प्रौद्योगिकी का एक उदाहरण शामिल है .</s>
लक्ष्य दर्शक सॉफ्टवेयर इंजीनियरों है जो एक मशीन सीखने या स्थैतिक टिक पृष्ठभूमि नहीं है , लेकिन तेजी से एक प्राप्त करना चाहते हैं और अपने उत्पाद या मंच में गहरी सीखने का उपयोग शुरू .</s>
डीप लर्निंग पहले से ही कई सॉफ्ट वेयर विधाओं में उपयोगी साबित हुई है , जिनमें कंप्यूटर विजन , स्पीच और ऑडियो प्रोसेसिंग , प्राकृतिक भाषा संसाधन , रोबोटिक्स , जैव सूचना विज्ञान और रसायन विज्ञान , वीडियो गेम्स , खोज इंजन ऑनलाइन विज्ञापन और वित्त शामिल हैं .</s>
इस पुस्तक को तीन भागों में संगठित किया गया है ताकि विभिन्न प्रकार के पाठकों को समायोजित किया जा सके ।</s>
भाग मैं बुनियादी गणितीय उपकरण और मशीन सीखने अवधारणाओं का परिचय देता है .</s>
भाग 2 सबसे स्थापित गहरी सीखने एल्गोरिदम , जो अनिवार्य रूप से प्रौद्योगिकियों हल कर रहे हैं का वर्णन करता है .</s>
भाग III अधिक सट्टा विचारों का वर्णन करता है जो व्यापक रूप से गहन शिक्षण में भविष्य के अनुसंधान के लिए महत्वपूर्ण माना जाता है .</s>
9 CHAPTER 1 .</s>
इनपुट इनपुट</s>
हैंड - डिजाइन प्रोग्राम</s>
इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट
सुविधाओं से आउटपुट इनपुट फीचर मैपिंग से हैंड - डिज़ाइन सुविधाओं मैपिंग</s>
विशेषताओं आउटपुट अतिरिक्त परतों से मैपिंग और अधिक अमूर्त सुविधाओं नियम - पस्त सिस्टम क्लासिक मशीन सीखने प्रतिनिधित्व चित्र 1 . 5 . फ्लोचर यह दर्शाता है कि कैसे एक एआई प्रणाली के विभिन्न भागों विभिन्न एआई विधाओं के भीतर एक दूसरे से संबंधित है .</s>
शेड बॉक्स उन घटकों को इंगित करते हैं जो डेटा से सीखने में सक्षम होते हैं .</s>
पाठकों को ऐसे हिस्सों को छोड़ने के लिए स्वतंत्र महसूस करना चाहिए जो उनके हितों या पृष्ठभूमि को देखते हुए प्रासंगिक न हों ।</s>
रैखिक बीजगणित , संभावना , और मौलिक मशीन सीखने अवधारणाओं से परिचित रीडर भाग मैं छोड़ सकते हैं , उदाहरण के लिए , जबकि जो लोग बस एक कार्य प्रणाली को लागू करना चाहते हैं भाग 2 से परे पढ़ने की जरूरत नहीं है .</s>
चुनने में मदद करने के लिए जो 10 CHAPTER 1 .</s>
INTRODUCING 1</s>
परिचय भाग I : अनुप्रयुक्त मठ और मशीन लर्निंग बेसिक 2 .</s>
रैखिक बीजगणित 3 .</s>
संभावना और सूचना सिद्धांत 4 .</s>
संख्यात्मक कंप्यूटिंग 5 .</s>
मशीन लर्निंग बेसिक पार्ट II :</s>
डीप नेटवर्कः आधुनिक अभ्यास 6 .</s>
डीप फीड सरल नेटवर्क 7 .</s>
नियमितीकरण 8 .</s>
अनुकूलन 9 .</s>
सीएनएन 10 .</s>
RNs 11 .</s>
व्यावहारिक पद्धति 12 . व्यावहारिक पद्धति 12 .</s>
अनुप्रयोग भाग III :</s>
डीप लर्निंग रिसर्च 13 .</s>
रैखिक फैक्टर मॉडल 14 .</s>
ऑटोनकोडर्स 15 .</s>
प्रतिनिधित्व अधिगम 16 .</s>
संरचित सम्भावित मॉडल 17 .</s>
मोंटे कार्लो विधि 18 .</s>
विभाजन कार्य 19 . विभाजन कार्य 19 . विभाजन कार्य 19 . विभाजन कार्य 19 . विभाजन कार्य 19 .</s>
अनुमान 20 .</s>
डीप जेनरेशन मॉडल चित्र 1 . 6 : पुस्तक का उच्च स्तरीय संगठन ।</s>
एक अध्याय से दूसरे अध्याय तक तीर से यह संकेत मिलता है कि पूर्व अध्याय को समझने के लिए पूर्वापेक्षित सामग्री है ।</s>
11 CHAPTER 1 .</s>
पढ़ने के लिए INTRODUCING अध्याय , आंकड़ा 1 . 6 पुस्तक के उच्च स्तर के संगठन को दर्शाता एक प्रवाह संचित्र प्रदान करता है .</s>
हम यह जरूर मान लेते हैं कि सभी पाठक कंप्यूटर विज्ञान की पृष्ठभूमि से आते हैं ।</s>
हम प्रोग्रामिंग , कम्प्यूटेशनल प्रदर्शन मुद्दों की एक बुनियादी समझ , जटिलता सिद्धांत , परिचयात्मक स्तर पथरी और ग्राफ सिद्धांत की कुछ शब्दावली के साथ परिचित मान लेते हैं .</s>
डीप लर्निंग में 1 . 2 ऐतिहासिक ट्रेंड्स</s>
किसी ऐतिहासिक संदर्भ के साथ गहन विद्या को समझना सबसे आसान है ।</s>
गहन शिक्षण का एक विस्तृत इतिहास प्रदान करने के बजाय , हम कुछ प्रमुख प्रवृत्तियों की पहचानः</s>
• गहन अध्ययन का एक लंबा और समृद्ध इतिहास रहा है , लेकिन कई नामों से जाना जाता है , विभिन्न दार्शनिक दृष्टिकोणों को प्रतिबिंबित , और लोकप्रियता में वृद्धि हुई है और</s>
• गहन शिक्षण अधिक उपयोगी हो गया है के रूप में उपलब्ध प्रशिक्षण डेटा की मात्रा में वृद्धि हुई है .</s>
• गहन शिक्षण मॉडल समय के साथ आकार में वृद्धि हुई है क्योंकि कंप्यूटर बुनियादी ढांचे ( गहन शिक्षण के लिए हार्डवेयर और सॉफ्टवेयर ) में सुधार हुआ है .</s>
• गहन शिक्षण समय के साथ बढ़ती सटीकता के साथ जटिल अनुप्रयोगों को हल किया है .</s>
1 . 2 . 1</s>
नेरल नेट वर्क्स के कई नाम और चंगिंग फ़ोर्ट्स</s>
हम उम्मीद करते हैं कि इस पुस्तक के कई पाठकों ने एक रोमांचक नई प्रौद्योगिकी के रूप में गहन शिक्षण के बारे में सुना है , और एक उभरते क्षेत्र के बारे में एक पुस्तक में “गैतिहासिक” का उल्लेख देखकर आश्चर्यचकित हैं .</s>
वास्तव में , गहरी सीखने 1940 के दशक के लिए वापस तिथियाँ .</s>
गहन अध्ययन केवल नया प्रतीत होता है , क्योंकि यह अपनी वर्तमान लोकप्रियता से पहले कई वर्षों के लिए अपेक्षाकृत अलोकप्रिय था , और क्योंकि यह कई अलग अलग नामों से चला गया है , अभी हाल ही में “देप अधिगम” कहा जा रहा है .</s>
इस क्षेत्र को कई बार पुनर्निर्मित किया गया है , जो विभिन्न शोधकर्ताओं और विभिन्न परिप्रेक्ष्यों के प्रभाव को दर्शाता है .</s>
गहन शिक्षण का एक व्यापक इतिहास इस पाठ्यपुस्तक के दायरे से बाहर है ।</s>
कुछ बुनियादी संदर्भ , तथापि , गहरी सीखने को समझने के लिए उपयोगी है .</s>
मोटे तौर पर , विकास की तीन लहरें आई हैंः</s>
1940 -1960 के दशक में साइबरनेटिक्स के नाम से जानी जाने वाली गहन शिक्षा , जिसे 12 सीएचएपीटर 1 में कनेक्शनवाद के नाम से जाना जाता है ।</s>
आईटीआरओडीयूसीएल 1940 , 1960 1980 2000 वर्ष</s>
0 . 000000 0 . 00050 0 . 000100 0 . 000150</s>
0 . 000200 0 . 000250 शब्द की आवृत्ति या वाक्यांश साइबरनेटिक्स ( संबंधवाद + तंत्रिका नेटवर्क )</s>
चित्र 1 . 7 :</s>
कृत्रिम तंत्रिका जाल अनुसंधान की तीन ऐतिहासिक तरंगों में से दो , जैसा कि वाक्यांशों की आवृत्ति से मापा जाता है “साइबरनेटिक्स” और “संयोजनवाद” या “नेरल नेटवर्क” , गूगल बुक्स के अनुसार ( तीसरी लहर अभी हाल ही में प्रकट हुई है )</s>
1940 के दशक में पहली लहर , 1940 के दशक में , जैविक , जैविक , जैविक , जैविक , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित</s>
दूसरी लहर 1980 -1995 की अवधि के संबंधवादी दृष्टिकोण के साथ शुरू हुई , जिसमें एक या दो छुपी हुई परतों वाले तंत्रिका नेटवर्क को प्रशिक्षित करने के लिए बैक - प्रचार ( Rumelhart एट अल , 1986 )</s>
वर्तमान तरंगदैर्ध्य गहन अध्ययन २००६</s>
अल</s>
और अभी 2016 के रूप में पुस्तक रूप में प्रकट हो रहा है</s>
इसी प्रकार अन्य दो तरंगें पुस्तक रूप में उसी वैज्ञानिक गतिविधि की अपेक्षा बहुत बाद में प्रकट हुईं ।</s>
1980 -1990 का दशक , और 2006 में गहन शिक्षण के नाम से वर्तमान पुनरुत्थान ।</s>
यह मात्रात्मक रूप से 1 . 7 अंक में चित्रित किया गया है ।</s>
आज हम जिन प्रारंभिक अधिगम एल्गोरिदमों को पहचानते हैं , उनमें से कुछ का उद्देश्य जैविक अधिगम के कम्प्यूटेशनल मॉडल होना था , अर्थात सीखने के तरीके मस्तिष्क में कैसे होते हैं या हो सकते हैं ।</s>
एक परिणाम के रूप में , एक नाम है कि गहरी सीखने के द्वारा चला गया है कृत्रिम तंत्रिका नेटवर्क (</s>
गहन शिक्षण मॉडलों पर तदनुरूप परिप्रेक्ष्य यह है कि वे जैविक मस्तिष्क से प्रेरित प्रणालियों ( चाहे मानव मस्तिष्क हो या किसी अन्य पशु का मस्तिष्क ।</s>
जबकि मशीन अधिगम के लिए प्रयुक्त तंत्रिका नेटवर्कों के प्रकारों का प्रयोग कभी - कभी मस्तिष्क के कार्य को समझने के लिए किया गया है ( हैनटन और शैलिस , 1991 , वे आम तौर पर जैविक कार्य के वास्तविक मॉडल नहीं बनाए गए हैं .</s>
गहन शिक्षण पर तंत्रिका परिप्रेक्ष्य दो मुख्य विचारों से प्रेरित है ।</s>
एक विचार यह है कि मस्तिष्क उदाहरण के द्वारा एक प्रमाण प्रदान करता है कि बुद्धिमान व्यवहार संभव है , और बुद्धि के निर्माण के लिए एक संकल्पनात्मक रूप से सीधा रास्ता है मस्तिष्क के पीछे कम्प्यूटेशनल सिद्धांतों को रिवर्स इंजीनियर और उसकी कार्यक्षमता डुप्लिकेट .</s>
एक और 13 CHAPTER 1 .</s>
अंतःक्रिया परिप्रेक्ष्य यह है कि मस्तिष्क और उन सिद्धांतों को समझना अत्यंत रोचक होगा जो मानव बुद्धि को कम करते हैं , इसलिए इन मूलभूत वैज्ञानिक प्रश्नों पर प्रकाश डालने वाले मशीन लर्निंग मॉडल इंजीनियरिंग अनुप्रयोगों को हल करने की उनकी क्षमता के अलावा उपयोगी हैं ।</s>
आधुनिक शब्द “देप लर्निंग” मशीन लर्निंग मॉडल की वर्तमान नस्ल पर तंत्रिका विज्ञानी परिप्रेक्ष्य से परे चला जाता है .</s>
यह रचना के एकाधिक स्तरों को सीखने के एक अधिक सामान्य सिद्धांत के लिए अपील करता है , जो मशीन सीखने के ढांचे में लागू किया जा सकता है जो अनिवार्य रूप से तंत्रिका प्रेरित नहीं हैं .</s>
आधुनिक गहन शिक्षण के प्रारंभिक पूर्ववर्ती सरल रैखिक मॉडल थे जो तंत्रिका विज्ञानी परिप्रेक्ष्य से प्रेरित थे .</s>
इन मॉडलों एन इनपुट मूल्यों एक्स 1 , का एक सेट लेने के लिए डिज़ाइन किया गया था .</s>
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
, x n और उन्हें एक आउटपुट y के साथ संबद्ध .</s>
इन मॉडलों वजन का एक सेट w 1 , सीखना होगा .</s>
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
w n और अपने आउटपुट f ( x , w ) की गणना करें</s>
1 w 1 w 1 w 1 w 1 w 1 w 1 w 1 w 1 w</s>
+ ··· + x n</s>
w n . n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</s>
तंत्रिका नेटवर्क अनुसंधान की इस पहली लहर को साइबरनेटिक्स के नाम से जाना जाता था , जैसा कि अंक 1 . 7 में दर्शाया गया है ।</s>
</s>
यह रैखिक मॉडल इनपुट की दो अलग अलग श्रेणियों को पहचान सकता है परीक्षण द्वारा कि क्या एफ ( x , w ) सकारात्मक या नकारात्मक है .</s>
बेशक , श्रेणियों की इच्छित परिभाषा के अनुरूप मॉडल के लिए , सही ढंग से सेट किए जाने के लिए आवश्यक भार .</s>
ये भार मानव संचालक द्वारा निर्धारित किया जा सकता था ।</s>
1950 के दशक में , पर्सेप्टॉन (Rosenblatt , 1958 , 1962 ) पहला मॉडल बन गया जो प्रत्येक श्रेणी के इनपुट के दिए गए उदाहरणों को परिभाषित करने वाले भार को सीख सकता था .</s>
अनुकूली रैखिक तत्व</s>
( ) स्वयं एक वास्तविक संख्या की भविष्यवाणी करने के लिए ( Widrow और Hoff , 1960 ) और भी डेटा से इन संख्याओं की भविष्यवाणी करने के लिए सीख सकता है .</s>
इन सरल अधिगम एल्गोरिदम ने मा - ज्या अधिगम के आधुनिक परिदृश्य को अत्यधिक प्रभावित किया ।</s>
प्रशिक्षण एल्गोरिथ्म का प्रयोग एड़ीएलआईईईई के भार को अनुकूलित करने के लिए किया जाता है ।</s>
एक एल्गोरिथ्म का एक विशेष मामला था जिसे stochastic प्रवणता अवतरण कहा जाता है ।</s>
Stochastic ग्रेडिएंट अवतरण एल्गोरिथ्म के हल्के संशोधित संस्करण आज गहन शिक्षण मॉडलों के लिए प्रमुख प्रशिक्षण एल्गोरिदम बने हुए हैं ।</s>
एफ ( x , w ) पर आधारित मॉडलों को पर्सेप्टॉन और एडीएलआईएन द्वारा प्रयोग किया जाता है , रैखिक मॉडल कहा जाता है .</s>
ये मॉडल कुछ सर्वाधिक व्यापक रूप से प्रयुक्त मशीन अधिगम मॉडल बने हुए हैं , हालांकि कई मामलों में उन्हें मूल मॉडलों की तुलना में विभिन्न तरीकों से प्रशिक्षित किया जाता है .</s>
रैखिक मॉडलों की कई सीमाएं हैं .</s>
सबसे प्रसिद्ध</s>
= 1 और f (</s>
[ 1 ] , 0 ]</s>
= 1 लेकिन f (</s>
[ 1 ] , 1 ] , 1 ]</s>
= = = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</s>
रैखिक मॉडलों में इन खामियों का अवलोकन करने वाले आलोचकों ने सामान्य रूप से जैविक रूप से प्रेरित सीखने के खिलाफ एक पीठ थपथपाई ( Minsky और पेपरेट , 1969</s>
तंत्रिका नेटवर्क की लोकप्रियता में यह पहला प्रमुख डुबकी था .</s>
14 CHAPTER 1 .</s>
INTRODUCING टुडे , तंत्रिका विज्ञान गहन शिक्षण शोधकर्ताओं के लिए प्रेरणा का एक महत्वपूर्ण स्रोत माना जाता है , लेकिन अब यह क्षेत्र के लिए प्रमुख मार्गदर्शक नहीं है ।</s>
आज गहन शिक्षण अनुसंधान में तंत्रिका विज्ञान की घटती भूमिका का मुख्य कारण यह है कि हमारे पास मस्तिष्क के बारे में पर्याप्त जानकारी नहीं है कि वह इसे गाइड के रूप में इस्तेमाल कर सके ।</s>
मस्तिष्क द्वारा प्रयुक्त वास्तविक एल्गोरिदम की गहरी समझ प्राप्त करने के लिए , हमें एक साथ हजारों अंतर्संबंधित न्यूरॉन्स की गतिविधि की निगरानी करने में सक्षम होना होगा ।</s>
क्योंकि हम ऐसा नहीं कर पा रहे हैं , हम मस्तिष्क के कुछ सबसे सरल और सुप्रमाणित भागों को भी समझने से दूर हैं ( ओलशासेन एंड फील्ड , 2005 )</s>
तंत्रिका विज्ञान हमें उम्मीद करने के लिए एक कारण दिया है कि एक एकल गहरी सीखने एल्गोरिथ्म कई अलग अलग कार्यों को हल कर सकते हैं .</s>
तंत्रिका विज्ञानियों ने पाया है कि फर्नेस अपने मस्तिष्क के श्रवण प्रसंस्करण क्षेत्र के साथ “सी” सीख सकते हैं यदि उनके मस्तिष्क को उस क्षेत्र में दृश्य संकेत भेजने के लिए दोहराया जाता है ( वॉन मेलचेर एट अल 2000 )</s>
यह सुझाव है कि स्तनधारी मस्तिष्क के बहुत से विभिन्न कार्यों है कि मस्तिष्क हल करता है के अधिकांश को हल करने के लिए एक एकल एल्गोरिथ्म का उपयोग कर सकता है .</s>
इस परिकल्पना से पहले , मशीन अधिगम अनुसंधान अधिक खंडित था , जिसमें प्राकृतिक भाषा संसाधन , दृष्टि , गति योजना और भाषण मान्यता का अध्ययन करने वाले शोधकर्ताओं के विभिन्न समुदाय थे .</s>
आज , इन अनुप्रयोग समुदायों अभी भी अलग हैं , लेकिन गहन शिक्षण अनुसंधान समूहों के लिए एक साथ कई या यहां तक कि इन सभी अनुप्रयोग क्षेत्रों का अध्ययन करना आम बात है .</s>
हम तंत्रिका विज्ञान से कुछ मोटे दिशानिर्देश बनाने में सक्षम हैं .</s>
कई कम्प्यूटेशनल इकाइयों है कि केवल एक दूसरे के साथ बातचीत के माध्यम से बुद्धिमान हो जाता है का मूल विचार मस्तिष्क से प्रेरित है .</s>
</s>
आज अधिकांश तंत्रिका नेटवर्क एक मॉडल न्यूरॉन पर आधारित हैं जिसे सुधारित रैखिक इकाई कहा जाता है .</s>
मूल cognitron ( Fukushima , 1975 ) एक और अधिक जटिल संस्करण है कि मस्तिष्क समारोह के बारे में हमारे ज्ञान से अत्यधिक प्रेरित था शुरू किया .</s>
सरलीकृत आधुनिक संस्करण कई दृष्टिकोणों से विचारों को समाहित करते हुए विकसित किया गया था , जिसमें नायर और हाइटन (2010 ) और ग्लोरोट एट अल .</s>
</s>
(2009 ) अधिक इंजीनियरिंग उन्मुख प्रभावों का हवाला देते हुए</s>
जबकि तंत्रिका विज्ञान प्रेरणा का एक महत्वपूर्ण स्रोत है , यह एक कठोर गाइड के रूप में लेने की जरूरत नहीं है .</s>
हम जानते हैं कि वास्तविक न्यूरॉन्स आधुनिक सुधारित रैखिक इकाइयों की तुलना में बहुत अलग कार्यों की गणना करते हैं , लेकिन अधिक तंत्रिका यथार्थवाद अभी तक मशीन सीखने के प्रदर्शन में सुधार के लिए नेतृत्व नहीं किया है .</s>
इसके अलावा , जबकि तंत्रिका विज्ञान सफलतापूर्वक कई तंत्रिका नेटवर्क वास्तुकला को प्रेरित किया है , हम अभी तक तंत्रिका विज्ञान के लिए जैविक सीखने के बारे में पर्याप्त नहीं जानते हैं सीखने एल्गोरिदम हम इन वास्तुकलाओं को प्रशिक्षित करने के लिए बहुत मार्गदर्शन की पेशकश करने के लिए उपयोग करते हैं .</s>
15 CHAPTER 1 .</s>
INTRODUCTER मीडिया लेख अक्सर मस्तिष्क के लिए गहरी सीखने की समानता पर जोर देते हैं .</s>
जबकि यह सच है कि गहन अध्ययन शोधकर्ताओं को अन्य मशीन सीखने के क्षेत्रों में काम कर रहे शोधकर्ताओं की तुलना में मस्तिष्क को एक प्रभाव के रूप में उद्धृत करने की संभावना अधिक है , कर्नेल मशीनों या बेसियन सांख्यिकी , एक मस्तिष्क का अनुकरण करने के प्रयास के रूप में गहन शिक्षण नहीं देखना चाहिए .</s>
आधुनिक गहन अधिगम कई क्षेत्रों से प्रेरणा लेता है , विशेष रूप से रैखिक बीजगणित , संभावना , सूचना सिद्धांत , और संख्यात्मक अनुकूलन जैसे अनुप्रयुक्त गणित मौलिक</s>
जबकि कुछ गहरे सीखने के शोधकर्ता तंत्रिका विज्ञान को प्रेरणा के एक महत्वपूर्ण स्रोत के रूप में उद्धृत करते हैं , दूसरों को बिल्कुल भी तंत्रिका विज्ञान से संबंधित नहीं है .</s>
गौरतलब है कि यह समझने का प्रयास कि मस्तिष्क एल्गोरिथम स्तर पर कैसे काम करता है , जीवित और अच्छी तरह से है ।</s>
इस प्रयास को मुख्य रूप से " कम्प्यूटेशनल तंत्रिका विज्ञान " के रूप में जाना जाता है और यह गहन शिक्षण से अध्ययन का एक अलग क्षेत्र है ।</s>
शोधकर्ताओं के लिए दोनों क्षेत्रों के बीच बार - बार आगे बढ़ना आम बात है ।</s>
गहन अधिगम का क्षेत्र मुख्य रूप से इस बात से संबंधित है कि कैसे कंप्यूटर प्रणालियों का निर्माण किया जाए जो बुद्धि की आवश्यकता वाले कार्यों को सफलतापूर्वक हल करने में सक्षम हों , जबकि कम्प्यूटेशनल तंत्रिका विज्ञान का क्षेत्र मुख्य रूप से मस्तिष्क वास्तव में कैसे कार्य करता है इसके अधिक सटीक मॉडल बनाने से संबंधित है ।</s>
1980 के दशक में , तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर एक आंदोलन के माध्यम से बड़े हिस्से में उभरी जिसे कनेक्शनवाद , या समानांतर वितरित प्रक्रिया -</s>
अल</s>
मैकक्लललैंड एट अल , 1995</s>
संज्ञानात्मक विज्ञान के संदर्भ में संबंधवाद का उदय हुआ ।</s>
संज्ञानात्मक विज्ञान , विश्लेषण के अनेक विभिन्न स्तरों को संयोजित करते हुए मन को समझने के लिए एक अंतर्विषयक दृष्टिकोण है ।</s>
1980 के दशक के प्रारंभ में , अधिकांश संज्ञानात्मक वैज्ञानिकों ने प्रतीकात्मक तर्क के मॉडलों का अध्ययन किया .</s>
अपनी लोकप्रियता के बावजूद , प्रतीकात्मक मॉडल कैसे मस्तिष्क वास्तव में न्यूरॉन्स का उपयोग कर उन्हें लागू कर सकता है के संदर्भ में व्याख्या करने के लिए मुश्किल थे .</s>
कनेक्शनिस्टों ने संज्ञान के मॉडलों का अध्ययन करना शुरू किया जो वास्तव में तंत्रिका कार्यान्वयनों में आधारित हो सकते थे ( Touretzky और मिंटन , 1985 , 1940 के दशक में मनोविज्ञानी डोनाल्ड हेब के कार्य के प्रति कई विचारों को पुनर्जीवित किया .</s>
कनेक्शनवाद में केंद्रीय विचार है कि सरल कम्प्यूटेशनल इकाइयों की एक बड़ी संख्या बुद्धिमान व्यवहार हासिल कर सकते हैं जब एक साथ नेटवर्क .</s>
यह अंतर्दृष्टि जैविक तंत्रिका प्रणालियों में न्यूरॉन्स के लिए समान रूप से लागू होती है , जैसा कि यह कम्प्यूटेशनल मॉडलों में छुपी इकाइयों के लिए करता है .</s>
1980 के दशक के संपर्क आंदोलन के दौरान कई प्रमुख अवधारणाओं का उदय हुआ जो आज के गहन अध्ययन के केंद्र में हैं ।</s>
इन अवधारणाओं में से एक है वितरित प्रतिनिधित्व ( हैन्टन एट अल , 1986 ) .</s>
यह विचार है कि एक प्रणाली के लिए प्रत्येक इनपुट कई सुविधाओं द्वारा प्रतिनिधित्व किया जाना चाहिए , और प्रत्येक विशेषता कई संभावित इनपुट के प्रतिनिधित्व में शामिल होना चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हमारे पास एक दृष्टि प्रणाली है जो 16 CHAPTER 1 को पहचान सकती है ।</s>
INTRODUCTER कारें , ट्रक , और पक्षी , और इन वस्तुओं प्रत्येक लाल , हरे , या नीले हो सकते हैं</s>
इन इनपुट का प्रतिनिधित्व करने का एक तरीका होगा एक अलग न्यूरॉन या छिपा इकाई है कि नौ संभावित संयोजनों में से प्रत्येक के लिए सक्रिय करता हैः लाल ट्रक , लाल पक्षी , हरे ट्रक , आदि</s>
इसके लिए नौ विभिन्न न्यूरॉन्स की आवश्यकता होती है , और प्रत्येक न्यूरॉन को स्वतंत्र रूप से रंग और वस्तु पहचान की अवधारणा सीखना चाहिए .</s>
इस स्थिति पर सुधार करने के लिए एक तरीका है एक वितरित प्रतिनिधित्व का उपयोग करने के लिए , तीन न्यूरॉन्स रंग का वर्णन और तीन न्यूरॉन्स वस्तु पहचान का वर्णन .</s>
इसके लिए नौ के स्थान पर केवल छह न्यूरॉन्स कुल की आवश्यकता होती है , और लालिमा का वर्णन करने वाला न्यूरॉन कारों , ट्रकों और पक्षियों की छवियों से लालिमा के बारे में जानने में सक्षम है , न कि केवल वस्तुओं की एक विशिष्ट श्रेणी की छवियों से .</s>
वितरित प्रतिनिधित्व की अवधारणा इस पुस्तक के लिए केंद्रीय है और अधिक विस्तार से अध्याय 15 में वर्णित है .</s>
कनेक्शनिस्ट आंदोलन की एक अन्य प्रमुख उपलब्धि थी , गहरे न्यूट्रीशन नेटवर्कों को प्रशिक्षित करने के लिए बैक - प्रोप्रेगेशन का व्यापक उपयोग , बैक - प्रोप्रेगेशन एल्गोरिथम , १९८७</s>
इस एल्गोरिथ्म की लोकप्रियता में वृद्धि हुई है और गिरावट आई है , लेकिन इस लेखन के रूप में , गहरे मॉडलों के प्रशिक्षण के लिए प्रमुख दृष्टिकोण है .</s>
1990 के दशक के दौरान , शोधकर्ताओं ने तंत्रिका नेटवर्क के साथ मॉडलिंग अनुक्रम में महत्वपूर्ण अग्रिम किया .</s>
होक्रिटर (1991 )</s>
(1994 )</s>
होक्रिटर और श्मिटुबर (1997 )</s>
आज , एलएसटीएम व्यापक रूप से कई अनुक्रम मॉडलिंग कार्यों के लिए प्रयोग किया जाता है , गूगल पर कई प्राकृतिक भाषा प्रसंस्करण कार्यों सहित .</s>
तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर1990 के दशक के मध्य तक चली .</s>
तंत्रिका नेटवर्क और अन्य एआई प्रौद्योगिकियों पर आधारित वेन - टर्स ने निवेश की मांग करते समय अवास्तविक सु महत्वाकांक्षी दावे करना शुरू कर दिया ।</s>
जब एआई शोध ने इन अनुचित उम्मीदों को पूरा नहीं किया तो निवेशक निराश हो गए ।</s>
इसके साथ ही मशीनी शिक्षा के अन्य क्षेत्रों ने भी प्रगति की ।</s>
</s>
इन दोनों कारणों से तंत्रिका नेटवर्कों की लोकप्रियता में गिरावट आई जो 2007 तक चली .</s>
इस दौरान , तंत्रिका नेटवर्क कुछ कार्यों पर प्रभावशाली प्रदर्शन प्राप्त करते रहे (</s>
लेकुन एट अल</s>
अल</s>
</s>
</s>
बहुसांस्कृतिक सीआईएफएआर एनएपी अनुसंधान पहल 17 सीएचएपीटर 1 ।</s>
अंतःविषय में मानव और कंप्यूटर दृष्टि के तंत्रिका विज्ञानी और विशेषज्ञ भी शामिल थे ।</s>
इस बिंदु पर , गहरे नेटवर्क आम तौर पर माना जाता था कि प्रशिक्षण देना बहुत कठिन होता है .</s>
अब हम जानते हैं कि एल्गोरिदम है कि 1980 के दशक के बाद से अस्तित्व में काफी अच्छी तरह से काम करते हैं , लेकिन यह स्पष्ट सिर्का 2006 नहीं था .</s>
मुद्दा शायद सिर्फ यह है कि इन एल्गोरिदम बहुत कम्प्यूटेशनल रूप से महंगा था उस समय उपलब्ध हार्डवेयर के साथ बहुत प्रयोग की अनुमति देने के लिए .</s>
तंत्रिका नेटवर्क अनुसंधान की तीसरी लहर 2006 में एक ब्रेक थ्रू के साथ शुरू हुई .</s>
</s>
अन्य संबद्ध अनुसंधान समूहों को शीघ्रता से उसी रणनीति को प्रदर्शित किया जा सकता है (</s>
तंत्रिका अनुसंधान की इस लहर ने “देप सीखने के लिए” के प्रयोग को लोकप्रिय बनाया , जिससे कि शोधकर्ताओं ने “देप सीखने के लिए” , “देप सीखने के लिए” , “देप सीखने के लिए” , और “देप सीखने के लिए”</s>
इस समय , गहरे तंत्रिका नेटवर्कों ने अन्य मशीन अधिगम प्रौद्योगिकियों के साथ - साथ हस्तनिर्मित कार्यक्षमता पर आधारित एआई सिस्टम को आउट किया .</s>
तंत्रिका नेटवर्क की लोकप्रियता की यह तीसरी लहर इस लेखन के समय तक जारी है , हालांकि गहन शिक्षण अनुसंधान का ध्यान इस लहर के समय के भीतर नाटकीय रूप से बदल गया है .</s>
तीसरी लहर की शुरुआत नई अनुपयुक्त अधिगम तकनीकों और छोटे डेटासेटों से अच्छी तरह सामान्य करने के लिए गहरे मॉडलों की क्षमता पर ध्यान केंद्रित करने के साथ हुई , लेकिन आज बहुत पुराने पर्यवेक्षित अधिगम एल्गोरिदम और बड़े लेबल वाले डेटासेटों का लाभ उठाने के लिए गहरे मॉडलों की क्षमता में अधिक रुचि है .</s>
1 . 2 . 2</s>
बढ़ते डेटासेट आकार</s>
कोई आश्चर्य की बात हो सकती है कि अभी हाल ही में गहन शिक्षण को एक महत्वपूर्ण प्रौद्योगिकी के रूप में मान्यता क्यों मिली है , भले ही 1950 के दशक में कृत्रिम तंत्रिका नेटवर्क के साथ पहले प्रयोग किए गए थे ।</s>
गहन शिक्षण 1990 के दशक के बाद से वाणिज्यिक अनुप्रयोगों में सफलतापूर्वक इस्तेमाल किया गया है , लेकिन अक्सर एक प्रौद्योगिकी और कुछ है कि केवल एक विशेषज्ञ का उपयोग कर सकते हैं की तुलना में एक कला के अधिक माना जाता था , हाल ही में .</s>
यह सच है कि एक गहरी सीखने एल्गोरिथ्म से अच्छा प्रदर्शन प्राप्त करने के लिए कुछ कौशल की आवश्यकता है .</s>
सौभाग्य से , प्रशिक्षण डेटा की मात्रा बढ़ने के रूप में आवश्यक कौशल की मात्रा कम हो जाती है .</s>
आज जटिल कार्यों पर मानव प्रदर्शन तक पहुंचने वाली अधिगम एल्गोरिदम , 1980 के दशक में खिलौना समस्याओं को हल करने के लिए संघर्ष करने वाली एल्गोरिदम के लगभग समान है , हालांकि इन एल्गोरिदम के साथ प्रशिक्षित मॉडलों में 18 CHAPTER 1 है .</s>
INTRODUCING में परिवर्तन किए गए जो बहुत गहरे वास्तुशिल्पों के प्रशिक्षण को सरल बनाते हैं ।</s>
सबसे महत्वपूर्ण नया विकास है कि आज हम इन एल्गोरिदम को सफल होने के लिए आवश्यक संसाधनों के साथ प्रदान कर सकते हैं .</s>
चित्र 1 . 8 से पता चलता है कि कैसे बेंचमार्क डेटासेट के आकार का समय के साथ उल्लेखनीय विस्तार हुआ है .</s>
यह प्रवृत्ति समाज के बढ़ते अंकीकरण से प्रेरित है ।</s>
जितनी अधिक से अधिक हमारी गतिविधियां कंप्यूटर पर होती हैं , उतनी ही अधिक से अधिक हम जो करते हैं , दर्ज की जाती हैं ।</s>
जैसे - जैसे हमारे कंप्यूटरों का एक साथ नेटवर्क बढ़ता जा रहा है , इन अभिलेखों को केंद्रीकृत करना और उन्हें मशीन अधिगम अनुप्रयोगों के लिए उपयुक्त डेटासेट में ठीक करना आसान होता जा रहा है ।</s>
</s>
</s>
सार्वजनिक एसवीएचएन छविनेट</s>
सीआईएफआर - 10</s>
</s>
</s>
समय के साथ बढ़ती डेटासेट आकार .</s>
में संकलित सांख्यिकीय सांख्यिकीय डेटा में संकलित सांख्यिकीय डेटा में 1900 के प्रारंभिक माप , 1900 के प्रारंभिक माप , 1900 के प्रारंभिक माप , 1900 के प्रारंभिक माप , 1900 के प्रारंभिक माप , 1900 के प्रारंभिक माप , 1900 के प्रारंभिक माप , और 1900 के प्रारंभिक माप , और 1900 के सांख्यिकीय डेटा का अध्ययन किया गया था ।</s>
</s>
१९८० और १९९० के दशक में , मशीनी शिक्षण अधिक सांख्यिकीय हो गया और इसने दसियों हज़ार उदाहरणों वाले बड़े डेटासेटों का लाभ उठाना शुरू कर दिया , जैसे कि MNIST डेटासेट ( आंकड़े १९८० के अंक में )</s>
2000 के दशक के पहले दशक में , इसी आकार के अधिक परिष्कृत डेटासेट , जैसे CIFAR - 10 डेटासेट ( Krizhevsky और Hinton , 2009 ) का उत्पादन जारी रहा .</s>
उस दशक के अंत में और 2010 के पूर्वार्ध में , उल्लेखनीय रूप से बड़े डेटासेट , जिसमें सैकड़ों हजारों से दसियों उदाहरण थे , पूरी तरह से बदल गया जो गहन अध्ययन के साथ संभव था .</s>
</s>
एम डेटासेट एम डेटासेट</s>
</s>
ग्राफ के शीर्ष पर , हम देखते हैं कि अनुवादित वाक्यों के डेटासेट , जैसे कि कनाडा के हंसार्ड से निर्मित आईबीएम के डेटासेट , 1990 और डब्ल्यूएमटी 2014 अंग्रेजी से फ्रांसीसी डेटासेट ( Schwenk 2014 ) आमतौर पर अन्य डेटासेट से काफी आगे हैं .</s>
19 CHAPTER 1 .</s>
चित्र 1 . 9</s>
MNIST डेटासेट से उदाहरण इनपुट</s>
“NIST” राष्ट्रीय मानक और प्रौद्योगिकी संस्थान के लिए खड़ा है , एजेंसी है कि मूल रूप से इस डेटा को एकत्र किया .</s>
“M” “मॉडेड” के लिए खड़ा है , क्योंकि डेटा मशीन सीखने एल्गोरिदम के साथ आसान उपयोग के लिए पूर्वप्रक्रमित किया गया है .</s>
MNIST डेटासेट में हस्तलिखित अंकों और संबद्ध लेबलों के स्कैन होते हैं , जो यह वर्णन करते हैं कि प्रत्येक छवि में 0 - 9 किस अंक में निहित है</s>
यह सरल वर्गीकरण समस्या गहन शिक्षण अनुसंधान में सबसे सरल और सबसे व्यापक रूप से इस्तेमाल किया परीक्षणों में से एक है .</s>
यह आधुनिक तकनीकों को हल करने के लिए काफी आसान होने के बावजूद लोकप्रिय रहता है .</s>
जिओफ्री हाइनटन ने इसे “ मशीन अधिगम का दर्शन” कहा है , जिसका अर्थ है कि यह मशीन अधिगम शोधकर्ताओं को नियंत्रित प्रयोगशाला परिस्थितियों में उनके एल्गोरिदम का अध्ययन करने में सक्षम बनाता है , बहुत कुछ जीवविज्ञानी अक्सर फल मक्खियों का अध्ययन करते हैं .</s>
मशीनी शिक्षण को काफी आसान बना दिया है क्योंकि सांख्यिकीय आकलन का मुख्य बोझ , जो केवल थोड़ी मात्रा में डेटा देखने के बाद नए डेटा को अच्छी तरह से उत्पन्न करता है , काफी हल्का कर दिया गया है ।</s>
2016 तक , अंगूठे का एक मोटा नियम यह है कि एक पर्यवेक्षित गहन शिक्षण एल्गोरिथ्म आम तौर पर लगभग 5 , 000 लेबल प्रति वर्ग के उदाहरणों के साथ स्वीकार्य प्रदर्शन हासिल करेगा और 20 CHAPTER 1 से मेल खाता है .</s>
INTRODUCING मानव निष्पादन से अधिक है जब एक डेटासेट के साथ प्रशिक्षित किया जाता है जिसमें कम से कम 10 मिलियन लेबल वाले उदाहरण होते हैं ।</s>
इससे छोटा डेटासेट के साथ सफलतापूर्वक कार्य करना एक महत्वपूर्ण अनुसंधान क्षेत्र है , विशेष रूप से इस बात पर ध्यान केंद्रित करना कि कैसे हम बड़ी मात्रा में अविश्वसनीय उदाहरणों का लाभ ले सकते हैं , जिसमें अनुपयुक्त या अर्ध - प्रसुप्त अधिगम है ।</s>
1 . 2 . 3</s>
बढ़ते मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज बेतहाशा सफल रहे हैं कि हम कम्प्यूटेशनल संसाधनों आज बहुत बड़े मॉडल चलाने के लिए .</s>
कनेक्शन - इस्म की एक मुख्य अंतर्दृष्टि है कि जानवर बुद्धिमान हो जाते हैं जब उनके न्यूरॉन्स में से कई एक साथ काम करते हैं .</s>
एक व्यक्तिगत न्यूरॉन या न्यूरॉन्स के छोटे संग्रह विशेष रूप से उपयोगी नहीं है .</s>
जैविक न्यूरॉन्स विशेष रूप से सघन रूप से जुड़े नहीं हैं .</s>
जैसा कि आंकड़ा 1 .10 में देखा गया है , हमारे मशीन सीखने मॉडलों भी दशकों के लिए स्तनधारी मस्तिष्क के परिमाण के एक क्रम के भीतर प्रति न्यूरॉन कनेक्शन की एक संख्या रहे हैं .</s>
न्यूरॉन्स की कुल संख्या के संदर्भ में , तंत्रिका नेटवर्क आश्चर्यजनक रूप से काफी हाल तक छोटे रहे हैं , जैसा कि आंकड़ा 1 .11 में दिखाया गया है .</s>
गुप्त इकाइयों के शुरू होने के बाद से , कृत्रिम तंत्रिका नेटवर्क मोटे तौर पर हर 2 . 4 साल में आकार में दोगुना हो गया है .</s>
यह वृद्धि तीव्र कंप्यूटरों द्वारा बड़ी स्मृति के साथ और बड़े डेटासेटों की उपलब्धता द्वारा संचालित होती है ।</s>
बड़े नेटवर्क अधिक जटिल कार्यों पर अधिक सटीकता प्राप्त करने में सक्षम होते हैं .</s>
यह प्रवृत्ति दशकों तक चलती रही ।</s>
जब तक नई प्रौद्योगिकियां तेजी से स्केलिंग को सक्षम नहीं करती हैं , कृत्रिम तंत्रिका नेटवर्कों में कम से कम 2050 के दशक तक मानव मस्तिष्क के समान न्यूरॉन्स की संख्या नहीं होगी .</s>
जैविक न्यूरॉन्स वर्तमान कृत्रिम न्यूरॉन्स की तुलना में अधिक जटिल कार्यों का प्रतिनिधित्व कर सकते हैं , इसलिए जैविक तंत्रिका नेटवर्क इस भूखंड चित्रण से भी बड़ा हो सकता है .</s>
पूर्वव्यापी में , यह विशेष रूप से आश्चर्य की बात नहीं है कि जोंक की तुलना में कम न्यूरॉन्स वाले तंत्रिका नेटवर्क परिष्कृत कृत्रिम बुद्धि प्रोब - लेम को हल करने में असमर्थ थे .</s>
आज के नेटवर्क भी , जिन्हें हम कम्प्यूटेशनल सिस्टम की दृष्टि से काफी बड़े मानते हैं , मेंढक जैसे अपेक्षाकृत आदिम कशेरुकी प्राणियों के तंत्रिका तंत्र से छोटे हैं ।</s>
समय के साथ मॉडल आकार में वृद्धि , तीव्र सीपीयू की उपलब्धता के कारण , सामान्य उद्देश्य जीपीयू ( सेक्शन 12 . 1 . 2 में उल्लिखित ) , तीव्र नेटवर्क कनेक्टिविटी और वितरित कंप्यूटिंग के लिए बेहतर सॉफ्टवेयर मूल संरचना , गहन शिक्षण के इतिहास की सबसे महत्वपूर्ण प्रवृत्तियों में से एक है .</s>
यह प्रवृत्ति आमतौर पर भविष्य में भी अच्छी तरह जारी रहने की उम्मीद है ।</s>
21 CHAPTER 1 .</s>
INTRODUCTER 1985 2000 2015 वर्ष 10 10 10 2 10 3 10 4 कनेक्शन प्रति न्यूरॉन 1 2 3 4 5 6 8 9 फल मक्खी माउस बिल्ली मानव चित्र 110</s>
: समय के साथ प्रति न्यूरॉन कनेक्शन की संख्या .</s>
प्रारंभ में , कृत्रिम तंत्रिका नेटवर्कों में न्यूरॉन्स के बीच कॉन्नेक - टीनों की संख्या हार्डवेयर क्षमताओं द्वारा सीमित थी .</s>
आज , न्यूरॉन्स के बीच कनेक्शन की संख्या ज्यादातर एक डिजाइन विचार है .</s>
कुछ कृत्रिम तंत्रिका नेटवर्कों में प्रति न्यूरॉन लगभग उतने ही कनेक्शन होते हैं जितने एक बिल्ली के रूप में होते हैं , और अन्य तंत्रिका नेटवर्कों के लिए चूहों जैसे छोटे स्तनधारियों के प्रति न्यूरॉन के अधिक से अधिक कनेक्शन होना आम बात है .</s>
यहां तक कि मानव मस्तिष्क प्रति न्यूरॉन कनेक्शन की एक अनाप - शनाप मात्रा नहीं है .</s>
विकिपीडिया से जैविक तंत्रिका नेटवर्क आकार (2015 )</s>
1 . 1</s>
अनुकूली रैखिक तत्व</s>
नियोकोग्निन ( Fukushima ) , 1980 में 3</s>
</s>
</s>
डीप बोल्ट्जमैन मशीन</s>
अनुपूरक संवलन नेटवर्क (</s>
जेरेट एट अल</s>
</s>
</s>
↑ 2010 का 7वां</s>
वितरित ऑटोनकोडर (</s>
ले एट अल</s>
↑ 2012 का 8वां</s>
बहु - जीपीयू संवलन नेटवर्क (</s>
Krizhevsky एट अल</s>
↑ 2012 का 9 .</s>
COTS HPC अनुपयुक्त संवलयन नेटवर्क (</s>
कोट्स एट अल , 2013 10</s>
गोगली</s>
1 . 2 . 4</s>
बढ़ती शुद्धता , जटिलता और वास्तविक प्रभाव</s>
1980 के दशक के बाद से , गहरी सीखने लगातार सटीक मान्यता और भविष्यवाणी प्रदान करने की क्षमता में सुधार हुआ है .</s>
इसके अलावा , गहरी सीखने लगातार आवेदनों के व्यापक और व्यापक सेट के लिए सफलता के साथ लागू किया गया है .</s>
सबसे पहले गहरे मॉडलों का उपयोग व्यक्तिगत वस्तुओं को कसकर फसल में पहचानने के लिए किया जाता था , अत्यंत छोटी छवियों (</s>
रूमाहार्ट एट अल</s>
तब से छवियों के आकार में क्रमिक वृद्धि हुई है तंत्रिका नेटवर्क प्रक्रिया कर सकता है .</s>
आधुनिक वस्तु अभिज्ञान नेटवर्क उच्च - विभेदन तस्वीरों को समृद्ध करता है और 22 सीएचएपीटर 1 नहीं करता है ।</s>
INTRODUCING 1950 2000 2015 2056 वर्ष 10 −2 10 −1</s>
10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</s>
10 11 न्यूरॉन्स (logarithmic पैमाने पर</s>
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</s>
</s>
गुप्त इकाइयों के शुरू होने के बाद से , कृत्रिम तंत्रिका नेटवर्क मोटे तौर पर हर 2 . 4 साल में आकार में दोगुना हो गया है .</s>
विकिपीडिया से जैविक तंत्रिका नेटवर्क आकार (2015 )</s>
1 . 1</s>
पर्सेप्टॉन (Rosenblatt , 1958 , 1962 ) 2</s>
अनुकूली रैखिक तत्व</s>
नियोकोग्निन ( Fukushima ) , 1980 ई . 4</s>
प्रारंभिक बैकप्रोपागेशन नेटवर्क</s>
भाषण मान्यता के लिए पुनरावर्ती तंत्रिका नेटवर्क</s>
6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6 . 6</s>
भाषण मान्यता के लिए मल्टीलेयर पर्सेप्टॉन ( Bengio एट अल )</s>
↑ 1991 का 7 .</s>
अर्थ क्षेत्र अवग्रह विश्वास नेटवर्क ( Saul एट अल , 1996 )</s>
८ . ८ . ८ .</s>
लेनेट - 5</s>
</s>
↑ 1998 का</s>
इको स्टेट नेटवर्क</s>
10 .</s>
डीप विश्वास नेटवर्क</s>
</s>
</s>
डीप बोल्ट्जमैन मशीन</s>
</s>
रैना एट अल</s>
↑ 2009 का 14वां</s>
अनुपूरक संवलन नेटवर्क (</s>
जेरेट एट अल</s>
</s>
</s>
16 . 16 .</s>
ओएमपी - 1 नेटवर्क</s>
वितरित ऑटोनकोडर (</s>
ले एट अल , 2012 18</s>
बहु - जीपीयू संवलन नेटवर्क (</s>
Krizhevsky एट अल</s>
↑ 2012 19 .</s>
COTS HPC अनुपयुक्त संवलयन नेटवर्क (</s>
कोट्स एट अल , 2013 20</s>
</s>
इसी प्रकार , प्रारंभिक नेटवर्क केवल दो प्रकार की वस्तुओं को पहचान सकता है ( कुछ मामलों में , एक ही प्रकार की वस्तु का अभाव या उपस्थिति , जबकि ये आधुनिक नेटवर्क आम तौर पर कम से कम 1 , 000 विभिन्न श्रेणियों की वस्तुओं को पहचानते हैं .</s>
वस्तु मान्यता में सबसे बड़ी प्रतियोगिता इमेजनेट 23 सीएचएपीटर 1 है ।</s>
INTRODUC सुविधाएँ</s>
बड़े पैमाने पर दृश्य अभिज्ञान चैलेंज (आईएलएसवीआरसीसी ) प्रत्येक वर्ष आयोजित किया जाता है .</s>
डीप लर्निंग के मौसमी उत्थान में एक नाटकीय क्षण आया जब एक संवलित नेटवर्क ने पहली बार इस चुनौती को जीता और एक व्यापक मार्जिन के द्वारा , राज्य के सबसे ऊपर - 5 त्रुटि दर 261 प्रतिशत से 153 प्रतिशत (</s>
तब से , ये प्रतियोगिताएं लगातार गहरे संवलित जालों द्वारा जीती जाती हैं , और इस लेखन के रूप में , गहन शिक्षण के अग्रिमों ने इस प्रतियोगिता में नवीनतम शीर्ष - ५ त्रुटि दर को ३ . ६ प्रतिशत तक पहुंचाया है , जैसा कि अंक ११२ में दिखाया गया है .</s>
गहन शिक्षण भी भाषण मान्यता पर एक नाटकीय प्रभाव पड़ा है .</s>
1990 के दशक भर में सुधार के बाद , भाषण मान्यता के लिए त्रुटि दरों लगभग 2000 में शुरू हुआ .</s>
परिचय</s>
हम इस इतिहास का अधिक विस्तार से से सेक्शन 12 . 3 में अन्वेषण करते हैं ।</s>
डीप नेटवर्कों को पैदल पता लगाने और छवि विभाजन के लिए भी शानदार सफलताएं मिली हैं ।</s>
</s>
कार्सन एट अल , 2012</s>
उसी समय , जब गहरे नेटवर्कों का पैमाना और सटीकता बढ़ी है , 2010 2011 2012 2014 2015 वर्ष 0 . 0 .05 0 .10 .15</s>
0 .20 0 .25 0 .30 ILSVRC वर्गीकरण त्रुटि दर चित्र 1 .12 : समय के साथ त्रुटि दर में कमी</s>
चूंकि गहरे नेटवर्क इमेजनेट लार्ज स्केल दृश्य मान्यता चैलेंज में प्रतिस्पर्धा करने के लिए आवश्यक पैमाने पर पहुंचे , उन्होंने लगातार प्रतिवर्ष प्रतियोगिता जीती है , जिससे हर बार कम और कम त्रुटि दरों का उत्पादन होता है .</s>
रसाकोवस्की एट अल से डेटा</s>
(2014 )</s>
और वह एट अल .</s>
(2015 )</s>
24 CHAPTER 1 .</s>
INTRODUCTER में ऐसे कार्यों की जटिलता है जिन्हें वे हल कर सकते हैं .</s>
गुडफेलो एट अल</s>
(2014 )</s>
पहले , यह व्यापक रूप से माना जाता था कि इस तरह के सीखने के लिए अनुक्रम के व्यक्तिगत तत्वों की लेबलिंग की आवश्यकता होती है ( Gülçhre और बेंजियो , 2013 )</s>
पुनरावर्ती तंत्रिका नेटवर्क , जैसे कि एलएसटीएम अनुक्रम मॉडल , जिसका उल्लेख ऊपर किया गया है , अब केवल स्थिर इनपुट के बजाय अनुक्रमों और अन्य अनुक्रमों के बीच संबंधों को मॉडल करने के लिए प्रयोग किया जाता है .</s>
Name</s>
अल</s>
बढ़ती जटिलता की इस प्रवृत्ति को तंत्रिका तंत्र मशीनों ( Graves एट अल , 2014 ) के आरंभ के साथ अपने तार्किक निष्कर्ष पर धकेल दिया गया है जो स्मृति कोशिकाओं से पढ़ना सीखते हैं और स्मृति कोशिकाओं को मनमाने ढंग से सामग्री लिखते हैं ।</s>
इस तरह के तंत्रिका नेटवर्क वांछित व्यवहार के उदाहरणों से सरल प्रोग्राम सीख सकते हैं .</s>
उदाहरण के लिए , वे स्क्रम्बल्ड और अनुक्रमित अनुक्रमों के उदाहरण दिए गए संख्याओं की सूचियों को छांटना सीख सकते हैं .</s>
यह स्व - प्रोग्रामिंग तकनीक अपने शैशव काल में है , लेकिन भविष्य में इसे सैद्धांतिक रूप से लगभग किसी भी कार्य के लिए लागू किया जा सकता है .</s>
गहन अधिगम की एक और प्रमुख उपलब्धि है , प्रवर्तन अधिगम के क्षेत्र में इसका विस्तार ।</s>
प्रवर्तन सीखने के संदर्भ में , एक स्वायत्त एजेंट को परीक्षण और त्रुटि के द्वारा एक कार्य करना सीखना चाहिए , मानव ऑपरेटर से किसी मार्गदर्शन के बिना .</s>
डीपमिन्ड ने यह प्रदर्शित किया कि गहन अधिगम पर आधारित एक प्रवर्तन अधिगम प्रणाली अतरी वीडियो गेम खेलने के लिए सीखने में सक्षम है , कई कार्यों पर मानव स्तर पर प्रदर्शन तक पहुँचने .</s>
</s>
गहन अध्ययन ने रोबोटिक्स ( एफिन एट अल , 2015 ) के लिए प्रवर्तन सीखने के प्रदर्शन में भी महत्वपूर्ण सुधार किया है ।</s>
गहन शिक्षण के इन अनुप्रयोगों में से कई अत्यधिक लाभप्रद हैं .</s>
डीप लर्निंग अब कई शीर्ष प्रौद्योगिकी कंपनियों द्वारा प्रयोग किया जाता है , जिनमें गूगल , माइक्रोसॉफ्ट , फेसबुक , आईबीएम , एप्पल , एडोब , नेटफ्लिक्स , एनवीआईए , और एनईसी शामिल हैं</s>
गहन शिक्षण में अग्रिम भी सॉफ्टवेयर बुनियादी ढांचे में अग्रिमों पर काफी निर्भर किया गया है .</s>
सॉफ्टवेयर पुस्तकालय जैसे</s>
</s>
</s>
टेन्सरफ्लो ( एबाडी एट अल , 2015 ) ने सभी महत्वपूर्ण अनुसंधान परियोजनाओं या वाणिज्यिक उत्पादों का समर्थन किया है ।</s>
गहन अध्ययन ने अन्य विज्ञानों में भी योगदान दिया है ।</s>
वस्तु अभिज्ञान के लिए आधुनिक संवलित नेटवर्क दृश्य प्रक्रमण का एक मॉडल प्रदान करते हैं जिसका तंत्रिका विज्ञानी अध्ययन कर सकते हैं (DiCarlo , 2013 )</s>
गहन अध्ययन भारी मात्रा में डेटा के प्रसंस्करण और वैज्ञानिक 25 सीएचएपीटर 1 में उपयोगी भविष्यवाणियां करने के लिए उपयोगी उपकरण भी प्रदान करता है ।</s>
INTRODUCING फ़ील्ड्स .</s>
इसका सफल प्रयोग यह भविष्यवाणी करने के लिए किया गया है कि कैसे अणु दवाओं के डिजाइन में मदद करने के लिए बातचीत करेंगे ( Dahl एट अल , 2014 )</s>
हमें उम्मीद है कि गहन शिक्षण भविष्य में अधिक से अधिक वैज्ञानिक क्षेत्रों में प्रकट होगा .</s>
सारांश में , गहन अधिगम मशीन अधिगम के प्रति एक दृष्टिकोण है जिसने पिछले कई दशकों में विकसित मानव मस्तिष्क , सांख्यिकी और अनुप्रयुक्त गणित के बारे में हमारे ज्ञान पर भारी खींचा है ।</s>
हाल के वर्षों में , गहन अधिगम ने अपनी लोकप्रियता और उपयोगिता में अत्यधिक वृद्धि देखी है , मुख्य रूप से अधिक शक्तिशाली कंप्यूटरों , गहन नेटवर्कों को प्रशिक्षित करने के लिए बड़े डेटासेटों और तकनीकों के परिणामस्वरूप .</s>
आगे के वर्ष गहरी शिक्षा में और भी सुधार लाने और उसे नई सीमाओं तक लाने की चुनौतियों और अवसरों से भरे हुए हैं ।</s>
26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26</s>
