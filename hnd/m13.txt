अध्याय 1 परिचय निवेशकों को लंबे समय से मशीनों है कि लगता है बनाने का सपना देखा है .</s>
यह इच्छा कम से कम प्राचीन यूनान के समय की है .</s>
पौराणिक पात्रों मसलन पिग्मैल , डैडलस और हेफेस्टस को शायद अनुश्रुत आविष्कारक माना जाए ।</s>
जब प्रोग्राम योग्य कंप्यूटरों की कल्पना पहली बार की गई तो लोगों ने सोचा कि क्या ऐसी मशीनें किसी के निर्माण से सौ वर्ष पहले बुद्धिमान बन सकती हैं ( लॉवलेसे , 1842 ) ।</s>
आज , कृत्रिम बुद्धि कई व्यावहारिक अनुप्रयोगों और सक्रिय अनुसंधान विषयों के साथ एक फलता - फूलता क्षेत्र है .</s>
हम सामान्य कार्य को स्वचालित करने के लिए बुद्धिमत्तापूर्ण सॉफ़्टवेयर की ओर देखते हैं , भाषण या छवियों को समझते हैं , चिकित्सा में निदान करते हैं और मूल वैज्ञानिक अनुसंधान को समर्थन देते हैं ।</s>
कृत्रिम बुद्धि के शुरुआती दिनों में , क्षेत्र तेजी से निपटने और समस्याओं है कि बौद्धिक रूप से मुश्किल है कि मानव के लिए लेकिन कंप्यूटर के लिए अपेक्षाकृत सीधे अग्रगामी है -</s>
समस्याओं है कि औपचारिक , गणित - विषयक नियमों की एक सूची द्वारा वर्णित किया जा सकता है .</s>
कृत्रिम बुद्धि के लिए सच्ची चुनौती उन कार्यों को हल करने में सिद्ध हुई जो लोगों के लिए आसान हैं , लेकिन लोगों के लिए औपचारिक रूप से वर्णन करने के लिए कठिन हैं - वे संकेत जिन्हें हम अंतर्ज्ञानात्मक रूप से हल करते हैं , जो बोलचाल के शब्दों को पहचानने या तस्वीरों में मुंह दिखाने के समान स्वतः अनुभव करते हैं ।</s>
यह पुस्तक इन अधिक सहज ज्ञान युक्त समस्याओं के एक समाधान के बारे में है .</s>
इस समाधान है कि कंप्यूटर अनुभव से सीखने के लिए और अवधारणाओं के एक पदानुक्रम के संदर्भ में दुनिया को समझने के लिए , सरल अवधारणाओं के साथ अपने संबंध के माध्यम से परिभाषित प्रत्येक अवधारणा के साथ .</s>
अनुभव से ज्ञान इकट्ठा करके , यह दृष्टिकोण सभी ज्ञान है कि कंप्यूटर की जरूरत है औपचारिक रूप से निर्दिष्ट करने के लिए मानव ऑपरेटरों की जरूरत से बचता है .</s>
संकल्पनाओं का पदानुक्रम कंप्यूटर को सरल अवधारणाओं से निर्मित करके जटिल अवधारणाओं को सीखने में सक्षम बनाता है ।</s>
यदि हम एक ग्राफ दिखा कैसे इन अवधारणाओं 1 अध्याय 1 आरेखित करें ।</s>
INTRODUCTION एक दूसरे के ऊपर बने हैं , ग्राफ गहरा है , कई परतों के साथ ।</s>
इस कारण के लिए , हम एआई गहन सीखने के लिए इस दृष्टिकोण कहते हैं ।</s>
एआई की कई प्रारंभिक सफलताएं अपेक्षाकृत बंध्य और औपचारिक वातावरण में हुई और इसके लिए कंप्यूटर को दुनिया के बारे में अधिक जानकारी रखने की आवश्यकता नहीं थी .</s>
उदाहरण के लिए , आईबीएम की डीप ब्लू शतरंज खेलने की प्रणाली ने 1997 में विश्व चैंपियन गैरी कास्परोव को हराया ( होसु , 2002 )</s>
निस्संदेह शतरंज एक बहुत ही सरल संसार है , जिसमें केवल चौंसठ स्थान और चौंतीस टुकड़े हैं , जो केवल कठोर परिधि में ही चल सकते हैं ।</s>
शतरंज की सफल रणनीति का वर्णन करना एक बहुत बड़ी उपलब्धि है , लेकिन चुनौती शतरंज के टुकड़े के सेट का वर्णन करने में कठिनाई और कंप्यूटर में स्वीकार्य चालों के कारण नहीं है .</s>
शतरंज पूरी तरह से औपचारिक नियमों की एक बहुत ही संक्षिप्त सूची द्वारा वर्णित किया जा सकता है , आसानी से प्रोग्रामर द्वारा समय से पहले प्रदान की .</s>
विडंबना यह है कि , अमूर्त और औपचारिक कार्य है कि एक मानव के लिए सबसे कठिन मानसिक उपक्रमों में से एक कंप्यूटर के लिए सबसे आसान में हैं .</s>
कंप्यूटर लंबे समय से सर्वश्रेष्ठ मानव शतरंज खिलाड़ी को भी हराने में सफल रहे हैं , लेकिन केवल हाल ही में वस्तुओं या बोली पहचान करने के लिए औसत मानव की क्षमताओं में से कुछ मिलान शुरू कर दिया है .</s>
एक व्यक्ति के दैनिक जीवन के लिए विश्व के बारे में अपार ज्ञान की आवश्यकता होती है ।</s>
इस ज्ञान का अधिकांश व्यक्तिपरक और सहज ज्ञान युक्त है , और इसलिए औपचारिक रूप से मुखर होना कठिन है .</s>
कम्प्यूटरों को बुद्धिमानी के तरीके से व्यवहार करने के लिए इसी ज्ञान को हथियाने की आवश्यकता होती है ।</s>
कृत्रिम बुद्धि में एक प्रमुख चुनौती यह है कि कम्प्यूटर में इस अनौपचारिक ज्ञान को कैसे प्राप्त किया जाए ।</s>
अनेक कृत्रिम बुद्धि परियोजनाओं ने औपचारिक भाषाओं में विश्व के बारे में कठोर ज्ञान प्राप्त करने का प्रयास किया है ।</s>
तार्किक अनुमान नियमों का प्रयोग करते हुए कंप्यूटर इन औपचारिक भाषाओं में वक्तव्यों के बारे में स्वतः ही तर्क दे सकता है ।</s>
इसे कृत्रिम बुद्धिमत्ता का ज्ञानाधार दृष्टिकोण कहा जाता है ।</s>
इनमें से किसी भी परियोजना से बड़ी सफलता नहीं मिली है ।</s>
इस तरह की सबसे प्रसिद्ध परियोजनाओं में से एक है साइक ( एलेनाट और गुहा , 1989 ) .</s>
साइक एक अनुमान इंजन है और एक भाषा में कथनों का डेटाबेस है , जिसे साइकल कहते हैं .</s>
इन बयानों को मानव पर्यवेक्षक के एक स्टाफ द्वारा दर्ज किया जाता है .</s>
यह एक दुर्वह प्रक्रिया है ।</s>
लोग दुनिया को सही ढंग से बताने के लिए पर्याप्त जटिलता के साथ औपचारिक नियम बनाने के लिए संघर्ष करते हैं .</s>
उदाहरण के लिए , साइक सुबह फ्रेड शेव नामक व्यक्ति के बारे में एक कहानी को समझने में असफल रहा ( लिंड , 1992 ) .</s>
इसके अनुमान इंजन को कहानी में एक असंगति का पता चलाः यह पता था कि लोगों के पास बिजली के पुर्जे नहीं हैं , लेकिन चूंकि फ्रेड बिजली के रेज़र था , इसलिए उसने इस इकाई का विश्वास किया “ फ्रेडव्हाइल - शैविंग” में विद्युत पुर्जे थे .</s>
इसलिए उसने पूछा कि क्या फ्रेड अभी भी व्यक्ति था जबकि वह शेव कर रहा था .</s>
कठोर ज्ञान पर भरोसा करने वाली प्रणालियों के सामने आने वाली कठिनाइयों से पता चलता है कि एआइएस प्रणालियों के लिए अपने स्वयं के ज्ञान को प्राप्त करने की क्षमता की जरूरत है , 2 अध्याय 1 का निष्कर्ष निकालने के द्वारा .</s>
कच्चे डेटा से सूचना पैटर्न</s>
इस क्षमता को मशीन लर्निंग के नाम से जाना जाता है ।</s>
मशीनी शिक्षा की शुरुआत से कंप्यूटर वास्तविक दुनिया की जानकारी से जुड़ी समस्याओं से निपटने में सक्षम हो गए और वे व्यक्तिपरक लगने वाले निर्णय ले सके ।</s>
एक साधारण मशीनी अधिगम एल्गोरिथ्म जिसे लॉजिस्टिक प्रतिगमन कहते हैं , सिजेरियन डिलीवरी ( मोर - योजफ एट अल , 1990 ) की सिफारिश कर सकता है ।</s>
एक सरल मशीन लर्निंग अल्गोरिथम जिसे भोले बेयॉ कहा जाता है , स्पैम ई - मेल से वैध इ - मेल को अलग कर सकता है ।</s>
इन सरल मशीन लर्निंग एल्गोरिदम का प्रदर्शन उनके द्वारा दिए गए डेटा के निरूपण पर काफी निर्भर करता है .</s>
उदाहरण के लिए , जब रसद प्रतिगमन सिजेरियन प्रसव की सिफारिश करने के लिए इस्तेमाल किया जाता है , एआई प्रणाली रोगी की सीधे जांच नहीं करता है .</s>
इसके बजाय , चिकित्सक सिस्टम को प्रासंगिक जानकारी के कई टुकड़े बताता है , जैसे गर्भाशय के निशान की उपस्थिति या अनुपस्थिति .</s>
रोगी के प्रतिनिधित्व में शामिल जानकारी के प्रत्येक टुकड़ा एक विशेषता के रूप में जाना जाता है .</s>
लॉजिस्टिक प्रतिगमन यह सीखता है कि रोगी की इन विशेषताओं में से प्रत्येक का विभिन्न परिणामों से संबंध कैसे होता है ।</s>
हालांकि , यह कैसे सुविधाओं को किसी भी तरह से परिभाषित कर रहे हैं प्रभावित नहीं कर सकते हैं .</s>
यदि रोगी को लॉजिस्टिक रीग्रेशन एमआरआई स्कैन दिया जाता , बजाय चिकित्सक की औपचारिक रिपोर्ट के , तो वह उपयोगी भविष्यवाणियां नहीं कर सकेगा ।</s>
एमआरआई स्कैन में व्यक्तिगत पिक्सल का प्रसव के दौरान होने वाली जटिलताओं के साथ नगण्य सहसंबंध होता है ।</s>
अभ्यावेदन पर यह निर्भरता एक सामान्य घटना है जो कंप्यूटर विज्ञान और यहां तक कि दैनिक जीवन भर दिखाई देती है .</s>
कंप्यूटर विज्ञान में , डेटा संग्रह की खोज जैसे ऑपरेशन घातीय रूप से तेजी से आगे बढ़ सकते हैं यदि कोलेक - टी का संरचित और अनुक्रमण समझदारी से हो ।</s>
लोग अरबी अंकों पर अंकगणित आसानी से कर सकते हैं लेकिन रोमन अंकों पर अंकगणित को बहुत अधिक समय लेने वाला पाते हैं ।</s>
यह आश्चर्य की बात नहीं है कि प्रतिनिधित्व के चयन का मशीन लर्निंग एल्गोरिदम के प्रदर्शन पर भारी प्रभाव पड़ता है .</s>
एक सरल दृश्य उदाहरण के लिए , देखें आंकड़ा 1 . 1 . 1</s>
कई कृत्रिम बुद्धि कार्यों उस कार्य के लिए निकालने के लिए सुविधाओं के सही सेट डिजाइन करने के द्वारा हल किया जा सकता है , तो एक सरल मशीन सीखने एल्गोरिथ्म के लिए इन सुविधाओं को प्रदान .</s>
उदाहरण के लिए , ध्वनि से वक्ता की पहचान के लिए एक उपयोगी विशेषता वक्ता के स्वर नली के आकार का अनुमान है .</s>
इस विशेषता से एक मजबूत सुराग मिलता है कि वक्ता एक पुरुष , महिला , या बच्चे है .</s>
कई कार्यों के लिए , तथापि , यह मुश्किल है पता करने के लिए क्या सुविधाओं को निकाला जाना चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हम फोटो में कारों का पता लगाने के लिए एक प्रोग्राम लिखना चाहेंगे .</s>
हम जानते हैं कि कारों पहिया है , तो हम एक विशेषता के रूप में एक पहिया की उपस्थिति का उपयोग करना चाहते हो सकता है .</s>
दुर्भाग्य से , यह वास्तव में क्या एक पहिया पिक्सेल मूल्यों के संदर्भ में की तरह लग रहा है का वर्णन करने के लिए मुश्किल है .</s>
पहिए की एक सरल ज्यामितीय आकृति होती है , लेकिन उसकी छवि व्हील पर पड़ने वाली छायाओं , पहिए के धातु भागों से चमकते सूर्य , कार का लिंग या किसी वस्तु से 3 अध्याय 1 में उलझ सकती है ।</s>
परिचय</s>
</s>
।</s>
</s>
।</s>
आकृति 1 . 1</s>
विभिन्न अभ्यावेदनों का उदाहरणः मान लीजिए कि हम एक स्कैटरपॉट में उनके बीच एक रेखा खींचकर दो श्रेणियों के डेटा को अलग करना चाहते हैं .</s>
बाईं ओर के भूखंड में , हम कार्टेसियन निर्देशांक का उपयोग कर कुछ डेटा का प्रतिनिधित्व करते हैं , और कार्य असंभव है .</s>
दाईं ओर के भूखंड में , हम ध्रुवीय निर्देशांकों के साथ डेटा का प्रतिनिधित्व करते हैं और कार्य एक ऊर्ध्वाधर रेखा के साथ हल करने के लिए सरल हो जाता है .</s>
।</s>
इस समस्या का एक समाधान यह है कि न केवल प्रतिनिधित्व से आउटपुट तक मानचित्रण का पता लगाने के लिए मशीन लर्निंग का उपयोग करें , बल्कि स्वयं प्रतिनिधित्व भी करें .</s>
इस दृष्टिकोण प्रतिनिधित्व सीखने के रूप में जाना जाता है .</s>
विद्वत अभ्यावेदन अक्सर हाथ से प्राप्त प्रतिनिधित्व की तुलना में कहीं बेहतर प्रदर्शन में परिणाम है .</s>
वे भी एआइ प्रणालियों को तेजी से नए कार्यों के लिए अनुकूलित करने के लिए सक्षम बनाता है , कम से कम मानव हस्तक्षेप के साथ .</s>
एक अभ्यावेदन अधिगम एल्गोरिथ्म , मिनटों में एक सरल कार्य के लिए या घंटों से महीनों में एक जटिल कार्य के लिए सुविधाओं के एक अच्छे सेट की खोज कर सकता है .</s>
डिजाइन जटिल कार्य की आवश्यकता है जटिल कार्य के लिए मानव समय की विशेषताओं और प्रयास के लिए एक महान हस्तचालित सौदा के लिए दशकों लेता है शोधकर्ताओं के पूरे समुदाय के लिए एक</s>
एक अभ्यावेदन शिक्षण एल्गोरिथ्म का वास्तविक उदाहरण ऑ - टोनकोडर है ।</s>
स्वतःकोडक , एक एनकोडर फलन का संयोजन है , जो इनपुट डेटा को भिन्न अभ्यावेदन , और एक विकोडक फलन में परिवर्तित करता है , जो नए निरूपण को वापस मूल प्रारूप में परिवर्तित करता है ।</s>
जब किसी इनपुट को एनकोडर और उसके बाद विकोडक के माध्यम से चलाया जाता है तो ऑटोनकोडर्स को यथासंभव अधिक से अधिक जानकारी सुरक्षित रखने के लिए प्रशिक्षित किया जाता है , लेकिन उन्हें नए निरूपण के विभिन्न अच्छे गुण बनाने के लिए भी प्रशिक्षित किया जाता है ।</s>
विभिन्न प्रकार के ऑटोनकोडर्स का लक्ष्य विभिन्न प्रकार की संपत्तियों को प्राप्त करना होता है ।</s>
जब सीखने सुविधाओं के लिए सुविधाओं या एल्गोरिदम डिजाइन , हमारा लक्ष्य आम तौर पर भिन्नता के कारकों को अलग करने के लिए है कि मनाया डेटा की व्याख्या .</s>
इस 4 अध्याय में 1 .</s>
आईटीआरओ संदर्भ , हम “फैक्टरर्स” का उपयोग करते हैं केवल “उपकरण” के गुणन शब्दों को सम्मिलित करने के लिए अलग - अलग स्रोतों को संदर्भित करते हैं ।</s>
इस तरह के कारक अक्सर ऐसी मात्राएं नहीं होती जो सीधे तौर पर देखी जाती हैं ।</s>
इसके स्थान पर वे भौतिक जगत में या तो अनपेक्षित वस्तुओं के रूप में विद्यमान हो सकते हैं या प्रेक्षण योग्य मात्राओं को प्रभावित करने वाली अगोचर शक्तियों के रूप में ।</s>
वे मानव मस्तिष्क में निर्मित वस्तुओं के रूप में भी विद्यमान हो सकते हैं जो प्रेक्षित आंकड़ों की व्याख्याओं या अनुमानित कारणों को सरल बनाती हैं ।</s>
उन्हें उन संकल्पनाओं या अमूर्तनों के रूप में समझा जा सकता है जो डेटा की समृद्ध भिन्नता का बोध कराने में हमारी सहायता करते हैं ।</s>
जब किसी वाक् अभिलेख का विश्लेषण किया जाता है तो विभिन्न कारकों में वक्ता की आयु , उनके लिंग , उनके लहजे और वे बोल रहे होते हैं ।</s>
किसी कार की छवि का विश्लेषण करने पर , विभिन्नता के कारकों में कार की स्थिति , उसके रंग , और सूर्य के कोण और चमक शामिल हैं .</s>
कृत्रिम बुद्धि के अनेक प्रयोगों में कठिनाई का एक बड़ा कारण यह है कि विभिन्नता के अनेक कारक ऐसे आंकड़ों के प्रत्येक टुकड़े को प्रभावित करते हैं जिन्हें हम देख सकते हैं ।</s>
लाल कार की छवि में व्यक्तिगत पिक्सल रात में काले रंग के बहुत करीब हो सकता है .</s>
कार के सिलहोट का आकार देखने के कोण पर निर्भर करता है ।</s>
अधिकांश अनुप्रयोगों की आवश्यकता होती है कि हम विभिन्नता के कारकों को सुलझाएं और उन कारकों को त्यागें जिनके बारे में हम परवाह नहीं करते .</s>
बेशक , कच्चे डेटा से इस तरह के उच्च स्तर , अमूर्त सुविधाओं को निकालना बहुत मुश्किल हो सकता है .</s>
विभिन्नता के इन कारकों , जैसे कि वक्ता के लहजे , की पहचान केवल परिष्कृत , लगभग मानव स्तर डेटा की समझ का उपयोग करके की जा सकती है .</s>
जब मूल समस्या को हल करने के लिए प्रतिनिधित्व प्राप्त करना लगभग उतना ही कठिन होता है , तब अभ्यावेदन सीखना , पहली नजर में , हमारी सहायता करता प्रतीत होता है ।</s>
गहन शिक्षण प्रतिनिधित्व शिक्षण में इस केंद्रीय समस्या का समाधान आत्म - द्वैध निरूपण के माध्यम से करता है जो अन्य , सरल निरूपण के संदर्भ में व्यक्त किया जाता है ।</s>
गहन अधिगम कंप्यूटर को सरल विचारों से जटिल संकल्पनाओं का निर्माण करने में सक्षम बनाता है ।</s>
चित्र 1 . 2 से पता चलता है कि कैसे एक गहन शिक्षण प्रणाली सरल अवधारणाओं , जैसे कोनों और रेखाओं , जो बदले में किनारों के संदर्भ में परिभाषित कर एक व्यक्ति की छवि की अवधारणा का प्रतिनिधित्व कर सकती है ।</s>
एक गहन अधिगम मॉडल का सारभूत उदाहरण फीड के गहरे नेटवर्क , या मल्टीलेयर पर्सेप्टॉन ( एमएलपी ) है ।</s>
एक मल्टीलेर परसेप्टरॉन , मात्र एक गणितीय फलन है , जो इनपुट मानों के कुछ समुच्चय को आउटपुट मानों के लिए प्रतिचित्रित करता है ।</s>
समारोह कई सरल कार्यों की रचना करके बनता है .</s>
हम इनपुट का एक नया प्रतिनिधित्व प्रदान करने के रूप में एक अलग गणितीय समारोह के प्रत्येक आवेदन के बारे में सोच सकते हैं .</s>
डेटा के लिए सही प्रतिनिधित्व सीखने का विचार गहन अधिगम पर एक प्रतिव्यापी प्रदान करता है .</s>
डीप लर्निंग का एक अन्य परिप्रेक्ष्य यह है कि डीप कंप्यूटर को मल्टीस्टेप कंप्यूटर प्रोग्राम सीखने में सक्षम बनाता है ।</s>
प्रतिनिधित्व की प्रत्येक परत को 5 अध्याय 1 के बाद कंप्यूटर की स्मृति की स्थिति माना जा सकता है ।</s>
दृश्य परत ( इनपुट पिक्सल )</s>
प्रथम प्रच्छन्न परत</s>
द्वितीय प्रच्छन्न परत</s>
Constellation name ( optional )</s>
चित्र 1 . 2 . गहन अध्ययन मॉडल का चित्रण ।</s>
कच्चे संवेदी इनपुट डेटा के अर्थ को समझना कंप्यूटर के लिए कठिन है , जैसे कि यह छवि पिक्सेल मूल्यों के संग्रह के रूप में प्रतिनिधित्व करती है .</s>
किसी वस्तु की पहचान के लिए पिक्सल के समुच्चय से फलन प्रतिचित्रण बहुत जटिल होता है ।</s>
इस मानचित्रण का अध्ययन या मूल्यांकन यदि सीधे तौर पर किया जाए तो यह अकाट्य प्रतीत होता है ।</s>
गहन अध्ययन इस कठिनाई का समाधान वांछित जटिल मानचित्रण को तोड़कर नेस्टेड सरल मानचित्रण की एक श्रृंखला , प्रत्येक मॉडल की एक अलग परत द्वारा वर्णित है .</s>
इनपुट दृश्य परत पर प्रस्तुत है , इसलिए नाम दिया है क्योंकि यह चर है कि हम निरीक्षण करने में सक्षम हैं शामिल हैं .</s>
फिर छिपा परतों की एक श्रृंखला छवि से लगातार सार सार .</s>
इन परतों को " इन परतों " कहा जाता है क्योंकि अपने मूल्यों को डेटा में नहीं दिया जाता है .</s>
यहाँ छवियों प्रत्येक छिपा इकाई द्वारा प्रतिनिधित्व विशेषता की तरह की कल्पना कर रहे हैं .</s>
पिक्सल को देखते हुए , पहली परत आसानी से किनारों की पहचान कर सकते हैं , पड़ोसी पिक्सल की चमक की तुलना करके .</s>
पहली छुपी परत के किनारों के वर्णन को देखते हुए , दूसरी छुपी परत आसानी से कोनों और विस्तारित कंटूरों को खोज सकती है , जो किनारों के संग्रह के रूप में पहचानी जा सकती है .</s>
कोनों और कोनों के संदर्भ में छवि की दूसरी छिपी परत के वर्णन को देखते हुए , तीसरी छुपी परत विशिष्ट वस्तुओं के संपूर्ण भागों का पता लगा सकती है , कोनों और कोनों के विशिष्ट संग्रहों का पता लगाकर .</s>
अंत में , यह जो ऑब्जेक्ट पार्ट्स इसमें शामिल हैं के संदर्भ में छवि के इस विवरण का उपयोग छवि में मौजूद वस्तुओं को पहचानने के लिए किया जा सकता है .</s>
जीइलर और फर्गस (2014 ) की अनुमति से चित्र पेश किए गए</s>
6 अध्याय 1 .</s>
सूचना का एक अन्य सेट समानांतर में निष्पादित करना ।</s>
अधिक गहराई वाले नेटवर्क अधिक अनुदेशों को अनुक्रम में निष्पादित कर सकते हैं ।</s>
अनुक्रमिक अनुदेश महान शक्ति प्रदान करते हैं क्योंकि बाद के अनुदेश पूर्व निर्देशों के परिणामों को वापस संदर्भित कर सकते हैं .</s>
एकड़</s>
गहन अधिगम के इस दृष्टिकोण को मजबूत करते हुए , एक परत सक्रियण में सभी जानकारी आवश्यक रूप से विभिन्नता के कारकों को एन्कोड करता है जो इनपुट की व्याख्या करते हैं .</s>
अभ्यावेदन अवस्था सूचना को भी संग्रहित करता है जो एक प्रोग्राम को निष्पादित करने में सहायता करता है जो इनपुट का बोध करा सकता है .</s>
यह अवस्था सूचना पारंपरिक कंप्यूटर प्रोग्राम में एक काउंटर या सूचक के अनुरूप हो सकती है ।</s>
इसका विशेष रूप से इनपुट की सामग्री से कोई संबंध नहीं है , लेकिन यह मॉडल को अपने प्रसंस्करण को व्यवस्थित करने में मदद करता है .</s>
एक मॉडल की गहराई मापने के दो मुख्य तरीके हैं .</s>
पहला दृश्य अनुक्रमिक अनुदेशों की संख्या पर आधारित है जिसे वास्तुकला का मूल्यांकन करने के लिए निष्पादित किया जाना चाहिए .</s>
हम इसके बारे में एक प्रवाह चार्ट के माध्यम से सबसे लंबे पथ की लंबाई के रूप में सोच सकते हैं जो यह वर्णन करता है कि कैसे प्रत्येक मॉडल के आउटपुट अपने इनपुट दिया की गणना करने के लिए .</s>
जिस प्रकार दो समतुल्य कंप्यूटर प्रोग्रामों की लंबाई भिन्न होती है , यह निर्भर करता है कि प्रोग्राम किस भाषा में लिखा गया है , उसी प्रकार एक ही फलन को विभिन्न गहराइयों के साथ फ्लो चार्ट के रूप में खींचा जा सकता है , जिस पर निर्भर करता है कि हम प्रवाह संचित्र में अलग - अलग चरणों के रूप में प्रयोग की अनुमति देते हैं ।</s>
चित्र 1 . 3 में स्पष्ट किया गया है कि किस प्रकार भाषा का यह चयन एक ही वास्तुकला के लिए दो अलग - अलग माप दे सकता है ।</s>
</s>
×</s>
x 2</s>
x 2</s>
त् वचा</s>
2number</s>
w 2 × + + + + + +</s>
तत्व सेट + ×</s>
x</s>
तत्व सेट लॉजिस्टिक प्रतिगमन लॉजिस्टिक प्रतिगमन चित्र 13 :</s>
एक आउटपुट में जहाँ प्रत्येक नोड एक ऑपरेशन निष्पादित करता है , कम्प्यूटेशनल ग्राफ का चित्रण ।</s>
गहराई इनपुट से आउटपुट तक के सबसे लंबे पथ की लंबाई है , लेकिन क्या एक संभव कम्प्यूटेशनल कदम का गठन करता है की परिभाषा पर निर्भर करता है .</s>
जिस अभिकलन मॉडल में प्रतिलोम को दर्शाया गया है वह गणनात्मक ग्राफ , वह लॉग , है ।</s>
यदि हम अपने कंप्यूटर भाषा के अवयवों के रूप में जोड़ , गुणन और लॉजिस्टिक अवग्रहों का उपयोग करते हैं , तो इस मॉडल में गहराई तीन है .</s>
यदि हम लॉजिस्टिक प्रतिगमन को एक तत्व के रूप में ही देखते हैं , तो इस मॉडल में गहराई एक है ।</s>
7 अध्याय 1 .</s>
INTRODUCTION एक अन्य दृष्टिकोण , जिसका प्रयोग गहन प्रायैविक मॉडल द्वारा किया जाता है , एक मॉडल की गहराई को कम्प्यूटेशनल ग्राफ की गहराई नहीं बल्कि कम्प्यूटेशनल ग्राफ की गहराई मानता है कि अवधारणाएं एक दूसरे से कैसे संबंधित हैं ।</s>
इस मामले में , प्रत्येक अवधारणा के निरूपण की गणना के लिए अपेक्षित संगणन के प्रवाह संचित्र की गहराई स्वयं संकल्पनाओं के ग्राफ से कहीं अधिक गहरी हो सकती है .</s>
इसका कारण यह है कि प्रणाली की सरल अवधारणाओं की समझ को अधिक जटिल अवधारणाओं के बारे में जानकारी देकर परिष्कृत किया जा सकता है ।</s>
उदाहरण के लिए , छाया में एक आंख वाले चेहरे के प्रतिबिंब को देखने वाली एआइ प्रणाली प्रारंभ में केवल एक आंख ही देख सकती है ।</s>
यह पता लगाने के बाद कि एक चेहरा मौजूद है , प्रणाली फिर अनुमान लगा सकते हैं कि एक दूसरी आंख शायद के रूप में अच्छी तरह से मौजूद है .</s>
इस मामले में , अवधारणाओं के ग्राफ में केवल दो परतें शामिल हैं - आंखों के लिए परत और चेहरों के लिए परत - लेकिन गणना के ग्राफ में 2 n परतें शामिल हैं अगर हम दूसरे n समय दिए गए प्रत्येक अवधारणा के अपने अनुमान को परिष्कृत करते हैं ।</s>
क्योंकि यह हमेशा स्पष्ट नहीं है कि इन दो दृश्यों में से कौन सा है - कम्प्यूटेशनल ग्राफ की गहराई या प्रोबाबिलिस्टिक मॉडलिंग ग्राफ की गहराई - सबसे अधिक प्रासंगिक है , और क्योंकि विभिन्न लोग अपने ग्राफों का निर्माण करने के लिए सबसे छोटे तत्वों के विभिन्न सेट चुनते हैं , वहाँ एक वास्तुकला की गहराई के लिए कोई एकल सही मूल्य नहीं है</s>
न ही इस बात पर आम सहमति है कि किसी मॉडल को “दीपावली” के रूप में कितनी गहराई के लिए अर्हता प्राप्त करने की आवश्यकता है ।</s>
हालांकि , गहन अधिगम को सुरक्षित रूप से मॉडलों का अध्ययन माना जा सकता है जिसमें पारंपरिक मशीन अधिगम की तुलना में अधिक मात्रा में विद्वत कार्यों या विद्वत अवधारणाओं की रचना शामिल होती है .</s>
संक्षेप करने के लिए , गहराई से सीखने , इस पुस्तक के विषय , एआई के लिए एक दृष्टिकोण है .</s>
विशेष रूप से , यह मशीन अधिगम का एक प्रकार है , एक तकनीक है जो अनुभव और डेटा के साथ कंप्यूटर सिस्टम को बेहतर बनाने में सक्षम बनाता है .</s>
हमारा तर्क है कि एआइ सिस्टम बनाने के लिए मशीन लर्निंग ही एक व्यवहार्य तरीका है जो जटिल वास्तविक दुनिया वातावरण में काम कर सकता है .</s>
गहन अधिगम एक विशेष प्रकार का मशीनी अधिगम है जो विश्व को संकल्पनाओं के एक नीस्टेड पदानुक्रम के रूप में प्रस्तुत करते हुए , सरल अवधारणाओं के संबंध में प्रत्येक अवधारणा के साथ , और कम अमूर्त निरूपण के संदर्भ में परिकलित अधिक अमूर्त निरूपण के साथ , महान शक्ति और लचीलापन प्राप्त करता है ।</s>
चित्र 1 . 4 इन विभिन्न एआइ विधाओं के बीच संबंधों को स्पष्ट करता है ।</s>
चित्र 1 . 5 में प्रत्येक व्यक्ति किस प्रकार कार्य करता है इसकी एक उच्च स्तर की योजना दी गई है ।</s>
1 . 1</s>
इस पुस्तक को किसने पढ़ा है ?</s>
यह पुस्तक पाठकों की एक किस्म के लिए उपयोगी हो सकता है , लेकिन हम मन में दो लक्षित दर्शकों के साथ लिखा था .</s>
इन लक्षित दर्शकों में से एक है विश्वविद्यालय के छात्र ( जो मशीन लर्निंग के बारे में स्नातक या स्नातक की उपाधि प्राप्त कर रहे हैं , उन सहित जो गहन अध्ययन और कृत्रिम बुद्धि अनुसंधान में एक कैरियर की शुरुआत कर रहे हैं .</s>
अन्य 8 अध्याय 1 .</s>
INTRODUCTION A मशीन लर्निंग प्रतिनिधित्व लर्निंग डीप लर्निंग उदाहरणः नॉलेज बेस उदाहरणः</s>
लॉजिस्टिक प्रतिगमन</s>
उदाहरणः</s>
शैलो ऑटोनकोडर्स</s>
उदाहरणः</s>
एमएलपी चित्र 1 . 4 :</s>
एक वेन आरेख जो दिखाता है कि कैसे गहरी अधिगम एक प्रकार का प्रतिनिधित्व अधिगम है , जो बारी में मशीन अधिगम , जो कई के लिए प्रयोग किया जाता है लेकिन एआई के लिए सभी दृष्टिकोण नहीं है .</s>
वेन आरेख के प्रत्येक अनुभाग में एक एआई प्रौद्योगिकी का एक उदाहरण शामिल है .</s>
लक्षित दर्शक उन सॉफ्टवेयर इंजीनियरों को कहते हैं जिनके पास मशीनी शिक्षण या स्थैतिक पृष्ठभूमि नहीं है लेकिन वे तेजी से एक को प्राप्त करना चाहते हैं और अपने उत्पाद या मंच में गहन शिक्षण का उपयोग शुरू कर देते हैं ।</s>
कंप्यूटर विजन , भाषण और ऑडियो प्रोसेसिंग , प्राकृतिक भाषा संसाधन , रोबोटिक्स , जैव सूचना विज्ञान और रसायन , वीडियो गेम्स , ऑनलाइन विज्ञापन और वित्त सहित कई सॉफ्ट वेयर विधाओं में डीप लर्निंग पहले से ही उपयोगी साबित हुई है ।</s>
इस पुस्तक को तीन भागों में बांटा गया है ताकि विभिन्न प्रकार के पाठकों को जगह मिल सके ।</s>
भाग I बुनियादी गणितीय उपकरणों और मशीन सीखने अवधारणाओं परिचय .</s>
भाग द्वितीय सबसे स्थापित गहन शिक्षण एल्गोरिदम , जो अनिवार्य रूप से हल प्रौद्योगिकियों का वर्णन करता है .</s>
भाग 3 में अधिक सट्टा विचारों का वर्णन है कि व्यापक रूप से माना जाता है गहन अध्ययन में भविष्य के अनुसंधान के लिए महत्वपूर्ण है .</s>
9 अध्याय 1 .</s>
परिचय</s>
हस्तनिर्मित प्रोग्राम</s>
आउटपुट इनपुट</s>
फीचर आउटपुट इनपुट से हस्तनिर्मित विशेषताएँ मैपिंग</s>
सुविधाओं से मैपिंग अधिक अमूर्त सुविधाओं के आउटपुट अतिरिक्त परतों नियम - पस्त सिस्टम्स क्लासिक मशीन लर्निंग प्रतिनिधित्व सीखने डीप लर्निंग चित्र 1 . 5 फ्लोचा दिखाता कैसे एक एआइ प्रणाली के विभिन्न भागों अलग एआइ विधाओं के भीतर एक दूसरे से संबंधित हैं .</s>
छायांकित बक्से उन घटकों को सूचित करते हैं जो डेटा से सीखने में सक्षम होते हैं .</s>
पाठकों को उन भागों को छोड़ने के लिए स्वतंत्र महसूस होना चाहिए जो उनके हितों या पृष्ठभूमि को देखते हुए प्रासंगिक नहीं हैं .</s>
रैखिक बीजगणित , संभावना , और मौलिक मशीन सीखने अवधारणाओं से परिचित पाठकों भाग I छोड़ सकते हैं , उदाहरण के लिए , जबकि जो लोग सिर्फ एक कार्य प्रणाली को लागू करना चाहते हैं उन्हें भाग II से आगे पढ़ने की जरूरत नहीं है .</s>
चुनने में मदद करने के लिए जो 10 अध्याय 1 .</s>
परिचय 1 .</s>
परिचय भाग - 1 : अनुप्रयुक्त मठ और मशीन लर्निंग बेसिक्स 2 .</s>
रेखीय बीजगणित 3 .</s>
संभावना और सूचना सिद्धांत 4 .</s>
संख्यात्मक संगणन 5 .</s>
मशीन लर्निंग बेसिक्स पार्ट - 2 :</s>
डीप नेटवर्क्सः आधुनिक प्रथाएं 6 .</s>
डीप फीडवर्ड नेटवर्क 7 .</s>
नियमितीकरण 8 .</s>
अनुकूलन 9 .</s>
सीएनएन १०</s>
आरएनएस 11</s>
व्यावहारिक पद्धति 12 .</s>
अनुप्रयोग भाग 3 :</s>
डीप लर्निंग रिसर्च 13 .</s>
रेखीय फैक्टर मॉडल्स 14 .</s>
ऑटोनोकोडर्स 15</s>
प्रतिनिधित्व सीखना 16 .</s>
संरचित सम्भाव्य मॉडल 17 .</s>
मोंटे कार्लो विधि १८</s>
पार्टीशन फंक्शन 19 .</s>
अनुमान</s>
डीप जेनरेशनेटिव मॉडल चित्र 1 . 6 पुस्तक का उच्चस्तर का संगठन ।</s>
एक अध्याय से दूसरे अध्याय तक का बाण इस बात का संकेत करता है कि पहला अध्याय दूसरे अध्याय को समझने के लिए आवश्यक सामग्री है ।</s>
11 अध्याय 1 .</s>
पढ़ने के लिए प्रति अध्याय 1 . 6 में एक फ्लो चार्ट दिया गया है जो पुस्तक के उच्चस्तर संगठन को दर्शाता है ।</s>
हम यह जरूर मान लेते हैं कि सभी पाठक कंप्यूटर विज्ञान की पृष्ठभूमि से आते हैं ।</s>
हम प्रोग्रामिंग , कम्प्यूटेशनल प्रदर्शन मुद्दों की एक बुनियादी समझ , जटिलता सिद्धांत , परिचयात्मक स्तर पथरी और ग्राफ सिद्धांत की कुछ शब्दावली के साथ परिचित हो लेते हैं .</s>
1 . 2 डीप लर्निंग में ऐतिहासिक वाक्यांश</s>
गहरे अध्ययन को किसी ऐतिहासिक संदर्भ के साथ समझना सबसे आसान है ।</s>
डीप लर्निंग का एक विस्तृत इतिहास प्रदान करने के बजाय , हम कुछ प्रमुख प्रवृत्तियों की पहचानः</s>
• गहन अध्ययन का एक लंबा और समृद्ध इतिहास रहा है , लेकिन कई नामों से चला गया है , विभिन्न दार्शनिक दृष्टिकोणों को प्रतिबिंबित , और लोकप्रियता में वृद्धि हुई है और इच्छुक है .</s>
• गहन शिक्षण अधिक उपयोगी हो गया है के रूप में उपलब्ध प्रशिक्षण डेटा की राशि में वृद्धि हुई है .</s>
• गहन अधिगम मॉडल समय के साथ आकार में वृद्धि हुई है के रूप में कंप्यूटर बुनियादी ढांचे ( दोनोंथ हार्डवेयर और गहन अधिगम के लिए सॉफ्टवेयर ) में सुधार हुआ है .</s>
• गहन सीखने समय के साथ बढ़ती सटीकता के साथ लगातार जटिल अनुप्रयोगों को हल किया है .</s>
1 . 2 . 1</s>
न्युरल नेट के अनेक नाम और परिवर्तनकारी कार्य</s>
हम उम्मीद है कि इस पुस्तक के कई पाठकों को एक रोमांचक नई प्रौद्योगिकी के रूप में गहरी सीखने के बारे में सुना है , और एक उभरते क्षेत्र के बारे में एक पुस्तक में “स्वास्थ्य” का उल्लेख देख आश्चर्य है .</s>
वास्तव में , गहरा ज्ञान 1940 के दशक में आया ।</s>
डीप लर्निंग केवल नया लगता है , क्योंकि यह अपनी वर्तमान लोकप्रियता से पहले कई वर्षों के लिए अपेक्षाकृत अलोकप्रिय था , और क्योंकि यह कई अलग अलग नामों से चला गया है , केवल हाल ही में “दीप लर्निंग” कहा जा रहा है .</s>
क्षेत्र का कई बार पुनर्गठन किया गया है , विभिन्न शोधकर्ताओं और विभिन्न परिप्रेक्ष्य के प्रभाव को प्रतिबिंबित .</s>
गहन अध्ययन का व्यापक इतिहास इस पाठ्यपुस्तक के दायरे से बाहर है ।</s>
कुछ बुनियादी संदर्भ , तथापि , गहरी सीखने को समझने के लिए उपयोगी है .</s>
मोटे तौर पर विकास की तीन लहरें रही हैंः</s>
1940 -1960 के दशक में साइबरनेटिक्स के रूप में जाना जाने वाला गहरा अध्ययन , 12 अध्याय 1 में कनेक्शनवाद के रूप में जाना जाता है ।</s>
परिचय 1940 1950 1960 1980 1990 2000 वर्ष</s>
</s>
0 . 000200 0 . 000250 शब्द या वाक्यांश की आवृत्ति ( कनेक्टेनिज्म + न्यूरल नेटवर्क )</s>
आंकड़े 1 . 7</s>
गूगल बुक्स ( गूगल बुक्स ) के अनुसार कृत्रिम तंत्रिका जाल अनुसंधान की तीन ऐतिहासिक तरंगों में से दो , वाक्यों “सभ्यता” और “संबंध” या “न्यूरल नेटवर्क” की आवृत्ति द्वारा मापी जाती हैं ( - तीसरी तरंग अभी हाल ही में दिखाई देती है ) .</s>
1940 के दशक में न्यूनेटिक्स के साथ शुरू हुई लहर ने , पहली बार न्यूरो - लॉजिस्टिक सिद्धांतो के सिद्धांतो के सिद्धांतो के साथ , 1958 से शुरू हुआ न्यूरो - साइक्लोलोजी के विकास , 1949 के क्रियान्वयन , बड़े - बड़े मॉडलों के न्यूरो - हेब्ब , 1949 से - इंजन ,</s>
दूसरी लहर 1980 -1995 अवधि के कनेक्शनवादी दृष्टिकोण के साथ शुरू हुई , पश्च - प्रचार के साथ ( रूमाल्ट एट अल , 1986 )</s>
तीसरे , गहरे & # 44 ; 2006 ( al . al . al . A ) ( 2006 )</s>
जनवरी 2007</s>
और अभी 2016 के रूप में पुस्तक के रूप में प्रकट हो रहा है .</s>
इसी प्रकार की अन्य दो तरंगें पुस्तकाकार में उस वैज्ञानिक गतिविधि से बहुत बाद में प्रकट हुईं ।</s>
1980 -1990 का दशक , और 2006 में डीप लर्निंग की शुरुआत के नाम से वर्तमान पुनरुत्थान ।</s>
यह संख्या 1 . 7 है ।</s>
कुछ आरंभिक अधिगम एल्गोरिदम जिन्हें हम आज पहचानते हैं , उनका उद्देश्य जैविक अधिगम का संगणकीय मॉडल होना था , अर्थात यह मॉडल कि अधिगम कैसे होता है या मस्तिष्क में हो सकता है ।</s>
इसके परिणामस्वरूप , एक नाम जो गहन अध्ययन में चला गया है वह है कृत्रिम तंत्रिका नेटवर्क ( एनएनएस )</s>
गहन अध्ययन मॉडलों पर तदनुरूप परिप्रेक्ष्य यह है कि उनके लिए जैविक मस्तिष्क से प्रेरित प्रणालियां बनाई जाती हैं ( चाहे वह मानव मस्तिष्क हो या किसी अन्य पशु का मस्तिष्क ।</s>
हालांकि मशीन सीखने के लिए इस्तेमाल किया तंत्रिक नेटवर्क के प्रकार कभी कभी मस्तिष्क समारोह को समझने के लिए इस्तेमाल किया गया है ( हाइटन और शैलस , 1991 ) , वे आमतौर पर जैविक समारोह के यथार्थवादी मॉडल नहीं बनाया गया है .</s>
गहन अध्ययन पर तंत्रिका परिप्रेक्ष्य दो मुख्य विचारों से प्रेरित है ।</s>
एक विचार यह है कि मस्तिष्क उदाहरण द्वारा एक प्रमाण प्रदान करता है कि बुद्धिमत्तापूर्ण व्यवहार संभव है , और बुद्धि के निर्माण के लिए वैचारिक रूप से सीधा मार्ग यह है कि मस्तिष्क के पीछे कम्प्यूटेशनल सिद्धांतों को अभियांत्रिक बनाया जाए और उसकी कार्यक्षमता को दोहराया जाए ।</s>
एक और 13 अध्याय 1 .</s>
फोरेंसिक परिप्रेक्ष्य यह है कि मस्तिष्क और मानव बुद्धि को नीचे ले जाने वाले सिद्धांतों को समझना अत्यंत रोचक होगा , इसलिए इन मूलभूत वैज्ञानिक प्रश्नों पर प्रकाश डालने वाले यंत्र शिक्षण मॉडल इंजीनियरी अनुप्रयोगों को हल करने की उनकी क्षमता के अलावा उपयोगी होंगे ।</s>
आधुनिक शब्द “दीप्त शिक्षण” मशीन लर्निंग मॉडल की वर्तमान नस्ल पर तंत्रिका विज्ञानी परिप्रेक्ष्य से परे चला जाता है .</s>
यह रचना के कई स्तरों को सीखने के एक और अधिक सामान्य सिद्धांत के लिए अपील करता है , जो मशीन शिक्षण ढांचे में लागू किया जा सकता है कि आवश्यक रूप से तंत्रिका प्रेरित नहीं हैं .</s>
आधुनिक गहन अध्ययन के सबसे पहले पूर्ववर्तियों सरल रैखिक मॉडल एक तंत्रिका विज्ञानी परिप्रेक्ष्य से प्रेरित थे .</s>
इन मॉडलों n इनपुट मूल्यों x 1 , 999 का एक सेट लेने के लिए डिज़ाइन किया गया था .</s>
1 . 743 . 549 शब्द 1 . 849 . 849 उच्चारण 299भाषा</s>
X n और उन्हें एक आउटपुट वाई के साथ सहयोगी ।</s>
इन मॉडलों से 1 , 999 पर वजन का एक सेट सीखना होगा .</s>
1 . 743 . 549 शब्द 1 . 849 . 849 उच्चारण 299भाषा</s>
, w n और अपने आउटपुट एफ की गणना ( x , w ) =</s>
</s>
+ ·· ·· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·</s>
w . n .</s>
तंत्रिका नेटवर्क अनुसंधान की इस पहली लहर को साइबरनेटिक्स के नाम से जाना जाता था , जैसा कि संख्या 1 . 7 में दर्शाया गया है ।</s>
मैकलोच - पिट्स न्यूरॉन ( मैकक्लोच और पिट्स , 1943 ) मस्तिष्क समारोह का एक प्रारंभिक मॉडल था .</s>
इस रैखिक मॉडल आदानों के दो अलग - अलग वर्गों को पहचान सकता है परीक्षण के द्वारा कि क्या एफ ( एक्स , डब्ल्यू ) सकारात्मक या नकारात्मक है ।</s>
बेशक , श्रेणियों की वांछित परिभाषा के अनुरूप करने के लिए मॉडल के लिए , सही ढंग से सेट किया जा करने के लिए आवश्यक वजन .</s>
इन भारों को मानव प्रचालक निर्धारित कर सकता था ।</s>
1950 के दशक में पेसेप्ट्रॉन ( Rosenblast , 1958 , 1962 ) पहला मॉडल बन गया जो उन भारों को सीख सका जो प्रत्येक श्रेणी से इनपुट के उदाहरण दिए गए श्रेणियों को परिभाषित करते हैं .</s>
अनुकूली रैखिक तत्व ( ADALINE ) , जो लगभग एक ही समय से तिथियाँ , बस च के मूल्य वापस</s>
( x ) स्वयं एक वास्तविक संख्या की भविष्यवाणी करना (Widro and Hoff , 1960 ) और इन संख्याओं की भविष्यवाणी डेटा से करना भी सीख सकते हैं .</s>
इन सरल शिक्षण एल्गोरिदम ने मा - चाइन लर्निंग के आधुनिक परिदृश्य को बहुत प्रभावित किया ।</s>
प्रशिक्षण एल्गोरिथ्म एडीएएलआई के वजन को अनुकूलित करने के लिए इस्तेमाल किया</s>
एक एल्गोरिथ्म का एक विशेष मामला था जिसे स्टोचिस्टिक प्रवणता कहते हैं ।</s>
आज गहन अधिगम मॉडलों के लिए स्टोचिस्टिक प्रवणन एल्गोरिथ्म प्रमुख प्रशिक्षण एल्गोरिदम बना हुआ है ।</s>
पेसेप्टॉन और ADALINE द्वारा प्रयुक्त f ( x , w ) पर आधारित मॉडलों को रैखिक मॉडल कहा जाता है ।</s>
इन मॉडलों सबसे व्यापक रूप से इस्तेमाल किया मशीन सीखने मॉडल में से कुछ बने हुए हैं , हालांकि कई मामलों में उन्हें मूल मॉडलों से अलग तरीके से प्रशिक्षित किया जाता है .</s>
रेखीय मॉडल की अनेक सीमाएं हैं ।</s>
सबसे प्रसिद्ध है , कि नहीं । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । ।</s>
= 1 और च (</s>
1 , 0 )</s>
= 1 , f (</s>
1 , 1 )</s>
= = च ; ( ० ) ; और ० ( ० ) ।</s>
जिन आलोचकों ने रैखिक मॉडलों में इन खामियों का पालन किया , उन्होंने सामान्य रूप से जैविक प्रेरित शिक्षण के विरुद्ध एक पृष्ठभूमि तैयार की ( एमिंक्सकी एंड पेपर्ट , 1969 ) .</s>
यह तंत्रिक नेटवर्क की लोकप्रियता की पहली बड़ी डुबकी थी ।</s>
14 अध्याय 1 .</s>
सूचना आज , तंत्रिका विज्ञान गहन अध्ययन शोधकर्ताओं के लिए एक महत्वपूर्ण प्रेरणा स्रोत माना जाता है , लेकिन अब यह क्षेत्र के लिए प्रमुख मार्गदर्शक नहीं है ।</s>
आज गहन अध्ययन अनुसंधान में तंत्रिका विज्ञान की घटती भूमिका का मुख्य कारण यह है कि हम बस एक गाइड के रूप में उपयोग करने के लिए मस्तिष्क के बारे में पर्याप्त जानकारी नहीं है .</s>
मस्तिष्क द्वारा प्रयोग किए जाने वाले वास्तविक एल्गोरिदम की गहरी समझ प्राप्त करने के लिए , हमें एक साथ ( कम से कम हजारों परस्पर संबद्ध न्यूरॉन्स की गतिविधि की निगरानी करने में सक्षम होना होगा ।</s>
क्योंकि हम ऐसा नहीं कर पा रहे हैं , इसलिए हम मस्तिष्क के कुछ अत्यंत सरल और सुशिक्षित भागों को भी नहीं समझ पा रहे हैं ।</s>
तंत्रिका विज्ञान ने हमें आशा करने का एक कारण दिया है कि एक ही गहन शिक्षण एल्गोरिथ्म कई अलग अलग कार्यों को हल कर सकता है ।</s>
तंत्रिका विज्ञानियों ने पाया है कि यदि उनके मस्तिष्क को दृश्य संकेतों को उस क्षेत्र ( वॉन मेल्कर एट अल , 2000 ) में भेजने के लिए पुनः प्रेरित किया जाता है तो वे अपने मस्तिष्क के श्रवण संसाधन क्षेत्र के साथ “सी” सीख सकते हैं ।</s>
यह सुझाव है कि स्तनधारी मस्तिष्क के बहुत सारे विभिन्न कार्यों है कि मस्तिष्क हल करता है के अधिकांश को हल करने के लिए एक एकल एल्गोरिथ्म का उपयोग हो सकता है .</s>
इस परिकल्पना से पहले , मशीनी शिक्षण अनुसंधान अधिक खंडित था , प्राकृतिक भाषा संसाधन , दृष्टि , गति योजना और भाषण मान्यता का अध्ययन कर रहे शोधकर्ताओं के विभिन्न समुदायों के साथ .</s>
आज , इन आवेदन समुदायों अभी भी अलग है , लेकिन गहरी सीखने अनुसंधान समूहों के लिए एक साथ कई या यहाँ तक कि इन सभी आवेदन क्षेत्रों का अध्ययन करने के लिए आम है .</s>
हम तंत्रिका विज्ञान से कुछ रूखे दिशा निर्देश बनाने में सक्षम हैं ।</s>
कई कम्प्यूटेशनल इकाइयों के होने का मूल विचार , जो केवल एक दूसरे के साथ बातचीत के माध्यम से बुद्धिमान बन जाते हैं , मस्तिष्क से प्रेरित होता है .</s>
नियोकग्नीट्रान</s>
अधिकांश तंत्रिका नेटवर्क आज एक मॉडल न्यूरॉन पर आधारित हैं जिसे संशोधित रैखिक इकाई कहा जाता है ।</s>
मूल कॉग्निट्रान ( Fukushima ) , 1975 में एक अधिक जटिल संस्करण पेश किया जो मस्तिष्क समारोह के हमारे ज्ञान से अत्यधिक प्रेरित था .</s>
सरलीकृत आधुनिक संस्करण का विकास कई दृष्टिकोणों से , नायर और हिंटन (2010 ) और ग्लोोट एट अल के साथ मिलकर किया गया ।</s>
(2011 ) ने तंत्रिका विज्ञान को एक प्रभाव के रूप में उद्धृत किया , और जेरेट्ट एट अल .</s>
(2009 ) अधिक इंजीनियरिंग उन्मुख प्रभाव का हवाला देते हुए .</s>
जबकि तंत्रिका विज्ञान प्रेरणा का एक महत्वपूर्ण स्रोत है , यह एक कठोर गाइड के रूप में लेने की जरूरत नहीं है .</s>
हम जानते हैं कि वास्तविक न्यूरॉन्स आधुनिक संशोधित रैखिक इकाइयों की तुलना में बहुत अलग कार्यों की परिकलन करते हैं , लेकिन अधिक तंत्रिका यथार्थवाद अभी तक मशीन सीखने के प्रदर्शन में सुधार का नेतृत्व नहीं किया है .</s>
इसके अलावा , जबकि तंत्रिका विज्ञान सफलतापूर्वक कई तंत्रिका नेटवर्क वास्तुकलाओं को प्रेरित किया है , हम अभी तक तंत्रिका विज्ञान के लिए जैविक सीखने के बारे में पर्याप्त जानकारी नहीं है कि इन वास्तुकलाओं को प्रशिक्षित करने के लिए हम उन सीखने एल्गोरिदम के लिए बहुत मार्गदर्शन की पेशकश .</s>
15 अध्याय 1 .</s>
INTRODUCTION मीडिया खातों अक्सर मस्तिष्क के लिए गहरे शिक्षण की समानता पर जोर देती है ।</s>
जबकि यह सच है कि गहन अध्ययन के शोधकर्ता अन्य मशीन अध्ययन क्षेत्रों , जैसे कर्नेल मशीन या बेयसेशियन सांख्यिकी में काम कर रहे शोधकर्ताओं की तुलना में मस्तिष्क को अधिक प्रभाव के रूप में उद्धृत करने की संभावना है , किसी को गहन अध्ययन को मस्तिष्क अनुकरण करने के प्रयास के रूप में नहीं देखना चाहिए ।</s>
आधुनिक गहन शिक्षण कई क्षेत्रों , विशेष रूप से लागू गणित बुनियादी जैसे रैखिक बीजगणित , संभावना , सूचना सिद्धांत , और संख्यात्मक अनुकूलन से प्रेरणा लेता है .</s>
जबकि कुछ गहन शिक्षण शोधकर्ताओं तंत्रिका विज्ञान को प्रेरणा का एक महत्वपूर्ण स्रोत के रूप में उद्धृत करते हैं , दूसरों को तंत्रिका विज्ञान के साथ बिल्कुल भी चिंतित नहीं है .</s>
यह ध्यान देने योग्य है कि मस्तिष्क एल्गोरिथम के स्तर पर कैसे काम करता है यह समझने का प्रयास जीवंत और अच्छी तरह से है ।</s>
इस प्रयास को मुख्य रूप से “कंप्यूटेशनल तंत्रिका विज्ञान” के रूप में जाना जाता है और गहन अध्ययन से एक अलग क्षेत्र है .</s>
शोधकर्ताओं के लिए दोनों क्षेत्रों के बीच आगे - पीछे घूमना आम बात है ।</s>
गहन अध्ययन का क्षेत्र मुख्य रूप से इस बात से संबंधित है कि कंप्यूटर सिस्टम का निर्माण कैसे किया जाए जो बुद्धि की आवश्यकता वाले कार्यों को सफलतापूर्वक हल करने में सक्षम हो , जबकि संगणकीय तंत्रिका विज्ञान का क्षेत्र मुख्य रूप से इस बात से संबंधित है कि मस्तिष्क वास्तव में किस प्रकार कार्य करता है ।</s>
1980 के दशक में , तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर कनेक्शनवाद , या समानांतर वितरित प्रक्रिया के माध्यम से एक आंदोलन के माध्यम से बड़े हिस्से में उभरा -</s>
</s>
मेक्सक्ललैंड एट अल . , 1995</s>
संज्ञानात्मक विज्ञान के संदर्भ में कनेक्शनवाद का उदय हुआ ।</s>
ज्ञानात्मक विज्ञान , मन को समझने के लिए , विश्लेषण के कई विभिन्न स्तरों के संयोजन के लिए एक अंतर्विषयक दृष्टिकोण है .</s>
1980 के दशक के प्रारंभ में , अधिकांश संज्ञानात्मक वैज्ञानिकों ने प्रतीकात्मक तर्क के मॉडलों का अध्ययन किया .</s>
उनकी लोकप्रियता के बावजूद , प्रतीकात्मक मॉडल कैसे मस्तिष्क वास्तव में न्यूरॉन्स का उपयोग कर उन्हें लागू कर सकता है के संदर्भ में व्याख्या करने के लिए मुश्किल थे .</s>
संबंधियों ने संज्ञान के मॉडलों का अध्ययन करना शुरू किया जो वास्तव में तंत्र क्रियान्वयन में आधारित हो सकता है ( टौरेटिंग्स्की एंड मिंटन , 1985 ) . 1940 के दशक में मनोविज्ञानी डोनाल्ड हेब के काम में कई विचारों को पुनर्जीवित किया .</s>
कनेक्शनवाद में केंद्रीय विचार है कि सरल कम्प्यूटेशनल इकाइयों की एक बड़ी संख्या में एक साथ नेटवर्क करने पर बुद्धिमान व्यवहार प्राप्त कर सकते हैं .</s>
यह अंतर्दृष्टि जैविक तंत्रिका प्रणालियों में न्यूरॉन्स के लिए समान रूप से लागू होता है के रूप में यह कम्प्यूटेशनल मॉडल में छिपा इकाइयों के लिए करता है .</s>
1980 के दशक के कनेक्शनवाद आंदोलन के दौरान कई प्रमुख अवधारणाएं उभरीं जो आज के गहन अध्ययन का केंद्र बनी हुई हैं ।</s>
इनमें से एक अवधारणा वितरित प्रतिनिधित्व की है ( हाइटन एट अल , 1986 ) .</s>
यह विचार है कि एक सिस्टम के प्रत्येक इनपुट को कई विशेषताओं द्वारा प्रतिनिधित्व किया जाना चाहिए , और प्रत्येक विशेषता को कई संभावित इनपुट के प्रतिनिधित्व में शामिल किया जाना चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हमारे पास एक दृष्टि प्रणाली है जो 16 अध्याय 1 को पहचान सकती है ।</s>
इनट्राडयूसींस कारों , ट्रकों , और पक्षियों , और इन वस्तुओं प्रत्येक लाल , हरे , या नीले हो सकता है ।</s>
इन आदानों का प्रतिनिधित्व करने का एक तरीका है एक अलग न्यूरॉन या छिपा इकाई है कि नौ संभावित संयोजनों में से प्रत्येक के लिए सक्रिय करता है लाल ट्रक , लाल कार , लाल पक्षी , हरे ट्रक , और इतने पर .</s>
यह नौ विभिन्न न्यूरॉन्स की आवश्यकता है , और प्रत्येक न्यूरॉन स्वतंत्र रूप से रंग और वस्तु पहचान की अवधारणा सीखना होगा .</s>
इस स्थिति पर सुधार करने के लिए एक वितरित प्रतिनिधित्व का उपयोग करने के लिए , तीन न्यूरॉन्स के साथ रंग का वर्णन और तीन न्यूरॉन्स ऑब्जेक्ट पहचान का वर्णन .</s>
इसके लिए नौ के स्थान पर केवल छह न्यूरॉन्स कुल की आवश्यकता होती है , और लालिमा का वर्णन करने वाले न्यूरॉन कारों , ट्रकों और पक्षियों की छवियों से लालिमा के बारे में सीखने में सक्षम है , न केवल वस्तुओं की एक विशिष्ट श्रेणी की छवियों से .</s>
वितरित प्रतिनिधित्व की अवधारणा इस पुस्तक के लिए केंद्रीय है और अध्याय 15 में अधिक विस्तार से वर्णित है .</s>
कनेक्शनवादी आंदोलन की एक अन्य प्रमुख उपलब्धि थी - आंतरिक पुनः प्रचार के साथ तंत्रिका नेटवर्क को प्रशिक्षित करने के लिए पश्च - प्रचार का सफल प्रयोग और पश्च - प्रचार एल्गोरिथ्म (Rumelhart ) & # 44 ; 1987 & # 44 ; गहरे & # 44 ; कार्डCuna .</s>
इस एल्गोरिथ्म ने लोकप्रियता में वृद्धि की है और कमी आई है , लेकिन इस लेखन के रूप में , गहरे मॉडलों के प्रशिक्षण का प्रमुख दृष्टिकोण है ।</s>
1990 के दशक के दौरान , शोधकर्ताओं ने तंत्रिका नेटवर्क के साथ मॉडलिंग अनुक्रम में महत्वपूर्ण प्रगति की .</s>
हॉकरेइटर ( १९९१ ) और बेन्जियो एट अल .</s>
(1994 ) लंबे अनुक्रम मॉडलिंग में कुछ मौलिक गणितीय कठिनाइयों की पहचान की , जो अनुभाग 10 .7 में वर्णित है .</s>
होफ्रीटर और शम्मीदबर (1997 ) ने इनमें से कुछ कठिनाइयों को दूर करने के लिए लंबी अल्पकालिक स्मृति ( एलएसटीएम नेटवर्क ) की शुरूआत की ।</s>
आज , एलएसटीएम व्यापक रूप से कई अनुक्रम मॉडलिंग कार्यों के लिए प्रयोग किया जाता है , गूगल में कई प्राकृतिक भाषा प्रसंस्करण कार्यों सहित .</s>
तंत्रिका नेटवर्क की दूसरी लहर 1990 के दशक के मध्य तक चली ।</s>
तंत्रिका नेटवर्क तथा अन्य एआई प्रौद्योगिकियों पर आधारित वेन - ट्यून्स ने निवेश की मांग करते समय अवास्तविक सु महत्वाकांक्षी दावे करने शुरू कर दिए ।</s>
जब एआई अनुसंधान इन अनुचित अपेक्षाओं को पूरा नहीं किया , निवेशकों निराश थे .</s>
इसके साथ ही मशीनी शिक्षा के अन्य क्षेत्रों में भी प्रगति हुई ।</s>
१९९९ मे देश के विभिन्न भागों से संबंधित कई महत्वपूर्ण कार्य - शालाएं आईं जैसे प्रमुख सहयोगी कंपनियां उत्प्रेरित हुईं १९९९ से प्रारंभ हुईं ।</s>
इन दो कारणों से तंत्रिका नेटवर्क की लोकप्रियता में गिरावट आई जो 2007 तक चली ।</s>
इस दौरान , तंत्रिका नेटवर्क कुछ कार्यों पर प्रभावशाली प्रदर्शन प्राप्त करते रहे (</s>
लेकुन एत अल</s>
↑ अल .</s>
कनाडा के उन्नत अनुसंधान संस्थान ( सीआईएफएआर ) ने तंत्रिका नेटवर्कों के अनुसंधान को इसके तंत्रिका संगणन और अनुकूलन अनुसंधान पहल के माध्यम से जीवित रखने में मदद की ।</s>
यह कार्यक्रम टोरंटो विश्वविद्यालय , मॉन्ट्रियल विश्वविद्यालय में जिओफ्री हाइटन के नेतृत्व में मशीन शिक्षण अनुसंधान समूहों , और न्यूयॉर्क विश्वविद्यालय में यॉसुआ बेंजियो</s>
बहु - अनुशासनिक सीआईएफएआर एनसीएपी अनुसंधान पहल 17 अध्याय 1 .</s>
सूचना में तंत्रिका विज्ञानी और मानव तथा कंप्यूटर दृष्टि के विशेषज्ञ भी शामिल थे ।</s>
इस बिंदु पर , डीप नेटवर्क आम तौर पर माना जाता था कि प्रशिक्षण के लिए बहुत मुश्किल है .</s>
हम अब जानते हैं कि एल्गोरिदम कि 1980 के दशक के बाद से अस्तित्व में है काफी अच्छी तरह से काम करते हैं , लेकिन यह जाहिरा सिरका 2006 नहीं था .</s>
मुद्दा शायद बस यह है कि इन एल्गोरिदम उस समय उपलब्ध हार्डवेयर के साथ बहुत प्रयोग की अनुमति देने के लिए बहुत अधिक कम्प्यूटेशनल रूप से महंगा था .</s>
तंत्रिका नेटवर्क अनुसंधान की तीसरी लहर 2006 में एक सफलता के साथ शुरू हुई ।</s>
जिओफ्री हाइंटन ने दिखाया कि गहन विश्वास नेटवर्क नामक एक प्रकार के तंत्रिका नेटवर्क को लालची पर्तवार पूर्वसंकेतन ( हाइनटन एट अल , 2006 ) नामक रणनीति का उपयोग करते हुए कुशलतापूर्वक प्रशिक्षित किया जा सकता है , जिसका वर्णन हम अधिक विस्तार से धारा 151 में करते हैं ।</s>
अन्य अनुसन्धान समूहों ने शीघ्रता से वही कार्यनीति प्रदर्शित की ताकि अन्य अनेक प्रकार के अनुसंधान समूहों को भी उसी कार्यनीति का प्रशिक्षण दिया जा सके ।</s>
अनुसन्धान की इस लहर ने तंत्र के प्रयोग को लोकप्रिय बनाते हुए कहा कि अब शोधकर्ता न्यूट्रल नेटवर्कों का प्रशिक्षण पाने में सक्षम हो गए हैं इससे पहले तक संभव नहीं था ।</s>
इस समय , गहन तंत्रिका नेटवर्क अन्य मशीन सीखने की प्रौद्योगिकियों और साथ ही हाथ पर निर्भर कार्यक्षमता पर आधारित एआई प्रणालियों को बाहर की ओर संकेत दिया .</s>
तंत्रिका नेटवर्क की लोकप्रियता की यह तीसरी लहर इस लेखन के समय तक जारी है , हालांकि गहन शिक्षण अनुसंधान का ध्यान इस लहर के समय के भीतर नाटकीय रूप से बदल गया है .</s>
तीसरी लहर की शुरुआत नए अप्रशिक्षित अधिगम तकनीकों और छोटे डेटासेटों से अच्छा सामान्य बनाने के लिए गहरे मॉडलों की क्षमता पर ध्यान केंद्रित करने के साथ हुई , लेकिन आज काफी पुराने पर्यवेक्षित अधिगम एल्गोरिदमों और बड़े लेबल वाले डेटासेटों के लाभ उठाने की क्षमता में अधिक रुचि है ।</s>
1 . 2 . 2</s>
डाटासेट आकार बढ़ाना</s>
कोई भी सोच सकता है कि हाल ही में गहन अध्ययन को महत्वपूर्ण प्रौद्योगिकी के रूप में क्यों स्वीकार किया गया है यद्यपि 1950 के दशक में कृत्रिम तंत्रिका नेटवर्कों के पहले प्रयोग किए गए थे ।</s>
गहन अध्ययन सफलतापूर्वक 1990 के दशक के बाद से वाणिज्यिक अनुप्रयोगों में इस्तेमाल किया गया है , लेकिन अक्सर एक प्रौद्योगिकी और कुछ है कि केवल एक विशेषज्ञ का उपयोग कर सकता है की तुलना में एक कला के अधिक माना जाता था , हाल ही तक .</s>
यह सच है कि एक गहरी सीखने एल्गोरिथ्म से अच्छा प्रदर्शन प्राप्त करने के लिए कुछ कौशल की आवश्यकता है .</s>
सौभाग्य से , आवश्यक कौशल की मात्रा को कम कर देता है के रूप में प्रशिक्षण डेटा की मात्रा बढ़ जाती है .</s>
जटिल कार्यों पर मानव प्रदर्शन तक पहुँचने सीखने एल्गोरिदम लगभग सीखने एल्गोरिदम है कि 1980 के दशक में खिलौना समस्याओं को हल करने के लिए संघर्ष किया , हालांकि मॉडल हम इन एल्गोरिदम के साथ 18 अध्याय 1 .</s>
इनट्रोडयूसेंस में ऐसे परिवर्तन हुए जो बहुत गहरे आर्किटेक्चर के प्रशिक्षण को सरल बनाते हैं ।</s>
सबसे महत्वपूर्ण नया विकास है कि आज हम इन एल्गोरिदम वे सफल होने की जरूरत संसाधनों के साथ प्रदान कर सकते हैं .</s>
चित्र 1 . 8 यह दर्शाता है कि कैसे बेंचमार्क डेटासेट के आकार का समय के साथ उल्लेखनीय विस्तार हुआ है ।</s>
यह प्रवृत्ति समाज के बढ़ते डिजिटाइजेशन से प्रेरित है ।</s>
जैसे - जैसे हमारी अधिक से अधिक गतिविधियां कंप्यूटर पर होती हैं , अधिक से अधिक हम जो करते हैं वह दर्ज होता जाता है ।</s>
चूंकि हमारे कंप्यूटर लगातार एक साथ नेटवर्क कर रहे हैं , इसलिए इन अभिलेखों को केंद्रीकृत करना और उन्हें मशीन सीखने के अनुप्रयोगों के लिए उपयुक्त डेटासेट में ठीक करना आसान हो जाता है ।</s>
“बिग डेटा” १९०० १९५० १९७५ वर्ष 2015 वर्ष १० ० १० १ १० १० २ १० १० ३ १० ४ १० ५ १० ६ ७ ८ १० ९ डाटासेट आकार ( असंख्य उदाहरण )</s>
आईरिस मेलनसूची</s>
सार्वजनिक SVHN छवि - नेट</s>
सीआईएफएआर - 10</s>
इमेजनेट10क आईएलएसवीआरसी 2014 स्पोर्ट्स - 1</s>
</s>
समय के साथ डेटासेट आकार में वृद्धि .</s>
के प्रारंभिक सांख्यिकीविदों में सामग्री संकलित किया में डेटा सेटों 1908 अध्ययन किया या सांख्यिकीविदों का अध्ययन किया आंकड़ों या सांख्यिकीविदों 500 सांख्यिकीय डेटा का अध्ययन किया 1908 अध्ययन किया , या सांख्यिकीय डेटा 50 1935 के शुरू में सांख्यिकीय डेटा का अध्ययन किया । हजारों सांख्यिकीय डेटा अनुमानित सांख्यिकीय डेटा 1908 अध्ययन किया , 1935 में आया ।</s>
के दशक में , 1980 के दशक के दौरान जैविक प्रेरित मशीन सीखने के अग्रदूतों अक्सर छोटे सिंथेटिक डेटासेट के साथ काम किया , जैसे कि अक्षरों की लो - रिसोल्यूशन बिटमैप , जिन्हें कम कम्प्यूटेशनल लागत उठाने के लिए तैयार किया गया था और बताया कि तंत्रिका नेटवर्क के विशिष्ट प्रकार के कार्य सीखने में सक्षम थे .</s>
के दशक में मशीनी अधिगम अधिक सांख्यिकीय बन गया और हजारों उदाहरणों के दसियों हजार से अधिक डेटासेटों का लाभ उठाने लगा , जैसे MNIST डेटासेट ( हस्तलिखित संख्या के 1 . 998 में )</s>
2000 के दशक के पहले दशक में इसी आकार के अधिक परिष्कृत डेटासेट , जैसे कि सीआईएफएआर - 10 डेटासेट ( Krizhevsky और Hinton , 2009 ) का उत्पादन जारी रहा .</s>
उस दशक के अंत में और 2010 के दशक के पूर्वार्ध में , उल्लेखनीय रूप से बड़े डेटासेट , लाखों उदाहरणों के सैकड़ों हजारों से दसियों , पूरी तरह से बदल दिया , जो गहन सीखने के साथ संभव था .</s>
</s>
एम डाटासेट</s>
केरल</s>
ग्राफ़ के शीर्ष पर , हम देखते हैं कि अनुदित वाक्यों के डेटासेट , जैसे कि कनाडा के हेन्सार्ड ( ब्राउन एट अल , 1990 ) से निर्मित आईबीएम के डेटासेट और डब्ल्यूएमटी 2014 इंग्लिश से फ्रेंच डेटासेट ( Schwenkset ) , विशेष रूप से अन्य डेटा से काफी आगे हैं .</s>
अध्याय .</s>
परिचय चित्र 1 . 9</s>
MNIST डेटासेट से उदाहरण इनपुट</s>
जिस एजेंसी ने मूल रूप से यह डेटा एकत्र किया था , उसका नाम राष्ट्रीय मानक और प्रौद्योगिकी संस्थान है ।</s>
चूंकि मशीन लर्निंग एल्गोरिदम के साथ डेटा के आसान उपयोग के लिए पूर्वप्रसंस्करित किया गया है , “M” " मॉडिफाइड , " के लिए खड़ा है .</s>
MNIST डेटासेट में हस्तलिखित अंकों तथा संबद्ध लेबलों के स्कैन होते हैं जो यह बताता है कि प्रत्येक छवि में 0–9 किस अंक को समाहित किया गया है ।</s>
इस सरल वर्गीकरण समस्या गहन शिक्षण अनुसंधान में सबसे सरल और सबसे व्यापक रूप से इस्तेमाल परीक्षणों में से एक है .</s>
यह आधुनिक तकनीकों को हल करने के लिए काफी आसान होने के बावजूद लोकप्रिय रहता है .</s>
जेफ्री हिंटन ने इसे " मशीन अधिगम का समाज दर्शन " कहा है जिसका अर्थ है कि यह मशीन अधिगम शोधकर्ताओं को नियंत्रित प्रयोगशाला परिस्थितियों में अपने एल्गोरिदम का अध्ययन करने में सक्षम बनाता है , जैसे कि जीवविज्ञानी अक्सर फल मक्खियों का अध्ययन करते हैं ।</s>
मशीनी शिक्षा को अधिक आसान बना दिया है क्योंकि सांख्यिकीय आकलन का प्रमुख भार - एक छोटी मात्रा के आंकड़ों का अवलोकन करने के बाद नए आंकड़ों तक अच्छी तरह से पहुँचाना काफी हल्का हो गया है ।</s>
2016 के रूप में , अंगूठे का एक मोटा नियम है कि एक पर्यवेक्षित गहन सीखने एल्गोरिथ्म आम तौर पर लगभग 5 , 000 लेबल उदाहरण प्रति वर्ग के साथ स्वीकार्य प्रदर्शन प्राप्त होगा और मेल या 20 अध्याय 1 .</s>
सूचना मानव कार्य निष्पादन से अधिक होती है जब उसे कम से कम 10 मिलियन लेबल वाले उदाहरण वाले डेटासेट के साथ प्रशिक्षित किया जाता है ।</s>
इससे छोटा डाटासेट के साथ सफलतापूर्वक कार्य करना एक महत्वपूर्ण अनुसंधान क्षेत्र है , जिसमें विशेष रूप से इस बात पर ध्यान दिया जाता है कि हम बड़ी मात्रा में अविश्वसनीय उदाहरणों का लाभ कैसे उठा सकते हैं , जिसमें अप्रशिक्षित या अर्ध - प्रयोज्य अधिगम होता है ।</s>
1 . 2 . 3</s>
बढ़ते मॉडल आकार एक और प्रमुख कारण है कि तंत्रिका नेटवर्क आज अपेक्षाकृत कम सफलता का आनंद लेने के बाद बेतहाशा सफल हो रहे हैं 1980 के दशक के बाद से है कि हम बहुत बड़ा मॉडल चलाने के कम्प्यूटेशनल संसाधनों आज .</s>
कनेक्शन - इस्म की एक मुख्य अंतर्दृष्टि है कि जानवर बुद्धिमान हो जाते हैं जब उनके कई न्यूरॉन्स एक साथ काम करते हैं .</s>
एक व्यक्ति न्यूरॉन या न्यूरॉन्स का छोटा संग्रह विशेष रूप से उपयोगी नहीं है .</s>
जैविक न्यूरॉन्स विशेष रूप से सघन रूप से जुड़े नहीं हैं .</s>
जैसा कि आंकड़ा 1 . 10 में देखा गया है , हमारे मशीनी शिक्षण मॉडलों में दशकों से स्तनधारियों के मस्तिष्क तक के परिमाण के एक क्रम के भीतर न्यूरॉन के प्रति कई कनेक्शन रहे हैं ।</s>
न्यूरॉन्स की कुल संख्या के संदर्भ में , तंत्रिका नेटवर्क काफी हाल ही में , के रूप में आंकड़ा 1 . 11 में दिखाया गया है आश्चर्यजनक रूप से छोटे रहे हैं .</s>
छुपी इकाइयों के शुरू होने के बाद से , कृत्रिम तंत्रिका नेटवर्क का आकार मोटे तौर पर हर 2 . 4 साल में दोगुना हो गया है .</s>
यह वृद्धि बड़ी स्मृति वाले और बड़े डेटासेट की उपलब्धता वाले तीव्र कंप्यूटरों से संचालित होती है ।</s>
बड़े नेटवर्क अधिक जटिल कार्यों पर उच्च सटीकता प्राप्त करने में सक्षम होते हैं .</s>
ऐसा लगता है कि यह सिलसिला दशकों तक चलता रहेगा ।</s>
जब तक नई प्रौद्योगिकियां तेजी से स्केलिंग सक्षम नहीं करतीं , कृत्रिम तंत्रिका नेटवर्कों में कम से कम 2050 के दशक तक न्यूरॉन्स की उतनी संख्या नहीं होगी ।</s>
जैविक न्यूरॉन्स वर्तमान कृत्रिम न्यूरॉन्स की तुलना में अधिक जटिल कार्यों का प्रतिनिधित्व कर सकते हैं , इसलिए जैविक तंत्रिका नेटवर्क भी इस साजिश चित्रित करता है की तुलना में बड़ा हो सकता है .</s>
पुनर्निरीक्षण में , यह विशेष रूप से आश्चर्य की बात नहीं है कि जोंक की तुलना में कम न्यूरॉन्स के साथ तंत्रिका नेटवर्क परिष्कृत कृत्रिम बुद्धिमत्ता प्रोब - लेम को हल करने में असमर्थ थे .</s>
आज भी नेटवर्क , जिसे हम संगणकीय प्रणालियों की दृष्टि से काफी बड़े मानते हैं , मेंढ़क जैसे अपेक्षाकृत आदिम कशेरुकी जंतुओं के तंत्रिका तंत्र से भी छोटे हैं ।</s>
समय के साथ मॉडल के आकार में वृद्धि , तेजी से सीपीयू की उपलब्धता के कारण , सामान्य उद्देश्य के आगमन ( धारा 12 . 1 . 2 में निहित ) जीपीयू , तेजी से नेटवर्क कनेक्टिविटी और वितरित कंप्यूटिंग के लिए बेहतर सॉफ्टवेयर मूल संरचना , गहन शिक्षण के इतिहास में सबसे महत्वपूर्ण प्रवृत्तियों में से एक है .</s>
यह प्रवृत्ति आमतौर पर भविष्य में भी अच्छी तरह से जारी रहने की उम्मीद है .</s>
अध्याय</s>
परिचय 1950 वर्ष 2000 2015 वर्ष 10 1 10 2 10 10 3 10 4 4 4 कनेक्शन प्रति न्यूरॉन 1 2 3 4 5 6 7 8 10 फल मक्खी माउस बिल्ली मानव चित्र 110</s>
: समय के साथ न्यूरॉन प्रति कनेक्शन की संख्या .</s>
प्रारंभ में , कृत्रिम तंत्रिका नेटवर्कों में न्यूरॉन्स के बीच होने वाले यौगिकों की संख्या हार्डवेयर क्षमताओं द्वारा सीमित थी .</s>
आज , न्यूरॉन्स के बीच कनेक्शन की संख्या ज्यादातर एक डिजाइन विचार है .</s>
कुछ कृत्रिम तंत्रिका नेटवर्क में लगभग एक बिल्ली के रूप में न्यूरॉन प्रति कई कनेक्शन है , और यह काफी आम है अन्य तंत्रिका नेटवर्क के लिए चूहों जैसे छोटे स्तनधारियों के रूप में न्यूरॉन प्रति कई कनेक्शन है .</s>
यहां तक कि मानव मस्तिष्क में प्रति न्यूरॉन कनेक्शन की अत्यधिक मात्रा नहीं होती है .</s>
विकिपीडिया से जैविक तंत्रिका नेटवर्क आकार (2015 )</s>
1 .</s>
अनुकूली रैखिक तत्व (Widro and Hoff , 1960 2 .</s>
नियोकॉग्नाइट्रॉन ( फुकुशिमा , 1980 3 .</s>
Comment</s>
चेलाबिंस्क</s>
डीप बोल्ज़मैन मशीन</s>
अविकसित संवलनीय नेटवर्क (</s>
जेरेट एट अल . , 2009 6 .</s>
Comment</s>
( कुरियन एट अल )</s>
२०१० १ः२०६ , ०००००००००६</s>
वितरित ऑटोनकोडर (</s>
ली एट अल</s>
वर्ष 2012</s>
मल्टी - जीपीयू संवलित नेटवर्क (</s>
किरिज़ेव्स्की एट अल</s>
वर्ष 2012</s>
COTS HPC अनुपयुक्त convolutional संजाल (</s>
कोट्स एट अल , 2013 10 .</s>
गूगलनेट</s>
1 . 2 . 4</s>
बढ़ती परिशुद्धता , जटिलता और वास्तविक - वर्ल्ड प्रभाव</s>
1980 के दशक के बाद से , गहरी सीखने लगातार सटीक मान्यता और भविष्यवाणी प्रदान करने की अपनी क्षमता में सुधार किया है .</s>
इसके अलावा , गहरे शिक्षण लगातार व्यापक और व्यापक अनुप्रयोगों के सेट करने के लिए सफलता के साथ लागू किया गया है .</s>
सबसे पहले गहरे मॉडल का उपयोग कसकर फसली में अलग - अलग वस्तुओं की पहचान के लिए किया गया , अत्यंत छोटे चित्रों (</s>
रेमेलहार्ट एट अल , 1986</s>
तब से वहाँ छवियों तंत्रिका नेटवर्क प्रक्रिया कर सकता है के आकार में क्रमिक वृद्धि हुई है .</s>
आधुनिक वस्तु अभिज्ञान नेटवर्क प्रक्रम में प्रचुर उच्च - प्रत्यावर्तन फोटो होते हैं तथा 22 अध्याय 1 नहीं होते ।</s>
प्रति वर्ष १९५० वर्ष २००० वर्ष २०५६ वर्ष १० - २ ।</s>
10 0 10 1 10 10 10 10 2 10 3 10 4 10 10 5 10 6 10 10 7 10 8 10 9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10</s>
न्यूरॉन्स की 10 11 संख्या</s>
1</s>
बढ़ 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 20 स्पंज गोलकृमि ली ची ची ची ची ची ची ची ची ची ची ची ची ची ची ची फीरोज ओक्टोपस मानव चित्र 111 : समय पर तंत्रिका नेटवर्क आकार</s>
छुपी इकाइयों के शुरू होने के बाद से , कृत्रिम तंत्रिका नेटवर्क का आकार मोटे तौर पर हर 2 . 4 साल में दोगुना हो गया है .</s>
विकिपीडिया से जैविक तंत्रिका नेटवर्क आकार (2015 )</s>
1 .</s>
पेप्टॉन ( १९५८ )</s>
अनुकूली रैखिक तत्व (Widro and Hoff , 1960 3 .</s>
नियोकॉग्नाइट्रॉन ( फुकुशिमा , 1980 4 .</s>
शीघ्र बैक - प्रोपगेशन नेटवर्क ( रूमेलहार्ट एट अल , 1986bd 5 .</s>
भाषण पहचान के लिए पुनरावर्ती तंत्रिका नेटवर्क ( रॉबिन्सन एंड फाल्सड ) , 1991</s>
6 .</s>
भाषण पहचान के लिए मल्टीलेयर परसेप्टरॉन ( Bengio एट अल )</s>
१९९१</s>
अर्थ फील्ड अवग्रहित विश्वास नेटवर्क (Saul एट अल , 1996 )</s>
8 .</s>
लेनेट - ५</s>
( लेट - सीएन एट अल )</s>
१९९८बी</s>
इको स्टेट नेटवर्क ( जेजर एंड हास , 2004 )</s>
10 .</s>
गहरा विश्वास नेटवर्क</s>
Comment</s>
चेलाबिंस्क</s>
डीप बोल्ज़मैन मशीन</s>
GPU - फैसलाती गहरा विश्वास नेटवर्क (</s>
रैना एट अल</s>
↑ 2009 के . सी . ई . , 14 .</s>
अविकसित संवलनीय नेटवर्क (</s>
ज्रेट एट अल , 2009 15 .</s>
Comment</s>
समाप्तप्रायः २०१०</s>
१६</s>
ओएमपी - 1 नेटवर्क ( कॉट्स एंड एनजी ) , 2011 17 .</s>
वितरित ऑटोनकोडर (</s>
लेइट अल , 2012 18 .</s>
मल्टी - जीपीयू संवलित नेटवर्क (</s>
किरिज़ेव्स्की एट अल</s>
वर्ष 2012</s>
COTS HPC अनुपयुक्त convolutional संजाल (</s>
कोट्स एट अल , 2013 20</s>
योग - नेट</s>
इसी प्रकार , आदि नेटवर्क केवल दो प्रकार की वस्तुओं को पहचान सकते थे ( जैसे कि कुछ मामलों में , एक एकल प्रकार की वस्तु की उपस्थिति या अनुपस्थिति , जबकि ये आधुनिक नेटवर्क आमतौर पर वस्तुओं की कम से कम 1 , 000 विभिन्न श्रेणियों को पहचानते हैं .</s>
वस्तु पहचान में सबसे बड़ी प्रतियोगिता इमेजनेट 23 अध्याय 1 है .</s>
परिचय</s>
प्रत्येक वर्ष बड़े पैमाने पर दृश्य मान्यता चैलेंज आयोजित किया जाता है ।</s>
के मौसमी वृद्धि में एक नाटकीय क्षण आया जब एक संवलित नेटवर्क पहली बार के लिए इस चुनौती को जीत लिया और एक व्यापक मार्जिन से , के रूप में राज्य - बाहर शीर्ष - 5 त्रुटि दर से संभावित उदाहरणों को 15 . 1 प्रतिशत )</s>
तब से , ये प्रतियोगिताएं लगातार गहरे संवलात्मक जालों से जीती जाती हैं , और इस लेखन के रूप में , गहन अध्ययन में हुई प्रगति ने इस प्रतियोगिता में नवीनतम शीर्ष - 5 त्रुटि दर को 36 प्रतिशत तक पहुंचा दिया है , जैसा कि 1 . 12 अंक में दर्शाया गया है ।</s>
गहन अध्ययन का वाचिक अभिज्ञान पर भी नाटकीय प्रभाव पड़ा है ।</s>
1990 के दशक भर में सुधार के बाद , भाषण मान्यता के लिए त्रुटि दरों में लगभग 2000 में शुरू करने के लिए रुका .</s>
।</s>
हम इस इतिहास का अन्वेषण अनुभाग 12 . 3 में और अधिक विस्तार से करते हैं ।</s>
डीप नेटवर्क को पैड्रियन संसूचन और प्रतिबिंब विभाजन के लिए भी शानदार सफलता मिली है ।</s>
अल - नमाज़े क़ुरैश २०१३ ) के महीने में हुमायुं के गुटखे में गुटखे में गुटखे में गुटखे में गुटखे का व्यापार</s>
सेट्स एट अल , 2012</s>
इसी के साथ डीप नेटवर्कों की मात्रा और सटीकता में वृद्धि हुई है , 2010 2011 2012 2013 2014 2015 वर्ष 000005 0 .10 015</s>
0 . 20 0 .25 0 .30 ILSVRC वर्गीकरण त्रुटि दर चित्र 1 . 12 : समय के साथ त्रुटि दर घटाना</s>
चूंकि डीप नेटवर्क इमेजननेट बड़े पैमाने पर दृश्य अभिज्ञान चैलेंज में प्रतिस्पर्धा करने के लिए आवश्यक पैमाने पर पहुँच गए , उन्होंने लगातार प्रतिवर्ष कम और कम त्रुटि दरों का उत्पादन करते हुए प्रतियोगिता जीत ली है .</s>
रसाकोव्स्की वगैरह का डाटा</s>
( २०१४ )</s>
और एट अल</s>
१२०</s>
24 अध्याय 1 .</s>
INTRODUCTION में कार्य की जटिलता है जिसे वे हल कर सकते हैं ।</s>
गुडफेलो एट अल .</s>
(2014 में प्रदर्शित किया गया कि तंत्रिका नेटवर्क किसी छवि से अंकित वर्णों के एक पूरे अनुक्रम को केवल एक वस्तु की पहचान करने के बजाय , आउटपुट करना सीख सकता है .</s>
पहले , यह व्यापक रूप से माना जाता था कि इस प्रकार के सीखने के लिए अनुक्रम के अलग - अलग तत्वों की लेबलिंग की आवश्यकता होती थी ( Gülçhre और बेंजियो , 2013 .</s>
पुनरावर्ती तंत्रिका नेटवर्क , जैसे कि ऊपर उल्लिखित LSTM अनुक्रम मॉडल , अब केवल निश्चित इनपुट के बजाय अनुक्रम और अन्य अनुक्रमों के बीच संबंधों को मॉडल करने के लिए प्रयोग किया जाता है .</s>
यह अनुक्रम - से - अनुक्रम अधिगम एक अन्य अनुप्रयोग में क्रांति लाने के मुहाने पर प्रतीत होता है : मशीन अनुवाद ( सूट्स - एडी एट अल .</s>
वर्ष 2014</s>
बढ़ती जटिलता की यह प्रवृत्ति , न्युरल ट्यूरिंग मशीन (Graves एट अल , 2014 जो स्मृति कोशिकाओं से पढ़ने के लिए और स्मृति कोशिकाओं के लिए मनमाना सामग्री लिखने के लिए सीखने के साथ अपने तार्किक निष्कर्ष पर धकेल दिया है .</s>
इस तरह के तंत्रिका नेटवर्क वांछित व्यवहार के उदाहरणों से सरल प्रोग्राम सीख सकते हैं ।</s>
उदाहरण के लिए , वे स्कॉर्पल्ड और अनुक्रमित अनुक्रमों के उदाहरण दिए गए संख्याओं की सूचियों को छांटना सीख सकते हैं .</s>
यह आत्म - प्रोग्रामन प्रौद्योगिकी अपनी शैशवावस्था में है , लेकिन भविष्य में सिद्धांत रूप में इसे लगभग किसी भी कार्य के लिए लागू किया जा सकता है ।</s>
गहन अध्ययन की एक और प्रमुख उपलब्धि है प्रवर्तन शिक्षण के क्षेत्र में इसका विस्तार ।</s>
प्रवर्तन सीखने के संदर्भ में , एक स्वायत्त एजेंट को मानव ऑपरेटर से किसी मार्गदर्शन के बिना , परीक्षण और त्रुटि द्वारा , एक कार्य करना सीखना होगा .</s>
दीपमण्ड ने प्रदर्शित किया कि गहन अध्ययन पर आधारित एक प्रवर्तन शिक्षण प्रणाली अटारी वीडियो गेम्स खेलना सीखने में सक्षम है , जो कई कार्यों पर मानव स्तर के प्रदर्शन तक पहुंचता है ।</s>
( प्रमुख अल , 2015 )</s>
गहन अध्ययन ने रोबोटिक्स (फिन एट अल , 2015 ) के लिए प्रवर्तन सीखने के कार्य में भी महत्वपूर्ण सुधार किया है ।</s>
गहन अध्ययन के इन अनुप्रयोगों में से कई अत्यधिक लाभदायक हैं .</s>
गहन अधिगम का उपयोग अब कई शीर्ष प्रौद्योगिकी कंपनियों द्वारा किया जाता है , जिनमें गूगल , माइक्रोसॉफ्ट , फेसबुक , आईबीएम , बैदु , एप्पल , एडोब , नेटफ्लिक्स , एनवीआईए , और एनईसी शामिल हैं .</s>
गहन अध्ययन में प्रगति सॉफ्टवेयर मूल संरचना में हुई प्रगति पर भी काफी निर्भर करती है ।</s>
सॉफ्टवेयर</s>
Written By & # 124 ; Administrator</s>
Written By & # 44 ; DistBelief</s>
TensorF्लो ( Aaabi एट अल , 2015 ) सभी महत्वपूर्ण अनुसंधान परियोजनाओं या वाणिज्यिक उत्पादों का समर्थन किया है .</s>
गहन अध्ययन ने अन्य विज्ञानों में भी योगदान दिया है ।</s>
वस्तु अभिज्ञान हेतु आधुनिक संयुग्मी नेटवर्क दृश्य प्रक्रमण का एक मॉडल उपलब्ध कराते हैं जिसका अध्ययन तंत्रिका विज्ञानी कर सकते हैं ( डीईसीकेर्लो , 2013 ) ।</s>
गहन अध्ययन भी भारी मात्रा में डेटा के प्रसंस्करण और वैज्ञानिक 25 अध्याय 1 में उपयोगी भविष्यवाणियां करने के लिए उपयोगी उपकरण प्रदान करता है .</s>
परिचय क्षेत्र</s>
इसका सफल प्रयोग इस बात की भविष्यवाणी करने में किया गया है कि किस प्रकार अणु दवा कंपनियों को नई दवाओं के निर्माण में सहायता करने के लिए आपस में विचार - विमर्श करेंगे (डल एट अल , 2014 )</s>
हम भविष्य में अधिक से अधिक वैज्ञानिक क्षेत्रों में गहरी सीखने की उम्मीद है .</s>
सारांश में , गहन अधिगम मशीनी अधिगम का एक दृष्टिकोण है जो पिछले कई दशकों में विकसित मानव मस्तिष्क , सांख्यिकी और अनुप्रयुक्त गणित के हमारे ज्ञान पर बहुत अधिक आकर्षित किया है .</s>
हाल के वर्षों में गहन अध्ययन ने अपनी लोकप्रियता और उपयोगिता में अत्यधिक वृद्धि देखी है , मोटे तौर पर अधिक शक्तिशाली कंप्यूटरों , बड़े डेटासेटों और गहन नेटवर्कों को प्रशिक्षित करने की तकनीकों के परिणामस्वरूप ।</s>
आने वाले वर्षों में गहन शिक्षा में और सुधार लाने तथा इसे नए मोर्चों पर लाने की चुनौतियों और अवसरों का सामना करना पड़ रहा है ।</s>
26</s>
