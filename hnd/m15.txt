अध्याय 1 परिचय निवेशकों लंबे समय से मशीनों है कि लगता है बनाने का सपना देखा है .</s>
यह इच्छा कम से कम प्राचीन यूनान के समय की है ।</s>
</s>
जब प्रोग्राम योग्य कंप्यूटरों की पहली कल्पना की गई तो लोगों ने सोचा कि क्या ऐसी मशीनें बुद्धिमान हो सकती हैं , एक के निर्माण से सौ साल पहले ( लोवेलेस , 1842 )</s>
आज , कृत्रिम बुद्धिमत्ता ( एआईआई ) एक फलता - फूलता क्षेत्र है जिसमें कई व्यावहारिक अनुप्रयोग और सक्रिय अनुसंधान विषय हैं ।</s>
हम सामान्य श्रम को स्वचालित करने के लिए बुद्धिमान सॉफ्टवेयर की ओर देखते हैं , भाषण या छवियों को समझते हैं , चिकित्सा में निदान करते हैं और बुनियादी वैज्ञानिक अनुसंधान का समर्थन करते हैं .</s>
कृत्रिम बुद्धि के आरंभिक दिनों में इस क्षेत्र ने तेजी से उन समस्याओं का समाधान और समाधान किया जो मनुष्यों के लिए बौद्धिक रूप से कठिन हैं लेकिन कंप्यूटर के लिए अपेक्षाकृत सीधे - सीधे आगे हैं ।</s>
समस्याओं है कि औपचारिक , गणितीय नियमों की एक सूची द्वारा वर्णित किया जा सकता है .</s>
कृत्रिम बुद्धि के लिए सच्ची चुनौती उन कार्यों को हल करने में साबित हुई जो लोगों के लिए करना आसान है लेकिन लोगों के लिए औपचारिक रूप से वर्णन करना कठिन है - उन प्रतीकों का जो हम सहज भाव से हल करते हैं , जो बोलचाल के शब्दों या चेहरों को पहचानने की तरह स्वचालित महसूस करते हैं ।</s>
यह पुस्तक इन अधिक सहज ज्ञान युक्त समस्याओं के समाधान के बारे में है ।</s>
यह समाधान कंप्यूटर को अनुभव से सीखने और अवधारणाओं के एक पदानुक्रम के संदर्भ में दुनिया को समझने के लिए अनुमति देने के लिए है , सरल अवधारणाओं के लिए अपने संबंध के माध्यम से परिभाषित प्रत्येक अवधारणा के साथ .</s>
अनुभव से ज्ञान इकट्ठा करके , यह दृष्टिकोण मानव ऑपरेटरों के लिए औपचारिक रूप से कंप्यूटर की जरूरत के सभी ज्ञान को निर्दिष्ट करने की आवश्यकता से बचता है .</s>
अवधारणाओं का पदानुक्रम कंप्यूटर को सरल अवधारणाओं से बाहर बनाकर जटिल अवधारणाओं को सीखने में सक्षम बनाता है .</s>
यदि हम एक ग्राफ आकर्षित कैसे इन अवधारणाओं को दर्शाता है 1 CHAPTER 1 .</s>
INTRODUCTER एक दूसरे के ऊपर निर्मित कर रहे हैं , ग्राफ गहरा है , कई परतों के साथ .</s>
इस कारण से , हम एआई गहरी सीखने के लिए इस दृष्टिकोण कहते हैं .</s>
एआई की कई प्रारंभिक सफलताएं अपेक्षाकृत बंध्य और औपचारिक वातावरण में हुईं और उन्हें दुनिया के बारे में अधिक जानकारी रखने के लिए कंप्यूटर की आवश्यकता नहीं पड़ी .</s>
उदाहरण के लिए , आईबीएम की डीप ब्लू शतरंज खेलने की प्रणाली ने 1997 में विश्व चैंपियन गैरी कास्परोव को हराया ( हासु , 2002 )</s>
वास्तव में शतरंज एक बहुत ही सरल संसार है , जिसमें केवल साठ - चौंसठ स्थान और चौंसठ टुकड़े हैं जो केवल कठोर रूप से निर्धारित तरीकों से चल सकते हैं ।</s>
एक सफल शतरंज रणनीति को छोड़ना एक जबरदस्त उपलब्धि है , लेकिन चुनौती शतरंज के टुकड़ों के सेट का वर्णन करने और कंप्यूटर के लिए अनुमति योग्य चालों की कठिनाई के कारण नहीं है .</s>
शतरंज पूरी तरह से औपचारिक नियमों की एक बहुत ही संक्षिप्त सूची द्वारा वर्णित किया जा सकता है , आसानी से प्रोग्रामर द्वारा समय से आगे प्रदान की .</s>
विडंबना यह है कि , अमूर्त और औपचारिक कार्य जो एक मानव के लिए सबसे कठिन मानसिक उपक्रमों में से हैं , एक कंप्यूटर के लिए सबसे आसान में से हैं .</s>
कंप्यूटर लंबे समय से सर्वश्रेष्ठ मानव शतरंज खिलाड़ी को भी हराने में सक्षम रहे हैं , लेकिन केवल हाल ही में वस्तुओं या भाषण को पहचानने के लिए औसत मानव की कुछ क्षमताओं का मिलान शुरू कर दिया है .</s>
व्यक्ति के दैनिक जीवन के लिए विश्व के बारे में अपार ज्ञान की आवश्यकता होती है ।</s>
इस ज्ञान का बहुत कुछ व्यक्तिपरक और सहज ज्ञान युक्त है , और इसलिए औपचारिक रूप से व्यक्त करना कठिन है ।</s>
कंप्यूटर एक बुद्धिमान तरीके से व्यवहार करने के क्रम में इस एक ही ज्ञान पर कब्जा करने की जरूरत है .</s>
कृत्रिम बुद्धि में एक प्रमुख चुनौती यह है कि इस अनौपचारिक ज्ञान को कंप्यूटर में कैसे प्राप्त किया जाए ।</s>
कई कृत्रिम बुद्धिमत्ता परियोजनाओं ने दुनिया के बारे में औपचारिक भाषाओं में जानकारी को कड़ी जानकारी देने की कोशिश की है ।</s>
एक कंप्यूटर तार्किक अनुमान नियमों का उपयोग कर इन औपचारिक भाषाओं में बयानों के बारे में स्वचालित रूप से तर्क कर सकते हैं .</s>
इसे कृत्रिम बुद्धि का ज्ञान आधार दृष्टिकोण कहा जाता है ।</s>
इनमें से किसी भी परियोजना को बड़ी सफलता नहीं मिली है ।</s>
ऐसी सबसे प्रसिद्ध परियोजनाओं में से एक है सीक ( Lyc ) और गुह , 1989 .</s>
साइक एक अनुमान इंजन है और साइकल नामक भाषा में कथनों का डेटाबेस है ।</s>
इन बयानों को मानव पर्यवेक्षकों के एक स्टाफ द्वारा दर्ज किया जाता है .</s>
यह एक अपरिच्छिन्न प्रक्रिया है ।</s>
लोग दुनिया का सही वर्णन करने के लिए पर्याप्त जटिलता के साथ औपचारिक नियम बनाने के लिए संघर्ष करते हैं .</s>
उदाहरण के लिए , Cyc सुबह ( Linde , 1992 ) फ्रेड शेव नामक व्यक्ति के बारे में एक कहानी समझने में विफल रहा .</s>
इसके अनुमान इंजन ने कहानी में एक असंगति का पता लगायाः यह जानता था कि लोगों के पास विद्युत पुर्जे नहीं हैं , लेकिन चूंकि फ्रेड एक विद्युत रज़र थामे हुए था , इसलिए यह विश्वास करता था कि एंटिटी “फ़्रेडवाहिलशिंग” में विद्युत पुर्जे थे .</s>
इसलिए यह पूछा गया कि क्या फ्रेड अभी भी शेव कर रहे थे ?</s>
कठोर ज्ञान पर निर्भर प्रणालियों के सामने आने वाली कठिनाइयों से यह संकेत मिलता है कि एआई सिस्टम को 2 सीएचएपीटर 1 निकालकर अपना ज्ञान प्राप्त करने की क्षमता की आवश्यकता है ।</s>
कच्चे डेटा से सामान्य पैटर्न</s>
इस क्षमता को मशीन लर्निंग के नाम से जाना जाता है ।</s>
मशीनी शिक्षण की शुरुआत ने कंप्यूटरों को वास्तविक दुनिया के ज्ञान से संबंधित समस्याओं से निपटने और व्यक्तिपरक दिखने वाले निर्णय लेने में सक्षम बनाया .</s>
एक सरल मशीन अधिगम एल्गोरिथ्म जिसे संभार प्रतिगमन कहा जाता है , यह निर्धारित कर सकता है कि क्या सीजेरियन डिलीवरी की सिफारिश की जाए ( मोर - योज़फ एट अल , 1990 )</s>
एक सरल मशीन लर्निंग एल्गोरिथम जिसे नैव बेयस कहा जाता है , स्पैम ई - मेल से वैध ई - मेल को अलग कर सकता है ।</s>
इन सरल मशीन अधिगम एल्गोरिदम का प्रदर्शन उनके द्वारा दिए गए डेटा के निरूपण पर अत्यधिक निर्भर करता है .</s>
उदाहरण के लिए , जब संभारात्मक प्रतिगमन का प्रयोग सीजेरियन डिलीवरी की सिफारिश करने के लिए किया जाता है , तो एआई सिस्टम सीधे रोगी की जांच नहीं करता है .</s>
इसके बजाय , डॉक्टर प्रणाली प्रासंगिक जानकारी के कई टुकड़े बताता है , जैसे कि गर्भाशय के निशान की उपस्थिति या अनुपस्थिति .</s>
रोगी के प्रतिनिधित्व में शामिल जानकारी के प्रत्येक टुकड़े एक विशेषता के रूप में जाना जाता है .</s>
लॉजिस्टिक प्रतिगमन सीखता है कि कैसे रोगी की इन विशेषताओं में से प्रत्येक विभिन्न परिणामों के साथ सहसंबंधित करता है .</s>
हालांकि , यह कैसे सुविधाओं को किसी भी तरह से परिभाषित कर रहे हैं प्रभावित नहीं कर सकते हैं .</s>
यदि संभारिक प्रतिगमन को रोगी का एमआरआई स्कैन दिया जाता , बजाय चिकित्सक की औपचारिक रिपोर्ट के , तो यह उपयोगी भविष्यवाणियां नहीं कर पाता .</s>
एमआरआई स्कैन में व्यक्तिगत पिक्सल का प्रसव के दौरान होने वाली किसी भी जटिलताओं से नगण्य सहसंबंध होता है ।</s>
अभ्यावेदन पर यह निर्भरता एक सामान्य घटना है जो पूरे कंप्यूटर विज्ञान और यहां तक कि दैनिक जीवन में प्रकट होती है .</s>
कंप्यूटर विज्ञान में , डेटा संग्रह की खोज जैसे कार्य घातीय रूप से तेजी से आगे बढ़ सकते हैं यदि कोलेक - टीन संरचित और बुद्धिमानी से अनुक्रमित हो .</s>
लोग अरबी अंकों पर अंकगणित आसानी से कर सकते हैं लेकिन रोमन अंकों पर अंकगणित को अधिक समय लेने पर पाते हैं ।</s>
यह आश्चर्य की बात नहीं है कि प्रतिनिधित्व के चयन का मशीन लर्निंग एल्गोरिदम के प्रदर्शन पर भारी प्रभाव पड़ता है .</s>
एक सरल दृश्य उदाहरण के लिए , आकृति 1 . 1 देखें</s>
कई कृत्रिम बुद्धि कार्यों को उस कार्य के लिए निकालने के लिए सुविधाओं के सही सेट को डिजाइन करके हल किया जा सकता है , फिर एक सरल मशीन सीखने एल्गोरिथ्म के लिए इन सुविधाओं को प्रदान करता है .</s>
उदाहरण के लिए , ध्वनि से वक्ता पहचान के लिए एक उपयोगी विशेषता वक्ता के मुखर पथ के आकार का अनुमान है .</s>
इस विशेषता से इस बात का पुख्ता सुराग मिलता है कि वक्ता पुरुष है , महिला है या बच्चा ।</s>
कई कार्यों के लिए , हालांकि , यह जानना मुश्किल है कि क्या सुविधाएँ निकाली जानी चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हम तस्वीरों में कारों का पता लगाने के लिए प्रोग्राम लिखना चाहेंगे ।</s>
हम जानते हैं कि कारों में पहिये होते हैं , इसलिए हम एक विशेषता के रूप में एक पहिये की उपस्थिति का उपयोग करना पसंद कर सकते हैं .</s>
दुर्भाग्य से , यह वास्तव में क्या एक पहिया पिक्सेल मूल्यों के संदर्भ में की तरह दिखता है का वर्णन करने के लिए मुश्किल है .</s>
एक पहिये का आकार सरल ज्यामितीय होता है , लेकिन इसकी छवि पहिये पर गिरने वाली छायाओं से जटिल हो सकती है , सूर्य पहिये के धातु भागों से बाहर निकलता है , कार का फेंडर या 3 CHAPTER 1 में कोई वस्तु</s>
Name</s>
. . . . . . . . .</s>
. . . . . . . . . . . . . . . . . . . . . . . . . . .</s>
. . . . . . . . .</s>
. . . . . . . . . . . . . . . . . . . . . . . . . . .</s>
चित्र 1 . 1 :</s>
विभिन्न अभ्यावेदनों का उदाहरणः मान लीजिए कि हम प्रकीर्णन में उनके बीच एक रेखा खींचकर डेटा की दो श्रेणियों को अलग करना चाहते हैं .</s>
बाईं ओर के भूखंड में , हम कार्टेसियन निर्देशांक का उपयोग कर कुछ डेटा का प्रतिनिधित्व करते हैं , और कार्य असंभव है .</s>
दाहिनी ओर के भूखंड में , हम ध्रुवीय निर्देशांकों के साथ डेटा का प्रतिनिधित्व करते हैं और एक ऊर्ध्वाधर रेखा के साथ हल करने के लिए कार्य सरल हो जाता है .</s>
</s>
इस समस्या का एक समाधान यह है कि न केवल प्रतिनिधित्व से आउटपुट तक मानचित्रण का पता लगाने के लिए मशीनी अधिगम का उपयोग किया जाए बल्कि स्वयं निरूपण भी किया जाए ।</s>
इस दृष्टिकोण प्रतिनिधित्व सीखने के रूप में जाना जाता है .</s>
विद्वत अभ्यावेदन अक्सर हस्तलेखित अभ्यावेदनों के साथ प्राप्त की जा सकने वाली तुलना में कहीं बेहतर प्रदर्शन में परिणामित होते हैं .</s>
वे एआई सिस्टम को नए कार्यों के लिए तेजी से अनुकूलित करने में भी सक्षम बनाते हैं , जिसमें कम से कम मानवीय हस्तक्षेप होता है .</s>
एक प्रतिनिधित्व सीखने एल्गोरिथ्म मिनट में एक सरल कार्य के लिए , या घंटे से महीनों में एक जटिल कार्य के लिए सुविधाओं के एक अच्छे सेट की खोज कर सकते हैं .</s>
मैनुअल डिजाइन जटिल कार्य के लिए मानव समय की विशेषताओं की आवश्यकता होती है और एक महान प्रयास के लिए पूरे समुदाय के शोधकर्ताओं के दशकों ले सकते हैं ।</s>
अभ्यावेदन अधिगम एल्गोरिथ्म का सारभूत उदाहरण ऑ - टोनकोडर है ।</s>
एक ऑटोनकोडर , एक एनकोडर फलन का संयोजन होता है , जो इनपुट डेटा को भिन्न निरूपण में परिवर्तित करता है , और एक विकोडक फलन , जो नए निरूपण को वापस मूल प्रारूप में परिवर्तित करता है .</s>
जब किसी इनपुट को एनकोडर के माध्यम से चलाया जाता है और फिर डिकोडर के माध्यम से चलाया जाता है तो अधिक से अधिक जानकारी को सुरक्षित रखने के लिए ऑटोनकोडर्स को प्रशिक्षित किया जाता है , लेकिन उन्हें नए प्रतिनिधित्व को विभिन्न अच्छे गुण बनाने के लिए भी प्रशिक्षित किया जाता है .</s>
विभिन्न प्रकार के ऑटोनकोडर्स का उद्देश्य विभिन्न प्रकार के गुणों को प्राप्त करना होता है ।</s>
जब सुविधाओं को सीखने के लिए सुविधाओं या एल्गोरिदम की डिजाइनिंग , हमारा लक्ष्य आमतौर पर मनाया डेटा की व्याख्या करने वाली भिन्नता के कारकों को अलग करना है .</s>
इसमें 4 CHAPTER 1 .</s>
InTRODC</s>
इस तरह के कारकों अक्सर मात्रा है कि सीधे मनाया जाता है नहीं कर रहे हैं .</s>
इसके बजाय , वे या तो अप्रतिरक्षित वस्तुओं के रूप में या भौतिक जगत की उन अप्रतिरक्षित शक्तियों के रूप में विद्यमान हो सकते हैं जो वेधीय मात्राओं को प्रभावित करती हैं .</s>
वे मानव मस्तिष्क में निर्माण के रूप में भी मौजूद हो सकते हैं जो अवलोकन डेटा के उपयोगी सरलीकरण स्पष्टीकरण या अनुमानित कारण प्रदान करते हैं .</s>
उन्हें अवधारणाओं या अमूर्तनों के रूप में सोचा जा सकता है जो हमें आंकड़ों में समृद्ध विविधता का बोध कराने में मदद करते हैं ।</s>
जब किसी भाषण की रिकार्डिंग का विश्लेषण किया जाता है तो विभिन्नता के कारकों में वक्ता की आयु , उनका लिंग , उनके लहजे और वे बोल रहे शब्दों का समावेश होता है ।</s>
जब किसी कार की छवि का विश्लेषण किया जाता है तो विभिन्नता के कारकों में कार की स्थिति , उसके रंग , और सूर्य के कोण और चमक शामिल हैं .</s>
कई वास्तविक दुनिया कृत्रिम बुद्धि अनुप्रयोगों में कठिनाई का एक प्रमुख स्रोत यह है कि विभिन्नता के कई कारक प्रत्येक डेटा के एक टुकड़े को प्रभावित करते हैं जिसे हम देख सकते हैं .</s>
लाल कार की छवि में व्यक्तिगत पिक्सल रात में काले के बहुत करीब हो सकता है .</s>
कार के सिलोयूट का आकार देखने के कोण पर निर्भर करता है ।</s>
अधिकांश अनुप्रयोगों के लिए हमें विभिन्नता के कारकों को अलग करने और उन कारकों को त्यागने की आवश्यकता होती है जिनके बारे में हम परवाह नहीं करते हैं .</s>
बेशक , कच्चे डेटा से इस तरह के उच्च स्तर , अमूर्त विशेषताओं को निकालना बहुत मुश्किल हो सकता है .</s>
विभिन्नता के इन कारकों में से कई , जैसे कि वक्ता के लहजे , की पहचान केवल परिष्कृत , लगभग मानव - स्तर की समझ के उपयोग से की जा सकती है .</s>
जब मूल समस्या के समाधान के लिए प्रतिनिधित्व प्राप्त करना लगभग उतना ही कठिन होता है , तो अभ्यावेदन विद्या नहीं होती , पहली नजर में , हमारी सहायता प्रतीत होती है ।</s>
गहन अधिगम इस केंद्रीय समस्या को निरूपण में अंतर्संबंधी निरूपण द्वारा हल करता है जो अन्य , सरल निरूपणों के संदर्भ में व्यक्त किए जाते हैं ।</s>
डीप लर्निंग कंप्यूटर को सरल शंकु से जटिल अवधारणाओं का निर्माण करने में सक्षम बनाता है ।</s>
चित्र 1 . 2 से पता चलता है कि कैसे एक गहरी सीखने की प्रणाली सरल अवधारणाओं , जैसे कोनों और contours , जो बारी में किनारों के संदर्भ में परिभाषित कर रहे हैं के संयोजन के द्वारा एक व्यक्ति की छवि की अवधारणा का प्रतिनिधित्व कर सकते हैं .</s>
डीप लर्निंग मॉडल का सारभूत उदाहरण है फीड एक गहरा नेटवर्क , या मल्टीलेयर पर्सेप्टॉन ( एम एल पी )</s>
एक मल्टीलेयर पर्सेप्टॉन सिर्फ एक गणितीय फलन मानचित्रण है जो आउटपुट मूल्यों के लिए इनपुट मूल्यों के कुछ सेट को मैप करता है .</s>
समारोह कई सरल कार्यों की रचना के द्वारा बनता है .</s>
हम इनपुट का एक नया प्रतिनिधित्व प्रदान करने के रूप में एक अलग गणितीय समारोह के प्रत्येक अनुप्रयोग के बारे में सोच सकते हैं .</s>
डेटा के लिए सही प्रतिनिधित्व सीखने के विचार गहरी सीखने पर एक प्रति - विशेष प्रदान करता है .</s>
गहन अधिगम पर एक अन्य परिप्रेक्ष्य यह है कि गहराई कंप्यूटर को बहुसंकेत कंप्यूटर प्रोग्राम सीखने में सक्षम बनाती है ।</s>
अभ्यावेदन की प्रत्येक परत को 5 CHAPTER 1 के बाद कंप्यूटर की स्मृति की स्थिति के रूप में सोचा जा सकता है .</s>
वस्तु दृश्य परत ( इनपुट पिक्सल )</s>
प्रथम छिपा हुआ</s>
द्वितीय प्रच्छन्न परत</s>
CAR PERSON ANIMAL आउटपुट ( घोर पहचान )</s>
चित्र 1 . 2 : एक गहन शिक्षण मॉडल का प्रदर्शन</s>
एक कंप्यूटर के लिए कच्चे संवेदी इनपुट डेटा के अर्थ को समझना कठिन है , जैसे कि यह छवि पिक्सेल मूल्यों के संग्रह के रूप में प्रतिनिधित्व करती है .</s>
पिक्सेल के सेट से वस्तु पहचान के लिए समारोह मानचित्रण बहुत जटिल है .</s>
यदि सीधे मुकाबला किया जाए तो इस मानचित्रण को सीखना या उसका मूल्यांकन करना असंदिग्ध प्रतीत होता है ।</s>
गहन अधिगम इस कठिनाई का समाधान वांछित जटिल प्रतिचित्रण को नीड़ित सरल प्रतिचित्रणों की श्रृंखला में तोड़कर करता है , प्रत्येक का वर्णन मॉडल की एक अलग परत द्वारा किया जाता है .</s>
इनपुट दृश्य परत पर प्रस्तुत किया जाता है , इसलिए नाम दिया गया है क्योंकि यह चर है कि हम निरीक्षण करने में सक्षम हैं शामिल हैं .</s>
तब छुपी हुई परतों की एक श्रृंखला छवि से लगातार अमूर्त विशेषताओं को निकालती है .</s>
इन परतों को इन परतों को नहीं दिया जाता है क्योंकि इन परतों को इन परतों को नहीं दिया जाता है क्योंकि इन परतों को इन परतों को इन परतों को नहीं दिया जाता है क्योंकि इन परतों को इन परतों को प्रयोग किया जाता है ।</s>
यहाँ छवियों प्रत्येक छिपा इकाई द्वारा प्रतिनिधित्व विशेषता के प्रकार की कल्पना कर रहे हैं .</s>
पिक्सेल को देखते हुए , पहली परत आसानी से किनारों की पहचान कर सकते हैं , पड़ोसी पिक्सेल की चमक की तुलना करके .</s>
किनारों की पहली छुपी परत के वर्णन को देखते हुए , दूसरी छुपी परत आसानी से कोनों और विस्तारित कोनों की खोज कर सकती है , जो किनारों के संग्रह के रूप में पहचानी जा सकती है .</s>
कोनों और कोनों के संदर्भ में छवि के दूसरे छुपे हुए परत के वर्णन को देखते हुए , तीसरी छुपी हुई परत विशिष्ट वस्तुओं के संपूर्ण भागों का पता लगा सकती है , जिसमें संचरों और कोनों के विशिष्ट संग्रह पाए जाते हैं .</s>
अंत में , वस्तु भागों के संदर्भ में छवि के इस वर्णन का उपयोग छवि में मौजूद वस्तुओं को पहचानने के लिए किया जा सकता है .</s>
ज़िलर और फर्गस (2014 ) से अनुमति के साथ छवियों का प्रजनन</s>
6 CHAPTER 1 .</s>
निर्देश के एक अन्य समुच्चय को समानांतर निष्पादित करना ।</s>
अधिक गहराई वाले नेटवर्क अनुक्रम में अधिक अनुदेश निष्पादित कर सकते हैं ।</s>
अनुक्रमिक निर्देश महान शक्ति प्रदान करते हैं क्योंकि बाद के निर्देश पूर्व अनुदेशों के परिणामों को वापस संदर्भित कर सकते हैं .</s>
ए - ए -</s>
गहन अधिगम के इस दृष्टिकोण के अनुरूप , एक परत के सक्रियण में सभी जानकारी आवश्यक रूप से इनपुट की व्याख्या करने वाले विभिन्नता के कारकों को कूटबद्ध नहीं करती है .</s>
अभ्यावेदन राज्य सूचना को भी भंडारित करता है जो इनपुट का बोध करा सकने वाले प्रोग्राम को निष्पादित करने में मदद करता है .</s>
यह राज्य सूचना एक पारंपरिक कंप्यूटर प्रोग्राम में एक काउंटर या संकेतक के अनुरूप हो सकती है .</s>
इसका इनपुट की सामग्री से विशेष रूप से कोई संबंध नहीं है , लेकिन यह मॉडल को अपने प्रसंस्करण को व्यवस्थित करने में मदद करता है .</s>
किसी मॉडल की गहराई मापने के दो मुख्य तरीके हैं ।</s>
पहला दृश्य अनुक्रमिक अनुदेशों की संख्या पर आधारित है जिसे वास्तुकला के मूल्यांकन के लिए निष्पादित किया जाना चाहिए ।</s>
हम इसे प्रवाह चार्ट के माध्यम से सबसे लंबे पथ की लंबाई के रूप में सोच सकते हैं जो यह वर्णन करता है कि कैसे मॉडल के प्रत्येक आउटपुट की गणना करने के लिए अपने इनपुट दिया .</s>
जिस प्रकार दो समतुल्य कंप्यूटर प्रोग्रामों की लंबाई अलग - अलग होगी , जिस आधार पर प्रोग्राम किस भाषा में लिखा जाता है , उसी प्रकार एक ही फलन को विभिन्न गहराइयों के साथ फ्लो चार्ट के रूप में खींचा जा सकता है , इस आधार पर कि फ्लो चार्ट में अलग - अलग चरणों के रूप में प्रयोग किया जाता है ।</s>
चित्र 1 . 3 स्पष्ट करता है कि कैसे भाषा का यह चयन एक ही वास्तुकला के लिए दो अलग अलग माप दे सकता है ।</s>
x 1 x 1 w 1 w 1 w</s>
 ×                                                                                                                                                                                                                                                   </s>
2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2</s>
2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2</s>
w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w</s>
2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
w 2 × +</s>
तत्व सेट + ×</s>
GenericName</s>
तत्व सेट लॉजिस्टिक प्रतिगमन लॉजिस्टिक प्रतिगमन चित्र 1 . 3ः</s>
कम्प्यूटेशनल रेखांकन का प्रदर्शन , इनपुट को आउटपुट में प्रतिचित्रित करता है , जहां प्रत्येक नोड एक संक्रिया निष्पादित करता है ।</s>
गहराई इनपुट से आउटपुट के लिए सबसे लंबे पथ की लंबाई है , लेकिन क्या एक संभावित कम्प्यूटेशनल कदम का गठन की परिभाषा पर निर्भर करता है .</s>
अभिकलन ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ ग्राफ लॉग इन है जो इन फंक्शन है ।</s>
यदि हम परिवर्धन , गुणन और रसद सिग्मिड का उपयोग हमारे कंप्यूटर भाषा के तत्वों के रूप में करते हैं , तो इस मॉडल की गहराई तीन</s>
यदि हम रसद प्रतिगमन को एक तत्व के रूप में ही देखते हैं , तो इस मॉडल की गहराई एक है .</s>
7 CHAPTER 1 .</s>
INTRODUCTER एक अन्य दृष्टिकोण , जिसका प्रयोग गहरे व्यावहारिक मॉडलों द्वारा किया जाता है , एक मॉडल की गहराई को कम्प्यूटेशनल ग्राफ की गहराई नहीं बल्कि ग्राफ की गहराई बताते हुए मानता है कि अवधारणाएं एक दूसरे से कैसे संबंधित हैं .</s>
इस मामले में , प्रत्येक अवधारणा के प्रतिनिधित्व की गणना करने के लिए आवश्यक अभिकलन के प्रवाह चार्ट की गहराई स्वयं अवधारणाओं के ग्राफ से कहीं अधिक गहरी हो सकती है .</s>
इसका कारण यह है कि सरल अवधारणाओं की प्रणाली की समझ को परिष्कृत किया जा सकता है और अधिक जटिल अवधारणाओं के बारे में जानकारी दी जा सकती है .</s>
उदाहरण के लिए , छाया में एक आँख वाले चेहरे की छवि का अवलोकन करने वाली एआई प्रणाली प्रारंभ में केवल एक आँख देख सकती है .</s>
यह पता लगाने के बाद कि एक चेहरा मौजूद है , सिस्टम तब अनुमान लगा सकता है कि एक दूसरी आंख शायद के रूप में अच्छी तरह से मौजूद है .</s>
इस मामले में , अवधारणाओं के ग्राफ में केवल दो परतें शामिल हैं - आंखों के लिए एक परत और चेहरों के लिए एक परत - लेकिन गणना के ग्राफ में 2 n परतें शामिल हैं यदि हम प्रत्येक अवधारणा के अपने अनुमान को दूसरे n समय दिए गए</s>
क्योंकि यह हमेशा स्पष्ट नहीं है कि इन दो दृश्यों में से कौन सा दृश्य है - कम्प्यूटेशनल ग्राफ की गहराई , या प्रोबेबिलिस्टिक मॉडलिंग ग्राफ की गहराई - सबसे अधिक प्रासंगिक है , और क्योंकि विभिन्न लोग सबसे छोटे तत्वों के समुच्चय चुनते हैं , जिनमें से अपने रेखांकन का निर्माण करने के लिए एक भी सही मान नहीं है ।</s>
न ही इस बारे में आम सहमति है कि किसी मॉडल को “देप” के रूप में कितनी गहराई से अर्हता प्राप्त करने की आवश्यकता है .</s>
हालांकि , गहरी सीखने को सुरक्षित रूप से उन मॉडलों के अध्ययन के रूप में माना जा सकता है जिनमें पारंपरिक मशीन लर्निंग की तुलना में या तो विद्वत कार्यों या विद्वत अवधारणाओं की अधिक मात्रा शामिल होती है .</s>
संक्षेप में , गहन अध्ययन , इस पुस्तक का विषय , एआई के लिए एक दृष्टिकोण है .</s>
विशेष रूप से , यह मशीन अधिगम का एक प्रकार है , एक तकनीक है जो कंप्यूटर सिस्टम को अनुभव और डेटा के साथ सुधार करने में सक्षम बनाती है .</s>
हम प्रतिवाद करते हैं कि मशीनी शिक्षण ही एआई सिस्टम के निर्माण का एकमात्र व्यवहार्य दृष्टिकोण है जो जटिल वास्तविक दुनिया के वातावरण में कार्य कर सकता है .</s>
गहन अधिगम , एक विशेष प्रकार का मशीन अधिगम है , जो अवधारणाओं के नीड़ित पदानुक्रम के रूप में विश्व का प्रतिनिधित्व करके महान शक्ति और लचीलापन प्राप्त करता है , प्रत्येक अवधारणा सरल अवधारणाओं के संबंध में परिभाषित होती है , और कम अमूर्त निरूपण के संदर्भ में परिकलित होती है ।</s>
चित्र 1 . 4 इन विभिन्न एआई विधाओं के बीच संबंधों को स्पष्ट करता है ।</s>
चित्र 1 . 5 प्रत्येक कृति के बारे में एक उच्च स्तरीय योजना प्रस्तुत करता है ।</s>
1 . 1</s>
इस पुस्तक को किसने पढ़ा ?</s>
यह पुस्तक कई तरह के पाठकों के लिए उपयोगी हो सकती है , लेकिन हमने इसे दो लक्षित दर्शकों को ध्यान में रखकर लिखा ।</s>
इन लक्षित दर्शकों में से एक है विश्वविद्यालय के छात्र ( मशीन सीखने के बारे में स्नातक या स्नातक</s>
अन्य 8 CHAPTER 1 .</s>
INTRODUCkar AI मशीन लर्निंग प्रतिनिधित्व सीखने डीप लर्निंग उदाहरणः ज्ञान आधार उदाहरणः</s>
लॉजिस्टिक प्रतिगमन</s>
उदाहरणः उदाहरणः</s>
Name</s>
उदाहरणः उदाहरणः</s>
MLP चित्र 1 . 4 :</s>
एक वेन आरेख यह दर्शाता है कि कितना गहरा अधिगम एक प्रकार का निरूपण अधिगम है , जो बदले में एक प्रकार का मशीन अधिगम है , जो कई लोगों के लिए प्रयोग किया जाता है लेकिन सभी एआई के लिए दृष्टिकोण नहीं है .</s>
वेन आरेख के प्रत्येक अनुभाग में एक एआई प्रौद्योगिकी का एक उदाहरण शामिल है .</s>
लक्षित दर्शक सॉफ्टवेयर इंजीनियरों है जो एक मशीन सीखने या स्थैतिक टिक पृष्ठभूमि नहीं है , लेकिन तेजी से एक प्राप्त करना चाहते हैं और अपने उत्पाद या मंच में गहरी सीखने का उपयोग शुरू .</s>
डीप लर्निंग पहले से ही कई सॉफ्ट वेयर विधाओं में उपयोगी साबित हुई है , जिसमें कंप्यूटर विजन , स्पीच और ऑडियो प्रोसेसिंग , प्राकृतिक भाषा संसाधन , रोबोटिक्स , जैव सूचना विज्ञान और रसायन विज्ञान , वीडियो गेम्स , खोज इंजन ऑनलाइन विज्ञापन और वित्त शामिल हैं .</s>
इस पुस्तक को तीन भागों में बांटकर विभिन्न प्रकार के पाठकों को सर्वोत्तम रूप से समायोजित किया गया है ।</s>
भाग मैं बुनियादी गणितीय उपकरणों और मशीन सीखने अवधारणाओं का परिचय .</s>
भाग 2 सबसे स्थापित गहन शिक्षण एल्गोरिदम , जो अनिवार्य रूप से हल प्रौद्योगिकियों का वर्णन करता है .</s>
भाग III अधिक सट्टा विचारों का वर्णन करता है जो व्यापक रूप से गहन शिक्षण में भविष्य के अनुसंधान के लिए महत्वपूर्ण माना जाता है .</s>
9 CHAPTER 1 .</s>
InTRODUC इनपुट</s>
हैंड - डिजाइन प्रोग्राम</s>
इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट आउटपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट इनपुट</s>
फीचर आउटपुट इनपुट आउटपुट फीचर मैपिंग से हैंड - डिजाइन किए गए फीचर मैपिंग</s>
विशेषताओं आउटपुट अतिरिक्त परतों से मैपिंग अधिक अमूर्त सुविधाओं नियम - पस्त सिस्टम क्लासिक मशीन लर्निंग प्रतिनिधित्व सीखने डीप लर्निंग चित्र 1 . 5 फ्लोचर यह दर्शाता है कि कैसे एक एआई सिस्टम के विभिन्न भागों विभिन्न एआई विधाओं के भीतर एक दूसरे से संबंधित है .</s>
छायांकित बक्से उन घटकों को इंगित करते हैं जो डेटा से सीखने में सक्षम होते हैं .</s>
पाठकों को उन हिस्सों को छोड़ने के लिए स्वतंत्र महसूस करना चाहिए जो उनके हितों या पृष्ठभूमि को देखते हुए प्रासंगिक नहीं हैं ।</s>
रैखिक बीजगणित , संभावना , और मौलिक मशीन सीखने अवधारणाओं से परिचित पाठकों भाग मैं छोड़ सकते हैं , उदाहरण के लिए , जबकि जो लोग सिर्फ एक कार्य प्रणाली को लागू करना चाहते हैं भाग 2 से परे पढ़ने की जरूरत नहीं है .</s>
चुनने में मदद करने के लिए जो 10 CHAPTER 1 .</s>
आईटीआरओडीयूसीएल 1</s>
परिचय भाग I : अनुप्रयुक्त मठ और मशीन लर्निंग बेसिक 2 .</s>
रैखिक बीजगणित 3 .</s>
संभावना और सूचना सिद्धांत 4 .</s>
संख्यात्मक कंप्यूटिंग 5 .</s>
मशीन लर्निंग बेसिक भाग 2 :</s>
डीप नेटवर्कः आधुनिक अभ्यास 6 .</s>
डीप फीड नेटवर्क 7 .</s>
नियमितीकरण ८ .</s>
अनुकूलन 9 .</s>
सीएनएन 10 .</s>
RNNs 11 .</s>
व्यावहारिक पद्धति 12 . व्यावहारिक पद्धति</s>
अनुप्रयोग भाग III :</s>
डीप लर्निंग रिसर्च 13 .</s>
रैखिक फैक्टर मॉडल 14 .</s>
ऑटोनकोडर्स 15 .</s>
प्रतिनिधित्व अधिगम 16 .</s>
संरचित संभावी मॉडल 17 .</s>
मोंटे कार्लो विधि 18 .</s>
विभाजन कार्य 19 . विभाजन कार्य 19 . विभाजन कार्य 19 .</s>
अनुमान 20 . अनुमान 20 .</s>
डीप जेनरेशन मॉडल चित्र 1 . 6 : पुस्तक का उच्च स्तरीय संगठन ।</s>
एक अध्याय से दूसरे अध्याय तक एक तीर इंगित करता है कि पहले अध्याय को समझने के लिए पूर्व निर्धारित सामग्री है .</s>
11 CHAPTER 1 .</s>
@ info अध्यायों को पढ़ने के लिए , अंक 1 . 6 पुस्तक के उच्च स्तरीय संगठन को दर्शाने वाला एक फ्लोचर उपलब्ध कराता है ।</s>
हम यह जरूर मान लेते हैं कि सभी पाठक कंप्यूटर विज्ञान की पृष्ठभूमि से आते हैं ।</s>
हम प्रोग्रामिंग , कम्प्यूटेशनल प्रदर्शन मुद्दों की एक बुनियादी समझ , जटिलता सिद्धांत , परिचयात्मक स्तर पथरी और ग्राफ सिद्धांत की कुछ शब्दावली के साथ परिचित मान लेते हैं .</s>
डीप लर्निंग में 1 . 2 ऐतिहासिक ट्रेंड्स</s>
किसी ऐतिहासिक संदर्भ के साथ गहरी विद्या को समझना सबसे आसान है ।</s>
गहन अध्ययन का एक विस्तृत इतिहास प्रदान करने के बजाय , हम कुछ प्रमुख प्रवृत्तियों की पहचानः</s>
• गहन अध्ययन का एक लंबा और समृद्ध इतिहास रहा है , लेकिन कई नामों से जाना जाता है , जो विभिन्न दार्शनिक दृष्टिकोणों को प्रतिबिंबित करता है , और लोकप्रियता में वृद्धि और बदल गया है</s>
• डीप लर्निंग अधिक उपयोगी हो गया है के रूप में उपलब्ध प्रशिक्षण डेटा की मात्रा में वृद्धि हुई है .</s>
• डीप लर्निंग मॉडल समय के साथ आकार में वृद्धि हुई है के रूप में कंप्यूटर बुनियादी ढांचे ( गहन सीखने के लिए हार्डवेयर और सॉफ्टवेयर में सुधार हुआ है .</s>
• गहन शिक्षण समय के साथ बढ़ती सटीकता के साथ जटिल अनुप्रयोगों को हल किया है</s>
1 . 2</s>
नेरल नेट के कई नाम और चंगिंग फॉर्च्यून काम करते हैं</s>
हम उम्मीद करते हैं कि इस पुस्तक के कई पाठकों को एक रोमांचक नई प्रौद्योगिकी के रूप में गहरी सीखने के बारे में सुना है , और एक उभरते क्षेत्र के बारे में एक पुस्तक में “हिस्टरी” का उल्लेख देखकर आश्चर्यचकित हैं .</s>
वास्तव में , गहरी सीखने 1940 के दशक के लिए वापस तिथियाँ .</s>
गहन अध्ययन केवल नया प्रतीत होता है , क्योंकि यह अपनी वर्तमान लोकप्रियता से पहले कई वर्षों के लिए अपेक्षाकृत अलोकप्रिय था , और क्योंकि यह कई अलग अलग नामों के माध्यम से चला गया है , हाल ही में “देप अधिगम” कहा जा रहा है .</s>
क्षेत्र कई बार rebred किया गया है , विभिन्न शोधकर्ताओं और विभिन्न परिप्रेक्ष्यों के प्रभाव को प्रतिबिंबित .</s>
गहन शिक्षण का एक व्यापक इतिहास इस पाठ्यपुस्तक के दायरे से बाहर है ।</s>
कुछ बुनियादी संदर्भ , तथापि , गहरी सीखने की समझ के लिए उपयोगी है .</s>
मोटे तौर पर , विकास की तीन लहरें आई हैंः</s>
1940 -1960 के दशक में साइबरनेटिक्स के नाम से जाना जाने वाला गहन अध्ययन , 12 सीएचएपीटर 1 में कनेक्शन के रूप में जाना जाता है ।</s>
१९५० , १९८० १९ २००० वर्ष</s>
0 . 000 0 . 00050 0 . 000100 0 . 000150</s>
</s>
चित्र 1 . 7 :</s>
कृत्रिम तंत्रिका जाल अनुसंधान की तीन ऐतिहासिक तरंगों में से दो , जैसा कि वाक्यांशों की आवृत्ति से मापा जाता है “साइबरनेटिक्स” और “संयोजन” या “नेरल नेटवर्क” , गूगल बुक्स के अनुसार ( तीसरी लहर अभी हाल ही में प्रकट हुई है )</s>
1940 के दशक में पहली लहर , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,</s>
दूसरी लहर 1980 -1995 की अवधि के कनेक्शनवादी दृष्टिकोण के साथ शुरू हुई , जिसमें बैक - प्रचार ( Rumelhart एट अल , 1986 ) एक या दो छुपी हुई परतों वाले तंत्रिका नेटवर्क को प्रशिक्षित करने के लिए</s>
वर्तमान वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव वेव</s>
अल</s>
और अभी 2016 के रूप में पुस्तक रूप में प्रकट हो रहा है</s>
इसी प्रकार अन्य दो तरंगें पुस्तकाकार रूप में उसी वैज्ञानिक गतिविधि से बहुत बाद में प्रकट हुई ।</s>
1980 -1990 के दशक में , और वर्तमान पुनरुत्थान गहरा सीखने के नाम से 2006 में शुरू हुआ .</s>
यह मात्रात्मक रूप से 1 . 7 अंक में चित्रित किया गया है ।</s>
आज हम जिस प्रारंभिक अधिगम एल्गोरिदम को पहचानते हैं , उनमें से कुछ का उद्देश्य जैविक अधिगम के कम्प्यूटेशनल मॉडल होना था , अर्थात सीखने का तरीका मस्तिष्क में कैसे होता है या हो सकता है ।</s>
एक परिणाम के रूप में , एक नाम है कि गहरी सीखने चला गया है कृत्रिम तंत्रिका नेटवर्क (</s>
गहन शिक्षण मॉडलों पर संगत परिप्रेक्ष्य यह है कि वे जैविक मस्तिष्क ( चाहे मानव मस्तिष्क हो या किसी अन्य पशु के मस्तिष्क ) से प्रेरित प्रणालियों इंजीनियर हैं .</s>
जबकि मशीनी शिक्षण के लिए प्रयुक्त तंत्रिका नेटवर्कों के प्रकारों का उपयोग कभी - कभी मस्तिष्क समारोह ( हैनटन और शैलिस , 1991 ) को समझने के लिए किया गया है , वे आम तौर पर जैविक कार्य के यथार्थवादी मॉडल नहीं बनाए गए हैं .</s>
गहन शिक्षण पर तंत्रिका परिप्रेक्ष्य दो मुख्य विचारों से प्रेरित है ।</s>
एक विचार यह है कि मस्तिष्क उदाहरण के द्वारा एक सबूत प्रदान करता है कि बुद्धिमान व्यवहार संभव है , और बुद्धि के निर्माण के लिए एक वैचारिक रूप से सीधा रास्ता है मस्तिष्क के पीछे कम्प्यूटेशनल सिद्धांतों को रिवर्स इंजीनियर और उसकी कार्यक्षमता डुप्लिकेट .</s>
एक और 13 CHAPTER 1 .</s>
अंतःविषय परिप्रेक्ष्य यह है कि मस्तिष्क और उन सिद्धांतों को समझना अत्यंत रोचक होगा जो मानव बुद्धि को कम करते हैं , इसलिए मशीनी शिक्षण मॉडल जो इन मूलभूत वैज्ञानिक प्रश्नों पर प्रकाश डालते हैं , इंजीनियरिंग अनुप्रयोगों को हल करने की उनकी क्षमता के अलावा उपयोगी हैं ।</s>
आधुनिक शब्द “डेप लर्निंग” मशीन लर्निंग मॉडल की वर्तमान नस्ल पर तंत्रिका विज्ञानी परिप्रेक्ष्य से परे चला जाता है .</s>
यह रचना के एकाधिक स्तरों को सीखने के एक अधिक सामान्य सिद्धांत के लिए अपील करता है , जो मशीन सीखने के ढांचे में लागू किया जा सकता है जो आवश्यक रूप से तंत्रिका प्रेरित नहीं हैं .</s>
आधुनिक गहन शिक्षण के प्रारंभिक पूर्ववर्ती सरल रैखिक मॉडल थे जो तंत्रिका विज्ञानी परिप्रेक्ष्य से प्रेरित थे .</s>
इन मॉडलों n इनपुट मूल्यों एक्स 1 , का एक सेट लेने के लिए डिज़ाइन किया गया था .</s>
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
, x n और उन्हें एक आउटपुट y के साथ संबद्ध ।</s>
इन मॉडलों वजन w 1 , का एक सेट सीखना होगा .</s>
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
w n और अपने आउटपुट f ( x , w ) = गणना</s>
</s>
+ ·· + x n</s>
w n . n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</s>
तंत्रिका नेटवर्क अनुसंधान की इस पहली लहर को साइबरनेटिक्स के नाम से जाना जाता था , जैसा कि अंक 1 . 7 में बताया गया है ।</s>
</s>
इस रैखिक मॉडल क्या एफ ( एक्स , w ) सकारात्मक या नकारात्मक है का परीक्षण करके इनपुट की दो अलग श्रेणियों को पहचान सकता है .</s>
निश्चित रूप से , मॉडल के लिए श्रेणियों की इच्छित परिभाषा के अनुरूप होने के लिए , भार को सही ढंग से सेट करने की आवश्यकता होती है .</s>
ये भार मानव संचालक द्वारा निर्धारित किया जा सकता था ।</s>
1950 के दशक में , पर्सेप्टॉन (Rosenblatt , 1958 , 1962 ) पहला मॉडल बन गया जो प्रत्येक श्रेणी के इनपुट के दिए गए वर्गों को परिभाषित करने वाले भार को सीख सकता था .</s>
अनुकूली रैखिक तत्व</s>
( x ) स्वयं एक वास्तविक संख्या की भविष्यवाणी करने के लिए ( WWidrow और Hoff , 1960 और भी डेटा से इन संख्याओं की भविष्यवाणी करने के लिए सीख सकता है .</s>
इन सरल शिक्षण एल्गोरिदम ने मा - ज्या शिक्षण के आधुनिक परिदृश्य को बहुत प्रभावित किया ।</s>
प्रशिक्षण एल्गोरिथ्म का प्रयोग एडीएएलआईएनई के भार को अनुकूलित करने के लिए किया जाता है ।</s>
एक एल्गोरिथ्म का एक विशेष मामला था जिसे stochastic प्रवणता अवतरण कहा जाता है ।</s>
Stochastic ग्रेडिएंट अवतरण एल्गोरिथ्म के हल्के संशोधित संस्करण आज गहरे सीखने के मॉडलों के लिए प्रमुख प्रशिक्षण एल्गोरिदम बने हुए हैं ।</s>
परिग्राही और एडीएएलआईएन द्वारा प्रयुक्त फ ( x , w ) पर आधारित मॉडल रैखिक मॉडल कहलाते हैं ।</s>
ये मॉडल कुछ सर्वाधिक व्यापक रूप से प्रयुक्त मशीन अधिगम मॉडल बने हुए हैं , हालांकि कई मामलों में उन्हें मूल मॉडलों की तुलना में विभिन्न तरीकों से प्रशिक्षित किया जाता है .</s>
रैखिक मॉडलों की कई सीमाएं हैं .</s>
सबसे प्रसिद्ध</s>
1 और f ( = 1 और f )</s>
[ 1 ] , 0 ] www . youtube . com</s>
1 लेकिन f ( = 1 लेकिन f (</s>
[ 1 ] 1 , 1 , 1 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2</s>
. . . . . . . . . . . . . . . . . . . . . . . . .</s>
जिन आलोचकों ने रैखिक मॉडलों में इन खामियों का अवलोकन किया , उन्होंने सामान्य रूप से जैविक रूप से प्रेरित शिक्षण के खिलाफ एक पीठ थपथपाई ( Minsky और पेपरेट , 1969</s>
तंत्रिका नेटवर्क की लोकप्रियता में यह पहली प्रमुख डुबकी थी .</s>
14 CHAPTER 1 .</s>
INTRODUCTER आज , तंत्रिका विज्ञान गहरी सीखने के शोधकर्ताओं के लिए प्रेरणा का एक महत्वपूर्ण स्रोत माना जाता है , लेकिन यह अब क्षेत्र के लिए प्रमुख गाइड नहीं है .</s>
आज गहरे सीखने के अनुसंधान में तंत्रिका विज्ञान की घटती भूमिका का मुख्य कारण यह है कि हम बस एक गाइड के रूप में इसका उपयोग करने के लिए मस्तिष्क के बारे में पर्याप्त जानकारी नहीं है .</s>
मस्तिष्क द्वारा प्रयुक्त वास्तविक एल्गोरिदम की गहरी समझ प्राप्त करने के लिए , हमें एक साथ कम से कम हजारों अंतर्संबंधित न्यूरॉन्स की गतिविधि की निगरानी करने में सक्षम होने की आवश्यकता होगी .</s>
क्योंकि हम ऐसा नहीं कर पा रहे हैं , हम मस्तिष्क के कुछ सबसे सरल और सुप्रमाणित भागों को भी समझने से दूर हैं ( ओलशासेन एंड फील्ड , 2005 )</s>
तंत्रिका विज्ञान हमें उम्मीद करने के लिए एक कारण दिया है कि एक एकल गहरी सीखने एल्गोरिथ्म कई अलग कार्यों को हल कर सकते हैं .</s>
तंत्रिका विज्ञानियों ने पाया है कि निषेचन अपने मस्तिष्क के श्रवण प्रसंस्करण क्षेत्र के साथ “सी” सीख सकते हैं यदि उनके मस्तिष्क को उस क्षेत्र में दृश्य संकेत भेजने के लिए दोहराया जाता है ( वॉन मेलचेर एट अल 2000 )</s>
यह सुझाव है कि स्तनधारी मस्तिष्क के बहुत से विभिन्न कार्यों है कि मस्तिष्क हल करता है के अधिकांश को हल करने के लिए एक एकल एल्गोरिथ्म का उपयोग कर सकता है .</s>
इस परिकल्पना से पहले , मशीन अधिगम अनुसंधान अधिक खंडित था , जिसमें प्राकृतिक भाषा संसाधन , दृष्टि , गति योजना और भाषण मान्यता का अध्ययन करने वाले शोधकर्ताओं के विभिन्न समुदाय थे .</s>
आज , इन अनुप्रयोग समुदायों अभी भी अलग हैं , लेकिन गहन शिक्षण अनुसंधान समूहों के लिए एक साथ कई या यहां तक कि इन सभी अनुप्रयोग क्षेत्रों का अध्ययन करना आम बात है .</s>
हम तंत्रिका विज्ञान से कुछ मोटे दिशानिर्देश निकालने में सक्षम हैं .</s>
कई कम्प्यूटेशनल इकाइयों है कि केवल एक दूसरे के साथ अपनी बातचीत के माध्यम से बुद्धिमान बन जाने का मूल विचार मस्तिष्क से प्रेरित है .</s>
नवयुगीन</s>
अधिकांश तंत्रिका नेटवर्क आज एक मॉडल न्यूरॉन पर आधारित हैं , जिसे संशोधित रैखिक इकाई कहा जाता है .</s>
मूल संज्ञानात्मक ( Fukushima , 1975 ) एक और अधिक जटिल संस्करण की शुरुआत की जो मस्तिष्क समारोह के हमारे ज्ञान से अत्यधिक प्रेरित था .</s>
सरलीकृत आधुनिक संस्करण कई दृष्टिकोणों से विचारों को समाहित करते हुए विकसित किया गया था , जिसमें नायर और हाइटन (2010 और ग्लोरोट एट अल ) थे .</s>
</s>
( 2009 ) अधिक इंजीनियरिंग उन्मुख प्रभावों का हवाला देते हुए</s>
जबकि तंत्रिका विज्ञान प्रेरणा का एक महत्वपूर्ण स्रोत है , यह एक कठोर गाइड के रूप में लेने की जरूरत नहीं है .</s>
हम जानते हैं कि वास्तविक न्यूरॉन्स आधुनिक संशोधित रैखिक इकाइयों की तुलना में बहुत अलग कार्यों की गणना करते हैं , लेकिन अधिक तंत्रिका यथार्थवाद अभी तक मशीन सीखने के प्रदर्शन में सुधार के लिए नेतृत्व नहीं किया है .</s>
इसके अलावा , जबकि तंत्रिका विज्ञान सफलतापूर्वक कई तंत्रिका नेटवर्क वास्तुकला को प्रेरित किया है , हम अभी तक तंत्रिका विज्ञान के लिए जैविक सीखने के बारे में पर्याप्त नहीं पता है सीखने एल्गोरिदम हम इन वास्तुकला को प्रशिक्षित करने के लिए उपयोग</s>
15 CHAPTER 1 .</s>
अंतःविषय मीडिया लेखे अक्सर मस्तिष्क के लिए गहरी सीखने की समानता पर जोर देते हैं ।</s>
जबकि यह सच है कि गहन अधिगम शोधकर्ताओं को मस्तिष्क को अन्य मशीन अधिगम क्षेत्रों , जैसे कर्नेल मशीन या बेसियन सांख्यिकी में काम करने वाले शोधकर्ताओं की तुलना में एक प्रभाव के रूप में उद्धृत करने की संभावना अधिक होती है , गहन अधिगम को मस्तिष्क का अनुकरण करने के प्रयास के रूप में नहीं देखना चाहिए .</s>
आधुनिक गहन शिक्षण कई क्षेत्रों , विशेष रूप से लागू गणित मौलिक जैसे रैखिक बीजगणित , संभावना , सूचना सिद्धांत , और संख्यात्मक अनुकूलन से प्रेरणा लेता है .</s>
जबकि कुछ गहरी सीखने के शोधकर्ता तंत्रिका विज्ञान को प्रेरणा का एक महत्वपूर्ण स्रोत बताते हैं , दूसरों को बिल्कुल भी तंत्रिका विज्ञान से संबंधित नहीं है .</s>
गौरतलब है कि यह समझने का प्रयास कि मस्तिष्क एल्गोरिथम स्तर पर कैसे काम करता है , जीवित और अच्छी तरह से है ।</s>
इस प्रयास को मुख्य रूप से “संपुटीय तंत्रिका विज्ञान” के रूप में जाना जाता है और यह गहन शिक्षण से अध्ययन का एक अलग क्षेत्र है .</s>
शोधकर्ताओं के लिए दोनों क्षेत्रों के बीच बार - बार आगे बढ़ना आम बात है ।</s>
गहन शिक्षण का क्षेत्र मुख्य रूप से इस बात से संबंधित है कि कैसे ऐसे कंप्यूटर सिस्टम का निर्माण किया जाए जो बुद्धि की आवश्यकता वाले कार्यों को सफलतापूर्वक हल करने में सक्षम हों , जबकि कम्प्यूटेशनल तंत्रिका विज्ञान का क्षेत्र मुख्य रूप से मस्तिष्क वास्तव में कैसे कार्य करता है , इसके अधिक सटीक मॉडल बनाने से संबंधित है ।</s>
1980 के दशक में , तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर बड़े हिस्से में कनेक्शनवाद , या समानांतर वितरित प्रक्रिया - नामक आंदोलन के माध्यम से उभरी .</s>
अल</s>
मैकक्लललैंड एट अल , 1995</s>
संज्ञानात्मक विज्ञान के संदर्भ में कनेक्शनवाद का उदय हुआ .</s>
संज्ञानात्मक विज्ञान मन को समझने के लिए एक अंतर्विषयक दृष्टिकोण है , विश्लेषण के कई अलग स्तरों के संयोजन .</s>
1980 के दशक के प्रारंभ में , अधिकांश संज्ञानात्मक वैज्ञानिकों ने प्रतीकात्मक तर्क के मॉडलों का अध्ययन किया .</s>
अपनी लोकप्रियता के बावजूद , प्रतीकात्मक मॉडल कैसे मस्तिष्क वास्तव में न्यूरॉन्स का उपयोग कर उन्हें लागू कर सकते हैं के संदर्भ में व्याख्या करने के लिए मुश्किल थे .</s>
कनेक्शनिस्टों ने संज्ञान के मॉडलों का अध्ययन करना शुरू किया जो वास्तव में तंत्रिका कार्यान्वयन में आधारित हो सकता है ( टोरेट्ज़की और मिंटन , 1985 , 1940 के दशक में मनोविज्ञानी डोनाल्ड हेब के कार्य से संबंधित कई विचारों को पुनः प्राप्त किया .</s>
कनेक्शनवाद में केंद्रीय विचार यह है कि सरल कम्प्यूटेशनल इकाइयों की एक बड़ी संख्या बुद्धिमान व्यवहार को प्राप्त कर सकते हैं जब एक साथ नेटवर्क .</s>
यह अंतर्दृष्टि जैविक तंत्रिका प्रणालियों में न्यूरॉन्स के लिए समान रूप से लागू होती है , जैसा कि यह कम्प्यूटेशनल मॉडलों में छुपी इकाइयों के लिए करता है .</s>
1980 के दशक के संपर्क आंदोलन के दौरान कई प्रमुख अवधारणाओं का उदय हुआ जो आज के गहन अध्ययन के केंद्र में हैं ।</s>
इन अवधारणाओं में से एक वितरित प्रतिनिधित्व ( हैनटन एट अल , 1986 ) का है .</s>
यह विचार है कि एक सिस्टम के लिए प्रत्येक इनपुट कई सुविधाओं द्वारा प्रतिनिधित्व किया जाना चाहिए , और प्रत्येक विशेषता कई संभावित इनपुट के प्रतिनिधित्व में शामिल होना चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हमारे पास एक दृष्टि प्रणाली है जो 16 CHAPTER 1 को पहचान सकती है ।</s>
INTRODUCbul कारें , ट्रक , और पक्षी , और ये वस्तुएं प्रत्येक लाल , हरे , या नीले रंग की हो सकती हैं .</s>
इन इनपुट का प्रतिनिधित्व करने का एक तरीका होगा एक अलग न्यूरॉन या छिपा इकाई है कि नौ संभावित संयोजनों में से प्रत्येक के लिए सक्रिय करता हैः लाल ट्रक , लाल कार , हरे ट्रक , और इतने पर</s>
इसके लिए नौ विभिन्न न्यूरॉन्स की आवश्यकता होती है , और प्रत्येक न्यूरॉन को स्वतंत्र रूप से रंग और वस्तु पहचान की अवधारणा सीखनी चाहिए .</s>
इस स्थिति पर सुधार करने के लिए एक तरीका है एक वितरित प्रतिनिधित्व का उपयोग करने के लिए , तीन न्यूरॉन्स रंग का वर्णन और तीन न्यूरॉन्स वस्तु पहचान का वर्णन .</s>
इसके लिए नौ के स्थान पर केवल छह न्यूरॉन्स की आवश्यकता होती है , और लालिमा का वर्णन करने वाला न्यूरॉन कारों , ट्रकों और पक्षियों की छवियों से लालिमा के बारे में जानने में सक्षम है , न कि केवल वस्तुओं की एक विशिष्ट श्रेणी की छवियों से .</s>
वितरित प्रतिनिधित्व की अवधारणा इस पुस्तक के लिए केंद्रीय है और अध्याय 15 में अधिक विस्तार से वर्णित है .</s>
कनेक्शनिस्ट आंदोलन की एक और प्रमुख उपलब्धि थी गहरे न्यूट्रल नेटवर्कों को प्रशिक्षित करने के लिए बैक - प्रोगेशन का सुदृढ उपयोग , बैक - प्रोगेशन एल्गोरिथम ( बैक - प्रोगेशन एल्गोरिथम , 1986 )</s>
इस एल्गोरिथ्म की लोकप्रियता में वृद्धि हुई है और गिरावट आई है , लेकिन इस लेखन के रूप में , गहरे मॉडलों के प्रशिक्षण के लिए प्रमुख दृष्टिकोण है .</s>
1990 के दशक के दौरान , शोधकर्ताओं ने तंत्रिका नेटवर्क के साथ मॉडलिंग अनुक्रम में महत्वपूर्ण प्रगति की .</s>
होकर</s>
(1994 ) लंबे अनुक्रमों के मॉडलिंग में कुछ मूलभूत गणितीय कठिनाइयों की पहचान की , जिसका वर्णन खंड 10 . 7 में किया गया है .</s>
</s>
आज , एलएसटीएम व्यापक रूप से कई अनुक्रम मॉडलिंग कार्यों के लिए प्रयोग किया जाता है , गूगल पर कई प्राकृतिक भाषा प्रसंस्करण कार्यों सहित .</s>
तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर1990 के दशक के मध्य तक चली .</s>
तंत्रिका नेटवर्कों और अन्य एआई प्रौद्योगिकियों पर आधारित वीन - टर्स ने निवेश की मांग करते समय अवास्तविक सुवाह्य महत्वाकांक्षी दावे करने शुरू कर दिए ।</s>
जब एआई शोध इन अनुचित उम्मीदों को पूरा नहीं करता था , तब निवेशक निराश हो जाते थे .</s>
इसके साथ ही मशीनी शिक्षण के अन्य क्षेत्रों ने भी प्रगति की ।</s>
</s>
इन दो कारकों से तंत्रिका नेटवर्क की लोकप्रियता में गिरावट आई जो 2007 तक चली .</s>
इस दौरान , तंत्रिका नेटवर्क कुछ कार्यों पर प्रभावशाली प्रदर्शन प्राप्त करने के लिए जारी रखा (</s>
लेकुन एट अल</s>
अल</s>
</s>
इस कार्यक्रम संयुक्त मशीन शिक्षण अनुसंधान समूहों टोरंटो विश्वविद्यालय में जिओफ्री हिन्टन के नेतृत्व में , मॉन्टट्रियल विश्वविद्यालय में योशु बेंजियो , और न्यूयॉर्क विश्वविद्यालय में यान लीकुन</s>
बहुसांस्कृतिक सीआईएफएआर एनएपी अनुसंधान पहल 17 सीएचएपीटर 1</s>
अंतःक्रिया में मानव और कंप्यूटर दृष्टि के तंत्रिका विज्ञानी और विशेषज्ञ भी शामिल थे ।</s>
इस बिंदु पर , गहरे नेटवर्क आम तौर पर माना जाता था कि प्रशिक्षण देना बहुत कठिन है .</s>
अब हम जानते हैं कि एल्गोरिदम है कि 1980 के दशक के बाद से किया गया है काफी अच्छी तरह से काम करते हैं , लेकिन यह स्पष्ट सिर्का 2006 नहीं था .</s>
मुद्दा शायद सिर्फ इतना है कि इन एल्गोरिदम बहुत कम्प्यूटेशनल रूप से महंगा था कि उस समय उपलब्ध हार्डवेयर के साथ बहुत अधिक प्रयोग की अनुमति दी .</s>
तंत्रिका नेटवर्क अनुसंधान की तीसरी लहर 2006 में एक ब्रेक थ्रू के साथ शुरू हुई .</s>
जिओफ्री हाइनटन ने दिखाया कि एक प्रकार का तंत्रिका नेटवर्क जिसे गहन विश्वास नेटवर्क कहा जाता है , लालची परत -वार पूर्वप्रवर्तन ( हैनटन एट अल 2006 ) नामक रणनीति का उपयोग करके कुशलतापूर्वक प्रशिक्षित किया जा सकता है , जिसका वर्णन हम धारा 151 में अधिक विस्तार से करते हैं .</s>
अन्य संबद्ध अनुसंधान समूह शीघ्रता से उसी कार्यनीति को प्रदर्शित कर सकते थे ( )</s>
तंत्रिका अनुसंधान की इस लहर ने “देप सीखने के लिए” को लोकप्रिय बनाया कि शोधकर्ताओं ने अब गहन तंत्रिका तंत्र को प्रशिक्षित करने में सक्षम थे ,</s>
इस समय , डीप नेरल नेटवर्क ने एआई सिस्टम को अन्य मशीन लर्निंग प्रौद्योगिकियों के साथ - साथ हाथ से दी गई कार्यक्षमता पर आधारित बताया .</s>
तंत्रिका नेटवर्क की लोकप्रियता की यह तीसरी लहर इस लेखन के समय तक जारी है , हालांकि गहन शिक्षण अनुसंधान का ध्यान इस लहर के समय के भीतर नाटकीय रूप से बदल गया है .</s>
तीसरी लहर की शुरुआत नई अनुपयुक्त अधिगम तकनीकों और गहरे मॉडलों की छोटे डेटासेटों से अच्छी तरह सामान्य करने की क्षमता पर ध्यान केंद्रित करने के साथ हुई , लेकिन आज बहुत पुराने पर्यवेक्षित अधिगम एल्गोरिदम और गहरे मॉडलों की बड़े लेबल वाले डेटासेटों का लाभ उठाने की क्षमता में अधिक रुचि है .</s>
1 . 2</s>
बढ़ते डेटासेट आकार</s>
कोई आश्चर्य की बात हो सकती है कि अभी हाल ही में डीप लर्निंग को एक निर्णायक प्रौद्योगिकी के रूप में मान्यता क्यों मिली है , भले ही 1950 के दशक में कृत्रिम तंत्रिका नेटवर्क के साथ पहले प्रयोग किए गए थे .</s>
गहन शिक्षण 1990 के दशक के बाद से वाणिज्यिक अनुप्रयोगों में सफलतापूर्वक इस्तेमाल किया गया है , लेकिन अक्सर एक प्रौद्योगिकी और कुछ है कि केवल एक विशेषज्ञ का उपयोग कर सकते हैं की तुलना में एक कला के अधिक माना जाता था , हाल ही में .</s>
यह सच है कि एक गहरी सीखने एल्गोरिथ्म से अच्छा प्रदर्शन प्राप्त करने के लिए कुछ कौशल की आवश्यकता है .</s>
सौभाग्य से , आवश्यक कौशल की मात्रा कम हो जाती है क्योंकि प्रशिक्षण डेटा की मात्रा बढ़ जाती है .</s>
आज जटिल कार्यों पर मानव प्रदर्शन तक पहुंचने वाली अधिगम एल्गोरिदम , 1980 के दशक में खिलौना समस्याओं को हल करने के लिए संघर्ष करने वाली अधिगम एल्गोरिदम के लगभग समान है , हालांकि इन एल्गोरिदम के साथ हम प्रशिक्षित मॉडलों में 18 CHAPTER 1 है .</s>
इन्ट्राऑडयूक्यूट में ऐसे परिवर्तन किए गए हैं जो बहुत गहरे वास्तुशिल्पों के प्रशिक्षण को सरल बनाते हैं ।</s>
सबसे महत्वपूर्ण नया विकास है कि आज हम इन एल्गोरिदम को सफल होने के लिए आवश्यक संसाधनों के साथ प्रदान कर सकते हैं .</s>
चित्र 1 . 8 से पता चलता है कि कैसे बेंचमार्क डेटासेट के आकार का समय के साथ उल्लेखनीय विस्तार हुआ है .</s>
यह प्रवृत्ति समाज के बढ़ते डिजिटाइजेशन से प्रेरित है ।</s>
जैसे - जैसे हमारी अधिक से अधिक गतिविधियां कंप्यूटर पर होती जाती हैं , वैसे - वैसे हम जो करते हैं उसका अधिक से अधिक विवरण दर्ज किया जाता है ।</s>
जैसे - जैसे हमारे कंप्यूटरों का एक साथ नेटवर्क बढ़ता जा रहा है , इन अभिलेखों को केंद्रीकृत करना और उन्हें मशीनी शिक्षण अनुप्रयोगों के लिए उपयुक्त डेटासेट में ठीक करना आसान होता जा रहा है ।</s>
</s>
</s>
सार्वजनिक SVHN छविनेट</s>
सीआईएफआर - 10</s>
</s>
</s>
समय के साथ बढ़ता डेटासेट आकार .</s>
में संकलित सांख्यिकीय डेटा में संकलित सांख्यिकीय डेटा में संकलित सांख्यिकीय डेटा में 1900 प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रारंभिक प्रस्तें , प्रस्तेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथेंथ
</s>
१९८० और १९९० के दशक में मशीनी शिक्षण अधिक सांख्यिकीय हो गया और उसने दसियों हजार उदाहरणों वाले बड़े डेटासेटों का लाभ उठाना शुरू किया , जैसे कि MNIST डाटासेट ( आंकड़े १९८० के अंक में )</s>
2000 के दशक के पहले दशक में , इसी आकार के अधिक परिष्कृत डेटासेट , जैसे CIFAR - 10 डेटासेट ( Krizhevsky और Hinton , 2009 ) का उत्पादन जारी रहा .</s>
उस दशक के अंत में और 2010 के पूर्वार्द्ध में , महत्वपूर्ण रूप से बड़े डेटासेट , जिसमें हजारों से दसियों लाख उदाहरण थे , पूरी तरह से बदल गया जो गहरी सीखने के साथ संभव था .</s>
</s>
एम डेटासेट एम डेटासेट</s>
</s>
ग्राफ के शीर्ष पर , हम देखते हैं कि अनुवादित वाक्यों के डेटासेट , जैसे कि कनाडा के हंसार्ड से निर्मित आईबीएम के डेटासेट , 1990 और डब्ल्यूएमटी 2014 अंग्रेजी से फ्रांसीसी डेटासेट ( Schwenk 2014 ) आमतौर पर अन्य डेटासेट से काफी आगे हैं .</s>
19 CHAPTER 1 .</s>
सामान्य चित्र 1 . 9</s>
MNIST डेटासेट से उदाहरण इनपुट</s>
“NIST” राष्ट्रीय मानक और प्रौद्योगिकी संस्थान , एजेंसी के लिए है जो मूल रूप से इस डेटा को एकत्र किया .</s>
“M” मशीन अधिगम एल्गोरिदम के साथ आसान उपयोग के लिए डेटा के पूर्वप्रमाणित किया गया है के बाद से “M” के लिए खड़ा है .</s>
MNIST डाटासेट में हस्तलिखित अंकों और संबद्ध लेबलों के स्कैन होते हैं , जिसमें यह वर्णन होता है कि प्रत्येक छवि में 0 - 9 किस अंक में निहित है</s>
यह सरल वर्गीकरण समस्या गहन शिक्षण अनुसंधान में सबसे सरल और सबसे व्यापक रूप से इस्तेमाल किया परीक्षणों में से एक है .</s>
आधुनिक तकनीकों को हल करना काफी आसान होने के बावजूद यह लोकप्रिय बना हुआ है ।</s>
जिओफ्री हाइनटन ने इसे “ मशीनी अधिगम का दर्शन” कहा है , जिसका अर्थ है कि यह मशीनी अधिगम शोधकर्ताओं को नियंत्रित प्रयोगशाला स्थितियों में उनकी एल्गोरिदम का अध्ययन करने में सक्षम बनाता है , जैसा कि जीवविज्ञानी अक्सर फल मक्खियों का अध्ययन करते हैं .</s>
मशीनी शिक्षण को काफी आसान बना दिया है क्योंकि सांख्यिकीय आकलन का मुख्य बोझ , जो केवल थोड़ी मात्रा में डेटा का अवलोकन करने के बाद नए डेटा को अच्छी तरह से उत्पन्न करता है , काफी हल्का हो गया है .</s>
वर्ष 2016 तक , अंगूठे का एक मोटा नियम यह है कि एक पर्यवेक्षित गहन शिक्षण एल्गोरिथ्म लगभग 5 , 000 लेबल किए गए उदाहरणों के साथ आम तौर पर स्वीकार्य प्रदर्शन प्राप्त करेगा और प्रति श्रेणी 20 CHAPTER 1 से मेल खाता होगा .</s>
INTRODUCect मानव प्रदर्शन से अधिक है जब डेटासेट के साथ प्रशिक्षित किया जाता है जिसमें कम से कम 10 मिलियन लेबल वाले उदाहरण होते हैं .</s>
इससे छोटा डेटासेट के साथ सफलतापूर्वक कार्य करना एक महत्वपूर्ण अनुसंधान क्षेत्र है , जो विशेष रूप से इस बात पर ध्यान केंद्रित करता है कि कैसे हम बड़ी मात्रा में अविश्वसनीय उदाहरणों का लाभ ले सकते हैं , जिसमें अनुपयुक्त या अर्ध - प्रसुप्त अधिगम होता है ।</s>
1 . 2 . 3</s>
बढ़ते मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज बेतहाशा सफल रहे हैं कि हम कम्प्यूटेशनल संसाधनों आज बहुत बड़े मॉडल चलाने के लिए .</s>
कनेक्शन - इस्म की एक मुख्य अंतर्दृष्टि यह है कि जब उनके कई न्यूरॉन्स एक साथ काम करते हैं तो जानवर बुद्धिमान हो जाते हैं .</s>
एक व्यक्तिगत न्यूरॉन या न्यूरॉन्स के छोटे संग्रह विशेष रूप से उपयोगी नहीं है .</s>
जैविक न्यूरॉन्स विशेष रूप से सघन रूप से जुड़े नहीं हैं .</s>
जैसा कि अंक 1 . 10 में देखा गया है , हमारे मशीन सीखने के मॉडलों में प्रति न्यूरॉन कई कनेक्शन थे दशकों के लिए भी स्तनधारी मस्तिष्क के परिमाण के एक क्रम के भीतर .</s>
न्यूरॉन्स की कुल संख्या के संदर्भ में , तंत्रिका नेटवर्क आश्चर्यजनक रूप से काफी हाल तक छोटे रहे हैं , जैसा कि अंक 1 .11 में दिखाया गया है .</s>
छुपी हुई इकाइयों के शुरू होने के बाद से कृत्रिम तंत्रिका नेटवर्कों का आकार मोटे तौर पर हर 2 . 4 साल में दोगुना हो गया है .</s>
यह वृद्धि बड़ी स्मृति वाले तीव्र कंप्यूटरों और बड़े डेटासेटों की उपलब्धता से प्रेरित होती है ।</s>
बड़े नेटवर्क अधिक जटिल कार्यों पर अधिक सटीकता प्राप्त करने में सक्षम होते हैं .</s>
यह प्रवृत्ति दशकों से जारी है ।</s>
जब तक नई प्रौद्योगिकियां तेजी से स्केलिंग सक्षम नहीं करती हैं , कृत्रिम तंत्रिका नेटवर्कों में कम से कम 2050 के दशक तक मानव मस्तिष्क के समान न्यूरॉन्स नहीं होंगे .</s>
जैविक न्यूरॉन्स वर्तमान कृत्रिम न्यूरॉन्स की तुलना में अधिक जटिल कार्यों का प्रतिनिधित्व कर सकते हैं , इसलिए जैविक तंत्रिका नेटवर्क इस भूखंड चित्रण से भी बड़ा हो सकता है .</s>
पुनर्विलोकन में , यह विशेष रूप से आश्चर्य की बात नहीं है कि जोच की तुलना में कम न्यूरॉन्स वाले तंत्रिका नेटवर्क परिष्कृत कृत्रिम बुद्धि प्रोब - लेम को हल करने में असमर्थ थे .</s>
आज के नेटवर्क भी , जिन्हें हम कम्प्यूटेशनल सिस्टम की दृष्टि से काफी बड़ा मानते हैं , मेंढक जैसे अपेक्षाकृत आदिम कशेरुकी जीवों के तंत्रिका तंत्र से छोटे हैं ।</s>
समय के साथ मॉडल आकार में वृद्धि , तीव्र सीपीयू की उपलब्धता के कारण , सामान्य प्रयोजन जीपीयू का आगमन ( खंड 12 . 1 . 2 में उल्लिखित ) , तीव्र नेटवर्क कनेक्टिविटी और वितरित कंप्यूटिंग के लिए बेहतर सॉफ्टवेयर मूल संरचना , गहन शिक्षण के इतिहास की सबसे महत्वपूर्ण प्रवृत्तियों में से एक है .</s>
इस प्रवृत्ति को आम तौर पर भविष्य में अच्छी तरह से जारी रखने की उम्मीद है .</s>
21 CHAPTER 1 .</s>
अंतःक्रिया 1950 2000 2015 वर्ष 10 10 2 10 3 10 4 कनेक्शन प्रति न्यूरॉन 1 2 3 4 5 6 8 9 फल मक्खी माउस मानव चित्र 110</s>
: समय के साथ प्रति न्यूरॉन कनेक्शन की संख्या .</s>
प्रारंभ में , कृत्रिम तंत्रिका नेटवर्कों में न्यूरॉन्स के बीच कॉन्नेक - टीशन की संख्या हार्डवेयर क्षमताओं द्वारा सीमित थी .</s>
आज , न्यूरॉन्स के बीच कनेक्शन की संख्या ज्यादातर एक डिजाइन विचार है .</s>
कुछ कृत्रिम तंत्रिका नेटवर्कों में प्रति न्यूरॉन लगभग उतने ही कनेक्शन होते हैं जितने एक बिल्ली के रूप में होते हैं , और अन्य तंत्रिका नेटवर्कों के लिए चूहों जैसे छोटे स्तनधारियों के रूप में प्रति न्यूरॉन कनेक्शन होना काफी आम बात है .</s>
यहां तक कि मानव मस्तिष्क प्रति न्यूरॉन कनेक्शन की एक अनाप - शनाप राशि नहीं है .</s>
विकिपीडिया से जैविक तंत्रिका नेटवर्क आकार (2015 )</s>
1 . 1</s>
अनुकूली रैखिक तत्व</s>
नियोकोग्निट्रॉन ( Fukushima ) , 1980 में 3</s>
</s>
</s>
डीप बोल्ट्जमैन मशीन</s>
अपूरणीय संवलन नेटवर्क (</s>
जेरेट एट अल , 2009 6</s>
</s>
</s>
↑ 2010 का इडियट 7 .</s>
वितरित ऑटोनकोडर (</s>
ले एट अल</s>
↑ 2012 का 8वां</s>
बहु - जीपीयू संवलन नेटवर्क (</s>
Krizhevsky एट अल</s>
↑ 2012 का 9 .</s>
COTS HPC अनुपयुक्त संवलयन नेटवर्क (</s>
कोट्स एट अल , 2013 10</s>
गोगली</s>
1 . 2 . 4</s>
बढ़ती शुद्धता , जटिलता और वास्तविक प्रभाव</s>
1980 के दशक के बाद से , गहरी सीखने लगातार सटीक मान्यता और भविष्यवाणी प्रदान करने की क्षमता में सुधार हुआ है .</s>
इसके अलावा , गहरी सीखने लगातार अनुप्रयोगों के व्यापक और व्यापक सेट के लिए सफलता के साथ लागू किया गया है .</s>
सबसे पहले गहरे मॉडलों का उपयोग व्यक्तिगत वस्तुओं को कसकर फसल में पहचानने के लिए किया गया था , अत्यंत छोटी छवियों (</s>
रूमाहार्ट एट अल , 1986</s>
तब से छवियों तंत्रिका नेटवर्क के आकार में क्रमिक वृद्धि हुई है प्रक्रिया कर सकते हैं .</s>
आधुनिक वस्तु पहचान नेटवर्क प्रक्रिया उच्च पुनर्संयोजन फोटोग्राफ को समृद्ध करता है और 22 सीएचएपीटर 1 नहीं करता है ।</s>
TRODUCect 1950 2000 2015 2056 वर्ष 10 −2 10 −1</s>
10 10 1 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10</s>
10 11 न्यूरॉन्स की संख्या (logarithmic पैमाने पर</s>
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</s>
</s>
छुपी हुई इकाइयों के शुरू होने के बाद से कृत्रिम तंत्रिका नेटवर्कों का आकार मोटे तौर पर हर 2 . 4 साल में दोगुना हो गया है .</s>
विकिपीडिया से जैविक तंत्रिका नेटवर्क आकार (2015 )</s>
1 . 1</s>
पर्सेप्टॉन (Rosenblatt , 1958 , 1962 ) 2</s>
अनुकूली रैखिक तत्व</s>
नियोकोग्निट्रॉन ( Fukushima ) , 1980 में 4</s>
प्रारंभिक बैक - प्रोपेगेशन नेटवर्क</s>
भाषण मान्यता के लिए समवर्ती तंत्रिका नेटवर्क ( रोबिनसन और फालसाइड , 1991</s>
6 . 6 . 6 . 6 . 6 . 6 . 6</s>
भाषण मान्यता के लिए मल्टीलेयर पर्सेप्टॉन ( Bengio एट अल )</s>
↑ 1991 का 7 .</s>
माध्य क्षेत्र अवग्रह विश्वास नेटवर्क</s>
8 . 8 . 8 . 8 . 8</s>
</s>
</s>
, 1998 का</s>
इको स्टेट नेटवर्क ( जेगर एंड हास , 2004 )</s>
10 . 10</s>
डीप विश्वास नेटवर्क</s>
</s>
</s>
डीप बोल्ट्जमैन मशीन</s>
</s>
रैना एट अल</s>
, 2009 का 14वां</s>
अपूरणीय संवलन नेटवर्क (</s>
जेरेट एट अल , 2009 15</s>
</s>
</s>
16 . 16 . 16 . 16 . 16 . 16 .</s>
ओएमपी - 1 नेटवर्क</s>
वितरित ऑटोनकोडर (</s>
ले एट अल , 2012 18</s>
बहु - जीपीयू संवलन नेटवर्क (</s>
Krizhevsky एट अल</s>
↑ 2012 का इडियट 19 .</s>
COTS HPC अनुपयुक्त संवलयन नेटवर्क (</s>
कोट्स एट अल , 2013 20</s>
गोगली</s>
इसी प्रकार , प्रारंभिक नेटवर्क केवल दो प्रकार की वस्तुओं को पहचान सकता है ( कुछ मामलों में , एक ही प्रकार की वस्तु की अनुपस्थिति या उपस्थिति , जबकि ये आधुनिक नेटवर्क आम तौर पर कम से कम 1 , 000 विभिन्न श्रेणियों की वस्तुओं को पहचानते हैं .</s>
वस्तु मान्यता में सबसे बड़ी प्रतियोगिता इमेजनेट 23 सीएचएपीटर 1 है ।</s>
Name</s>
बड़े पैमाने पर दृश्य अभिज्ञान चैलेंज (आईएलएसवीआरसी ) प्रत्येक वर्ष आयोजित किया जाता है .</s>
की मेटाडोरिक वृद्धि में एक नाटकीय पल आया जब एक संवलित नेटवर्क पहली बार इस चुनौती को जीता और एक व्यापक मार्जिन के द्वारा , यह 261 प्रतिशत से 153 प्रतिशत तक नीचे ले आया</s>
तब से , इन प्रतियोगिताओं को लगातार गहरे संवलयन जालों द्वारा जीता जाता है , और इस लेखन के रूप में , गहन शिक्षण में अग्रिम इस प्रतियोगिता में नवीनतम शीर्ष - 5 त्रुटि दर को 3 . 6 प्रतिशत तक ले आया है , जैसा कि अंक 112 में दिखाया गया है .</s>
डीप लर्निंग का भाषण मान्यता पर भी नाटकीय प्रभाव पड़ा है .</s>
1990 के दशक भर में सुधार के बाद , भाषण मान्यता के लिए त्रुटि दरों लगभग 2000 में शुरू हुआ .</s>
परिचय</s>
हम इस इतिहास का अधिक विस्तार से से सेक्शन 12 . 3 में अन्वेषण करते हैं ।</s>
डीप नेटवर्कों को भी पैदल पता लगाने और छवि खंडन के लिए शानदार सफलता मिली है ।</s>
</s>
कायर्सन एट अल , 2012</s>
इसी समय गहरे नेटवर्कों के पैमाने और सटीकता में वृद्धि हुई है , 2010 2011 2013 2014 वर्ष 0 . 0 .05 0 .10 .15</s>
0 . 20 0 .25 0 .30 ILSVRC वर्गीकरण त्रुटि दर चित्र 1 . 12 : समय के साथ त्रुटि दर में कमी</s>
चूंकि गहरे नेटवर्क इमेजनेट बड़े पैमाने पर दृश्य मान्यता चैलेंज में प्रतिस्पर्धा करने के लिए आवश्यक पैमाने पर पहुंचे , वे लगातार हर साल प्रतियोगिता जीत लिया है , हर बार कम और कम त्रुटि दरों का उत्पादन .</s>
रसाकोवस्की एट अल से डेटा</s>
(2014 )</s>
और वह एट अल .</s>
(2015 )</s>
24 CHAPTER 1 .</s>
INTRODUCTION उन कार्यों की जटिलता है जिन्हें वे हल कर सकते हैं .</s>
गुडफेलो एट अल</s>
</s>
पहले , यह व्यापक रूप से माना जाता था कि इस तरह के सीखने के लिए अनुक्रम के व्यक्तिगत तत्वों की लेबलिंग की आवश्यकता होती है ( Gülçhre और बेंगीओ , 2013</s>
पुनरावर्ती तंत्रिका नेटवर्क , जैसे कि एलएसटीएम अनुक्रम मॉडल , जिसका उल्लेख ऊपर किया गया है , अब केवल स्थिर इनपुट के बजाय अनुक्रमों और अन्य अनुक्रमों के बीच संबंधों को मॉडल करने के लिए प्रयोग किया जाता है .</s>
यह अनुक्रम - से - अनुक्रम अधिगम किसी अन्य अनुप्रयोग में क्रांति लाने के मुहाने पर प्रतीत होता हैः मशीनी अनुवाद</s>
2014</s>
बढ़ती जटिलता की इस प्रवृत्ति को तंत्रिका तंत्र मशीनों ( Graves एट अल , 2014 ) के लागू होने के साथ अपने तार्किक निष्कर्ष पर धकेल दिया गया है जो स्मृति कोशिकाओं से पढ़ना सीखते हैं और स्मृति कोशिकाओं को मनमाने ढंग से सामग्री लिखते हैं .</s>
ऐसे तंत्रिका नेटवर्क वांछित व्यवहार के उदाहरणों से सरल प्रोग्राम सीख सकते हैं ।</s>
उदाहरण के लिए , वे स्क्रम्बल्ड और अनुक्रमित अनुक्रमों के उदाहरण दिए गए संख्याओं की सूचियों को छांटना सीख सकते हैं .</s>
यह स्व - प्रोग्रामिंग तकनीक अपनी शैशवावस्था में है , लेकिन भविष्य में इसे सैद्धांतिक रूप से लगभग किसी भी कार्य के लिए लागू किया जा सकता है .</s>
गहन अधिगम की एक और प्रमुख उपलब्धि है , प्रवर्तन अधिगम के क्षेत्र में इसका विस्तार ।</s>
प्रवर्तन अधिगम के संदर्भ में , एक स्वायत्त अभिकर्ता को परीक्षण और त्रुटि द्वारा कार्य करना सीखना चाहिए , मानव प्रचालक से बिना किसी मार्गदर्शन के .</s>
डीपमिन्ड ने यह प्रदर्शित किया कि गहन अधिगम पर आधारित एक प्रवर्तन अधिगम प्रणाली एटरी वीडियो गेम खेलने के लिए सीखने में सक्षम है , जो कई कार्यों पर मानव - स्तर प्रदर्शन तक पहुंचता है ।</s>
</s>
डीप लर्निंग ने रोबोटिक्स ( Finn एट अल , 2015 ) के लिए प्रवर्तन सीखने के प्रदर्शन में भी महत्वपूर्ण सुधार किया है .</s>
गहन शिक्षण के इन अनुप्रयोगों में से कई अत्यधिक लाभप्रद हैं .</s>
डीप लर्निंग अब गूगल , माइक्रोसॉफ्ट , फेसबुक , आईबीएम , एप्पल , एडोब , नेटफ्लिक्स , एनवीआईए , और एनईसी सहित कई शीर्ष प्रौद्योगिकी कंपनियों द्वारा प्रयोग किया जाता है</s>
गहन शिक्षण में अग्रिम भी सॉफ्टवेयर बुनियादी ढांचे में अग्रिमों पर काफी निर्भर किया गया है .</s>
सॉफ्टवेयर पुस्तकालयों जैसे</s>
</s>
</s>
टेन्सरफ्लो</s>
डीप लर्निंग ने अन्य विज्ञानों में भी योगदान दिया है ।</s>
वस्तु अभिज्ञान के लिए आधुनिक संवलित - सामयिक नेटवर्क दृश्य प्रक्रमण का एक मॉडल प्रदान करते हैं जिसका तंत्रिका विज्ञानी अध्ययन कर सकते हैं (DiCarlo , 2013 )</s>
डीप लर्निंग भी भारी मात्रा में डेटा के प्रसंस्करण और वैज्ञानिक 25 सीएचएपीटर 1 में उपयोगी भविष्यवाणियों के लिए उपयोगी उपकरण प्रदान करता है .</s>
सामान्य क्षेत्र</s>
इसका सफलतापूर्वक उपयोग यह भविष्यवाणी करने के लिए किया गया है कि कैसे अणु दवा कंपनियों को नई दवाओं के डिजाइन में मदद करने के लिए बातचीत करेंगे ( Dahl एट अल , 2014 )</s>
हम उम्मीद करते हैं कि गहन शिक्षण भविष्य में अधिक से अधिक वैज्ञानिक क्षेत्रों में प्रकट होगा .</s>
सारांश में , गहरी सीखने मशीन सीखने के लिए एक दृष्टिकोण है कि मानव मस्तिष्क , सांख्यिकी और अनुप्रयुक्त गणित के हमारे ज्ञान पर भारी आकर्षित किया है के रूप में यह पिछले कई दशकों में विकसित किया है .</s>
हाल के वर्षों में , गहन शिक्षण ने अपनी लोकप्रियता और उपयोगिता में अत्यधिक वृद्धि देखी है , व्यापक रूप से अधिक शक्तिशाली कंप्यूटरों , गहरे नेटवर्कों को प्रशिक्षित करने के लिए बड़े डेटासेट और तकनीक के परिणाम के रूप में .</s>
आगे के वर्ष गहरी शिक्षा को और भी बेहतर बनाने और उसे नई सीमाओं तक लाने की चुनौतियों और अवसरों से भरे हुए हैं ।</s>
26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26</s>
