अध्याय 1 परिचय निवेशकों ने लंबे समय से ऐसी मशीनें बनाने का सपना देखा है जो सोचती हैं ।
यह इच्छा कम से कम प्राचीन यूनान के समय की है ।
मिथकीय
जब पहली बार प्रोग्राम योग्य कंप्यूटरों की कल्पना की गई तो लोगों ने सोचा कि क्या ऐसी मशीनें बुद्धिमान हो सकती हैं , एक के निर्माण से सौ साल पहले ( Lovlace , 1842 )
आज , कृत्रिम बुद्धि ( एएआई ) एक फलता - फूलता क्षेत्र है जिसमें कई व्यावहारिक अनुप्रयोग और सक्रिय अनुसंधान विषय हैं ।
हम रुटीन श्रम को स्वचालित करने के लिए बुद्धिमान सॉफ्टवेयर की ओर देखते हैं , भाषण या छवियों को समझते हैं , चिकित्सा में निदान करते हैं और बुनियादी वैज्ञानिक अनुसंधान का समर्थन करते हैं .
कृत्रिम बुद्धि के प्रारंभिक दिनों में , क्षेत्र तेजी से हल और समस्याओं है कि बौद्धिक रूप से मानव के लिए मुश्किल है , लेकिन कंप्यूटर के लिए अपेक्षाकृत सीधे आगे -
समस्याओं है कि औपचारिक , गणित विषयगत नियमों की एक सूची द्वारा वर्णित किया जा सकता है .
कृत्रिम बुद्धि की सच्ची चुनौती उन कार्यों को हल करने में सिद्ध हुई जो लोगों के लिए औपचारिक रूप से करने में आसान हैं लेकिन लोगों के लिए कठिन हैं - ऐसे प्रतीक जिन्हें हम सहज ही हल कर लेते हैं , जो बोलचाल के शब्दों या चेहरों को पहचानने जैसे स्वचालित महसूस करते हैं ।
यह पुस्तक इन अधिक सहज समस्याओं के समाधान के बारे में है ।
यह समाधान कंप्यूटर को अनुभव से सीखने और अवधारणाओं के एक पदानुक्रम के संदर्भ में दुनिया को समझने की अनुमति देने के लिए है , सरल अवधारणाओं के साथ अपने संबंध के माध्यम से परिभाषित प्रत्येक अवधारणा के साथ .
अनुभव से ज्ञान इकट्ठा करके , यह दृष्टिकोण मानव ऑपरेटरों के लिए औपचारिक रूप से कंप्यूटर की जरूरत के सभी ज्ञान को निर्दिष्ट करने की आवश्यकता से बचता है .
अवधारणाओं का पदानुक्रम कंप्यूटर को जटिल अवधारणाओं को सरल अवधारणाओं से निर्मित करके सीखने में सक्षम बनाता है .
यदि हम एक ग्राफ बनाते हैं जिससे पता चलता है कि कैसे इन अवधारणाओं 1 CHAPTER 1 .
INTRODUCING एक दूसरे के ऊपर बनाया जाता है , ग्राफ गहरा है , कई परतों के साथ ।
इस कारण से , हम एआई गहरी सीखने के लिए इस दृष्टिकोण कहते हैं .
एआई की कई प्रारंभिक सफलताएं अपेक्षाकृत बंध्य और औपचारिक वातावरण में हुईं और उन्हें दुनिया के बारे में अधिक जानकारी रखने के लिए कंप्यूटर की आवश्यकता नहीं पड़ी .
उदाहरण के लिए , आईबीएम के डीप ब्लू शतरंज खेल प्रणाली ने 1997 में विश्व चैंपियन गैरी कास्पारोव को हराया ( हासु , 2002 )
शतरंज निस्संदेह एक बहुत ही सरल संसार है , जिसमें केवल साठ स्थान और चौंतीस टुकड़े हैं , जो केवल कड़ाई से परिक्रमा कर सकते हैं ।
एक सफल शतरंज रणनीति प्रस्तुत करना एक जबरदस्त उपलब्धि है , लेकिन चुनौती शतरंज के टुकड़ों के सेट का वर्णन करने और कंप्यूटर में स्वीकार्य चालों के कारण नहीं है .
शतरंज पूरी तरह से पूरी तरह से औपचारिक नियमों की एक बहुत संक्षिप्त सूची द्वारा वर्णित किया जा सकता है , आसानी से प्रोग्रामर द्वारा समय से आगे प्रदान की .
विडंबना यह है कि , अमूर्त और औपचारिक कार्य जो मनुष्य के लिए सबसे कठिन मानसिक उपक्रमों में से हैं , कंप्यूटर के लिए सबसे आसान हैं .
कंप्यूटर लंबे समय से सर्वश्रेष्ठ मानव शतरंज खिलाड़ी को भी हराने में सफल रहे हैं , लेकिन हाल ही में वस्तुओं या भाषण को पहचानने के लिए औसत मानव की कुछ क्षमताओं का मिलान शुरू कर दिया है .
व्यक्ति के दैनिक जीवन के लिए विश्व के बारे में अत्यधिक ज्ञान की आवश्यकता होती है ।
इस ज्ञान का अधिकांश भाग व्यक्तिपरक और अन्तर्ज्ञानात्मक है , और इसलिए इसे औपचारिक रूप से व्यक्त करना कठिन है ।
कंप्यूटर एक बुद्धिमान तरीके से व्यवहार करने के लिए इसी ज्ञान पर कब्जा करने की जरूरत है .
कृत्रिम बुद्धि में एक प्रमुख चुनौती यह है कि इस अनौपचारिक ज्ञान को कंप्यूटर में कैसे प्राप्त किया जाए ।
कई कृत्रिम खुफिया परियोजनाओं ने औपचारिक भाषाओं में दुनिया के बारे में कड़ी जानकारी देने की कोशिश की है ।
एक कंप्यूटर तार्किक अनुमान नियमों का उपयोग कर इन औपचारिक भाषाओं में कथनों के बारे में स्वतः तर्क कर सकता है .
इसे कृत्रिम बुद्धि के ज्ञान का आधार दृष्टिकोण कहा जाता है ।
इनमें से किसी भी परियोजना को बड़ी सफलता नहीं मिली है ।
ऐसी सबसे प्रसिद्ध परियोजनाओं में से एक है साइक ( Lyc ) और गुह , 1989 .
साइक एक अनुमान इंजन है और साइकल नामक भाषा में कथनों का डेटाबेस है ।
इन बयानों को मानव पर्यवेक्षकों के एक स्टाफ द्वारा दर्ज किया जाता है .
यह एक अपरिच्छिन्न प्रक्रिया है ।
लोग दुनिया का सही वर्णन करने के लिए पर्याप्त जटिलता के साथ औपचारिक नियमों को ईजाद करने के लिए संघर्ष करते हैं .
उदाहरण के लिए , Cyc सुबह फ्रेड शेव नामक व्यक्ति के बारे में एक कहानी समझने में विफल रहा ( Linde , 1992 )
इसके अनुमान इंजन ने कहानी में एक असंगति का पता लगायाः यह जानता था कि लोगों के पास विद्युतीय भाग नहीं हैं , लेकिन चूंकि फ्रेड एक विद्युतीय रेज़र थामे हुए थे , इसलिए यह विश्वास करता था कि यह इकाई “फ़र्डवाहिलशिंग” में विद्युतीय भाग थे .
इसलिए इसमें पूछा गया कि क्या फ्रेड अभी भी शेव करते समय व्यक्ति थे ?
कठोर ज्ञान के आधार पर प्रणालियों के सामने आने वाली कठिनाइयों से यह संकेत मिलता है कि एआई सिस्टम को अपना ज्ञान प्राप्त करने की क्षमता की आवश्यकता है , 2 सीएचएपीटर 1 निकालकर ।
कच्चे डेटा से प्रविष्ट पैटर्न
इस क्षमता को मशीन लर्निंग के नाम से जाना जाता है ।
मशीन लर्निंग शुरू होने से कंप्यूटर वास्तविक दुनिया के ज्ञान से संबंधित समस्याओं से निपटने और व्यक्तिपरक दिखने वाले निर्णय लेने में सक्षम हो गए .
एक सरल मशीन अधिगम एल्गोरिथ्म जिसे संभार प्रतिगमन कहा जाता है , यह निर्धारित कर सकता है कि क्या सीजेरियन डिलीवरी की सिफारिश की जाए ( Mor -Yosf एट अल , 1990 )
एक सरल मशीन लर्निंग एल्गोरिथ्म जिसे नैव बेज़ कहा जाता है , स्पैम ई - मेल से वैध ई - मेल को अलग कर सकता है ।
इन सरल मशीन अधिगम एल्गोरिदम का प्रदर्शन उनके द्वारा दिए गए डेटा के निरूपण पर काफी निर्भर करता है .
उदाहरण के लिए , जब संभार प्रतिगमन का प्रयोग सीजेरियन प्रसव की सिफारिश करने के लिए किया जाता है , एआई प्रणाली रोगी की सीधे जांच नहीं करती है .
इसके बजाय , चिकित्सक सिस्टम को प्रासंगिक जानकारी के कई टुकड़े बताता है , जैसे गर्भाशय के निशान की उपस्थिति या अनुपस्थिति .
रोगी के प्रतिनिधित्व में शामिल जानकारी के प्रत्येक टुकड़े एक विशेषता के रूप में जाना जाता है .
लॉजिस्टिक प्रतिगमन यह सीखता है कि रोगी की इन विशेषताओं में से प्रत्येक का विभिन्न परिणामों से संबंध कैसे होता है ।
हालांकि , यह प्रभावित नहीं कर सकते कि कैसे सुविधाओं को किसी भी तरह से परिभाषित किया जाता है .
यदि संभार - प्रतिगमन को रोगी का एमआरआई स्कैन दिया जाता , न कि चिकित्सक की औपचारिक रिपोर्ट , तो यह उपयोगी भविष्यवाणियां नहीं कर पाता .
एमआरआई स्कैन में व्यक्तिगत पिक्सल का प्रसव के दौरान होने वाली जटिलताओं के साथ नगण्य सहसंबंध होता है ।
अभ्यावेदनों पर यह निर्भरता एक सामान्य घटना है जो पूरे कंप्यूटर विज्ञान और यहां तक कि दैनिक जीवन में प्रकट होती है .
संगणक विज्ञान में , डेटा संग्रह की खोज जैसे संक्रियाएं घातीय रूप से तेज गति से आगे बढ़ सकती हैं यदि कोलेक - टीन संरचित और अनुक्रमित बुद्धिमानी से हो ।
लोग अरबी अंकों पर अंकगणित आसानी से कर सकते हैं लेकिन रोमन अंकों पर अंकगणित को अधिक समय लेने वाला पाते हैं ।
यह आश्चर्य की बात नहीं है कि प्रतिनिधित्व के चयन मशीन सीखने एल्गोरिदम के प्रदर्शन पर एक विशाल प्रभाव है .
एक सरल दृश्य उदाहरण के लिए , देखें आंकड़ा 1 . 1 .
कई कृत्रिम बुद्धि कार्यों को उस कार्य के लिए निकालने के लिए सुविधाओं के सही सेट डिजाइन करके हल किया जा सकता है , तो इन सुविधाओं को एक सरल मशीन सीखने एल्गोरिथ्म के लिए प्रदान करता है .
उदाहरण के लिए , ध्वनि से वक्ता की पहचान के लिए एक उपयोगी विशेषता वक्ता के मुखर पथ के आकार का अनुमान है .
यह विशेषता इस बात का पुख्ता सुराग देती है कि वक्ता पुरुष है या स्त्री , या बच्चा ।
कई कार्यों के लिए , तथापि , यह जानना कठिन है कि किन विशेषताओं को निकाला जाना चाहिए .
उदाहरण के लिए , मान लीजिए कि हम तस्वीरों में कारों का पता लगाने के लिए एक प्रोग्राम लिखना चाहेंगे .
हम जानते हैं कि कारों में पहिये होते हैं , इसलिए हम एक विशेषता के रूप में पहिये की उपस्थिति का उपयोग करना पसंद कर सकते हैं .
दुर्भाग्य से , यह ठीक से वर्णन करने के लिए मुश्किल है क्या एक पहिया पिक्सेल मूल्यों के संदर्भ में की तरह दिखता है .
पहिये का आकार सरल ज्यामितीय होता है , लेकिन इसकी छवि पहिये पर पड़ने वाली छायाओं से जटिल हो सकती है , सूर्य पहिये के धातु भागों , कार के लिंग या किसी वस्तु को 3 सीएचएपीटर 1 में चमकता है ।
INTRODUC
                                                 
.
                                                 
.
चित्र 1 . 1
विभिन्न अभ्यावेदनों का उदाहरणः मान लीजिए कि हम दो श्रेणियों के डेटा को एक प्रकीर्णन में उनके बीच एक रेखा खींच कर अलग करना चाहते हैं .
बाईं ओर भूखंड में , हम कार्टेसियन निर्देशांक का उपयोग कर कुछ डेटा का प्रतिनिधित्व करते हैं , और कार्य असंभव है .
दाहिनी ओर भूखंड में , हम ध्रुवीय निर्देशांकों के साथ डेटा का प्रतिनिधित्व करते हैं और कार्य ऊर्ध्वाधर रेखा के साथ हल करने के लिए सरल हो जाता है .

इस समस्या का एक समाधान यह भी है कि न केवल प्रतिनिधित्व से आउटपुट तक मानचित्रण की खोज के लिए मशीनी शिक्षण का उपयोग किया जाए बल्कि स्वयं प्रतिनिधित्व भी किया जाए ।
इस दृष्टिकोण प्रतिनिधित्व सीखने के रूप में जाना जाता है .
विद्वत अभ्यावेदन अक्सर हस्तहस्ताक्षरित अभ्यावेदनों से प्राप्त किए जा सकने की तुलना में कहीं बेहतर प्रदर्शन का परिणाम होते हैं .
वे एआई सिस्टम को नए कार्यों के लिए तेजी से अनुकूलित करने में भी सक्षम बनाते हैं , जिसमें न्यूनतम मानवीय हस्तक्षेप होता है .
एक प्रतिनिधित्व सीखने एल्गोरिथ्म मिनट में एक सरल कार्य के लिए सुविधाओं का एक अच्छा सेट का पता लगा सकते हैं , या घंटे से महीनों में एक जटिल कार्य के लिए .
एक जटिल कार्य को डिजाइन करने के लिए मानवीय समय की आवश्यकता होती है ।
निरूपण अधिगम एल्गोरिथ्म का सारभूत उदाहरण ऑ - टोनकोडर है ।
एक ऑटोनकोडर , एक एनकोडर फलन का संयोजन होता है , जो इनपुट डेटा को भिन्न निरूपण में परिवर्तित करता है , और एक विकोडक फलन , जो नए निरूपण को वापस मूल प्रारूप में परिवर्तित करता है .
जब एक इनपुट एनकोडर और फिर डिकोडर के माध्यम से चलाया जाता है तो अधिक से अधिक जानकारी को सुरक्षित रखने के लिए ऑटोनोकोडर को प्रशिक्षित किया जाता है , लेकिन उन्हें यह भी प्रशिक्षित किया जाता है कि नए प्रतिनिधित्व में विभिन्न अच्छे गुण होते हैं .
विभिन्न प्रकार के ऑटोनकोडर्स का उद्देश्य विभिन्न प्रकार के गुण प्राप्त करना होता है ।
जब सीखने की सुविधाओं के लिए सुविधाओं या एल्गोरिदम डिजाइनिंग , हमारे लक्ष्य आमतौर पर विभिन्नता के कारकों को अलग करने के लिए है कि मनाया डेटा की व्याख्या .
इसमें ४ भा . प्रौ . सं . १
संदर्भ
इस तरह के कारक अक्सर मात्राओं है कि सीधे मनाया जाता है नहीं कर रहे हैं .
इसके बजाय , वे या तो अप्रतिरक्षित वस्तुओं के रूप में मौजूद हो सकते हैं या भौतिक जगत में अप्रतिरक्षित शक्तियों के रूप में , जो वेधीय मात्राओं को प्रभावित करती हैं .
वे मानव मस्तिष्क में निर्माण के रूप में भी मौजूद हो सकते हैं जो प्रेक्षित डेटा की उपयोगी सरल व्याख्या या अनुमानित कारण प्रदान करते हैं .
उन्हें उन अवधारणाओं या अमूर्तनों के रूप में सोचा जा सकता है जो हमें आंकड़ों में समृद्ध भिन्नता का बोध कराने में मदद करते हैं ।
जब किसी भाषण रिकॉर्डिंग का विश्लेषण किया जाता है तो विभिन्नता के कारकों में वक्ता की आयु , उनका लिंग , उनके लहजे और वे जो शब्द बोल रहे हैं शामिल हैं .
जब किसी कार की छवि का विश्लेषण किया जाता है , तो विभिन्नता के कारकों में कार की स्थिति , उसके रंग , और सूर्य के कोण और चमक शामिल हैं .
कई वास्तविक दुनिया कृत्रिम बुद्धि अनुप्रयोगों में कठिनाई का एक प्रमुख स्रोत यह है कि विभिन्नता के कई कारक डेटा के हर एक टुकड़े को प्रभावित करते हैं जिसका हम अवलोकन करने में सक्षम हैं .
एक लाल कार की छवि में व्यक्तिगत पिक्सल रात में काले के बहुत करीब हो सकता है .
कार के सिलहोट का आकार देखने के कोण पर निर्भर करता है ।
अधिकांश अनुप्रयोगों के लिए हमें विभिन्नता के कारकों को अलग करने और उन कारकों को त्यागने की आवश्यकता होती है जिनकी हम परवाह नहीं करते ।
बेशक , कच्चे डेटा से इस तरह के उच्च स्तर , अमूर्त विशेषताओं को निकालना बहुत मुश्किल हो सकता है .
विभिन्नता के इनमें से कई कारक , जैसे कि वक्ता के लहजे की पहचान केवल परिष्कृत , डेटा की लगभग मानवीय समझ का उपयोग करके की जा सकती है .
जब मूल समस्या के समाधान के लिए कोई निरूपण प्राप्त करना लगभग उतना ही कठिन होता है , तब अभ्यावेदन अधिगम पहली नजर में नहीं , हमारी सहायता करता प्रतीत होता है ।
गहन अधिगम इस केंद्रीय समस्या को अंतर्मुखी निरूपण द्वारा हल करता है जो अन्य सरल निरूपणों के संदर्भ में व्यक्त किए जाते हैं ।
डीप लर्निंग कंप्यूटर को सरल शंकु से जटिल अवधारणाओं का निर्माण करने में सक्षम बनाती है ।
चित्र 1 . 2 से पता चलता है कि कैसे एक गहरी शिक्षण प्रणाली सरल अवधारणाओं , जैसे कोनों और contours , जो बारी में किनारों के संदर्भ में परिभाषित कर रहे हैं के संयोजन के द्वारा एक व्यक्ति की छवि की अवधारणा का प्रतिनिधित्व कर सकते हैं .
एक गहन शिक्षण मॉडल का सारभूत उदाहरण है फीड एक गहरा नेटवर्क , या मल्टीलेयर पर्सेप्टॉन (
Name
समारोह कई सरल कार्यों की रचना के द्वारा बनता है .
हम इनपुट का एक नया प्रतिनिधित्व प्रदान करने के रूप में एक अलग गणितीय समारोह के प्रत्येक अनुप्रयोग के बारे में सोच सकते हैं .
डेटा के लिए सही प्रतिनिधित्व सीखने का विचार गहरे सीखने पर एक प्रति स्पेक्ट्रम प्रदान करता है .
गहन अधिगम पर एक और परिप्रेक्ष्य है कि गहराई कंप्यूटर को एक बहुसंकेत कंप्यूटर प्रोग्राम सीखने में सक्षम बनाता है .
अभ्यावेदन की प्रत्येक परत को 5 CHAPTER 1 के बाद कंप्यूटर की स्मृति की स्थिति के रूप में सोचा जा सकता है .
परिवर्तनीय परत
प्रथम प्रच्छन्न परत
द्वितीय प्रच्छन्न परत
CAR PERSON LANIMAL आउटपुट ( विषयगत पहचान )
चित्र 1 . 2 : गहन अध्ययन मॉडल का प्रदर्शन
एक कंप्यूटर के लिए कच्चे संवेदी इनपुट डेटा के अर्थ को समझना कठिन होता है , जैसे कि यह छवि पिक्सेल मूल्यों के संग्रह के रूप में प्रतिनिधित्व करती है .
पिक्सेल के सेट से वस्तु पहचान के लिए समारोह मानचित्रण बहुत जटिल है .
इस प्रतिचित्रण को सीखना या उसका मूल्यांकन करना यदि सीधे तरीके से किया जाए तो असंदिग्ध प्रतीत होता है ।
गहन अधिगम इस कठिनाई को नीड़ित सरल मानचित्रणों की श्रृंखला में वांछित जटिल प्रतिचित्रण को तोड़कर दूर करता है , प्रत्येक मॉडल की एक अलग परत द्वारा वर्णित होता है ।
इनपुट दृश्य परत पर प्रस्तुत किया जाता है , तो नामित क्योंकि यह चर है कि हम अवलोकन करने में सक्षम हैं शामिल हैं .
फिर छुपी परतों की एक श्रृंखला छवि से बढ़ती अमूर्त विशेषताओं को निकालती है .
इन परतों को इन परतों को इन परतों को इन परतों को इन परतों को नहीं दिया जाता है क्योंकि इन परतों को इन परतों को इन परतों को नहीं दिया जाता है क्योंकि इन परतों को इन परतों को इन परतों को इन परतों को इन परतों को इन परतों को नहीं दिया जाता है ।
यहाँ छवियों प्रत्येक छिपा इकाई द्वारा प्रतिनिधित्व विशेषता के प्रकार की कल्पना कर रहे हैं .
पिक्सेल को देखते हुए , पहली परत आसानी से किनारों की पहचान कर सकते हैं , पड़ोसी पिक्सेल की चमक की तुलना करके .
किनारों के पहले छिपे परत के वर्णन को देखते हुए , दूसरी छुपी परत आसानी से कोनों और विस्तारित contours की खोज कर सकती है , जो कि किनारों के संग्रह के रूप में पहचानी जा सकती है .
दूसरी छुपी हुई परत के कोनों और कोनों के संदर्भ में छवि के वर्णन को देखते हुए , तीसरी छुपी हुई परत विशिष्ट वस्तुओं के संपूर्ण भागों का पता लगा सकती है , जिसमें संधानों और कोनों के विशिष्ट संग्रहों का पता लगाया जा सकता है .
अंत में , इसमें निहित वस्तु भागों के संदर्भ में छवि का यह वर्णन छवि में मौजूद वस्तुओं को पहचानने के लिए इस्तेमाल किया जा सकता है .
ज़ाइलर और फर्गुस (2014 ) से अनुमति लेकर पेश किए गए चित्र
6 CHAPTER 1 .
निर्देश के एक अन्य सेट को समानांतर रूप से निष्पादित करने वाला INTRODUCING ।
अधिक गहराई वाले नेटवर्क अनुक्रम में अधिक अनुदेश निष्पादित कर सकते हैं ।
अनुक्रमिक अनुदेश महान शक्ति प्रदान करते हैं क्योंकि बाद के अनुदेश पूर्व अनुदेशों के परिणामों को वापस संदर्भित कर सकते हैं .
Ac
गहन अध्ययन के इस दृष्टिकोण के प्रति सौहार्द , एक परत के सक्रियण में सभी जानकारी अनिवार्य रूप से इनपुट की व्याख्या करने वाले विभिन्नता के कारकों को एन्कोड करता है .
अभ्यावेदन राज्य सूचना को भी भंडारित करता है जो इनपुट का बोध करा सकने वाले प्रोग्राम को निष्पादित करने में मदद करता है .
यह राज्य जानकारी एक पारंपरिक कंप्यूटर प्रोग्राम में एक काउंटर या सूचक के अनुरूप हो सकती है .
इसका विशेष रूप से इनपुट की सामग्री से कोई संबंध नहीं है , लेकिन यह मॉडल को अपने प्रसंस्करण को व्यवस्थित करने में मदद करता है .
एक मॉडल की गहराई को मापने के दो मुख्य तरीके हैं ।
पहला दृश्य अनुक्रमिक अनुदेशों की संख्या पर आधारित है जिसे वास्तुकला के मूल्यांकन के लिए निष्पादित किया जाना चाहिए .
हम इसे एक प्रवाह चार्ट के माध्यम से सबसे लंबे पथ की लंबाई के रूप में सोच सकते हैं कि कैसे मॉडल के आउटपुट में से प्रत्येक की गणना करने के लिए अपने इनपुट दिया का वर्णन करता है .
जिस प्रकार दो समतुल्य कंप्यूटर प्रोग्रामों की लंबाई अलग - अलग होगी , यह निर्भर करता है कि प्रोग्राम किस भाषा में लिखा गया है , उसी प्रकार एक फ्लो चार्ट के रूप में विभिन्न गहराई के साथ एक ही फंक्शन तैयार किया जा सकता है , जिस पर निर्भर करता है कि हम फ्लो चार्ट में व्यक्तिगत चरणों के रूप में प्रयोग करने की अनुमति देते हैं .
चित्र 1 . 3 इस बात को स्पष्ट करता है कि किस प्रकार भाषा का यह चयन एक ही वास्तुकला के लिए दो भिन्न माप दे सकता है ।
x 1 x 1 x 1 x 1 w 1 w 1 w
×


w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w w
2
w 2 × +
तत्व सेट + ×

तत्व सेट लॉजिस्टिक प्रतिगमन लॉजिस्टिक प्रतिगमन चित्र 1 . 3 :
कम्प्यूटेशनल रेखांकन का प्रदर्शन , एक आउटपुट के लिए इनपुट प्रतिचित्रण , जहां प्रत्येक नोड एक संक्रिया निष्पादित करता है ।
गहराई इनपुट से आउटपुट तक सबसे लंबे पथ की लंबाई है , लेकिन जो एक संभावित कम्प्यूटेशनल कदम का गठन करता है की परिभाषा पर निर्भर करता है .
संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संरेखण संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना संगणना सं
यदि हम जोड़ , गुणा और रसद सिग्मिड हमारे कंप्यूटर भाषा के तत्वों के रूप में उपयोग करते हैं , तो इस मॉडल की गहराई तीन
यदि हम संभार प्रतिगमन को एक तत्व के रूप में ही देखते हैं , तो इस मॉडल की गहराई एक है ।
7 CHAPTER 1 .
एक अन्य दृष्टिकोण , जिसका प्रयोग गहरे समर्थक मॉडलों द्वारा किया जाता है , एक मॉडल की गहराई को कम्प्यूटेशनल ग्राफ की गहराई नहीं बल्कि ग्राफ की गहराई को वर्णित करता है कि अवधारणाएं एक दूसरे से कैसे संबंधित हैं .
इस मामले में , प्रत्येक अवधारणा के निरूपण की गणना के लिए आवश्यक संगणनाओं के प्रवाह संचित्र की गहराई स्वयं अवधारणाओं के ग्राफ से कहीं अधिक गहरी हो सकती है .
इसका कारण यह है कि सरल अवधारणाओं की प्रणाली की समझ को अधिक जटिल अवधारणाओं के बारे में जानकारी दी जा सकती है ।
उदाहरण के लिए , छाया में एक आंख वाले चेहरे की छवि देखने वाला एआई सिस्टम शुरू में केवल एक आंख ही देख सकता है .
यह पता लगाने के बाद कि चेहरा मौजूद है , सिस्टम तब यह अनुमान लगा सकता है कि दूसरी आंख शायद भी मौजूद है .
इस मामले में , अवधारणाओं के ग्राफ में केवल दो परतें शामिल हैं - आंखों के लिए परत और चेहरों के लिए परत - लेकिन अभिकलन के ग्राफ में 2 n परतें शामिल हैं यदि हम प्रत्येक अवधारणा के अपने अनुमान को दूसरे n बार परिष्कृत करते हैं .
क्योंकि यह हमेशा स्पष्ट नहीं है कि इन दो विचारों में से कौन सा कम्प्यूटेशनल ग्राफ की गहराई , या प्रोबेबिलिस्टिक मॉडलिंग ग्राफ की गहराई , सबसे अधिक प्रासंगिक है , और क्योंकि विभिन्न लोग अपने रेखांकन का निर्माण करने के लिए एकल तत्वों के विभिन्न सेट चुनते हैं , जिसमें से एक ही सही प्रोग्राम की गहराई के लिए कोई सही मूल्य नहीं है .
न ही इस बारे में आम सहमति है कि किसी मॉडल को “देप” के रूप में कितनी गहराई तक अर्हता प्राप्त करने की आवश्यकता होती है .
हालांकि , गहरी सीखने को सुरक्षित रूप से उन मॉडलों का अध्ययन माना जा सकता है जिनमें पारंपरिक मशीन लर्निंग की तुलना में या तो विद्वत कार्यों या विद्वत अवधारणाओं के संयोजन की अधिक मात्रा शामिल होती है .
संक्षेप में , गहन अध्ययन के लिए , इस पुस्तक का विषय , एआई के लिए एक दृष्टिकोण है
विशेष रूप से , यह मशीन लर्निंग का एक प्रकार है , एक तकनीक है जो कंप्यूटर सिस्टम को अनुभव और डेटा के साथ सुधार करने में सक्षम बनाता है .
हम प्रतिवाद करते हैं कि मशीन लर्निंग एआई सिस्टम के निर्माण के लिए एकमात्र व्यवहार्य दृष्टिकोण है जो जटिल वास्तविक दुनिया वातावरण में संचालित कर सकता है .
गहन अधिगम , एक विशेष प्रकार का मशीन अधिगम है , जो अवधारणाओं के नीड़ित पदानुक्रम के रूप में विश्व का प्रतिनिधित्व करके महान शक्ति और लचीलापन प्राप्त करता है , जिसमें प्रत्येक अवधारणा सरल अवधारणाओं के संबंध में परिभाषित होती है , और कम अमूर्त निरूपण के संदर्भ में परिकलित होती है ।
चित्र 1 . 4 इन विभिन्न एआई विधाओं के बीच संबंध को स्पष्ट करता है ।
चित्र 1 . 5 प्रत्येक कृति का उच्च स्तर का वर्णन करता है ।
1 . 1
इस किताब को कौन पढ़ें ?
यह पुस्तक कई तरह के पाठकों के लिए उपयोगी हो सकती है , लेकिन हमने इसे दो लक्षित दर्शकों को ध्यान में रखकर लिखा ।
इन लक्षित दर्शकों में से एक है विश्वविद्यालय के छात्र ( स्नातक या मशीन सीखने के बारे में स्नातक
अन्य 8 CHAPTER 1 .
Comment
लॉजिस्टिक प्रतिगमन
उदाहरणः
Name
उदाहरणः
MLP चित्र 1 . 4
एक वेन आरेख , जो दिखाता है कि कितना गहरा अधिगम एक प्रकार का निरूपण अधिगम है , जो बदले में एक प्रकार का मशीन अधिगम है , जो कई के लिए प्रयोग किया जाता है लेकिन सभी दृष्टिकोण एआई के लिए नहीं .
वेन आरेख के प्रत्येक अनुभाग में एक एआई प्रौद्योगिकी का एक उदाहरण शामिल है .
लक्षित दर्शक सॉफ्टवेयर इंजीनियरों है जो एक मशीन सीखने या स्थैतिक पृष्ठभूमि नहीं है , लेकिन तेजी से एक का अधिग्रहण करना चाहते हैं और अपने उत्पाद या मंच में गहरी सीखने का उपयोग शुरू .
डीप लर्निंग पहले से ही कई सॉफ्ट - वेयर विधाओं में उपयोगी साबित हुई है , जिसमें कंप्यूटर विजन , भाषण और ऑडियो प्रोसेसिंग , प्राकृतिक भाषा संसाधन , रोबोटिक्स , जैव सूचना विज्ञान और रसायन विज्ञान , वीडियो गेम , ऑनलाइन विज्ञापन और वित्त शामिल हैं .
इस पुस्तक को तीन भागों में संगठित किया गया है ताकि विभिन्न प्रकार के पाठकों को समायोजित किया जा सके ।
भाग मैं बुनियादी गणितीय उपकरणों और मशीन सीखने अवधारणाओं का परिचय देता है .
भाग द्वितीय सबसे स्थापित गहरी सीखने एल्गोरिदम , जो अनिवार्य रूप से हल प्रौद्योगिकियों रहे हैं का वर्णन करता है .
भाग III अधिक सट्टा विचारों का वर्णन करता है जो व्यापक रूप से गहन अध्ययन में भविष्य के अनुसंधान के लिए महत्वपूर्ण माना जाता है .
९ CHAPTER १ .
Comment
हाथ से तैयार किया गया प्रोग्राम
आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट आउटपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट इनपुट आउटपुट
फीचर आउटपुट इनपुट फीचर मैपिंग से फीचर आउटपुट इनपुट मैपिंग से हैंड डिज़ाइन सुविधाएँ

छायादार बक्से उन घटकों को इंगित करते हैं जो डेटा से सीखने में सक्षम होते हैं .
पाठकों को उन हिस्सों को छोड़ने के लिए स्वतंत्र महसूस करना चाहिए जो उनके हितों या पृष्ठभूमि को देखते हुए प्रासंगिक नहीं हैं ।
रैखिक बीजगणित , संभावना , और मौलिक मशीन सीखने की अवधारणाओं से परिचित पाठकों भाग मैं छोड़ सकते हैं , उदाहरण के लिए , जबकि जो लोग सिर्फ एक कार्य प्रणाली को लागू करना चाहते हैं भाग द्वितीय से आगे पढ़ने की जरूरत नहीं है .
यह चुनने में मदद करने के लिए कि कौन से 10 CHAPTER 1 .
Comment
परिचय भाग
रैखिक बीजगणित 3 .
प्रायिकता और सूचना सिद्धांत 4 .
संख्यात्मक कंप्यूटिंग 5 .
मशीन लर्निंग बेसिक पार्ट 2 :
डीप नेटवर्कः आधुनिक अभ्यास 6 .
डीप फीड सरल नेटवर्क 7 .
नियमितीकरण 8 .
अनुकूलन 9 .
सीएनएन 10
RNs 11 .
व्यावहारिक पद्धति १२
अनुप्रयोग भाग 3 :
डीप लर्निंग रिसर्च 13 .
रैखिक फैक्टर मॉडल 14 .
ऑटोनकोडर 15 .
प्रतिनिधित्व अधिगम 16 .
संरचित सम्भावित मॉडल 17 .
मोंटे कार्लो विधियाँ 18 .
विभाजन कार्य 19 .
अनुमान 20 .
डीप जेनरेशन मॉडल चित्र 1 . 6 : पुस्तक का उच्च स्तरीय संगठन ।
एक अध्याय से दूसरे अध्याय तक तीर से यह संकेत मिलता है कि पूर्व अध्याय उत्तरार्द्ध को समझने के लिए आवश्यक सामग्री है ।
११ भा . प्रौ . सं . १
पढ़ने के लिए INTRODUCING अध्याय , आंकड़ा 1 . 6 पुस्तक के उच्च स्तरीय संगठन को दर्शाता एक फ्लो चार्ट प्रदान करता है ।
हम यह जरूर मान लेते हैं कि सभी पाठक कंप्यूटर विज्ञान की पृष्ठभूमि से आते हैं ।
हम प्रोग्रामिंग , कम्प्यूटेशनल प्रदर्शन मुद्दों , जटिलता सिद्धांत , परिचयात्मक स्तर पथरी और ग्राफ सिद्धांत की कुछ शब्दावली के साथ परिचित हैं .
डीप लर्निंग में 1 . 2 ऐतिहासिक ट्रेंड्स
किसी ऐतिहासिक संदर्भ के साथ गहन अध्ययन को समझना सबसे आसान है ।
गहन अध्ययन का विस्तृत इतिहास उपलब्ध कराने के बजाय , हम कुछ प्रमुख प्रवृत्तियों की पहचानः
• गहन अध्ययन का एक लंबा और समृद्ध इतिहास रहा है , लेकिन कई नामों से जाना जाता है , जो विभिन्न दार्शनिक दृष्टिकोणों को प्रतिबिंबित करते हैं , और लोकप्रियता में वृद्धि और गिरावट आई है .
• डीप लर्निंग अधिक उपयोगी हो गया है क्योंकि उपलब्ध प्रशिक्षण डेटा की मात्रा में वृद्धि हुई है .
• डीप लर्निंग मॉडल समय के साथ आकार में वृद्धि हुई है के रूप में कंप्यूटर बुनियादी ढांचे ( गहरे सीखने के लिए हार्डवेयर और सॉफ्टवेयर ) में सुधार हुआ है .
• डीप लर्निंग समय के साथ बढ़ती सटीकता के साथ तेजी से जटिल अनुप्रयोगों को हल किया है .
1 . 2 . 1
नेरल नेट - वर्क्स के कई नाम और चंगिंग फॉर्च्यून्स
हमें उम्मीद है कि इस पुस्तक के कई पाठकों ने एक रोमांचक नई प्रौद्योगिकी के रूप में गहरी शिक्षा के बारे में सुना है , और एक उभरते हुए क्षेत्र के बारे में एक पुस्तक में “हिस्टरी” का उल्लेख देखकर आश्चर्यचकित हैं .
वास्तव में , गहरी सीखने 1940 के दशक के लिए वापस तिथियाँ .
डीप लर्निंग केवल नई प्रतीत होती है , क्योंकि यह अपनी वर्तमान लोकप्रियता से पहले कई वर्षों के लिए अपेक्षाकृत अलोकप्रिय था , और क्योंकि यह कई अलग अलग नामों से चला गया है , केवल हाल ही में “डेप लर्निंग” कहा जा रहा है .
क्षेत्र को कई बार पुनर्निर्मित किया गया है , विभिन्न शोधकर्ताओं और विभिन्न परिप्रेक्ष्यों के प्रभाव को प्रतिबिंबित .
गहन अध्ययन का एक व्यापक इतिहास इस पाठ्यपुस्तक के दायरे से बाहर है ।
कुछ बुनियादी संदर्भ , तथापि , गहरी सीखने को समझने के लिए उपयोगी है .
मोटे तौर पर , विकास की तीन तरंगें हैंः
1940 -1960 के दशक में साइबरनेटिक्स के नाम से जाना जाने वाला गहन अध्ययन , जिसे 12 सीएचएपीटर 1 में कनेक्शनवाद के नाम से जाना जाता है ।
आईटीआरओडीयूसी 1940 1960 1970 1980 2000 वर्ष


चित्र 1 . 7
कृत्रिम तंत्रिका जाल अनुसंधान की तीन ऐतिहासिक तरंगों में से दो , जैसा कि वाक्यांशों “सैबरनेटिक्स” और “संयोजन” या “नेरल नेटवर्क” की आवृत्ति से मापा जाता है , गूगल बुक्स के अनुसार ( तीसरी तरंग बहुत हाल की है )
1940 के दशक में पहला , 1940 के दशक में शुरू हुआ , जैविक , जैविक , जैविक , जैविक , जैविक , जैविक , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित , प्रवर्तित
दूसरी लहर 1980 -1995 की अवधि के संबंधवादी दृष्टिकोण के साथ शुरू हुई , जिसका बैक - प्रचार ( Rumelhart एट अल , 1986 ) एक या दो छुपी हुई परतों वाले तंत्रिका नेटवर्क को प्रशिक्षित करने के लिए
तरंग गहन अध्ययन २००६ ( वर्तमान तरंग गहन अध्ययन २००६ ) ( वर्तमान तरंग गहन अध्ययन अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल अल
2007
और अभी 2016 के रूप में पुस्तक रूप में प्रदर्शित हो रहा है
इसी प्रकार अन्य दो तरंगें भी उसी प्रकार वैज्ञानिक गतिविधि की अपेक्षा बहुत बाद में पुस्तक रूप में प्रकट हुईं ।
1980 -1990 के दशक में , और वर्तमान पुनरुत्थान गहरा सीखने के नाम से 2006 में शुरू हुआ .
यह मात्रात्मक रूप से 1 . 7 अंक में चित्रित किया गया है ।
कुछ प्रारंभिक सीखने एल्गोरिदम हम आज पहचानते हैं जैविक सीखने के कम्प्यूटेशनल मॉडल , अर्थात् कैसे सीखने के लिए होता है या मस्तिष्क में हो सकता है के मॉडल थे .
एक परिणाम के रूप में , एक नाम है कि गहरी सीखने चला गया है कृत्रिम तंत्रिका नेटवर्क (
गहन शिक्षण मॉडलों पर संगत परिप्रेक्ष्य यह है कि वे जैविक मस्तिष्क ( चाहे मानव मस्तिष्क हो या किसी अन्य पशु के मस्तिष्क ) से प्रेरित सिस्टम इंजीनियर हैं .
जबकि मशीनी शिक्षण के लिए प्रयुक्त तंत्रिका नेटवर्कों के प्रकारों का उपयोग कभी - कभी मस्तिष्क कार्य ( हाइनटन और शैलिस , 1991 ) को समझने के लिए किया गया है , वे आम तौर पर जैविक कार्य के वास्तविक मॉडल नहीं बनाए गए हैं .
गहन अध्ययन पर तंत्रिका परिप्रेक्ष्य दो मुख्य विचारों से प्रेरित है ।
एक विचार यह है कि मस्तिष्क उदाहरण के द्वारा एक सबूत प्रदान करता है कि बुद्धिमान व्यवहार संभव है , और बुद्धि के निर्माण के लिए एक वैचारिक रूप से सीधा रास्ता मस्तिष्क के पीछे कम्प्यूटेशनल सिद्धांतों को रिवर्स इंजीनियर और उसकी कार्यक्षमता की नकल करने के लिए है .
एक और 13 CHAPTER 1 .
INTRODUCTER परिप्रेक्ष्य यह है कि मस्तिष्क और मानव बुद्धि को कम करने वाले सिद्धांतों को समझना अत्यंत रोचक होगा , इसलिए मशीन लर्निंग मॉडल जो इन बुनियादी वैज्ञानिक प्रश्नों पर प्रकाश डालते हैं , इंजीनियरिंग अनुप्रयोगों को हल करने की क्षमता के अलावा उपयोगी हैं .
आधुनिक शब्द “देप अधिगम” मशीन अधिगम मॉडलों की वर्तमान नस्ल पर तंत्रिका विज्ञानी परिप्रेक्ष्य से परे जाता है ।
यह रचना के एकाधिक स्तरों को सीखने के एक अधिक सामान्य सिद्धांत के लिए अपील करता है , जो मशीन सीखने के ढांचे में लागू किया जा सकता है जो अनिवार्य रूप से तंत्रिका प्रेरित नहीं हैं .
आधुनिक गहन अध्ययन के प्रारंभिक पूर्ववर्ती सरल रैखिक मॉडल थे जो तंत्रिका विज्ञानी परिप्रेक्ष्य से प्रेरित थे .
इन मॉडलों n इनपुट मूल्यों x 1 , का एक सेट लेने के लिए डिज़ाइन किया गया था .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
x n और उन्हें एक आउटपुट y के साथ संबद्ध ।
इन मॉडल वजन w 1 , का एक सेट सीखना होगा .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
w n और उनके आउटपुट f ( x , w ) =
1
+ ·· + x n
w n n . n . n . w . n . n . n . w . n . n . n . n . n . w . n . n . n .
तंत्रिका नेटवर्क अनुसंधान की इस पहली लहर को साइबरनेटिक्स के रूप में जाना जाता था , जैसा कि अंक 1 . 7 में दर्शाया गया है ।

यह रैखिक मॉडल इनपुट के दो अलग अलग वर्गों को पहचान सकता है परीक्षण के द्वारा चाहे एफ ( एक्स , w ) सकारात्मक या नकारात्मक है .
बेशक , मॉडल के लिए श्रेणियों की इच्छित परिभाषा के अनुरूप , सही ढंग से सेट किए जाने के लिए आवश्यक भार .
ये भार मानव संचालक द्वारा निर्धारित किया जा सकता था ।
1950 के दशक में , पर्सेप्टॉन (Rossenblatt , 1958 , 1962 ) पहला मॉडल बन गया जो प्रत्येक श्रेणी से इनपुट के दिए गए श्रेणियों को परिभाषित करने वाले भार को सीख सकता था .
अनुकूली रैखिक तत्व
( x ) स्वयं एक वास्तविक संख्या की भविष्यवाणी करने के लिए ( Widrow और Hoff , 1960 ) और भी डेटा से इन संख्याओं की भविष्यवाणी करने के लिए सीख सकता है .
इन सरल अधिगम एल्गोरिदम ने मा - चीनी अधिगम के आधुनिक परिदृश्य को बहुत प्रभावित किया ।
प्रशिक्षण एल्गोरिथ्म जिसका प्रयोग एडीएएलआईएनई के भार को अनुकूलित करने के लिए किया जाता है
एक एल्गोरिथ्म का एक विशेष मामला था जिसे stochastic प्रवणता अवतरण कहा जाता है ।
stochastic ग्रेडिएंट अवतरण एल्गोरिथ्म के हल्के संशोधित संस्करण आज गहरे सीखने मॉडल के लिए प्रमुख प्रशिक्षण एल्गोरिदम बने हुए हैं ।
पर्सेप्टॉन और एडीएएलआईएन द्वारा प्रयुक्त फ ( x , w ) पर आधारित मॉडल रैखिक मॉडल कहलाते हैं .
ये मॉडल कुछ सर्वाधिक व्यापक रूप से प्रयुक्त मशीन लर्निंग मॉडल बने हुए हैं , हालांकि कई मामलों में उन्हें मूल मॉडलों की तुलना में विभिन्न तरीकों से प्रशिक्षित किया जाता है .
रैखिक मॉडलों की कई सीमाएं हैं .
सबसे प्रसिद्ध सबसे प्रसिद्ध & # 44 ; सबसे प्रसिद्ध & # 44 ; सबसे प्रसिद्ध & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; वे & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; & # 44 ; &
= 1 और एफ (
[1 ] , 0 ] , 0 ]
= 1 लेकिन एफ (
[1 ] , 1 www . youtube . com ]
= 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 f 0 है ।
रैखिक मॉडलों में इन खामियों का अवलोकन करने वाले आलोचकों ने सामान्य रूप से जैविक रूप से प्रेरित शिक्षण के खिलाफ एक पीठ थपथपाई ( Minsky और पेपरेट , 1969 )
यह तंत्रिका नेटवर्क की लोकप्रियता में पहली बड़ी डुबकी थी .
14 CHAPTER 1 .
INTRODUCction टुडे , तंत्रिका विज्ञान को गहन शिक्षण शोधकर्ताओं के लिए प्रेरणा का एक महत्वपूर्ण स्रोत माना जाता है , लेकिन अब यह क्षेत्र के लिए प्रमुख मार्गदर्शक नहीं है .
गहन शिक्षण अनुसंधान में तंत्रिका विज्ञान की घटती भूमिका का मुख्य कारण आज यह है कि हमारे पास मस्तिष्क के बारे में पर्याप्त जानकारी नहीं है कि वह इसे गाइड के रूप में इस्तेमाल कर सके ।
मस्तिष्क द्वारा प्रयुक्त वास्तविक एल्गोरिदम की गहरी समझ प्राप्त करने के लिए , हमें एक साथ कम से कम हजारों अंतर्संबंधित न्यूरॉन्स की गतिविधि की निगरानी करने में सक्षम होने की आवश्यकता होगी ।
क्योंकि हम ऐसा नहीं कर पा रहे हैं , हम मस्तिष्क के कुछ सबसे सरल और सुप्रमाणित भागों को भी समझने से दूर हैं ( ओलशसन एंड फील्ड , 2005 )
तंत्रिका विज्ञान हमें उम्मीद करने के लिए एक कारण दिया है कि एक एकल गहरी सीखने एल्गोरिथ्म कई अलग अलग कार्यों को हल कर सकते हैं .
तंत्रिका विज्ञानियों ने पाया है कि फर्नेस अपने मस्तिष्क के श्रवण प्रसंस्करण क्षेत्र के साथ “सी” करना सीख सकते हैं यदि उनके मस्तिष्क को उस क्षेत्र में दृश्य संकेत भेजने के लिए दोहराया जाता है ( वॉन मेलचेर एट अल 2000 )
यह सुझाव है कि स्तनधारी मस्तिष्क के बहुत से विभिन्न कार्यों है कि मस्तिष्क हल करता है के अधिकांश को हल करने के लिए एक एकल एल्गोरिथ्म का उपयोग कर सकता है .
इस परिकल्पना से पहले , मशीन अधिगम अनुसंधान अधिक खंडित था , जिसमें प्राकृतिक भाषा संसाधन , दृष्टि , गति नियोजन और भाषण मान्यता का अध्ययन करने वाले शोधकर्ताओं के विभिन्न समुदाय थे .
आज , इन अनुप्रयोग समुदायों अभी भी अलग हैं , लेकिन गहन सीखने अनुसंधान समूहों के लिए एक साथ कई या यहां तक कि इन सभी अनुप्रयोग क्षेत्रों का अध्ययन करना आम बात है .
हम तंत्रिका विज्ञान से कुछ रूखे दिशानिर्देश निकालने में सक्षम हैं ।
कई कम्प्यूटेशनल इकाइयों है कि केवल एक दूसरे के साथ अपनी बातचीत के माध्यम से बुद्धिमान बन जाते हैं के बुनियादी विचार मस्तिष्क से प्रेरित है .
नवयुग्मी
आज अधिकांश तंत्रिका नेटवर्क एक मॉडल न्यूरॉन पर आधारित हैं जिसे संशोधित रैखिक इकाई कहा जाता है .
मूल cognitron ( Fukushima , 1975 ) एक और अधिक जटिल संस्करण शुरू किया जो मस्तिष्क समारोह के हमारे ज्ञान से अत्यधिक प्रेरित था .
सरलीकृत आधुनिक संस्करण कई दृष्टिकोणों से विचारों को समाहित करते हुए विकसित किया गया था , जिसमें नायर और हाइनटन (2010 और ग्लोरोट एट अल ) थे .

( 2009 ) अधिक इंजीनियरिंग उन्मुख प्रभावों का हवाला देते हुए
जबकि तंत्रिका विज्ञान प्रेरणा का एक महत्वपूर्ण स्रोत है , यह एक कठोर गाइड के रूप में लेने की जरूरत नहीं है .
हम जानते हैं कि वास्तविक न्यूरॉन्स आधुनिक संशोधित रैखिक इकाइयों की तुलना में बहुत अलग कार्यों की गणना , लेकिन अधिक तंत्रिका यथार्थवाद अभी तक मशीन सीखने के प्रदर्शन में सुधार के लिए नेतृत्व नहीं किया है .
इसके अलावा , जबकि तंत्रिका विज्ञान सफलतापूर्वक कई तंत्रिका नेटवर्क वास्तुकला को प्रेरित किया है , हम अभी तक तंत्रिका विज्ञान के लिए जैविक सीखने के बारे में पर्याप्त नहीं पता है सीखने एल्गोरिदम हम इन वास्तुकलाओं को प्रशिक्षित करने के लिए उपयोग
१५ भा . प्रौ . सं . १
INTRODUCING मीडिया लेख अक्सर मस्तिष्क के लिए गहरी सीखने की समानता पर जोर देते हैं .
जबकि यह सच है कि गहन शिक्षण शोधकर्ताओं को मस्तिष्क को अन्य मशीनी शिक्षण क्षेत्रों , जैसे कर्नेल मशीन या बेसियन सांख्यिकी में काम करने वाले शोधकर्ताओं की तुलना में एक प्रभाव के रूप में उद्धृत करने की संभावना अधिक होती है , मस्तिष्क का अनुकरण करने के प्रयास के रूप में गहन शिक्षण को नहीं देखना चाहिए .
आधुनिक गहन अधिगम कई क्षेत्रों , विशेष रूप से लागू गणित मौलिक जैसे रैखिक बीजगणित , संभावना , सूचना सिद्धांत , और संख्यात्मक अनुकूलन से प्रेरणा लेता है .
जबकि कुछ गहरे सीखने के शोधकर्ता तंत्रिका विज्ञान को प्रेरणा का एक महत्वपूर्ण स्रोत बताते हैं , दूसरों को बिल्कुल भी तंत्रिका विज्ञान से संबंधित नहीं हैं .
गौरतलब है कि मस्तिष्क एल्गोरिथम स्तर पर कैसे काम करता है यह समझने का प्रयास जीवित और अच्छी तरह से है ।
इस प्रयास को मुख्य रूप से “संक्रामक तंत्रिका विज्ञान” के रूप में जाना जाता है और यह गहन अध्ययन से अध्ययन का एक अलग क्षेत्र है ।
शोधकर्ताओं के लिए दोनों क्षेत्रों के बीच बार - बार आगे बढ़ना आम बात है ।
गहन अधिगम का क्षेत्र मुख्य रूप से इस बात से संबंधित है कि कैसे ऐसे कंप्यूटर सिस्टम का निर्माण किया जाए जो बुद्धि की आवश्यकता वाले कार्यों को सफलतापूर्वक हल करने में सक्षम हों , जबकि कम्प्यूटेशनल तंत्रिका विज्ञान का क्षेत्र मुख्य रूप से मस्तिष्क वास्तव में कैसे कार्य करता है इसके अधिक सटीक मॉडल बनाने से संबंधित है ।
1980 के दशक में , तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर बड़े भाग में कनेक्शनवाद , या समानांतर वितरित प्रक्रिया - नामक आंदोलन के माध्यम से उभरी .


संज्ञानात्मक विज्ञान के संदर्भ में कनेक्शनवाद का उदय हुआ .
संज्ञानात्मक विज्ञान मन को समझने के लिए एक अंतर्विषयक दृष्टिकोण है , विश्लेषण के कई अलग अलग स्तरों के संयोजन .
1980 के दशक के प्रारंभ में , अधिकांश संज्ञानात्मक वैज्ञानिकों ने प्रतीकात्मक तर्क के मॉडलों का अध्ययन किया .
अपनी लोकप्रियता के बावजूद , प्रतीकात्मक मॉडल कैसे वास्तव में न्यूरॉन्स का उपयोग कर उन्हें लागू कर सकते हैं के संदर्भ में व्याख्या करने के लिए मुश्किल थे .
कनेक्शनिस्टों ने संज्ञान के ऐसे मॉडलों का अध्ययन करना शुरू किया जो वास्तव में तंत्रिका कार्यान्वयनों (Touretzky और मिंटन , 1985 ) में आधारित हो सकते थे , जो 1940 के दशक में मनोविज्ञानी डोनाल्ड हेब के कार्य से संबंधित कई विचारों को पुनः प्राप्त कर रहे थे .
कनेक्शनवाद में केंद्रीय विचार यह है कि सरल कम्प्यूटेशनल इकाइयों की एक बड़ी संख्या बुद्धिमान व्यवहार प्राप्त कर सकते हैं जब एक साथ नेटवर्क .
यह अंतर्दृष्टि जैविक तंत्रिका प्रणालियों में न्यूरॉन्स के लिए समान रूप से लागू होती है , जैसा कि यह कम्प्यूटेशनल मॉडलों में छुपी इकाइयों के लिए करता है .
1980 के दशक के संबंधवाद आंदोलन के दौरान कई प्रमुख अवधारणाएं उत्पन्न हुईं जो आज के गहन अध्ययन का केंद्र बनी हुई हैं ।
इनमें से एक अवधारणा वितरित प्रतिनिधित्व की है ( हैनटन एट अल , 1986 )
यह विचार है कि एक सिस्टम के लिए प्रत्येक इनपुट कई सुविधाओं द्वारा प्रतिनिधित्व किया जाना चाहिए , और प्रत्येक विशेषता कई संभावित इनपुट के प्रतिनिधित्व में शामिल होना चाहिए .
उदाहरण के लिए , मान लीजिए कि हमारे पास एक दृष्टि प्रणाली है जो 16 CHAPTER 1 को पहचान सकती है ।
INTRODUCING कारों , ट्रकों , और पक्षियों , और इन वस्तुओं प्रत्येक लाल , हरे , या नीले हो सकता है
इन आदानों का प्रतिनिधित्व करने का एक तरीका होगा एक अलग न्यूरॉन या छिपा इकाई है कि नौ संभावित संयोजनों में से प्रत्येक के लिए सक्रिय करता हैः लाल ट्रक , लाल कार , लाल पक्षी , हरे ट्रक , आदि
इसके लिए नौ विभिन्न न्यूरॉन्स की आवश्यकता होती है , और प्रत्येक न्यूरॉन को स्वतंत्र रूप से रंग और वस्तु पहचान की अवधारणा सीखनी चाहिए .
इस स्थिति में सुधार के लिए एक तरीका है एक वितरित प्रतिनिधित्व का उपयोग करने के लिए , तीन न्यूरॉन्स के साथ वस्तु पहचान का वर्णन रंग और तीन न्यूरॉन्स के साथ .
इसके लिए नौ के स्थान पर केवल छह न्यूरॉन्स की आवश्यकता होती है , और लालिमा का वर्णन करने वाला न्यूरॉन कारों , ट्रकों और पक्षियों की छवियों से लालिमा के बारे में जानने में सक्षम है , न कि केवल वस्तुओं की एक विशिष्ट श्रेणी की छवियों से .
वितरित प्रतिनिधित्व की अवधारणा इस पुस्तक के लिए केंद्रीय है और अध्याय 15 में अधिक विस्तार से वर्णित है .
कनेक्शनिस्ट आंदोलन की एक अन्य प्रमुख उपलब्धि थी , गहरे न्यूट्रल नेटवर्कों को प्रशिक्षित करने के लिए बैकप्रोप का उपयोग , बैकप्रोपेशन एल्गोरिदम एल्गोरिदम एल्गोरिदम एल्गोरिदम एल्गोरिदम , 1986
यह एल्गोरिथ्म लोकप्रियता में वृद्धि और गिरावट आई है , लेकिन इस लेखन के रूप में , गहरे मॉडलों के प्रशिक्षण के लिए प्रमुख दृष्टिकोण है .
1990 के दशक के दौरान , शोधकर्ताओं ने तंत्रिका नेटवर्क के साथ मॉडलिंग अनुक्रम में महत्वपूर्ण प्रगति की .
होक्रिटर
(1994 ई . )
होकरिटर और श्मिटहुबर (1997 )
आज , एलएसटीएम व्यापक रूप से कई अनुक्रम मॉडलिंग कार्यों के लिए इस्तेमाल किया जाता है , गूगल पर कई प्राकृतिक भाषा प्रसंस्करण कार्यों सहित .
तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर1990 के दशक के मध्य तक चली .
तंत्रिका नेटवर्क तथा अन्य एआई प्रौद्योगिकियों पर आधारित वीन टर्स ने निवेश की मांग करते समय अवास्तविक सुवाह्य महत्वाकांक्षी दावे करना शुरू कर दिया ।
जब एआई शोध ने इन अनुचित उम्मीदों को पूरा नहीं किया तो निवेशक निराश हो गए ।
इसके साथ ही मशीनी शिक्षण के अन्य क्षेत्रों ने भी प्रगति की ।

इन दो कारकों से तंत्रिका नेटवर्क की लोकप्रियता में गिरावट आई जो 2007 तक चली .
इस दौरान , तंत्रिका नेटवर्क कुछ कार्यों पर प्रभावशाली प्रदर्शन प्राप्त करते रहे (
लेकुन एट अल
अल

इस कार्यक्रम में जेफ्री हाइनटन के नेतृत्व में टोरंटो विश्वविद्यालय , मांट्रियल विश्वविद्यालय में योशु बेंजियो , और न्यूयॉर्क विश्वविद्यालय में यान लेकुन के नेतृत्व में संयुक्त मशीन शिक्षण अनुसंधान समूहों को शामिल किया गया .
बहुसांस्कृतिक सीआईएफएपी अनुसंधान पहल 17 सीएचएपीटर 1
INTRODUCING में तंत्रिका विज्ञानी और मानव और कंप्यूटर दृष्टि के विशेषज्ञ भी शामिल थे ।
इस बिंदु पर , गहरे नेटवर्क आम तौर पर माना जाता था कि प्रशिक्षण के लिए बहुत मुश्किल है .
अब हम जानते हैं कि एल्गोरिदम है कि 1980 के दशक के बाद से मौजूद है काफी अच्छी तरह से काम करते हैं , लेकिन यह स्पष्ट सिर्का 2006 नहीं था .
मुद्दा शायद सिर्फ यह है कि ये एल्गोरिदम इतने कम्प्यूटेशनल रूप से महंगे थे कि उस समय उपलब्ध हार्डवेयर के साथ बहुत अधिक प्रयोग की अनुमति नहीं दे सकते थे .
तंत्रिका नेटवर्क अनुसंधान की तीसरी लहर 2006 में एक सफलता के साथ शुरू हुई .
जिओफ्री हाइनटन ने दिखाया कि एक प्रकार का तंत्रिका नेटवर्क जिसे गहरा विश्वास नेटवर्क कहा जाता है , लालची परत - वार पूर्वप्रवर्तन ( हैनटन एट अल 2006 ) नामक रणनीति का उपयोग करके कुशलतापूर्वक प्रशिक्षित किया जा सकता है , जिसका हम अधिक विस्तार से खंड 151 में वर्णन करते हैं .
अन्य संबद्ध अनुसंधान शीघ्रता से वही कार्यनीति शीघ्रता से प्रदर्शित की जा सकती है ( वही कार्यनीति शीघ्रता से प्रदर्शित की जा सकती है )
अनुसंधान की इस लहर ने इस बात पर जोर देने के लिए “देईप शिक्षण” शब्द का प्रयोग लोकप्रिय बनाया कि शोधकर्ता अब गहन तंत्रिका तंत्र को प्रशिक्षित करने में सक्षम हो गए हैं , इससे पहले कि शोधकर्ताओं ने इसके पहले और अधिक गहन तंत्रिका तंत्रों को प्रशिक्षित किया था ।
इस समय , गहरे तंत्रिका नेटवर्कों ने अन्य मशीनी शिक्षण प्रौद्योगिकियों के साथ - साथ हस्तनिर्मित कार्यक्षमता पर आधारित एआई प्रणालियों को आगे बढ़ाया .
तंत्रिका नेटवर्क की लोकप्रियता की यह तीसरी लहर इस लेखन के समय तक बनी हुई है , हालांकि गहन शिक्षण अनुसंधान का ध्यान इस लहर के समय के भीतर नाटकीय रूप से बदल गया है .
तीसरी लहर नई अनुपयुक्त अधिगम तकनीकों और गहरे मॉडलों की छोटे डेटासेटों से अच्छी तरह से सामान्यीकरण करने की क्षमता पर ध्यान केंद्रित के साथ शुरू हुई , लेकिन आज बहुत पुराने पर्यवेक्षित अधिगम एल्गोरिदम और बड़े लेबल वाले डेटा को कम करने के लिए गहरे मॉडलों की क्षमता में अधिक रुचि है .
1 . 2
बढ़ते डेटासेट आकार
आश्चर्य की बात यह है कि अभी हाल ही में गहन शिक्षण को एक महत्वपूर्ण प्रौद्योगिकी के रूप में मान्यता क्यों मिली है , भले ही 1950 के दशक में कृत्रिम तंत्रिका नेटवर्कों के साथ पहले प्रयोग किए गए थे ।
गहन शिक्षण वाणिज्यिक अनुप्रयोगों में 1990 के दशक के बाद से सफलतापूर्वक इस्तेमाल किया गया है , लेकिन अक्सर एक प्रौद्योगिकी और कुछ है कि केवल एक विशेषज्ञ का उपयोग कर सकते हैं की तुलना में एक कला के अधिक माना जाता था , हाल ही में
यह सच है कि एक गहरी सीखने एल्गोरिथ्म से अच्छा प्रदर्शन प्राप्त करने के लिए कुछ कौशल की आवश्यकता है .
सौभाग्य से , प्रशिक्षण डेटा की मात्रा में वृद्धि के रूप में आवश्यक कौशल की मात्रा को कम कर देता है .
आज जटिल कार्यों पर मानव प्रदर्शन तक पहुंचने वाली शिक्षण एल्गोरिदम लगभग सीखने की एल्गोरिदम के समान है जो 1980 के दशक में खिलौना समस्याओं को हल करने के लिए संघर्ष करती थी , हालांकि इन एल्गोरिदम के साथ हम जिन मॉडलों को प्रशिक्षित करते हैं उनमें 18 CHAPTER 1
INTRODUCTION में ऐसे बदलाव किए गए जो बहुत गहरे वास्तुशिल्पों के प्रशिक्षण को सरल बनाते हैं ।
सबसे महत्वपूर्ण नया विकास है कि आज हम इन एल्गोरिदम संसाधनों के साथ प्रदान कर सकते हैं वे सफल होने की जरूरत है .
आंकड़े 1 . 8 से पता चलता है कि कैसे बेंचमार्क डेटासेट के आकार का समय के साथ उल्लेखनीय विस्तार हुआ है ।
यह प्रवृत्ति समाज के बढ़ते डिजिटाइजेशन से प्रेरित है ।
हमारी अधिक से अधिक गतिविधियों के रूप में कंप्यूटर पर जगह लेता है , अधिक से अधिक हम क्या करते हैं दर्ज की जाती है .
जैसे - जैसे हमारे कंप्यूटरों का नेटवर्क बढ़ता जा रहा है , इन अभिलेखों को केंद्र में रखकर मशीनी शिक्षण अनुप्रयोगों के लिए उपयुक्त डेटासेट में ठीक करना आसान होता जा रहा है ।
“बिग डेटा” 1900 1950 2015 वर्ष 10 0 10 10 10 10 10 10 10 10 10 10 10 10 10 10 9 डेटासेट आकार ( संख्या उदाहरण

पब्लिक एसवीएचएन इमेजनेट
सीआईएफएआर - 10
Name

समय के साथ बढ़ती डेटासेट आकार .
संकलित आंकड़ों में संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित आंकड़ों में , संकलित
१९८० के दशक में , जैविक रूप से प्रेरित मशीनी शिक्षण के अग्रदूत अक्सर छोटे सिंथेटिक डेटासेट के साथ काम करते थे , जैसे कि निम्न रिसोल्यूशन बिटमैप , जो निम्न संगणनात्मक लागत और १९८६ के निम्न संगणनात्मक कार्यों को सीखने में सक्षम थे .
१९८० और १९९० के दशक में मशीनी शिक्षण अधिक सांख्यिकीय हो गया और दसियों हज़ार उदाहरणों वाले बड़े डेटासेटों का लाभ उठाने लगा , जैसे कि MNIST डेटासेट ( हस्तलिखित संख्याओं के स्कैन १९८० के अंक में )
2000 के दशक के पहले दशक में , इस आकार के अधिक परिष्कृत डेटासेट , जैसे CIFAR - 10 डेटासेट ( Krizhevsky और Hinton , 2009 ) का उत्पादन जारी रहा .
उस दशक के अंत में और 2010 के दशक के पूर्वार्ध में , उल्लेखनीय रूप से बड़े डेटासेट , जिसमें सैकड़ों हजारों से दसियों लाख उदाहरण थे , पूरी तरह से बदल गया जो गहन अध्ययन के साथ संभव था .

एम डेटासेट

ग्राफ के शीर्ष पर , हम देखते हैं कि अनुवादित वाक्यों के डेटासेट , जैसे कि आईबीएम के डेटासेट का निर्माण कनाडा के हंसार्ड ( ब्राउन एट अल , 1990 ) और डब्ल्यूएमटी 2014 अंग्रेजी से फ्रेंच डेटासेट ( Schwenk 2014 ) के अन्य डेटासेट से काफी आगे है .
19 CHAPTER 1 .
Comment
MNIST डेटासेट से उदाहरण इनपुट
“NIST” राष्ट्रीय मानक और प्रौद्योगिकी संस्थान , एजेंसी के लिए खड़ा है जो मूल रूप से इस डेटा को एकत्रित करती थी .
“M” मशीन लर्निंग एल्गोरिदम के साथ आसान उपयोग के लिए डेटा का पूर्वप्रक्रमण किया गया है के बाद से “M” के लिए खड़ा है .
MNIST डेटासेट में हस्तलिखित अंकों और संबद्ध लेबलों के स्कैन होते हैं , जिनमें यह बताया जाता है कि प्रत्येक छवि में कौन सा अंक 0 - 9 निहित है ।
यह सरल वर्गीकरण समस्या गहन शिक्षण अनुसंधान में सबसे सरल और सबसे व्यापक रूप से इस्तेमाल किया परीक्षणों में से एक है .
यह आधुनिक तकनीकों को हल करने के लिए काफी आसान होने के बावजूद लोकप्रिय रहता है .
जिओफ्री हाइनटन ने इसे “ मशीन लर्निंग का डेरोसोफिला” कहा है , इसका अर्थ है कि यह मशीन लर्निंग शोधकर्ताओं को नियंत्रित प्रयोगशाला स्थितियों में उनके एल्गोरिदम का अध्ययन करने में सक्षम बनाता है , बहुत कुछ जीवविज्ञानी अक्सर फल मक्खियों का अध्ययन करते हैं .
मशीनी शिक्षण को काफी आसान बना दिया है क्योंकि सांख्यिकीय आकलन का मुख्य बोझ - केवल थोड़ी मात्रा में डेटा देखने के बाद नए डेटा को अच्छी तरह से उत्पन्न करना - काफी हल्का हो गया है ।
वर्ष 2016 तक , अंगूठे का एक मोटा नियम यह है कि पर्यवेक्षित गहन शिक्षण एल्गोरिथ्म सामान्यतः प्रति श्रेणी लगभग 5 , 000 लेबल वाले उदाहरणों के साथ स्वीकार्य प्रदर्शन प्राप्त करेगा और 20 सीएचएपीटर 1 से मेल खाता है ।
INTRODUCING मानव प्रदर्शन से अधिक है जब कम से कम 10 मिलियन लेबल वाले उदाहरणों वाले डेटासेट के साथ प्रशिक्षित किया जाता है .
इससे छोटे डेटासेट के साथ सफलतापूर्वक कार्य करना एक महत्वपूर्ण अनुसंधान क्षेत्र है , जो विशेष रूप से इस बात पर ध्यान केंद्रित करता है कि हम बड़ी मात्रा में अविश्वसनीय उदाहरणों का लाभ कैसे उठा सकते हैं , जिसमें अनुपयुक्त या अर्ध - प्रतीक्षित अधिगम होता है ।
1 . 2 . 3
बढ़ते मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद तंत्रिका नेटवर्क बेतहाशा सफल हैं कि हम कम्प्यूटेशनल संसाधनों आज बहुत बड़े मॉडल चलाने के लिए
कनेक्शन - इसम की एक मुख्य अंतर्दृष्टि है कि जानवर बुद्धिमान हो जाते हैं जब उनके कई न्यूरॉन्स एक साथ काम करते हैं .
एक व्यक्तिगत न्यूरॉन या न्यूरॉन्स के छोटे संग्रह विशेष रूप से उपयोगी नहीं है .
जैविक न्यूरॉन्स विशेष रूप से सघन रूप से जुड़े नहीं हैं .
जैसा कि आंकड़ा 1 .10 में देखा गया है , हमारे मशीन सीखने के मॉडलों में दशकों से भी स्तनधारी मस्तिष्क के परिमाण के एक क्रम के भीतर प्रति न्यूरॉन कई कनेक्शन रहे हैं .
न्यूरॉन्स की कुल संख्या के संदर्भ में , तंत्रिका नेटवर्क आश्चर्यजनक रूप से काफी हाल तक छोटे रहे हैं , जैसा कि 1 .11 अंक में दिखाया गया है .
गुप्त इकाइयों की शुरुआत के बाद से , कृत्रिम तंत्रिका नेटवर्क मोटे तौर पर हर 2 . 4 साल में आकार में दोगुना हो गया है .
यह वृद्धि बड़ी स्मृति वाले तीव्र कंप्यूटरों और बड़े डेटासेटों की उपलब्धता से प्रेरित होती है ।
बड़े नेटवर्क अधिक जटिल कार्यों पर अधिक सटीकता प्राप्त करने में सक्षम होते हैं .
यह प्रवृत्ति दशकों तक चलती नजर आ रही है ।
जब तक नई प्रौद्योगिकियां तेजी से स्केलिंग सक्षम नहीं करती , कृत्रिम तंत्रिका नेटवर्कों में कम से कम 2050 के दशक तक मानव मस्तिष्क के समान न्यूरॉन्स नहीं होंगे .
जैविक न्यूरॉन्स वर्तमान कृत्रिम न्यूरॉन्स की तुलना में अधिक जटिल कार्यों का प्रतिनिधित्व कर सकते हैं , इसलिए जैविक तंत्रिका नेटवर्क इस भूखंड चित्रण से भी बड़ा हो सकता है .
पूर्वव्यापी में , यह विशेष रूप से आश्चर्य की बात नहीं है कि जोंक की तुलना में कम न्यूरॉन्स वाले तंत्रिका नेटवर्क परिष्कृत कृत्रिम बुद्धि प्रोब - लेम्स को हल करने में असमर्थ थे .
आज के नेटवर्क भी , जिन्हें हम कम्प्यूटेशनल सिस्टम की दृष्टि से काफी बड़ा मानते हैं , मेंढक जैसे अपेक्षाकृत आदिम कशेरुकी प्राणियों के तंत्रिका तंत्र से छोटे हैं ।
समय के साथ मॉडल आकार में वृद्धि , तीव्र सीपीयू की उपलब्धता के कारण , सामान्य प्रयोजन जीपीयू ( सेक्शन 12 . 12 में उल्लिखित ) , तीव्र नेटवर्क कनेक्टिविटी और वितरित कंप्यूटिंग के लिए बेहतर सॉफ्टवेयर मूल संरचना , गहन शिक्षण के इतिहास की सबसे महत्वपूर्ण प्रवृत्तियों में से एक है .
यह प्रवृत्ति आम तौर पर भविष्य में अच्छी तरह से जारी रहने की उम्मीद है .
21 CHAPTER 1 .
INTRODUCction 1950 वर्ष 2000 2015 वर्ष 10 10 10 10 10 3 10 4 कनेक्शन प्रति न्यूरॉन 1 2 3 4 5 8 9 फल मक्खी माउस बिल्ली मानव चित्र 1 . 10
: समय के साथ प्रति न्यूरॉन कनेक्शन की संख्या .
प्रारंभ में , कृत्रिम तंत्रिका नेटवर्क में न्यूरॉन्स के बीच कॉन्नेक - टीनों की संख्या हार्डवेयर क्षमताओं द्वारा सीमित थी .
आज , न्यूरॉन्स के बीच कनेक्शन की संख्या ज्यादातर एक डिजाइन विचार है .
कुछ कृत्रिम तंत्रिका नेटवर्कों में प्रति न्यूरॉन लगभग उतने ही कनेक्शन होते हैं जितने एक बिल्ली के रूप में होते हैं , और अन्य तंत्रिका नेटवर्कों के लिए प्रति न्यूरॉन उतने ही कनेक्शन होना आम बात है जितने छोटे स्तनधारी जैसे चूहे ।
यहां तक कि मानव मस्तिष्क प्रति न्यूरॉन कनेक्शन की एक अनाप - शनाप मात्रा नहीं है .
विकिपीडिया से जैविक तंत्रिका नेटवर्क आकार (2015 )
१ .
अनुकूली रैखिक तत्व
नियोकोग्निटॉन


डीप बोल्ट्जमैन मशीन
अप्रयुक्त संवलयन नेटवर्क (
जेरेट एट अल


↑ 2010 का
वितरित स्वचालित
ली एट अल
↑ 2012 का 8 .
बहु - जीपीयू संवलयन नेटवर्क (
Krizhevsky एट अल .
↑ 2012 का 9 .
COTS HPC अनुपयुक्त संवलयन नेटवर्क (
कोट्स एट अल , 2013 10
गोगलीनेट
1 . 2 . 4
बढ़ती हुई परिशुद्धता , जटिलता और वास्तविक - वर्टल्ड प्रभाव
1980 के दशक के बाद से , गहरी सीखने सटीक मान्यता और भविष्यवाणी प्रदान करने की अपनी क्षमता में लगातार सुधार हुआ है .
इसके अलावा , गहरी सीखने लगातार अनुप्रयोगों के व्यापक और व्यापक सेट के लिए सफलता के साथ लागू किया गया है .
सबसे पहले गहरे मॉडलों का प्रयोग व्यक्तिगत वस्तुओं को कसकर फसल में पहचानने के लिए किया जाता था , अत्यंत छोटी छवियों (
रूमाहार्ट एट अल , 1986
तब से वहाँ धीरे धीरे छवियों तंत्रिका नेटवर्क के आकार में वृद्धि हुई है प्रक्रिया कर सकते हैं .
आधुनिक वस्तु पहचान नेटवर्क समृद्ध उच्च - पुनर्संयोजन तस्वीरों की प्रक्रिया करता है और 22 CHAPTER 1 नहीं करता है ।
आईटीआरओडीयूसी 1950 वर्ष 2015 2056 वर्ष 10 −2 10 −1
10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10
10 11 नंबर के न्यूरॉन्स (logarithmic पैमाने पर
1

गुप्त इकाइयों की शुरुआत के बाद से , कृत्रिम तंत्रिका नेटवर्क मोटे तौर पर हर 2 . 4 साल में आकार में दोगुना हो गया है .
विकिपीडिया से जैविक तंत्रिका नेटवर्क आकार (2015 )
१ .
पर्स्टेरॉन
अनुकूली रैखिक तत्व
नियोकोग्निटॉन
प्रारंभिक पश्च - उत्पादन नेटवर्क
भाषण मान्यता के लिए आवर्ती तंत्रिका नेटवर्क
६
भाषण मान्यता के लिए मल्टीलेयर पर्सेप्ट्रॉन
१९९१ का
माध्य क्षेत्र अवग्रह विश्वास नेटवर्क
८ .
लीनेट ५

१९९८ का

१० . १०
डीप विश्वास नेटवर्क


डीप बोल्ट्जमैन मशीन
जीपीयू
रैना एट अल
↑ 2009 का , 14 .
अप्रयुक्त संवलयन नेटवर्क (
जेरेट एट अल


16 . 16 . 16 . 16 . 16 . 16 . 16
ओएमपी - 1 नेटवर्क
वितरित स्वचालित
ली एट अल , 2012
बहु - जीपीयू संवलयन नेटवर्क (
Krizhevsky एट अल .
↑ 2012 का 19 वां
COTS HPC अनुपयुक्त संवलयन नेटवर्क (
कोट्स एट अल , २०१३

इसी प्रकार , प्रारंभिक नेटवर्क केवल दो प्रकार की वस्तुओं को पहचान सकते थे ( कुछ मामलों में , एक ही प्रकार की वस्तु की अनुपस्थिति या उपस्थिति , जबकि ये आधुनिक नेटवर्क आम तौर पर वस्तुओं की कम से कम 1 , 000 विभिन्न श्रेणियों को पहचानते हैं .
वस्तु मान्यता में सबसे बड़ा मुकाबला इमेजनेट 23 CHAPTER 1 है ।
INTRODUC
बड़े पैमाने पर दृश्य अभिज्ञान चैलेंज (आईएलएसवीआरसीसी ) प्रत्येक वर्ष आयोजित किया जाता है .
गहरे शिक्षण के मौसमी उत्थान में एक नाटकीय क्षण आया जब एक संवलयिक नेटवर्क ने पहली बार इस चुनौती को जीता और एक व्यापक मार्जिन से , राज्य के सबसे ऊपर - 5 त्रुटि दर को 261 प्रतिशत से घटाकर 153 प्रतिशत (
तब से , इन प्रतियोगिताओं को लगातार गहरे संवलयिक जालों द्वारा जीता जाता है , और इस लेखन के रूप में , गहन अध्ययन में अग्रिम इस प्रतियोगिता में नवीनतम शीर्ष - ५ त्रुटि दर को 3 . 6 प्रतिशत तक ले आया है , जैसा कि अंक 112 में दिखाया गया है .
गहरी शिक्षा का भाषण मान्यता पर भी नाटकीय प्रभाव पड़ा है ।
1990 के दशक भर में सुधार के बाद , भाषण मान्यता के लिए त्रुटि दरों लगभग 2000 में बंद कर दिया .
परिचय
हम इस इतिहास की अधिक विस्तार से धारा १२ . ३ में खोज करते हैं ।
डीप नेटवर्क को पैदल पता लगाने और छवि विभाजन के लिए भी शानदार सफलताएं मिली हैं ।


साथ ही , कि गहरे नेटवर्क के पैमाने और सटीकता में वृद्धि हुई है , 2010 2011 2013 2014 वर्ष 0 . 0 .05 0 .10 0 .15
0 .20 0 .25 0 .30 ILSVRC वर्गीकरण त्रुटि दर चित्र 1 .12 : समय के साथ त्रुटि दर घटाना
चूंकि गहरे नेटवर्क इमेजनेट बड़े पैमाने पर दृश्य मान्यता चैलेंज में प्रतिस्पर्धा करने के लिए आवश्यक पैमाने पर पहुंचे , उन्होंने लगातार प्रतिवर्ष प्रतियोगिता जीती है , जिससे हर बार निम्न और निम्न त्रुटि दर प्राप्त होती है .
रसाकोवस्की एट अल का डाटा
(2014 )
और वह एट अल
(2015 )
24 CHAPTER 1 .
INTRODUC फ़ोल्डर में उन कार्यों की जटिलता है जिन्हें वे हल कर सकते हैं .
गुडफेलो एट अल

पहले , यह व्यापक रूप से माना जाता था कि इस तरह के सीखने के लिए अनुक्रम के व्यक्तिगत तत्वों की लेबलिंग आवश्यक है (Gülçhre और बेंगीओ , 2013
पुनरावर्ती तंत्रिका नेटवर्क , जैसे कि एलएसटीएम अनुक्रम मॉडल , जिसका उल्लेख ऊपर किया गया है , अब केवल निश्चित इनपुट के बजाय अनुक्रमों और अन्य अनुक्रमों के बीच संबंधों को मॉडल करने के लिए प्रयोग किया जाता है .
यह अनुक्रम - से - अनुक्रम अधिगम किसी अन्य अनुप्रयोग में क्रांति लाने के कगार पर प्रतीत होता हैः मशीनी अनुवाद

जटिलता में वृद्धि की इस प्रवृत्ति को तंत्रिका ट्यूरिंग मशीनों ( Graves एट अल , 2014 ) की शुरुआत के साथ अपने तार्किक निष्कर्ष पर धकेल दिया गया है जो स्मृति कोशिकाओं से पढ़ना सीखते हैं और स्मृति कोशिकाओं को मनमानी सामग्री लिखते हैं .
इस तरह के तंत्रिका नेटवर्क वांछित व्यवहार के उदाहरण से सरल प्रोग्राम सीख सकते हैं .
उदाहरण के लिए , वे हाथापाई और अनुक्रमों के उदाहरण दिए गए संख्याओं की सूचियों को छांटना सीख सकते हैं .
यह आत्म - प्रदर्शन प्रौद्योगिकी अपनी शैशवावस्था में है , लेकिन भविष्य में इसे सैद्धांतिक रूप से लगभग किसी भी कार्य के लिए लागू किया जा सकता है .
गहन अधिगम की एक और प्रमुख उपलब्धि है उसका प्रवर्तन अधिगम के क्षेत्र में विस्तार ।
प्रवर्तन अधिगम के संदर्भ में , एक स्वायत्त अभिकर्ता को परीक्षण और त्रुटि द्वारा कार्य करना सीखना चाहिए , मानव प्रचालक से बिना किसी मार्गदर्शन के ।
डीपमिंड ने यह प्रदर्शित किया कि गहन अधिगम पर आधारित एक प्रवर्तन अधिगम प्रणाली अतरी वीडियो गेम खेलने के लिए सीखने में सक्षम है , जो कई कार्यों पर मानव स्तर के प्रदर्शन तक पहुंचता है ।

गहन अध्ययन ने रोबोटिक्स ( फिन एट अल , 2015 ) के लिए पुनः प्रवर्तन शिक्षण के प्रदर्शन में भी उल्लेखनीय सुधार किया है ।
गहन शिक्षण के इन अनुप्रयोगों में से कई अत्यधिक लाभप्रद हैं .
डीप लर्निंग का उपयोग अब कई शीर्ष प्रौद्योगिकी कंपनियों द्वारा किया जाता है , जिनमें गूगल , माइक्रोसॉफ्ट , फेसबुक , आईबीएम , बैदु , एप्पल , एडोब , नेटफ्लिक्स , एनवीआईए , और एनईसी शामिल हैं
गहन अध्ययन में प्रगति भी सॉफ्टवेयर मूल संरचना के अग्रिमों पर काफी निर्भर रही है ।



टेन्सरफ्लो
गहन अध्ययन ने अन्य विज्ञानों में भी योगदान दिया है ।
वस्तु अभिज्ञान के लिए आधुनिक संवलित नेटवर्क दृश्य प्रक्रमण का एक मॉडल प्रदान करते हैं जिसका तंत्रिका विज्ञानी अध्ययन कर सकते हैं (DiCarlo , 2013 )
गहन अध्ययन भी डेटा की भारी मात्रा के प्रसंस्करण और वैज्ञानिक 25 CHAPTER 1 में उपयोगी भविष्यवाणियां करने के लिए उपयोगी उपकरण प्रदान करता है
Comment
इसका उपयोग यह भविष्यवाणी करने के लिए सफलतापूर्वक किया गया है कि कैसे अणु दवा कंपनियों को नई दवाएं डिजाइन करने में मदद करने के लिए बातचीत करेंगे ( Dahl एट अल , 2014 ) , उप - भौतिक कणों की खोज करने के लिए ( Baldiet al . 2014 )
हम उम्मीद करते हैं कि भविष्य में अधिक से अधिक वैज्ञानिक क्षेत्रों में गहरी शिक्षा का प्रादुर्भाव होगा ।
सारांश में , गहन अधिगम मशीन अधिगम का एक दृष्टिकोण है जिसने पिछले कई दशकों में विकसित मानव मस्तिष्क , सांख्यिकी और अनुप्रयुक्त गणित के हमारे ज्ञान पर भारी प्रभाव डाला है ।
हाल के वर्षों में , गहन अधिगम ने अपनी लोकप्रियता और उपयोगिता में अत्यधिक वृद्धि देखी है , जो अधिकांशतः अधिक शक्तिशाली कंप्यूटरों , बड़े डेटासेटों और तकनीकों के परिणामस्वरूप गहरे नेटवर्कों को प्रशिक्षित करने के लिए
आगे के वर्ष गहरी सीखने में और भी सुधार लाने और उसे नई सीमाओं तक लाने के लिए चुनौतियों और अवसरों से भरे हुए हैं ।

