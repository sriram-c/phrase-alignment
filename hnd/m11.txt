अध्याय 1 परिचय निवेशकों लंबे समय से सोच मशीनों बनाने के सपने देखा है .</s>
यह इच्छा कम से कम प्राचीन यूनान के समय की है ।</s>
मिथकीय आंकड़े पिग्मलियन , दादलस , और हेफेस्टस सभी को प्रख्यात आविष्कारक और गैलेटा , टैलोस , और पांडोरा सभी को कृत्रिम जीवन माना जा सकता है ( ओविड और मार्टिन , 1997 , काउन्टी , 1996 )</s>
जब प्रोग्राम किये जाने वाले कंप्यूटरों की कल्पना की गई तो लोगों ने सोचा कि क्या ऐसी मशीनें बुद्धिमान हो सकती हैं , एक के निर्माण से सौ साल पहले ( लैवॅल्स , 1842 )</s>
आज कृत्रिम बुद्धिमत्ता का एक फलता - फूलता क्षेत्र है जिसमें कई व्यावहारिक अनुप्रयोग और सक्रिय अनुसंधान विषय हैं ।</s>
हम सामान्य प्रसव को स्वचालित करने के लिए , भाषण या छवियों को समझने के लिए , चिकित्सा में निदान करने के लिए और बुनियादी वैज्ञानिक अनुसंधान का समर्थन करने के लिए बुद्धिमान सॉफ्टवेयर के लिए देखो .</s>
कृत्रिम बुद्धिमत्ता के शुरुआती दिनों में , इस क्षेत्र ने तेजी से उन समस्याओं को हल किया और हल किया जो मनुष्यों के लिए बौद्धिक रूप से कठिन हैं , लेकिन कंप्यूटर के लिए अपेक्षाकृत सीधे - सीधे आगे हैं -</s>
समस्याओं है कि औपचारिक , गणित - कायिक नियमों की एक सूची द्वारा वर्णित किया जा सकता है .</s>
कृत्रिम बुद्धि के लिए सच्ची चुनौती उन कार्यों को हल करने की साबित हुई जो लोगों के लिए आसान हैं , लेकिन औपचारिक रूप से वर्णन करने के लिए लोगों के लिए कठिन हैं - उन प्रतीकों को , जो हम अन्तर्ज्ञानात्मक रूप से हल करते हैं , जो बोलचाल के शब्दों या तस्वीरों में चेहरे को पहचानने की तरह स्वतः महसूस करते हैं ।</s>
यह पुस्तक इन अधिक सहज ज्ञान युक्त समस्याओं के समाधान के बारे में है .</s>
यह समाधान कंप्यूटर को अनुभव से सीखने और अवधारणाओं के एक पदानुक्रम के संदर्भ में दुनिया को समझने की अनुमति देने के लिए है , प्रत्येक अवधारणा के साथ सरल अवधारणाओं के अपने संबंध के माध्यम से परिभाषित .</s>
अनुभव से ज्ञान इकट्ठा करके , यह दृष्टिकोण मानव ऑपरेटरों के लिए उन सभी ज्ञान को औपचारिक रूप से निर्दिष्ट करने की आवश्यकता से बचता है जो कंप्यूटर की आवश्यकता है .</s>
अवधारणाओं का पदानुक्रम कंप्यूटर को जटिल अवधारणाओं को सरल बनाने के द्वारा सीखने में सक्षम बनाता है .</s>
यदि हम एक ग्राफ बनाते हैं जो दिखाता है कि कैसे इन अवधारणाओं 1 अध्याय 1</s>
INTRODUCTION एक दूसरे के ऊपर निर्मित हैं , ग्राफ गहरा है , कई परतों के साथ ।</s>
इस कारण से , हम इस दृष्टिकोण एआई गहरी सीखने के लिए कहते हैं .</s>
एआई की आरंभिक सफलताओं में से कई अपेक्षाकृत बांझ और औपचारिक वातावरण में हुई और कंप्यूटरों की आवश्यकता नहीं थी कि उन्हें दुनिया के बारे में अधिक ज्ञान हो ।</s>
उदाहरण के लिए , आईबीएम की डीप ब्लू शतरंज खेलने की प्रणाली ने 1997 में विश्व चैंपियन गार् कास्परोव को हरा दिया ( 2002 )</s>
शतरंज निस्संदेह एक बहुत ही सरल संसार है , जिसमें केवल साठ - चार स्थान और तीस - तीस टुकड़े होते हैं , जो केवल कठोरता से सीमित तरीकों से चल सकते हैं ।</s>
एक सफल शतरंज रणनीति को बदलना एक बड़ी उपलब्धि है , लेकिन चुनौती यह नहीं है कि शतरंज के टुकड़े के सेट का वर्णन करने में कठिनाई और कंप्यूटर के लिए अनुमति देने योग्य चालें .</s>
शतरंज पूरी तरह से औपचारिक नियमों की एक संक्षिप्त सूची द्वारा वर्णित किया जा सकता है , आसानी से प्रोग्रामर द्वारा समय से पहले प्रदान की जाती है .</s>
विडंबना यह है कि एक मानव के लिए सबसे कठिन मानसिक उपक्रमों में से एक अमूर्त और औपचारिक कार्य कंप्यूटर के लिए सबसे आसान है .</s>
कंप्यूटर लंबे समय से सर्वश्रेष्ठ मानव शतरंज खिलाड़ी को भी हराने में सफल रहे हैं , लेकिन हाल ही में उन्होंने वस्तुओं या भाषण को पहचानने के लिए औसत मानव की कुछ योग्यताओं का मिलान करना शुरू कर दिया है .</s>
व्यक्ति की रोजमर्रा की जिंदगी में दुनिया के बारे में काफी ज्ञान की जरूरत होती है ।</s>
इस ज्ञान का अधिकांश हिस्सा व्यक्तिपरक और अन्तर्ज्ञानात्मक है , और इसलिए औपचारिक रूप से व्यक्त करना कठिन है ।</s>
कम्प्यूटरों को बुद्धिमान तरीके से व्यवहार करने के लिए इसी ज्ञान को प्राप्त करने की आवश्यकता है ।</s>
कृत्रिम बुद्धि में एक प्रमुख चुनौती यह है कि इस अनौपचारिक ज्ञान को कंप्यूटर में कैसे लाया जाए ।</s>
कई कृत्रिम खुफिया परियोजनाओं ने औपचारिक भाषाओं में दुनिया के बारे में ज्ञान को कड़ा करने की कोशिश की है ।</s>
तार्किक अनुमान नियमों का उपयोग करके कंप्यूटर इन औपचारिक भाषाओं में कथनों के बारे में स्वतः तर्क दे सकता है .</s>
इसे कृत्रिम बुद्धि के लिए ज्ञान आधार दृष्टिकोण के रूप में जाना जाता है .</s>
इनमें से किसी भी परियोजना को बड़ी सफलता नहीं मिली है ।</s>
इस प्रकार की एक सर्वाधिक प्रसिद्ध परियोजना सीक ( एलेनट और गुहा , 1989 ) है ।</s>
सीसी एक अनुमान इंजन है और सीसीएल नामक भाषा में कथनों का डेटाबेस है ।</s>
इन बयानों को मानव पर्यवेक्षकों के एक स्टाफ द्वारा दर्ज किया जाता है .</s>
यह एक अनिच्छुक प्रक्रिया है ।</s>
लोग दुनिया का सही वर्णन करने के लिए पर्याप्त जटिलता के साथ औपचारिक नियमों को तैयार करने के लिए संघर्ष करते हैं .</s>
उदाहरण के लिए , सिंक सुबह फ्रेड शेवर नामक व्यक्ति के बारे में एक कहानी समझने में विफल रहा ( लिनड , 1992 )</s>
इसका अनुमान इंजन कहानी में एक असंगति का पता लगाता थाः यह जानता था कि लोगों के पास विद्युतीय भाग नहीं हैं , लेकिन फ्रेड के पास एक विद्युत किरण पुंज है , यह मानता था कि “FredWhileSh आचरण” में विद्युतीय भाग निहित हैं .</s>
इसलिए उसने पूछा कि क्या फ्रेड अभी भी एक व्यक्ति था जब वह शेव कर रहा था .</s>
कठोर ज्ञान के आधार पर प्रणालियों के सामने आने वाली कठिनाइयों से यह संकेत मिलता है कि एआई प्रणालियों को अपना ज्ञान प्राप्त करने की क्षमता की आवश्यकता है , 2 अध्याय 1 निकालकर ।</s>
कच्चे आंकड़ों से कार्रवाई पैटर्न</s>
इस क्षमता को मशीनी सीखने के नाम से जाना जाता है ।</s>
मशीनी अधिगम की शुरुआत से कंप्यूटर वास्तविक दुनिया के ज्ञान से जुड़ी समस्याओं से निपटने और ऐसे निर्णय लेने में सक्षम हो गए जो व्यक्तिपरक प्रतीत होते हैं .</s>
लॉजिस्टिक प्रतिगमन नामक एक साधारण मशीन अधिगम एल्गोरिथ्म यह निर्धारित कर सकता है कि सीजेरियन डिलीवरी की सिफारिश करना है या नहीं ।</s>
एक साधारण मशीन लर्निंग एल्गोरिथम जिसे भोले बे कहते हैं , स्पैम ई - मेल से वैध ई - मेल को अलग कर सकता है .</s>
इन सरल मशीन लर्निंग एल्गोरिदम का प्रदर्शन अत्यधिक उन डेटा के प्रतिनिधित्व पर निर्भर करता है जो उन्हें दिए गए हैं .</s>
उदाहरण के लिए , जब संभारी वितरण की सिफारिश करने के लिए लॉजिस्टिक प्रतिगमन का प्रयोग किया जाता है , एआई सिस्टम रोगी की प्रत्यक्ष जांच नहीं करता है .</s>
इसके बजाय , चिकित्सक इस प्रणाली को प्रासंगिक जानकारी के कई टुकड़े बताता है , जैसे गर्भाशय के निशान की उपस्थिति या अनुपस्थिति .</s>
रोगी के प्रतिनिधित्व में शामिल जानकारी के प्रत्येक टुकड़ा एक विशेषता के रूप में जाना जाता है .</s>
लॉजिस्टिक प्रतिगमन से पता चलता है कि रोगी की इन विशेषताओं में से प्रत्येक लक्षण किस प्रकार विभिन्न परिणामों से संबद्ध है ।</s>
हालांकि , यह प्रभाव नहीं कर सकता कि लक्षणों को किसी भी तरह परिभाषित किया जाता है .</s>
यदि संभारी प्रतिगमन को रोगी का एमआरआई स्कैन दिया गया , बजाय चिकित्सक की औपचारिक रिपोर्ट के , यह उपयोगी भविष्यवाणियां करने में सक्षम नहीं होगा .</s>
एमआरआई स्कैन में व्यक्तिगत पिक्सल में प्रसव के दौरान होने वाली किसी भी जटिलताओं के साथ नगण्य सहसंबंध होता है ।</s>
अभ्यावेदन पर यह निर्भरता एक सामान्य घटना है जो कंप्यूटर विज्ञान और यहां तक कि दैनिक जीवन भर दिखाई देती है .</s>
कंप्यूटर विज्ञान में , डेटा के संग्रह की खोज जैसे संक्रियाएं घातीय रूप से तेज गति से आगे बढ़ सकती हैं यदि कोलेक - ट्यून संरचित और अनुक्रमित समझदारी से किया जाए ।</s>
लोग अरबी अंकों पर अंकगणित आसानी से कर सकते हैं लेकिन रोमन अंकों पर अंकगणित को अधिक समय लेने वाले अंक मिलते हैं ।</s>
यह कोई आश्चर्य की बात नहीं है कि प्रतिनिधित्व का चयन मशीन अधिगम एल्गोरिदम के प्रदर्शन पर एक बड़ा प्रभाव है .</s>
एक साधारण दृश्य उदाहरण के लिए , आकृति 1 . 1 देखें .</s>
कई कृत्रिम बुद्धि कार्यों को उस कार्य के लिए निकालने के लिए सुविधाओं के सही सेट डिजाइन करके हल किया जा सकता है , फिर एक सरल मशीन अधिगम एल्गोरिथ्म के लिए इन सुविधाओं को प्रदान करते हैं .</s>
उदाहरण के लिए , ध्वनि से स्पीकर पहचान के लिए एक उपयोगी विशेषता वक्ता के मुखर पथ के आकार का अनुमान है .</s>
यह विशेषता इस बात का प्रबल संकेत देती है कि वक्ता पुरुष है , स्त्री है या बच्चा ।</s>
कई कार्यों के लिए , तथापि , यह जानना कठिन है कि किन विशेषताओं को निकाला जाना चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हम तस्वीरों में कारों का पता लगाने के लिए एक प्रोग्राम लिखना चाहेंगे .</s>
हम जानते हैं कि कारों में पहिए होते हैं , इसलिए हम व्हील की उपस्थिति का उपयोग एक विशेषता के रूप में करना चाहते हो सकता है .</s>
दुर्भाग्य से , यह वास्तव में क्या एक पहिया पिक्सेल मूल्यों के संदर्भ में लगता है वर्णन करने के लिए मुश्किल है .</s>
पहिये का आकार सरल ज्यामितीय होता है , लेकिन पहिये पर पड़ने वाली छायाओं , पहिये के धातु भागों से चमकता हुआ सूर्य , कार या वस्तु के 3 अध्याय 1 में होने से इसकी छवि जटिल हो सकती है ।</s>
सूचना अधिकार</s>
 </s>
 </s>
 </s>
 </s>
चित्र 1 . 1</s>
विभिन्न अभ्यावेदनों का उदाहरणः मान लीजिए कि हम डेटा की दो श्रेणियों को अलग करना चाहते हैं एक रेखा खींचकर एक स्कैटरपॉट में उनके बीच एक रेखा खींचकर .</s>
बाईं ओर के प्लॉट में , हम कार्टियन निर्देशांक का उपयोग कर कुछ डेटा का प्रतिनिधित्व करते हैं , और कार्य असंभव है .</s>
दाएं भूखंड में , हम ध्रुवीय निर्देशांक के साथ डेटा का प्रतिनिधित्व करते हैं और कार्य एक ऊर्ध्वाधर लाइन के साथ हल करने के लिए सरल हो जाता है .</s>
</s>
इस समस्या का एक समाधान यह है कि न केवल प्रतिनिधित्व से उत्पादन के लिए मानचित्रण का पता लगाने के लिए मशीन अधिगम का उपयोग किया जाए , बल्कि स्वयं प्रतिनिधित्व भी किया जाए ।</s>
इस दृष्टिकोण प्रतिनिधित्व सीखने के रूप में जाना जाता है .</s>
विद्वत अभ्यावेदन अक्सर हस्त - लिखित अभ्यावेदनों से प्राप्त की जा सकने वाली तुलना में कहीं बेहतर प्रदर्शन का परिणाम होते हैं .</s>
वे भी एआई प्रणालियों को नए कार्यों के लिए तेजी से अनुकूलित करने में सक्षम बनाता है , कम से कम मानवीय हस्तक्षेप के साथ .</s>
एक प्रतिनिधित्व अधिगम एल्गोरिथ्म , मिनट में एक सरल कार्य के लिए , या महीनों में एक जटिल कार्य के लिए सुविधाओं का एक अच्छा सेट की खोज कर सकते हैं .</s>
एक जटिल कार्य के लिए मैन्युअल रूप से डिजाइन विशेषताओं को मानव समय का एक बड़ा सौदा की आवश्यकता है और प्रयास यह शोधकर्ताओं के पूरे समुदाय में दशकों लग सकता है .</s>
एक अभ्यावेदन अधिगम एल्गोरिथ्म का बहुसांस्कृतिक उदाहरण औ - तोंकोडर है ।</s>
एक स्वचलितकोडक , एक एनकोडर फलन का संयोजन है , जो इनपुट डेटा को भिन्न प्रतिनिधित्व में बदलता है , और एक डीकोडर फलन है , जो नए अभ्यावेदन को वापस मूल प्रारूप में बदलता है .</s>
जब इनपुट एनकोडर और उसके बाद डीकोडर के माध्यम से चलाया जाता है , तो स्वचलित कोडकों को अधिक से अधिक जानकारी सुरक्षित रखने के लिए प्रशिक्षित किया जाता है , लेकिन वे भी नए प्रतिनिधित्व के विभिन्न अच्छा गुण है बनाने के लिए प्रशिक्षित किया जाता है .</s>
विभिन्न प्रकार के स्वानुकरणों का लक्ष्य विभिन्न प्रकार के गुण प्राप्त करना होता है ।</s>
जब विशेषताओं या एल्गोरिदम को सीखने के लिए डिजाइन किया जाता है , हमारा लक्ष्य आमतौर पर विभिन्नता के कारकों को अलग करने के लिए है कि मनाया डेटा की व्याख्या करते हैं .</s>
इसमें ४ अध्याय १</s>
INTRODUCTION संदर्भ में , हम “फैक्टर” शब्द का प्रयोग करते हैं , जिससे कि विभिन्न घटकों द्वारा संयुक्त रूप से प्रभावित होने वाले विभिन्न स्रोतों का उल्लेख किया जा सके , आमतौर पर गुणन नहीं होते .</s>
इस तरह के कारकों अक्सर मात्रा नहीं है कि सीधे देखा जाता है .</s>
इसके बजाय , वे या तो अनारक्षित वस्तुओं के रूप में मौजूद हो सकते हैं या भौतिक जगत में अनारक्षित शक्तियों के रूप में , जो देखने योग्य मात्राओं को प्रभावित करते हैं .</s>
वे मानव मस्तिष्क में निर्माणों के रूप में भी मौजूद हो सकते हैं जो परीक्षित आंकड़ों की उपयोगी सरल व्याख्या या अनुमानित कारण प्रदान करते हैं .</s>
वे अवधारणाओं या अमूर्त के रूप में सोचा जा सकता है जो हमें डेटा में समृद्ध विविधता का एहसास करने में मदद करता है .</s>
जब किसी वाक् रिकॉर्डिंग का विश्लेषण किया जाता है तो उसमें वक्ता की आयु , उनके लिंग , लहजे और वे बोल रहे शब्दों को शामिल किया जाता है .</s>
जब किसी कार की छवि का विश्लेषण किया जाता है , तो परिवर्तन के कारकों में कार की स्थिति , उसका रंग , और सूर्य का कोण और चमक शामिल होती है .</s>
कई वास्तविक दुनिया के कृत्रिम खुफिया अनुप्रयोगों में कठिनाई का एक बड़ा कारण यह है कि विभिन्नता के अनेक कारक प्रत्येक आंकड़े को प्रभावित करते हैं जिसका हम पालन कर सकते हैं ।</s>
लाल रंग की कार की इमेज में अलग - अलग पिक्सल रात में काली के काफी करीब हो सकते हैं ।</s>
कार के सिलोफेट का आकार देखने के कोण पर निर्भर करता है ।</s>
अधिकांश अनुप्रयोगों के लिए हमें विभिन्नता के कारकों को अलग करने और उन कारकों को त्यागने की आवश्यकता होती है जिनकी हमें परवाह नहीं है ।</s>
अलबत्ता , कच्चे आंकड़ों से इतनी ऊंची - ऊंची , अमूर्त विशेषताओं को निकालना बहुत मुश्किल हो सकता है ।</s>
विभिन्नता के इन कारकों , जैसे वक्ता के लहजे , को केवल परिष्कृत , लगभग मानवीय समझ का उपयोग करके पहचाना जा सकता है .</s>
जब मूल समस्या को हल करने के लिए प्रतिनिधित्व प्राप्त करना लगभग उतना ही कठिन होता है , तब अभ्यावेदन अधिगम , पहली नजर में , हमारी सहायता करने के लिए प्रतीत नहीं होता ।</s>
गहन अध्ययन इस केंद्रीय समस्या का समाधान करता है प्रतिनिधित्व सीखने में आत्मनिरीक्षण द्वारा , जो अन्य , सरल निरूपण के संदर्भ में व्यक्त किए जाते हैं .</s>
गहन अध्ययन कंप्यूटर को सरल संकल्पनाओं से जटिल अवधारणाओं का निर्माण करने में सक्षम बनाता है ।</s>
चित्र 1 . 2 से पता चलता है कि कैसे एक गहरी अधिगम प्रणाली सरल अवधारणाओं , जैसे कि कोनों और कंटूर , जो बारी में किनारों के संदर्भ में परिभाषित कर रहे हैं के संयोजन द्वारा एक व्यक्ति की छवि की अवधारणा का प्रतिनिधित्व कर सकते हैं .</s>
एक गहन अधिगम मॉडल का सारभूत उदाहरण है फीडलर गहरा नेटवर्क या मल्टीलेयर पर्सेप्टॉन ( एमएलपी ) ।</s>
Name</s>
फंक्शन कई सरल क्रियाओं की रचना करके बनता है ।</s>
हम इनपुट का एक नया प्रतिनिधित्व प्रदान करने के रूप में एक अलग गणितीय समारोह के प्रत्येक आवेदन के बारे में सोच सकते हैं .</s>
डेटा के लिए सही प्रतिनिधित्व सीखने के विचार गहरा सीखने पर एक प्रति - पहलू प्रदान करता है .</s>
गहन अधिगम पर एक अन्य परिप्रेक्ष्य यह है कि गहराई कंप्यूटर को एक बहुप्रतीक्षित कंप्यूटर प्रोग्राम सीखने में सक्षम बनाता है .</s>
प्रतिनिधित्व की प्रत्येक परत 5 अध्याय 1 के बाद कंप्यूटर की स्मृति की स्थिति के रूप में सोचा जा सकता है .</s>
दृश्य परत ( इनपुट पिक्सल )</s>
प्रथम गुप्त परत</s>
द्वितीय गुप्त परत ( संदूक और संदूक )</s>
उत्कृष्ट पहचान</s>
चित्र 1 . 2 . 2 : गहन अध्ययन मॉडल का चित्रण</s>
एक कंप्यूटर के लिए कच्चे संवेदी इनपुट डेटा का अर्थ समझना कठिन होता है , जैसे कि यह छवि पिक्सेल मूल्यों के संग्रह के रूप में प्रदर्शित होती है .</s>
फंक्शन मैपिंग पिक्सेल के सेट से वस्तु पहचान तक बहुत जटिल है .</s>
इस मानचित्रण को सीखना या उसका मूल्यांकन करना यदि प्रत्यक्ष रूप से हल किया जाए तो असंभव प्रतीत होता है ।</s>
गहन अध्ययन नेस्टेड सरल मानचित्रणों की एक श्रृंखला में वांछित जटिल मानचित्रण को तोड़कर इस कठिनाई को दूर करता है , प्रत्येक मॉडल की एक अलग परत द्वारा वर्णित है .</s>
इनपुट को दृश्य स्तर पर प्रस्तुत किया जाता है , इसलिए नाम दिया जाता है क्योंकि इसमें वे चर होते हैं जो हम निरीक्षण करने में सक्षम होते हैं .</s>
फिर गुप्त परतों की एक श्रृंखला छवि से लगातार अमूर्त विशेषताओं को निकालती है .</s>
इन परतों को “हिdden” कहा जाता है क्योंकि उनके मूल्य डेटा में नहीं दिए गए हैं , क्योंकि मॉडल को उन अवधारणाओं का निर्धारण करना चाहिए जो प्रेक्षित आंकड़ों में संबंधों की व्याख्या करने के लिए उपयोगी हैं .</s>
यहाँ छवियों प्रत्येक गुप्त इकाई द्वारा प्रतिनिधित्व विशेषता की तरह की कल्पना कर रहे हैं .</s>
पिक्सेल को देखते हुए , पहली परत पड़ोसी पिक्सेल की चमक की तुलना करके किनारों को आसानी से पहचान सकती है .</s>
किनारों के प्रथम गुप्त परत के विवरण को देखते हुए , दूसरी गुप्त परत कोनों और विस्तारित संदूकों को आसानी से खोज सकती है , जो कि किनारों के संग्रह के रूप में पहचाने जा सकते हैं .</s>
कोनों और स्पर्शों के संदर्भ में छवि के दूसरे गुप्त परतों के वर्णन को देखते हुए , तीसरी छुपी पर्त विशिष्ट वस्तुओं के विशिष्ट संग्रहों और कोनों का पता लगाकर , विशिष्ट वस्तुओं के संपूर्ण भागों का पता लगा सकती है .</s>
अंत में , छवि के इस वर्णन को वस्तु भागों के संदर्भ में , जिसमें यह समाहित है , छवि में मौजूद वस्तुओं को पहचानने के लिए प्रयोग किया जा सकता है .</s>
ज़ीलर और फर्गस (2014 ) की अनुमति से चित्र प्रस्तुत किये गये ।</s>
6 अध्याय 1</s>
समानांतर रूप से अनुदेशों के एक अन्य सेट को निष्पादित करना ।</s>
अधिक गहराई वाले नेटवर्क अनुक्रम में अधिक अनुदेशों को निष्पादित कर सकते हैं ।</s>
अनुक्रमिक अनुदेश महान शक्ति प्रदान करते हैं , क्योंकि बाद के अनुदेश पूर्व अनुदेशों के परिणामों का उल्लेख कर सकते हैं .</s>
एक्स</s>
गहन अध्ययन के इस दृष्टिकोण को सूत्रित करते हुए , परतों के सक्रियण में सभी सूचनाएं आवश्यक रूप से भिन्नता के कारकों को कूटबद्ध करती हैं जो इनपुट की व्याख्या करते हैं .</s>
अभ्यावेदन में राज्य सूचना भी संग्रहित होती है जो एक प्रोग्राम को निष्पादित करने में सहायता करती है जो इनपुट का बोध करा सकता है .</s>
यह राज्य सूचना एक पारंपरिक कंप्यूटर प्रोग्राम में एक काउंटर या सूचक के अनुरूप हो सकती है .</s>
इसका इनपुट की सामग्री के साथ विशेष रूप से कोई संबंध नहीं है , लेकिन यह मॉडल को अपने प्रसंस्करण को व्यवस्थित करने में मदद करता है .</s>
मॉडल की गहराई को मापने के दो मुख्य तरीके हैं ।</s>
पहला दृश्य अनुक्रमिक अनुदेशों की संख्या पर आधारित है जिसे वास्तुकला का मूल्यांकन करने के लिए निष्पादित किया जाना चाहिए .</s>
हम इसे एक प्रवाह चार्ट के माध्यम से सबसे लंबे पथ की लंबाई के रूप में सोच सकते हैं कि कैसे मॉडल के प्रत्येक आउटपुट की गणना करने के लिए अपने इनपुट दिए हैं का वर्णन करता है .</s>
जिस प्रकार दो समतुल्य कंप्यूटर प्रोग्रामों की लंबाई अलग - अलग होती है , इस आधार पर कि प्रोग्राम किस भाषा में लिखा जाता है , एक ही फंक्शन को भिन्न - भिन्न गहराइयों से फ्लोचार्ट के रूप में खींचा जा सकता है , इस आधार पर कि हम फ्लोचार्ट में अलग - अलग चरणों के रूप में किस प्रकार्यों का प्रयोग करने की अनुमति देते हैं ।</s>
चित्र 1 . 3 इस बात को स्पष्ट करता है कि एक ही वास्तुकला के लिए भाषा का यह विकल्प दो अलग - अलग माप कैसे दे सकता है ।</s>
x 1 x 1 σ w 1 w 1 w</s>
×</s>
x 2</s>
x 2</s>
www . google . nl " and " http : / / www . google . nl / www . google . nl / search ? q = % s</s>
2</s>
w 2 × +</s>
तत्व समुच्चय + ×</s>
σ xx  www</s>
तत्व समुच्चय लॉजिस्टिक प्रतिगमन चित्र 1 . 3 :</s>
कम्प्यूटेशनल रेखांकन का विश्लेषण एक आउटपुट में इनपुट मानचित्रण जहां प्रत्येक नोड एक संक्रिया निष्पादित करता है ।</s>
गहराई इनपुट से आउटपुट तक सबसे लंबा पथ की लंबाई है लेकिन यह उस परिभाषा पर निर्भर करता है जो एक संभावित कम्प्यूटेशनल चरण का गठन करती है .</s>
इन रेखांकन में दर्शाया गया अभिकलन एक लॉजिस्टिक प्रतिगमन मॉडल का आउटपुट है , σ ( w T x ) , जहां σ लॉजिस्टिक अवग्रही फलन है .</s>
यदि हम अपनी कंप्यूटर भाषा के तत्वों के रूप में जोड़ , गुणा और रसद अवग्रहों का उपयोग करें , तो इस मॉडल की गहराई तीन है .</s>
यदि हम लॉजिस्टिक प्रतिगमन को एक तत्व के रूप में ही देखते हैं , तो इस मॉडल में गहराई एक है .</s>
7 अध्याय 1</s>
INTRODUCTION एक अन्य दृष्टिकोण , जिसका प्रयोग गहरे प्रोबबिलिस्टिक मॉडलों द्वारा किया जाता है , एक मॉडल की गहराई को कम्प्यूटेशनल ग्राफ की गहराई नहीं बल्कि ग्राफ की गहराई को दर्शाता है कि अवधारणाओं का एक दूसरे से कैसे संबंध है .</s>
इस मामले में , प्रत्येक अवधारणा के प्रतिनिधित्व की गणना करने के लिए आवश्यक गणनाओं के प्रवाह संचित्र की गहराई स्वयं अवधारणाओं के ग्राफ की तुलना में कहीं अधिक गहरी हो सकती है .</s>
इसका कारण यह है कि सरल अवधारणाओं की प्रणाली की समझ को परिष्कृत किया जा सकता है और अधिक जटिल अवधारणाओं के बारे में जानकारी दी जा सकती है .</s>
उदाहरण के लिए , एक एआई सिस्टम जो छाया में एक आंख के साथ एक चेहरे की छवि देख रहा है शुरू में केवल एक आंख देख सकता है .</s>
यह पता लगाने के बाद कि चेहरा मौजूद है , तब प्रणाली अनुमान लगा सकती है कि दूसरी आंख संभवतः</s>
इस मामले में , अवधारणाओं के ग्राफ में केवल दो परतें शामिल हैं - आंखों के लिए परत और चेहरे के लिए एक परत ।</s>
क्योंकि यह हमेशा स्पष्ट नहीं है कि इन दो दृश्यों में से कौन - सा कम्प्यूटेशनल ग्राफ की गहराई , या प्रोबबिलिटी मॉडलिंग ग्राफ की गहराई - यह सबसे अधिक प्रासंगिक है , और क्योंकि अलग लोग छोटे तत्वों के विभिन्न सेट चुनते हैं जिनसे उनके ग्राफ बनाने के लिए , एक ही सही लंबाई नहीं है ।</s>
न ही किसी मॉडल को “दीप” के रूप में अर्हता प्राप्त करने के लिए कितनी गहराई की आवश्यकता है के बारे में एक आम सहमति है .</s>
तथापि , गहन अधिगम को सुरक्षित रूप से मॉडलों के अध्ययन के रूप में माना जा सकता है जिसमें पारंपरिक मशीन अधिगम की तुलना में या तो सीखा कार्यों या सीखा अवधारणाओं के संयोजन की अधिक मात्रा शामिल होती है .</s>
संक्षेप में , गहन अध्ययन के लिए , इस पुस्तक के विषय में , एआइ के लिए दृष्टिकोण है .</s>
विशेष रूप से , यह मशीन अधिगम का एक प्रकार है , एक तकनीक है कि अनुभव और डेटा के साथ कंप्यूटर प्रणालियों में सुधार करने के लिए सक्षम बनाता है .</s>
हम तर्क देते हैं कि मशीनी शिक्षा ही एआई प्रणालियों के निर्माण का एकमात्र व्यवहार्य तरीका है जो जटिल वास्तविक दुनिया के वातावरण में कार्य कर सकता है ।</s>
गहन अधिगम एक विशेष प्रकार का मशीनी अधिगम है , जो दुनिया को अवधारणाओं के नीड़ित पदानुक्रम के रूप में , सरल अवधारणाओं के संबंध में परिभाषित प्रत्येक अवधारणा के साथ , और कम अमूर्त अवधारणाओं के संदर्भ में परिकलित अधिक अमूर्त प्रतिनिधित्व द्वारा महान शक्ति और लचीलापन प्राप्त करता है ।</s>
चित्र 1 . 4 इन विभिन्न एआई विधाओं के बीच संबंध को स्पष्ट करता है ।</s>
चित्र 1 .5 में प्रत्येक कैसे काम करता है की एक उच्च स्तरीय योजना है ।</s>
१०००</s>
यह पुस्तक किसने पढ़ी ?</s>
यह पुस्तक विभिन्न पाठकों के लिए उपयोगी हो सकती है , लेकिन हमने इसे दो लक्षित दर्शकों को ध्यान में रखकर लिखा ।</s>
इन लक्षित दर्शकों में से एक है विश्वविद्यालय के छात्र ( स्नातक या स्नातक ) मशीन सीखने के बारे में सीखने , जिनमें वे भी शामिल हैं जो गहन अध्ययन और कृत्रिम बुद्धि अनुसंधान में एक कैरियर शुरू कर रहे हैं .</s>
अन्य 8 अध्याय 1</s>
INTRODUCTION AI मशीन लर्निंग प्रतिनिधित्व अधिगम गहन अधिगम उदाहरणः ज्ञान आधार उदाहरणः</s>
तार्किक प्रतिगमन</s>
उदाहरणः</s>
Name</s>
उदाहरणः</s>
MLP चित्र 1 . 4 :</s>
एक वेन आरेख जो दिखाता है कि कितना गहन अधिगम है , एक प्रकार का प्रतिनिधित्व अधिगम , जो बारी में मशीन अधिगम , जो कई के लिए प्रयोग किया जाता है , लेकिन सभी एआई के लिए दृष्टिकोण नहीं है .</s>
वेन आरेख के प्रत्येक खंड में एआई प्रौद्योगिकी का एक उदाहरण शामिल है .</s>
लक्षित दर्शक , सॉफ्टवेयर इंजीनियर होते हैं , जिनके पास मशीन अधिगम या स्थैतिक पृष्ठभूमि नहीं होती , लेकिन वे तेजी से एक प्राप्त करना चाहते हैं और अपने उत्पाद या मंच में गहन अधिगम का उपयोग करना शुरू कर देते हैं .</s>
कंप्यूटर दृष्टि , भाषण और ऑडियो संसाधन , प्राकृतिक भाषा संसाधन , रोबोटिक्स , जैव सूचना विज्ञान और रसायन विज्ञान , वीडियो गेम्स , खोज इंजन , ऑनलाइन विज्ञापन और वित्त सहित कई सॉफ्ट वेयर विधाओं में गहन अध्ययन पहले से ही उपयोगी साबित हुआ है .</s>
इस पुस्तक को तीन भागों में आयोजित किया गया है , ताकि विभिन्न पाठकों को सर्वोत्तम स्थान मिल सके ।</s>
भाग मैं बुनियादी गणितीय उपकरण और मशीन सीखने अवधारणाओं का परिचय .</s>
भाग II में सर्वाधिक स्थापित गहन अधिगम एल्गोरिदमों का वर्णन है , जो अनिवार्य रूप से हल की गई प्रौद्योगिकियां हैं ।</s>
भाग 3 अधिक सट्टेबाजी विचारों का वर्णन करता है जो व्यापक रूप से गहन अध्ययन में भविष्य के अनुसंधान के लिए महत्वपूर्ण माना जाता है .</s>
9 अध्याय 1</s>
इनपुट</s>
हस्तनिर्मित प्रोग्राम</s>
आउटपुट इनपुट</s>
सुविधाओं आउटपुट इनपुट विशेषताओं मानचित्रण से सुविधाओं आउटपुट इनपुट इनपुट इनपुट इनपुट</s>
सामान्य लक्षणों का मानचित्रण अधिक अमूर्त विशेषताओं के आउटपुट अतिरिक्त परतों के नियम - पस्त सिस्टम क्लासिक मशीन अधिगम अधिगम गहन अधिगम चित्र 1 .5 : प्रवाह संचित्र यह दर्शाता है कि एआई सिस्टम के विभिन्न भाग विभिन्न एआई विधाओं के भीतर किस प्रकार एक दूसरे से संबंधित हैं ।</s>
छायांकित बक्से उन घटकों को इंगित करते हैं जो डेटा से सीखने में सक्षम होते हैं .</s>
पाठकों को उन हिस्सों को छोड़ने के लिए स्वतंत्र महसूस करना चाहिए जो उनके हितों या पृष्ठभूमि को ध्यान में रखते हुए प्रासंगिक नहीं हैं .</s>
रैखिक बीजगणित , संभावना , और मौलिक मशीन अधिगम अवधारणाओं से परिचित पाठक भाग 1 को छोड़ सकते हैं , उदाहरण के लिए , जबकि जो लोग सिर्फ एक कार्य प्रणाली को लागू करना चाहते हैं , उन्हें भाग 2 से आगे नहीं पढ़ना चाहिए .</s>
चुनने में मदद करने के लिए जो 10 अध्याय 1 .</s>
INTRODUCTION 1</s>
परिचय भाग 1 : अनुप्रयुक्त मठ और मशीन लर्निंग बेसिक 2 .</s>
रेखीय बीजगणित 3</s>
संभाव्यता और सूचना सिद्धांत 4 .</s>
संख्यात्मक गणना 5 .</s>
मशीन लर्निंग बेसिक्स पार्ट - 2 :</s>
डीप नेटवर्क्सः आधुनिक अभ्यास 6 .</s>
डीप फीडर नेटवर्क 7</s>
नियमितीकरण 8 .</s>
ऑप्टीमाइजेशन 9 .</s>
सीएनएन</s>
आरएनएस 11</s>
व्यावहारिक पद्धति 12 .</s>
अनुप्रयोग पार्ट - III</s>
डीप लर्निंग रिसर्च 13</s>
रेखीय फैक्टर मॉडल 14 .</s>
स्वनकोडर १५ .</s>
प्रतिनिधित्व अधिगम 16 .</s>
संरचित संभाव्यता मॉडल 17 .</s>
मोंटे कार्लो मेथड 18</s>
पार्टीशन फंक्शन 19</s>
अनुमान 20</s>
डीप जेनरेशन मॉडल चित्र 1 . 6 : पुस्तक का उच्च स्तरीय संगठन ।</s>
एक अध्याय से दूसरे अध्याय के तीर से यह ज्ञात होता है कि पूर्व अध्याय को समझने के लिए पूर्वापेक्षित सामग्री है ।</s>
11 अध्याय 1</s>
अध्यायों को पढ़ने के लिए , आंकड़ा 1 . 6 एक प्रवाह संचित्र प्रदान करता है जो पुस्तक के उच्च स्तर के संगठन को दर्शाता है ।</s>
हम यह जरूर मान लेते हैं कि सभी पाठक कंप्यूटर विज्ञान की पृष्ठभूमि से आते हैं ।</s>
हम प्रोग्रामिंग , कम्प्यूटेशनल निष्पादन मुद्दों की एक बुनियादी समझ , जटिलता सिद्धांत , परिचयात्मक स्तर पथरी और ग्राफ सिद्धांत की शब्दावली के साथ परिचित मान लेते हैं .</s>
1 . 2 डीप लर्निंग में ऐतिहासिक रुझान</s>
गहन अध्ययन को किसी ऐतिहासिक संदर्भ के साथ समझना सबसे आसान है ।</s>
गहन अध्ययन का विस्तृत इतिहास प्रदान करने के बजाय , हम कुछ प्रमुख प्रवृत्तियों की पहचान करते हैंः</s>
• गहन अध्ययन का एक लंबा और समृद्ध इतिहास रहा है , लेकिन कई नामों से जाना जाता है , जो विभिन्न दार्शनिक दृष्टिकोणों को प्रतिबिंबित करते हैं , और लोकप्रियता में गिरावट और गिरावट आई है .</s>
• गहन अध्ययन अधिक उपयोगी हो गया है क्योंकि उपलब्ध प्रशिक्षण डेटा की मात्रा में वृद्धि हुई है .</s>
• डीप लर्निंग मॉडलों का आकार समय के साथ बढ़ता गया है क्योंकि कंप्यूटर इन्फ्रास्ट्रक्चर ( गहन सीखने के लिए हार्डवेयर और सॉफ्टवेयर ) में सुधार हुआ है ।</s>
• गहन अध्ययन समय के साथ बढ़ती सटीकता के साथ जटिल अनुप्रयोगों को हल किया है .</s>
१ . २ . १</s>
नेरल नेट के अनेक नाम और परिवर्तनशील फोरन</s>
हम उम्मीद करते हैं कि इस पुस्तक के कई पाठकों ने एक रोमांचक नई तकनीक के रूप में गहन अध्ययन के बारे में सुना है , और एक उभरते हुए क्षेत्र के बारे में एक पुस्तक में “ थ्योरी” का उल्लेख देखकर आश्चर्य होता है .</s>
दरअसल , 1940 के दशक में गहन अध्ययन की शुरुआत हुई थी ।</s>
डीप लर्निंग केवल नया प्रतीत होता है , क्योंकि यह अपनी वर्तमान लोकप्रियता से पहले कई वर्षों के लिए अपेक्षाकृत अलोकप्रिय था , और क्योंकि यह कई अलग अलग नामों से गुजरा है , केवल हाल ही में “दीप लर्निंग” कहा जाता है .</s>
इस क्षेत्र को कई बार पुनर्गठित किया गया है , जो विभिन्न शोधकर्ताओं और विभिन्न परिप्रेक्ष्यों के प्रभाव को प्रतिबिंबित करता है .</s>
गहन अध्ययन का एक व्यापक इतिहास इस पाठ्यपुस्तक के दायरे से बाहर है .</s>
कुछ बुनियादी संदर्भ , तथापि , गहरी सीखने को समझने के लिए उपयोगी है .</s>
मोटे तौर पर कहें तो विकास की तीन लहरें रही हैं :</s>
1940 -1960 के दशक में साइबरनेटिक्स के नाम से जाना जाने वाला गहन अध्ययन , 12 अध्याय 1 में संबंधवाद के रूप में जाना जाता है ।</s>
भाग 1940 1960 1970 1990 2000 वर्ष</s>
</s>
</s>
चित्र 1 .7</s>
कृत्रिम तंत्रिका जाल अनुसंधान की तीन ऐतिहासिक तरंगों में से दो , जैसा कि गूगल पुस्तकों के अनुसार “साइबरनेटिक्स” और “न्यूरीय नेटवर्क” की आवृत्ति से मापा जाता है ( तीसरी लहर भी हाल ही में दिखाई देने वाली है ) .</s>
प्रथम तरंग की शुरुआत 1940 -1960 के दशक में जैव विज्ञान के सिद्धांतों के विकास के साथ हुई ( तंत्रिका विज्ञान और पिट्स , १९४३ , १९४९ , हेब्बन मॉडलों के प्रथम अनुकूलीकरण और कार्यान्वयन जैसे एकल सक्षमीकरण प्रशिक्षण )</s>
दूसरी लहर की शुरुआत 1980 -1995 के संपर्कवादी दृष्टिकोण से हुई , जिसमें पश्च - प्रचार ( Rumelhart एट अल , 1986 ) एक या दो छिपे परतों वाले तंत्रिका नेटवर्क को प्रशिक्षित करने के लिए हुआ ।</s>
वर्तमान तीसरी लहर , गहन अध्ययन , 2006 के आसपास शुरू हुआ ( एचिन एट अल , 2006 बेंजियो एट</s>
एट अल Ranzato & # 44 ; 2007 & # 44 ;</s>
और अभी पुस्तक रूप में 2016 के रूप में प्रकट हो रहा है .</s>
अन्य दो तरंगें भी इसी प्रकार पुस्तकाकार रूप में प्रकाशित हुई , जो तदनुरूप वैज्ञानिक गतिविधि से बहुत बाद में प्राप्त हुई ।</s>
१९८० - १९९० , और वर्तमान पुनरुत्थान के नाम से सन् २००६ में आरम्भ हुआ ।</s>
यह मात्रात्मक रूप से 1 .7 अंक में दर्शाया गया है ।</s>
आज हम जिन प्रारंभिक अधिगम एल्गोरिदमों को पहचानते हैं उनमें से कुछ का उद्देश्य जैविक अधिगम का कम्प्यूटेशनल मॉडल होना था , अर्थात मस्तिष्क में कैसे अधिगम होता है या हो सकता है इसके मॉडल ।</s>
एक परिणाम के रूप में , एक नाम है कि गहन अध्ययन किया गया है कृत्रिम तंत्रिका नेटवर्क (</s>
गहन अध्ययन मॉडलों के बारे में इसी परिप्रेक्ष्य में कहा जाता है कि वे जैविक मस्तिष्क ( चाहे मानव मस्तिष्क हो या किसी अन्य पशु के मस्तिष्क ) से प्रेरित प्रणालियों के इंजीनियर हैं .</s>
मशीनी सीखने के लिए जिन प्रकार के तंत्रिका नेटवर्कों का प्रयोग किया जाता है , वे कभी - कभी मस्तिष्क के कार्य को समझने के लिए प्रयोग किए जाते हैं ( Hinton and Canice , 1991 ) , वे आमतौर पर जैविक कार्य के वास्तविक मॉडल नहीं माने जाते हैं .</s>
गहन अध्ययन पर तंत्रिका परिप्रेक्ष्य दो मुख्य विचारों से प्रेरित है .</s>
एक विचार है कि मस्तिष्क उदाहरण के द्वारा एक सबूत प्रदान करता है कि बुद्धिमान व्यवहार संभव है , और बुद्धि का निर्माण करने के लिए एक वैचारिक रूप से सीधा रास्ता है मस्तिष्क के पीछे कम्प्यूटेशनल सिद्धांतों को उलटना और उसकी कार्यक्षमता की नकल .</s>
अन्य 13 अध्याय 1</s>
अंतःविषय परिप्रेक्ष्य यह है कि मस्तिष्क और उन सिद्धांतों को समझना बहुत दिलचस्प होगा जो मानव बुद्धि को कमजोर करते हैं , इसलिए मशीनी शिक्षण मॉडल जो इन बुनियादी वैज्ञानिक प्रश्नों पर प्रकाश डालते हैं , इंजीनियरिंग अनुप्रयोगों को हल करने की उनकी क्षमता के अलावा उपयोगी हैं .</s>
आधुनिक शब्द “दीप अधिगम” मशीन अधिगम मॉडल की वर्तमान नस्ल पर तंत्रिका विज्ञानी परिप्रेक्ष्य से परे चला जाता है .</s>
यह रचना के एकाधिक स्तरों को सीखने के एक अधिक सामान्य सिद्धांत के लिए अपील करता है , जिसे मशीन अधिगम ढांचे में लागू किया जा सकता है जो अनिवार्य रूप से तंत्रिका प्रेरित नहीं हैं .</s>
आधुनिक गहन अध्ययन के प्रारंभिक पूर्ववर्ती सरल रैखिक मॉडल थे जो तंत्रिका विज्ञानी परिप्रेक्ष्य से प्रेरित थे ।</s>
इन मॉडलों को एन इनपुट मूल्यों का एक सेट x 1 , 008 लेने के लिए डिज़ाइन किया गया था .</s>
1 . 735 . 737 शब्द 1 . 837 . 837 उच्चारण 299भाषा</s>
एक्स एन , और उन्हें एक आउटपुट वाई के साथ जोड़ें ।</s>
इन मॉडलों को 1 , 070 से वजन का एक सेट सीखना होगा .</s>
1 . 735 . 737 शब्द 1 . 837 . 837 उच्चारण 299भाषा</s>
W n और उनके आउटपुट एफ ( x , w ) की गणना =</s>
x 1 से 1</s>
+ ···· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·</s>
www . n . n . n . n . n</s>
तंत्रिका नेटवर्क अनुसंधान की यह पहली लहर साइबरनेटिक्स के रूप में जानी जाती थी , जैसा कि 1 .7 के आंकड़े में दर्शाया गया है ।</s>
मैकक्लोच - पिट्स न्यूरॉन ( McCulloch and Pitts , 1943 ) मस्तिष्क कार्य का एक प्रारंभिक मॉडल था .</s>
यह रैखिक मॉडल एफ ( x , w ) सकारात्मक या नकारात्मक है की जाँच करके इनपुट की दो अलग - अलग श्रेणियों को पहचान सकता है .</s>
बेशक , मॉडल के लिए श्रेणियों की वांछित परिभाषा के अनुरूप करने के लिए , वजन सही ढंग से सेट करने की जरूरत है .</s>
इन भारों को मानव ऑपरेटर द्वारा निर्धारित किया जा सकता है ।</s>
1950 के दशक में , पेरोसिनबॉटन ( Rosenblogt , 1958 , 1962 ) पहला मॉडल बन गया , जो भार सीख सकता था जिसने प्रत्येक वर्ग से इनपुट के उदाहरण दिए गए श्रेणियों को परिभाषित किया .</s>
अनुकूली रैखिक तत्व ( ADALINE ) , जो लगभग उसी समय से प्रारंभ हुआ , केवल f का मूल्य लौटा दिया .</s>
( x ) वास्तविक संख्या की भविष्यवाणी करने के लिए ( Widrow and Hoff , 1960 ) और भी डेटा से इन संख्याओं की भविष्यवाणी करने के लिए सीख सकते हैं .</s>
इन सरल शिक्षण एल्गोरिदमों ने मा - चाईन सीखने के आधुनिक परिदृश्य को बहुत प्रभावित किया ।</s>
प्रशिक्षण एल्गोरिथ्म एडीएएलआईएन के भार को अनुकूलित करने के लिए प्रयोग किया जाता है</s>
एक एल्गोरिथ्म का एक विशेष मामला था जिसे stochastic ढाल अवतरण कहते हैं ।</s>
stochastic ढाल अवतरण एल्गोरिथ्म के हल्के संशोधित संस्करण आज गहन अधिगम मॉडलों के लिए प्रमुख प्रशिक्षण एल्गोरिदम हैं ।</s>
पेरीसिप्टॉन और एडीएएलआईएन द्वारा प्रयुक्त फ ( x , w ) पर आधारित मॉडलों को रैखिक मॉडल कहा जाता है ।</s>
ये मॉडल कुछ सबसे व्यापक रूप से इस्तेमाल किए गए मशीन लर्निंग मॉडलों में से हैं , हालांकि कई मामलों में उन्हें मूल मॉडलों की तुलना में अलग - अलग तरीकों से प्रशिक्षित किया जाता है .</s>
रेखीय मॉडलों की कई सीमाएं हैं ।</s>
अधिकांश प्रसिद्ध , वे सीख नहीं सकते समारोह , जहां f ( ध्0 ) , ध्0 , ध्0 , ध्0 , ध्0 , ध्0</s>
= 1 और एफ (</s>
[1 , 0“ , ”उन्होंने कहा”</s>
= 1 लेकिन एफ</s>
[1 , 1 ] , शावक , शावक , शावक , शावक</s>
= 0 और f ( झुकना0 , झुकना , लहराना , लहराना ) = 0</s>
रैखिक मॉडलों में इन खामियों को देखने वाले आलोचकों ने सामान्य रूप से जैविक रूप से प्रेरित अधिगम के विरुद्ध एक पीठ का निर्माण किया ( एमिन्सकी एंड पेपर्ट , 1969 ) .</s>
तंत्रिका नेटवर्कों की लोकप्रियता में यह पहली बड़ी डुबकी थी ।</s>
14 अध्याय 1</s>
आज , तंत्रिका विज्ञान गहन अध्ययन शोधकर्ताओं के लिए प्रेरणा का एक महत्वपूर्ण स्रोत माना जाता है , लेकिन यह अब क्षेत्र के लिए प्रमुख मार्गदर्शक नहीं है .</s>
आज गहन अध्ययन अनुसंधान में तंत्रिका विज्ञान की कम भूमिका का मुख्य कारण यह है कि हमारे पास मस्तिष्क के बारे में पर्याप्त जानकारी नहीं है कि हम इसे एक गाइड के रूप में इस्तेमाल कर सकें ।</s>
मस्तिष्क द्वारा प्रयोग किए गए वास्तविक एल्गोरिदम की गहरी समझ प्राप्त करने के लिए हमें एक साथ हजारों अंतर्संबंधित न्यूरॉन्स की गतिविधि की निगरानी करने में सक्षम होने की आवश्यकता होगी ।</s>
क्योंकि हम ऐसा नहीं कर पा रहे हैं , इसलिए हम मस्तिष्क के कुछ अत्यंत सरल और विद्वत्तापूर्ण भागों को भी नहीं समझ पा रहे हैं ( ओएलशसन एंड फील्ड , 2005 ) ।</s>
तंत्रिका विज्ञान ने हमें आशा करने के लिए एक कारण दिया है कि एक एकल गहन अधिगम एल्गोरिथ्म कई अलग अलग कार्यों को हल कर सकते हैं .</s>
तंत्रिका विज्ञानियों ने पाया है कि नृट्स अपने मस्तिष्क के श्रवण संसाधन क्षेत्र के साथ “सांख्य” सीख सकते हैं यदि उनके मस्तिष्क को उस क्षेत्र में दृश्य संकेत भेजने के लिए पुनरावर्तित किया जाता है ( वान मेल्चर एट अल 2000 ) .</s>
इससे यह संकेत मिलता है कि अधिकतर स्तनधारी मस्तिष्क उन विभिन्न कार्यों को हल करने के लिए एक ही एल्गोरिथ्म का प्रयोग कर सकता है जो मस्तिष्क द्वारा हल किए जाते हैं ।</s>
इस परिकल्पना से पहले , मशीन अधिगम अनुसंधान अधिक खंडित था , शोधकर्ताओं के विभिन्न समुदायों के साथ जो प्राकृतिक भाषा संसाधन , दृष्टि , गति योजना और भाषण मान्यता का अध्ययन कर रहे थे .</s>
आज , इन अनुप्रयोग समुदायों अभी भी अलग हैं , लेकिन यह कई या यहां तक कि इन सभी अनुप्रयोग क्षेत्रों का एक साथ अध्ययन करने के लिए गहन अध्ययन अनुसंधान समूहों के लिए आम है .</s>
हम तंत्रिका विज्ञान से कुछ रूखे दिशा निर्देश तैयार करने में सक्षम हैं ।</s>
कई कम्प्यूटेशनल इकाइयों है कि केवल एक दूसरे के साथ बातचीत के माध्यम से बुद्धिमान बनने का मूल विचार मस्तिष्क से प्रेरित है .</s>
नवोन्मेष ( Fukushima , 1980 ) ने स्तनधारी दृश्य प्रणाली की संरचना से प्रेरित छवियों के प्रसंस्करण के लिए एक शक्तिशाली मॉडल वास्तुकला की शुरुआत की और बाद में आधुनिक संयुग्मन नेटवर्क ( LeCun et al . 9 , 1998 ) का आधार बना ।</s>
अधिकांश तंत्रिका नेटवर्क आज एक मॉडल न्यूरॉन पर आधारित हैं , जिसे संशोधित रैखिक इकाई कहा जाता है ।</s>
मूल कोग्निट्रॉन ( Fukushima , 1975 ) ने एक अधिक जटिल संस्करण प्रस्तुत किया जो मस्तिष्क के कार्य के हमारे ज्ञान से अत्यधिक प्रेरित था ।</s>
सरलीकृत आधुनिक संस्करण कई दृष्टिकोणों से विचारों को समाहित करते हुए विकसित किया गया था , जिसमें नायर और हाइटन (2010 का आसन्न और ग्लोट एट अल ) शामिल थे .</s>
(2011 ) प्रभाव के रूप में तंत्रिका विज्ञान का हवाला देते हुए , और जेरेट्ट एट अल .</s>
(2009 ) अधिक इंजीनियरिंग उन्मुख प्रभावों का हवाला देते हुए</s>
जबकि तंत्रिका विज्ञान प्रेरणा का एक महत्वपूर्ण स्रोत है , यह एक कठोर गाइड के रूप में लेने की जरूरत नहीं है .</s>
हम जानते हैं कि वास्तविक न्यूरॉन्स आधुनिक संशोधित रैखिक इकाइयों की तुलना में बहुत अलग कार्यों की गणना , लेकिन अधिक तंत्रिका यथार्थवाद अभी तक मशीन अधिगम प्रदर्शन में सुधार के लिए नेतृत्व नहीं किया है .</s>
इसके अलावा , जबकि तंत्रिका विज्ञान सफलतापूर्वक कई तंत्रिका नेटवर्क वास्तुकला को प्रेरित किया है , हम अभी तक तंत्रिका विज्ञान के लिए जैविक सीखने के बारे में पर्याप्त नहीं पता है कि हम इन वास्तुकला के प्रशिक्षण के लिए उपयोग एल्गोरिदम के लिए बहुत मार्गदर्शन प्रदान करते हैं .</s>
अध्याय 1</s>
INTRODUCTION मीडिया खातों में अक्सर मस्तिष्क को गहन अध्ययन की समानता पर बल दिया जाता है ।</s>
जबकि यह सच है कि गहन अध्ययन करने वाले शोधकर्ता मस्तिष्क को अन्य मशीनी शिक्षण क्षेत्रों में काम करने वाले शोधकर्ताओं की तुलना में अधिक प्रभावित मानते हैं , जैसे कर्नेल मशीन या बेसेशियन सांख्यिकी , गहन अध्ययन को मस्तिष्क का अनुकरण करने के प्रयास के रूप में नहीं देखना चाहिए ।</s>
आधुनिक गहन अध्ययन कई क्षेत्रों , विशेष रूप से लागू गणित मौलिक जैसे रैखिक बीजगणित , संभावना , सूचना सिद्धांत , और संख्यात्मक अनुकूलन से प्रेरणा लेता है .</s>
जबकि कुछ गहन अध्ययन शोधकर्ता तंत्रिका विज्ञान को प्रेरणा का एक महत्वपूर्ण स्रोत बताते हैं , दूसरों को तंत्रिका विज्ञान से बिल्कुल संबंधित नहीं है .</s>
गौरतलब है कि एल्गोरिथम के स्तर पर मस्तिष्क कैसे काम करता है , यह समझने का प्रयास जीवित है और अच्छी तरह से है ।</s>
इस प्रयास को मुख्य रूप से “संयोजक तंत्रिका विज्ञान” के रूप में जाना जाता है और गहन अध्ययन से अध्ययन का एक अलग क्षेत्र है .</s>
शोधकर्ताओं के लिए दोनों क्षेत्रों के बीच पीछे और आगे बढ़ना आम बात है ।</s>
गहन अध्ययन का क्षेत्र मुख्य रूप से इस बात से संबंधित है कि कंप्यूटर प्रणालियों का निर्माण कैसे किया जाए जो बुद्धि की आवश्यकता वाले कार्यों को सफलतापूर्वक हल कर सकें , जबकि कम्प्यूटेशनल तंत्रिका विज्ञान का क्षेत्र मुख्य रूप से इस बात से संबंधित है कि मस्तिष्क वास्तव में कैसे काम करता है ।</s>
1980 के दशक में , तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर , कनेक्शनवाद , या समानांतर वितरित प्रक्रिया के माध्यम से बड़े भाग में उभरा -</s>
</s>
मेक्ललैंड एट अल , 1995</s>
संज्ञानात्मक विज्ञान के संदर्भ में संबंधवाद का उदय हुआ ।</s>
संज्ञानात्मक विज्ञान मस्तिष्क को समझने के लिए एक अंतःविषय दृष्टिकोण है , जो विश्लेषण के कई विभिन्न स्तरों को जोड़ता है .</s>
1980 के दशक के प्रारंभ में , अधिकांश संज्ञानात्मक वैज्ञानिकों ने प्रतीकात्मक तर्क के मॉडलों का अध्ययन किया .</s>
उनकी लोकप्रियता के बावजूद , प्रतीकात्मक मॉडलों को इस संदर्भ में समझाना मुश्किल था कि कैसे मस्तिष्क वास्तव में न्यूरॉन्स का उपयोग कर उन्हें लागू कर सकता है .</s>
कनेक्शनकारों ने संज्ञान के ऐसे मॉडलों का अध्ययन करना शुरू किया जो वास्तव में तंत्रिका कार्यान्वयन ( टोर्नेट्जकी और मिंटो , 1985 ) में आधारित थे , जिन्होंने 1940 के दशक में मनोविज्ञानी डोनाल्ड हेब के काम से संबंधित कई विचारों को पुनः जीवित किया . ( 1949 )</s>
कनेक्शन में केंद्रीय विचार है कि सरल कम्प्यूटेशनल इकाइयों की एक बड़ी संख्या में बुद्धिमान व्यवहार प्राप्त कर सकते हैं जब एक साथ नेटवर्क किया .</s>
यह अंतर्दृष्टि जैविक तंत्रिका तंत्र में न्यूरॉन्स के लिए समान रूप से लागू होती है , क्योंकि यह कम्प्यूटेशनल मॉडलों में छुपी इकाइयों के लिए करता है .</s>
1980 के दशक के संबंधवाद आंदोलन के दौरान कई प्रमुख अवधारणाओं का जन्म हुआ जो आज के गहन अध्ययन के केंद्र में हैं .</s>
इन अवधारणाओं में से एक है वितरित प्रतिनिधित्व ( हैटन एट अल , 1986 ) ।</s>
यह विचार है कि एक प्रणाली के लिए प्रत्येक इनपुट कई सुविधाओं द्वारा प्रतिनिधित्व किया जाना चाहिए , और प्रत्येक विशेषता कई संभव इनपुट के प्रतिनिधित्व में शामिल किया जाना चाहिए .</s>
उदाहरण के लिए , मान लीजिए कि हमारे पास एक दृष्टि प्रणाली है जो 16 अध्याय 1 को पहचान सकती है ।</s>
INTRODUCTION कारों , ट्रकों और पक्षियों , और इन वस्तुओं में से प्रत्येक लाल , हरा या नीला हो सकता है ।</s>
इन आदानों का प्रतिनिधित्व करने का एक तरीका यह होगा कि एक अलग न्यूरॉन या छुपी इकाई हो जो नौ संभव संयोजनों में से प्रत्येक के लिए सक्रिय हो : लाल ट्रक , लाल कार , लाल पक्षी , हरे ट्रक , आदि .</s>
इसके लिए नौ अलग - अलग न्यूरॉन्स की आवश्यकता होती है , और प्रत्येक न्यूरॉन को स्वतंत्र रूप से रंग और वस्तु पहचान की अवधारणा सीखना होगा .</s>
इस स्थिति में सुधार का एक तरीका है एक वितरित प्रतिनिधित्व का उपयोग करने के लिए , तीन न्यूरॉन्स के साथ रंग का वर्णन और तीन न्यूरॉन्स ऑब्जेक्ट पहचान का वर्णन .</s>
इसके लिए नौ के बजाय कुल छह न्यूरॉन्स की आवश्यकता होती है , और लालिमा का वर्णन करने वाला न्यूरॉन कारों , ट्रकों और पक्षियों की छवियों से लालिमा के बारे में जानने में सक्षम है , न कि सिर्फ एक विशिष्ट श्रेणी की वस्तुओं की छवियों से .</s>
वितरित प्रतिनिधित्व की अवधारणा इस पुस्तक के लिए केंद्रीय है और अध्याय 15 में अधिक विस्तार में वर्णित है .</s>
कनेक्शनिस्ट आंदोलन की एक अन्य प्रमुख उपलब्धि थी , आंतरिक उद्वेगों से युक्त गहन तंत्रिका नेटवर्कों को प्रशिक्षित करने के लिए बैकुंठ प्रयोग और बैकुंठ कलन विधि का लोकप्रियीकरण ( आरुमलहार्ट एट अल , 1987 , लेसन , 1986 ) ।</s>
इस एल्गोरिथ्म की लोकप्रियता में वृद्धि हुई है और कमी आई है , लेकिन , इस लेखन की तरह , गहन मॉडलों के प्रशिक्षण के लिए प्रमुख दृष्टिकोण है .</s>
1990 के दशक के दौरान , शोधकर्ताओं ने तंत्रिका नेटवर्क के साथ मॉडलिंग अनुक्रम में महत्वपूर्ण प्रगति की .</s>
होचरीकारक</s>
(1994 ) लंबे अनुक्रमों के मॉडलिंग में कुछ मूलभूत गणितीय कठिनाइयों की पहचान की , जिनका वर्णन खंड 10 .7 में किया गया है .</s>
हूकर और शम्मीदुद्दीन ने इनमें से कुछ कठिनाइयों को दूर करने के लिए लम्बी लघु स्मृति ( एल टी एम नेटवर्क ) शुरू की ।</s>
आज , LSTM व्यापक रूप से कई अनुक्रम मॉडलिंग कार्यों के लिए प्रयोग किया जाता है , जिसमें गूगल पर कई प्राकृतिक भाषा संसाधन कार्य शामिल हैं .</s>
तंत्रिका नेटवर्क अनुसंधान की दूसरी लहर १९९० के दशक के मध्य तक चली ।</s>
तंत्रिका नेटवर्क और अन्य एआई प्रौद्योगिकियों पर आधारित वेन ने निवेश की मांग करते समय अवास्तविक महत्वाकांक्षी दावे करना शुरू कर दिया ।</s>
जब एआई अनुसंधान ने इन अनुचित अपेक्षाओं को पूरा नहीं किया तो निवेशक निराश हो गए ।</s>
साथ ही मशीनी शिक्षा के अन्य क्षेत्रों में भी प्रगति हुई ।</s>
कर्नेल मशीनें ( १९९२ महत्वपूर्ण कोर और वेपें और १९९५ ) & # 44 ; एस एम एस & # 44 ; १९९९९ & # 44 ; एस एम एस & # 44 ; ग्राफिकल मॉडल ( १९९८ ) & # 44 ; जिन कार्यों से कई अच्छे परिणाम प्राप्त हुए ।</s>
इन दो कारणों से 2007 तक चलने वाले तंत्रिका नेटवर्कों की लोकप्रियता में कमी आई .</s>
इस दौरान , तंत्रिका नेटवर्क कुछ कार्यों पर प्रभावशाली प्रदर्शन प्राप्त करते रहे (</s>
लेउन एट अल</s>
1998 , बेनजीर भुट्टो हत्याकांड , 2001</s>
कनाडा के इंस्टीट्यूट फॉर एडवांस्ड रिसर्च ( सीएफएआर ) ने तंत्रिका नेटवर्क अनुसंधान को उसके तंत्रिका संगणन और अनुकूली अनुसन्धान के माध्यम से जीवित रखने में मदद की ।</s>
यह कार्यक्रम टोरंटो विश्वविद्यालय में जिओफ्री हैनटन के नेतृत्व में संयुक्त मशीन अधिगम अनुसंधान समूहों , मॉन्ट्रियल विश्वविद्यालय में योसुआ बेंजीनो , और न्यूयॉर्क विश्वविद्यालय में यान लेकन</s>
बहुसांस्कृतिक अनुसंधान पहल 17 अध्याय 1 ।</s>
INTRODUCTION में तंत्रिका विज्ञानी और मानव और कंप्यूटर दृष्टि के विशेषज्ञ भी शामिल थे ।</s>
इस बिंदु पर , डीप नेटवर्क आम तौर पर माना जाता था कि प्रशिक्षण के लिए बहुत मुश्किल है .</s>
अब हम जानते हैं कि 1980 के दशक के बाद से मौजूद एल्गोरिदम काफी अच्छी तरह से काम करते हैं , लेकिन यह स्पष्ट सर्किट 2006 नहीं था .</s>
मुद्दा शायद यह है कि ये एल्गोरिदम इतने कम्प्यूटेशनल रूप से महंगे थे कि उस समय उपलब्ध हार्डवेयर के साथ अधिक प्रयोग करने की अनुमति नहीं दी जा सकती थी .</s>
तंत्रिका नेटवर्क अनुसंधान की तीसरी लहर 2006 में एक सफलता के साथ शुरू हुई .</s>
जिओफ़्री हाइटन ने दिखाया कि एक प्रकार का तंत्रिका नेटवर्क जिसे गहरी आस्था नेटवर्क कहा जाता है , लालची परतवार पूर्वरेखण ( हैटन एट अल , 2006 ) नामक रणनीति का उपयोग करके कुशलतापूर्वक प्रशिक्षित किया जा सकता है , जिसका हम धारा 151 में विस्तार से वर्णन करते हैं ।</s>
अन्य संबद्ध अनुसंधान समूहों ने शीघ्र ही यह दिखाया कि इसी रणनीति का उपयोग अन्य अनेक नेटवर्कों को प्रशिक्षित करने के लिए किया जा सकता है ।</s>
तंत्रिका नेटवर्क अनुसंधान की इस लहर ने इस बात पर जोर देने के लिए “दीप लर्निंग” शब्द के प्रयोग को लोकप्रिय बनाया कि शोधकर्ता पहले की तुलना में गहरे तंत्रिका नेटवर्कों को प्रशिक्षित करने में सक्षम थे , और गहराई के सैद्धांतिक महत्व पर ध्यान केंद्रित करने के लिए (</s>
इस समय , गहन तंत्रिका नेटवर्क अन्य मशीनी अधिगम प्रौद्योगिकियों तथा हाथ से दी गई कार्यशीलता के आधार पर एआई सिस्टमों को सूचित करते हैं ।</s>
तंत्रिका नेटवर्क की लोकप्रियता की यह तीसरी लहर इस लेखन के समय तक जारी है , हालांकि गहन अध्ययन अनुसंधान का ध्यान इस लहर के समय के भीतर नाटकीय रूप से बदल गया है .</s>
तीसरी लहर की शुरुआत नई अप्रयुक्त अधिगम तकनीकों और छोटे डेटासेटों से अच्छी तरह सामान्य करने के लिए गहरे मॉडलों की क्षमता पर ध्यान केंद्रित करने के साथ हुई , लेकिन आज बहुत पुराने पर्यवेक्षित अधिगम एल्गोरिदमों और बड़े लेबलीकृत डेटासेटों को प्राप्त करने के लिए गहरे मॉडलों की क्षमता में अधिक रुचि है .</s>
१ . २ . २</s>
डाटासेट आकार बढ़ाना</s>
आश्चर्य हो सकता है कि गहन अध्ययन केवल हाल ही में एक महत्वपूर्ण प्रौद्योगिकी के रूप में मान्यता प्राप्त हो गया है , भले ही कृत्रिम तंत्रिका नेटवर्क के साथ पहला प्रयोग 1950 के दशक में किया गया था .</s>
गहन अध्ययन 1990 के दशक के बाद से वाणिज्यिक अनुप्रयोगों में सफलतापूर्वक इस्तेमाल किया गया है , लेकिन अक्सर एक तकनीक और कुछ है कि केवल एक विशेषज्ञ का उपयोग कर सकता है के रूप में माना जाता था , हाल ही तक .</s>
यह सच है कि एक गहरी सीखने एल्गोरिथ्म से अच्छा प्रदर्शन प्राप्त करने के लिए कुछ कौशल की आवश्यकता है .</s>
सौभाग्य से , प्रशिक्षण डेटा की राशि बढ़ने के रूप में आवश्यक कौशल की मात्रा कम हो जाती है .</s>
आज जटिल कार्यों पर मानव प्रदर्शन करने वाले अधिगम एल्गोरिदम 1980 के दशक में खिलौनों की समस्याओं को हल करने के लिए संघर्ष करने वाले अधिगम एल्गोरिदम के लगभग समान हैं , हालांकि इन एल्गोरिदमों के साथ हम जिन मॉडलों को प्रशिक्षित करते हैं उनमें 18 अध्याय 1 है ।</s>
इन्फर्मेशन में ऐसे बदलाव किए गए हैं जो बहुत गहरे वास्तुशिल्प के प्रशिक्षण को सरल बनाते हैं ।</s>
सबसे महत्वपूर्ण नया विकास है कि आज हम इन एल्गोरिदम को सफल होने के लिए आवश्यक संसाधनों के साथ प्रदान कर सकते हैं .</s>
चित्र 1 . 8 से पता चलता है कि कैसे बेंचमार्क डेटासेट के आकार में समय के साथ उल्लेखनीय विस्तार हुआ है .</s>
यह प्रवृत्ति समाज के बढ़ते डिजिटाइजेशन से प्रेरित है ।</s>
जैसे - जैसे हमारी गतिविधियों का अधिक से अधिक भाग कंप्यूटर पर होता है , वैसे - वैसे हम जो करते हैं उसका अधिक से अधिक विवरण दर्ज किया जाता है ।</s>
चूंकि हमारे कंप्यूटर एक साथ अधिकाधिक नेटवर्क कर रहे हैं , इसलिए इन अभिलेखों को केंद्रीकृत करना और उन्हें मशीन अधिगम अनुप्रयोगों के लिए उपयुक्त डेटासेट में कम करना आसान हो जाता है .</s>
बोग डेटा की आयु १९०० १९८५ १९८५ १९८५ २००० वर्ष १० ० १० १ १० २ १० ३ १० ४ १० ५ १० ६ १० ८ १० ९ डेटासेट आकार ( अनुगणित उदाहरण )</s>
परितारिका</s>
सार्वजनिक एसवीएचएन इमेजन -नेट</s>
CIFAR - १०</s>
इमेजनट10k ILSVRC 2014 स्पोर्ट्स - 1</s>
</s>
समय के साथ बढ़ता डेटासेट आकार</s>
मैन्युअल रूप से संकलित आंकड़ों के सैकड़ों के उपयोग का प्रयोग करते हुए सांख्यिकीविदों ने 1900 के आरंभिक आंकड़ों का अध्ययन किया ( गसरसन या 1900 के प्रारंभिक आंकड़े मापने वाले हजारों गोसैट 1908 , मत्स्य एंडरसन , 1935</s>
1950 के दशक में 1980 के दशक में , जैविक रूप से प्रेरित मशीनी शिक्षण के अग्रदूत अक्सर छोटे सिंथेटिक डेटासेटों के साथ काम करते थे , जैसे कि कम कम्प्यूटेशनल लागत उठाने के लिए डिज़ाइन किया गया था और यह दर्शाते थे कि तंत्रिका नेटवर्क विशिष्ट प्रकार्यों को सीखने में सक्षम थे (</s>
1980 और 1990 के दशक में मशीनी अधिगम अधिक सांख्यिकीय बन गया और उसने हजारों उदाहरणों से युक्त बड़े डेटासेटों का उपयोग करना शुरू किया , जैसे MNIST आंकड़ासेट ( MNIST आंकड़ासेट ) , हस्तलिखित संख्याओं के स्कैन के 1 अंक में ( LLCunbunet ) .</s>
2000 के पहले दशक में , इसी आकार के और अधिक परिष्कृत डेटासेट , जैसे कि CIFAR -10 डेटासेट ( क्रिजह्वेस्की और हैनटन , 2009 ) का उत्पादन जारी रहा .</s>
उस दशक के अंत की ओर और 2010 के दशक के पूर्वार्ध में , उल्लेखनीय रूप से बड़ी डेटासेट , जिसमें सैकड़ों हजारों से दसियों लाख उदाहरण थे , गहन अध्ययन के साथ जो संभव था , पूरी तरह से बदल दिया .</s>
इन आकडों में सार्वजनिक स्ट्रीट व्यू डेटासेट्स ( नीत्ज़र एट अल , विभिन्न इमेज नेनेट अल , 2009 , 2010 , रूसोकोव्स्क अल स्पोर्ट्स और 2014 संस्करणों की संख्याएं शामिल थीं ।</s>
M डाटासेट</s>
( सहानुभूति एट अल , 2014 )</s>
ग्राफ के शीर्ष पर , हम देखते हैं कि अनुदित वाक्यों के डेटासेट , जैसे कि कनाडा के हंसार्ड से बनाया गया आईबीएम का डेटासेट ( बॉर्न एट अल , 1990 ) और डब्ल्यूएमटी 2014 अंग्रेजी से फ्रेंच डेटासेट ( स्वेज़ , 2014 से आगे के आंकड़े विशेष रूप से बहुत दूर हैं .</s>
19 अध्याय 1</s>
INTRODUCTION चित्र 1 . 9 :</s>
MNIST डाटासेट से उदाहरण इनपुट</s>
“NIST” राष्ट्रीय मानक और प्रौद्योगिकी संस्थान के लिए खड़ा है , एजेंसी है कि मूल रूप से इस डेटा एकत्र किया .</s>
मशीन अधिगम एल्गोरिदम के साथ आसान उपयोग के लिए डेटा को पूर्व - प्रक्रमित किया गया है के बाद से “M” के लिए खड़ा है .</s>
MNIST आकडासेट में हस्तलिखित अंकों और संबद्ध लेबलों का स्कैन होता है जिसमें यह बताया जाता है कि प्रत्येक छवि में कौन से अंक 0–9 समाहित है ।</s>
इस सरल वर्गीकरण समस्या गहन अध्ययन अनुसंधान में सबसे सरल और सबसे व्यापक रूप से इस्तेमाल परीक्षणों में से एक है .</s>
आधुनिक तकनीकों के हल के लिए काफी आसान होने के बावजूद यह लोकप्रिय बना हुआ है .</s>
जेफ्री हैनटन ने इसे मशीनी अधिगम का द्वरा दर्शन कहा है , जिसका अर्थ है कि यह मशीनी अधिगम शोधकर्ताओं को नियंत्रित प्रयोगशाला स्थितियों में उनके एल्गोरिदम का अध्ययन करने में सक्षम बनाता है , हालांकि जीव विज्ञानी अक्सर फल मक्खियों का अध्ययन करते हैं ।</s>
मशीनी अधिगम को बहुत आसान बना दिया है , क्योंकि सांख्यिकीय आकलन का मुख्य बोझ - केवल थोड़ी मात्रा में आंकड़ों का अवलोकन करने के बाद अच्छी तरह से नए आंकड़ों का सृजन करना - काफी हल्का हो गया है ।</s>
के रूप में 2016 , अंगूठे का एक मोटे नियम है कि एक पर्यवेक्षित गहरी अधिगम एल्गोरिथ्म आमतौर पर लगभग 5 , 000 लेबल उदाहरण प्रति वर्ग के साथ स्वीकार्य प्रदर्शन प्राप्त करेगा और मैच या 20 अध्याय 1 .</s>
जब कम से कम 10 मिलियन लेबल किए उदाहरणों वाले डेटासेट के साथ प्रशिक्षित किया जाता है , तो यह मानव प्रदर्शन से अधिक होता है .</s>
इससे छोटे डेटासेट के साथ सफलतापूर्वक कार्य करना एक महत्वपूर्ण अनुसंधान क्षेत्र है , जो विशेष रूप से इस पर ध्यान केंद्रित करता है कि कैसे हम बड़ी मात्रा में अविश्वसनीय उदाहरणों का लाभ उठा सकते हैं , जिनमें अकुशल या अर्धसुरक्षित अधिगम शामिल हैं ।</s>
1 . 2 . 3</s>
बढ़ती मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज तंत्रिका नेटवर्क बेतहाशा सफल रहे हैं कि हम कम्प्यूटेशनल संसाधनों है कि आज बहुत बड़े मॉडलों को चलाने के लिए .</s>
कनेक्शन - एम की एक मुख्य अंतर्दृष्टि है कि जानवर बुद्धिमान हो जाते हैं जब उनके न्यूरॉन्स के कई एक साथ काम करते हैं .</s>
एक अलग न्यूरॉन या न्यूरॉन्स के छोटे संग्रह विशेष रूप से उपयोगी नहीं है .</s>
जैविक न्यूरॉन्स विशेष रूप से घने जुड़े नहीं हैं .</s>
जैसा कि 1 .10 के आंकड़े में देखा गया है , हमारे मशीन अधिगम मॉडलों में दशकों से स्तनधारी मस्तिष्कों के परिमाण के क्रम में प्रति न्यूरॉन कई कनेक्शन थे .</s>
न्यूरॉन्स की कुल संख्या के संदर्भ में , तंत्रिका नेटवर्क आश्चर्यजनक रूप से बहुत हाल ही तक छोटे रहे हैं , जैसा कि 1 . 11 के आंकड़े में दिखाया गया है .</s>
गुप्त इकाइयों की शुरुआत के बाद से , कृत्रिम तंत्रिका नेटवर्क लगभग हर 2 . 4 साल में आकार में दोगुना हो गया है .</s>
यह वृद्धि तीव्र कंप्यूटरों द्वारा बड़ी स्मृति और बड़ी डेटासेट की उपलब्धता से प्रेरित होती है .</s>
बड़े नेटवर्क अधिक जटिल कार्यों पर उच्च सटीकता प्राप्त करने में सक्षम हैं .</s>
यह प्रवृत्ति दशकों तक जारी रहने की प्रतीत होती है ।</s>
जब तक नई प्रौद्योगिकियों तेजी से स्केलिंग सक्षम , कृत्रिम तंत्रिका नेटवर्क मानव मस्तिष्क के रूप में न्यूरॉन्स की एक ही संख्या 2050 तक नहीं होगा .</s>
जैविक न्यूरॉन्स वर्तमान कृत्रिम न्यूरॉन्स की तुलना में अधिक जटिल कार्यों का प्रतिनिधित्व कर सकते हैं , तो जैविक तंत्रिका नेटवर्क इस भूखंड चित्रण से भी बड़ा हो सकता है .</s>
आत्मविश्लेषण में , यह विशेष रूप से आश्चर्य की बात नहीं है कि तंत्रिका नेटवर्क जोंक की तुलना में कम न्यूरॉन्स के साथ परिष्कृत कृत्रिम बुद्धिमत्ता प्रोब फाइम को हल करने में असमर्थ थे .</s>
आज के नेटवर्क , जिन्हें हम कम्प्यूटेशनल सिस्टम की दृष्टि से काफी बड़ा मानते हैं , अपेक्षाकृत आदिम कशेरुकी प्राणियों जैसे तंत्रिका तंत्र से भी छोटे हैं ।</s>
समय के साथ मॉडल आकार में वृद्धि , तीव्र सीपीयू की उपलब्धता के कारण , सामान्य प्रयोजन GPUs ( खंड 12 . 1 . 2 में वर्णित ) , तीव्र नेटवर्क कनेक्टिविटी और वितरित कंप्यूटिंग के लिए बेहतर सॉफ्टवेयर अवसंरचना , गहन अध्ययन के इतिहास में सबसे महत्वपूर्ण रुझानों में से एक है .</s>
इस प्रवृत्ति आमतौर पर भविष्य में अच्छी तरह से जारी रखने की उम्मीद है .</s>
अध्याय 1</s>
INTRODUCTION 1950 1985 2015 वर्ष 10 1 10 2 10 3 10 10 3 10 10 10 10 10 3 10 4 4 कनेक्शन प्रति न्यूरॉन 1 2 3 4 5 6 7 8 9 10 फल मक्खी चूहे बिल्ली मानव चित्र 1</s>
: समय के साथ प्रति न्यूरॉन कनेक्शन की संख्या .</s>
आरंभ में , कृत्रिम तंत्रिका नेटवर्क में न्यूरॉन्स के बीच संयुग्मन की संख्या हार्डवेयर क्षमताओं द्वारा सीमित थी .</s>
आज , न्यूरॉन्स के बीच कनेक्शन की संख्या ज्यादातर एक डिजाइन विचार है .</s>
कुछ कृत्रिम तंत्रिका नेटवर्कों में एक बिल्ली के रूप में न्यूरॉन के प्रति लगभग कई कनेक्शन होते हैं , और अन्य तंत्रिका नेटवर्कों के लिए यह काफी आम है कि प्रति न्यूरॉन जितने छोटे स्तनधारी होते हैं ।</s>
मानव मस्तिष्क में भी प्रति न्यूरॉन कनेक्शन की अनाप - शनाप मात्रा नहीं होती ।</s>
जैविक तंत्रिका नेटवर्क का आकार विकिपीडिया से (2015 ई . )</s>
1</s>
अनुकूली रेखीय तत्व ( Widrow and Hoff , 1960 )</s>
नेकोग्निट्रॉन ( Fukushima , 1980 ) 3 .</s>
जीपीयू - सैक्सरेटेड कॉन्वोल्यूशनल नेटवर्क</s>
कैवेलपिल्ला एट अल</s>
डीप ब्लॉट्जमैन मशीन ( Salakhutdinov और हैन्टन , २००९ ) 5 .</s>
अपसंयोजित समूह नेटवर्क (</s>
जेरेट्ट एट अल , 2009 6 .</s>
Name</s>
( क्रिस्टियन एट अल )</s>
January 2010 7 , 2010 7 : 00 पूर्वाह्न</s>
वितरित स्वचलितकोडर (</s>
ले एट अल</s>
Archive & # 124 ; October 0 8 th , 2012</s>
बहु - जीपीयू संकेन्द्रण नेटवर्क (</s>
किरिज़ेव्स्की एट अल</s>
Archive & # 124 ; October 0 9 th , 2012</s>
COTS HPC असंयोजित संयुग्मी नेटवर्क (</s>
समागम एट अल , 2013 10</s>
गोगलेनेट</s>
1 . 2 . 4</s>
बढ़ती सटीकता , जटिलता और वास्तविक प्रभाव</s>
1980 के दशक के बाद से , गहन अध्ययन लगातार सटीक मान्यता और भविष्यवाणी प्रदान करने की क्षमता में सुधार किया है .</s>
इसके अलावा , अनुप्रयोगों के व्यापक और व्यापक सेट करने के लिए सफलता के साथ गहन अध्ययन लगातार लागू किया गया है .</s>
सबसे पहले गहरे नमूनों को कसकर अलग अलग वस्तुओं को पहचानने के लिए इस्तेमाल किया गया था , बेहद छोटे चित्रों (</s>
रियालहार्ट एट अल , 1986</s>
तब से छवियों के आकार में एक क्रमिक वृद्धि हुई है तंत्रिका नेटवर्क प्रक्रिया कर सकता है .</s>
आधुनिक वस्तु पहचान नेटवर्क समृद्ध उच्च - रेखाचित्रों की प्रक्रिया करते हैं और 22 अध्याय 1 नहीं करते हैं ।</s>
वर्ष 1950 1985 2000 2015 2056 वर्ष 10 −2 10 −1</s>
10 0 10 1 10 2 10 10 3 10 4 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10</s>
१० ११ न्यूरॉन्स की संख्या ( वैज्ञानिक पैमाने )</s>
1</s>
</s>
गुप्त इकाइयों की शुरुआत के बाद से , कृत्रिम तंत्रिका नेटवर्क लगभग हर 2 . 4 साल में आकार में दोगुना हो गया है .</s>
जैविक तंत्रिका नेटवर्क का आकार विकिपीडिया से (2015 ई . )</s>
1</s>
पर्सेप्टन</s>
अनुकूली रेखीय तत्व ( Widrow and Hoff , 1960 ३ .</s>
नेकोग्निट्रॉन ( Fukushima , 1980 ) 4 .</s>
आरंभिक बैक - अप नेटवर्क ( Rumelhart एट अल , 1986b 5 .</s>
स्पीच पहचान के लिए सतत तंत्रिका नेटवर्क ( Robson and Fallside ) 1991</s>
६</s>
स्पीच पहचान के लिए मल्टीलेयर पर्सेप्टॉन (Bengio एट अल )</s>
१९९१ ७</s>
माध्य क्षेत्र अवग्रह विश्वास नेटवर्क</s>
८</s>
लेनेट - ५</s>
( LeCun et al . )</s>
सन् १९९८ ई .</s>
इको स्टेट नेटवर्क (Jeger and Haas , 2004 )</s>
दस</s>
गहन विश्वास नेटवर्क</s>
जीपीयू - सैक्सरेटेड कॉन्वोल्यूशनल नेटवर्क</s>
कैवेलपिल्ला एट अल</s>
डीप ब्लॉट्जमैन मशीन ( Salakhutdinov और हैनटन , 2009 ) 13 .</s>
जीपीयू - सैक्सलर गहरा विश्वास नेटवर्क (</s>
रैना एट अल</s>
Posted in Bollywood , tagged bollywood babe , 2009 14 । Leave a Comment »</s>
अपसंयोजित समूह नेटवर्क (</s>
जेरेट एट अल , 2009 15</s>
Name</s>
( क्रिस्टन एट अल , 2010 )</s>
१६</s>
OMP - 1</s>
वितरित स्वचलितकोडर (</s>
ली एट अल</s>
बहु - जीपीयू संकेन्द्रण नेटवर्क (</s>
किरिज़ेव्स्की एट अल</s>
Archive & # 124 ; September 19 th , 2012</s>
COTS HPC असंयोजित संयुग्मी नेटवर्क (</s>
काइट्स एट अल , 2013 20</s>
गोगलेनेट</s>
इसी प्रकार , प्रारंभिक नेटवर्क केवल दो प्रकार की वस्तुओं को पहचान सकता था ( कुछ मामलों में , एक ही प्रकार की वस्तु की अनुपस्थिति या उपस्थिति , जबकि ये आधुनिक नेटवर्क वस्तुओं की कम से कम 1 , 000 विभिन्न श्रेणियों को पहचानते हैं .</s>
वस्तु पहचान की सबसे बड़ी प्रतियोगिता इमेजनेट 23 अध्याय 1 है ।</s>
सूचना अधिकार</s>
बड़े पैमाने पर दृश्य पहचान चुनौतियां (आईएलएसवीआरसी ) प्रतिवर्ष आयोजित की जाती हैं ।</s>
गहन अध्ययन का एक नाटकीय क्षण तब आया जब एक संयुग्मक नेटवर्क ने पहली बार इस चुनौती को जीता और एक व्यापक मार्जिन से राज्य के शीर्ष - 5 त्रुटि दर को 261 प्रतिशत से घटाकर 153 प्रतिशत (</s>
उसके बाद से , इन प्रतियोगिताओं को लगातार गहरे सामूहिक जालों द्वारा जीता जाता है , और इस लेखन के रूप में , गहन अध्ययन में प्रगति ने इस प्रतियोगिता में नवीनतम टॉप फाइव त्रुटि दर को 3 . 6 प्रतिशत तक लाया है , जैसा कि 1 . 12 अंक में दिखाया गया है .</s>
गहन अध्ययन का वाचिक मान्यता पर भी नाटकीय प्रभाव पड़ा है ।</s>
1990 के दशक भर में सुधार के बाद , भाषण मान्यता के लिए त्रुटि दरों लगभग 2000 में शुरू हो गया .</s>
डीडह्ल एट अल 2010 डीडह्ल डींग एट अल डींग एतट , 2010 डीडब ऐट अल सीड , 2011 एचनटन के साथ कुछ बोलने की अनुमति देने के कारण अचानक गलती दरों में कटौती की गई ।</s>
हम इस इतिहास की अधिक विस्तार से धारा 12 . 3 में खोज करते हैं ।</s>
डीप नेटवर्कों को पैदल यात्री पहचान और छवि विभाजन के लिए भी शानदार सफलता मिली है ।</s>
</s>
टायर्सन एट अल , 2012</s>
साथ ही , गहरे नेटवर्कों के पैमाने और सटीकता में वृद्धि हुई है , 2010 2011 2013 2014 2014 2014 2014 2015 वर्ष 0 .0005 0 .10 0 .15</s>
0 .20 0 .25 0 . 30 ILSVRC वर्गीकरण त्रुटि दर चित्र 1 .12 : समय के साथ त्रुटि दर घटाना</s>
चूंकि गहरे नेटवर्क इमेजनेट बड़े पैमाने पर दृश्य पहचान चैलेंज में प्रतिस्पर्धा करने के लिए आवश्यक पैमाने पर पहुंचे , उन्होंने प्रति वर्ष लगातार इस प्रतियोगिता को जीता है , जिससे हर बार कम और त्रुटि दरों में कमी आती है .</s>
रसस्कोवस्की एट अल से डेटा</s>
(2014 )</s>
और वह एट अल</s>
(2015 )</s>
24 अध्याय 1</s>
INTRODUCTION में कार्यों की जटिलता है जिसे वे हल कर सकते हैं ।</s>
गुडफ़्लो एट अल</s>
</s>
पहले यह व्यापक रूप से माना जाता था कि इस प्रकार के ज्ञान के लिए अनुक्रम के अलग - अलग तत्वों के लेबलिंग की आवश्यकता होती है ( Gülçhre और बेंजियो , 2013 ) .</s>
बारंबार तंत्रिका नेटवर्क , जैसे कि एलएसटीएम अनुक्रम मॉडल जिसका उल्लेख ऊपर किया गया है , अब केवल स्थिर इनपुट के बजाय अनुक्रमों और अन्य अनुक्रमों के बीच संबंधों को मॉडल करने के लिए इस्तेमाल किया जाता है .</s>
यह अनुक्रम - अनुक्रम अधिगम किसी अन्य अनुप्रयोग के क्रांतिकारी होने के कगार पर प्रतीत होता है ।</s>
↑ बी . बी . डी . अल . , 2015</s>
बढ़ती जटिलता की इस प्रवृत्ति को अपने तार्किक निष्कर्ष की ओर धकेल दिया गया है जिसमें तंत्रिका तंत्र ( ग्रेव्स एट अल 2014 ) की शुरुआत की गई है जो स्मृति कोशिकाओं से पढ़ना सीखते हैं और स्मृति कोशिकाओं को मनमाना सामग्री लिखते हैं ।</s>
ऐसे तंत्रिका नेटवर्क वांछित व्यवहार के उदाहरणों से सरल प्रोग्राम सीख सकते हैं ।</s>
उदाहरण के लिए , वे उन संख्याओं की सूचियों को छांटना सीख सकते हैं , जो क्रमबद्ध और अनुक्रमित होती हैं ।</s>
यह स्व - प्रोग्रामिंग तकनीक अपनी शैशवावस्था में है , लेकिन भविष्य में इसे सैद्धांतिक रूप से लगभग किसी भी कार्य के लिए लागू किया जा सकता है ।</s>
गहन अध्ययन की एक और प्रमुख उपलब्धि यह है कि इसका विस्तार प्रवर्तन अधिगम के क्षेत्र में है ।</s>
प्रवर्तन अधिगम के संदर्भ में , एक स्वायत्त एजेंट को मानव ऑपरेटर से बिना किसी मार्गदर्शन के परीक्षण और त्रुटि द्वारा कार्य करना सीखना चाहिए .</s>
डीपम्ड ने यह प्रदर्शित किया कि गहन अधिगम पर आधारित एक प्रवर्तन अधिगम प्रणाली , अतुलरी वीडियो गेम्स खेलना सीखने में सक्षम है , जो कई कार्यों पर मानवीय स्तर तक पहुंचता है .</s>
अनल</s>
डीप लर्निंग ने रोबोटिक्स ( एफिन एट अल , 2015 ) के लिए प्रवर्तन अधिगम के प्रदर्शन में भी उल्लेखनीय सुधार किया है ।</s>
गहन अध्ययन के इन अनुप्रयोगों में से कई अत्यधिक लाभदायक हैं .</s>
डीप लर्निंग का उपयोग अब गूगल , माइक्रोसॉफ्ट , फेसबुक , आईबीबीएम , बैदु , एप्पल , एडोब , नेटफोलिक्स , एनवीआईए , और एनईसी सहित कई शीर्ष प्रौद्योगिकी कंपनियों द्वारा किया जाता है .</s>
गहन अधिगम में अग्रिम भी सॉफ्टवेयर मूल संरचना के अग्रिमों पर काफी निर्भर रहे हैं ।</s>
सॉफ्टवेयर पुस्तकालय जैसे कि Theergtra एट अल , 2010 rutien एट अल , PyLearn2</s>
↑ 2013क , टोर्च</s>
↑ 2011ब</s>
टेन्सोरफ्लो ( अबादी एट अल , 2015 ) ने सभी महत्वपूर्ण अनुसंधान परियोजनाओं या वाणिज्यिक उत्पादों का समर्थन किया है ।</s>
गहन अध्ययन ने अन्य विज्ञानों में भी योगदान दिया है ।</s>
वस्तु पहचान के लिए आधुनिक कॉन्वोलुअल नेटवर्क दृश्य प्रक्रमण का एक मॉडल प्रदान करते हैं जिसका तंत्रिका विज्ञानी अध्ययन कर सकते हैं ( डीईआरसी , 2013 ) ।</s>
गहन अध्ययन भी डेटा की भारी मात्रा के प्रसंस्करण और वैज्ञानिक 25 अध्याय 1 में उपयोगी भविष्यवाणियां करने के लिए उपयोगी उपकरण प्रदान करता है .</s>
INTRODUCTION OF TRANSLATORS</s>
यह सफलतापूर्वक भविष्यवाणी करने के लिए इस्तेमाल किया गया है कि कैसे अणु दवा कंपनियों को नई दवाओं के डिजाइन में मदद करने के लिए बातचीत करेंगे ( डेहल एट अल , 2014 ) उपत्वीय कणों की खोज के लिए</s>
हम उम्मीद करते हैं कि भविष्य में अधिक से अधिक वैज्ञानिक क्षेत्रों में गहन अध्ययन की उपस्थिति होगी ।</s>
सारांश में , गहन अधिगम मशीनी अधिगम का एक दृष्टिकोण है जिसने मानव मस्तिष्क , सांख्यिकी और अनुप्रयुक्त गणित के हमारे ज्ञान को पिछले कई दशकों में विकसित किया है ।</s>
हाल के वर्षों में , गहन अध्ययन ने इसकी लोकप्रियता और उपयोगिता में अत्यधिक वृद्धि देखी है , मोटे तौर पर अधिक शक्तिशाली कंप्यूटरों , अधिक डेटासेट और तकनीकों के परिणामस्वरूप गहरे नेटवर्कों को प्रशिक्षित करने के लिए .</s>
आने वाले वर्ष चुनौतियों और अवसरों से भरे हैं ताकि गहन अध्ययन को और आगे बढ़ाया जा सके और इसे नई सीमाओं में लाया जा सके ।</s>
२६</s>
