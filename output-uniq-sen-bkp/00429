English --> Increasing Model Sizes Another key reason that neural networks are wildly successful today after enjoying comparatively little success since the 1980s is that we have the computational resources to run much larger models today.
NMT:  बढ़ते मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज तंत्रिका नेटवर्क बेतहाशा सफल हो रहे हैं कि हम आज बहुत बड़े मॉडल चलाने के लिए कम्प्यूटेशनल संसाधनों है .
NMT:  बढ़ती मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज तंत्रिका नेटवर्क बेतहाशा सफल रहे हैं कि हम कम्प्यूटेशनल संसाधनों है कि आज बहुत बड़े मॉडलों को चलाने के लिए .
NMT:  बढ़ते मॉडल आकार एक और प्रमुख कारण है कि तंत्रिका नेटवर्क आज 1980 के दशक के बाद से तुलनात्मक रूप से कम सफलता का आनंद लेने के बाद बेतहाशा सफल हो रहे हैं है कि हम बहुत बड़ा मॉडल आज चलाने के लिए कम्प्यूटेशनल संसाधन है .
NMT:  बढ़ते मॉडल आकार एक और प्रमुख कारण है कि तंत्रिका नेटवर्क आज अपेक्षाकृत कम सफलता का आनंद लेने के बाद बेतहाशा सफल हो रहे हैं 1980 के दशक के बाद से है कि हम बहुत बड़ा मॉडल चलाने के कम्प्यूटेशनल संसाधनों आज .
NMT:  बढ़ती मॉडल आकार एक और प्रमुख कारण है कि तंत्रिका नेटवर्क आज अपेक्षाकृत कम सफलता का आनंद लेने के बाद बेतहाशा सफल रहे हैं 1980 के दशक से है कि हम आज बहुत बड़े मॉडल चलाने के लिए कम्प्यूटेशनल संसाधनों है .
NMT:  बढ़ते मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज बेतहाशा सफल रहे हैं कि हम कम्प्यूटेशनल संसाधनों आज बहुत बड़े मॉडल चलाने के लिए .
NMT:  बढ़ते मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद तंत्रिका नेटवर्क बेतहाशा सफल रहे हैं कि हम कम्प्यूटेशनल संसाधनों आज काफी बड़े मॉडल चलाने के लिए है .
NMT:  बढ़ते मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज तंत्रिका नेटवर्क बेतहाशा सफल हैं कि हम कम्प्यूटेशनल संसाधनों आज बहुत बड़े मॉडल चलाने के लिए है .
NMT:  बढ़ते मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज तंत्रिका नेटवर्क बेतहाशा सफल हैं कि हम कम्प्यूटेशनल संसाधनों आज बहुत बड़े मॉडल चलाने के लिए
NMT:  बढ़ती मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज तंत्रिका नेटवर्क बेतहाशा सफल हो रहे हैं कि हम कम्प्यूटेशनल संसाधनों आज बहुत बड़े मॉडल चलाने के लिए है .
NMT:  बढ़ते मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद तंत्रिका नेटवर्क बेतहाशा सफल हैं कि हम कम्प्यूटेशनल संसाधनों आज बहुत बड़े मॉडल चलाने के लिए
NMT:  बढ़ते मॉडल आकार एक अन्य प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज तंत्रिका नेटवर्क बेतहाशा सफल हो रहे हैं कि हम कम्प्यूटेशनल संसाधनों आज बहुत बड़े मॉडल चलाने के लिए है .
NMT:  बढ़ते मॉडल आकार एक और प्रमुख कारण है कि 1980 के दशक के बाद से अपेक्षाकृत कम सफलता का आनंद लेने के बाद आज तंत्रिका नेटवर्क बेतहाशा सफल हो रहे हैं कि हम आज बहुत बड़े मॉडल चलाने के लिए कम्प्यूटेशनल संसाधन है .
NMT: 


